"""
NEXUS TMC Engine - Parallel Processing Module

This module provides parallel processing capabilities for the TMC engine.
"""

import asyncio
import time
import threading
import queue
import psutil
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass

from ..core.data_types import AsyncTask

__all__ = ['ParallelPipelineProcessor']

# Configuration constants
MAX_WORKERS = min(psutil.cpu_count() or 4, 16)
PIPELINE_QUEUE_SIZE = 1000
ASYNC_BATCH_SIZE = 8
DEFAULT_CHUNK_SIZE = 64 * 1024


class ParallelPipelineProcessor:
    """
    TMC v9.0 é©æ–°çš„ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
    çœŸã®ä¸¦åˆ—å‡¦ç† (ProcessPoolExecutor) + éåŒæœŸI/O + ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
    """
    
    def __init__(self, max_workers: int = MAX_WORKERS):
        self.max_workers = max_workers
        self.pipeline_queue = queue.Queue(maxsize=PIPELINE_QUEUE_SIZE)
        self.result_queue = queue.Queue()
        self.active_tasks = {}
        self.performance_stats = {
            'total_processed': 0,
            'average_throughput': 0.0,
            'pipeline_efficiency': 0.0
        }
        
        # çœŸã®ä¸¦åˆ—å‡¦ç†ãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–ï¼ˆCPUãƒã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç”¨ï¼‰
        self.process_pool = ProcessPoolExecutor(max_workers=max_workers)
        # I/Oãƒã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç”¨ï¼ˆè»½é‡ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆ¶å¾¡
        self.pipeline_active = True
        self.pipeline_thread = None
        
        print(f"ğŸš€ ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†: {max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼ (Process+Thread Hybrid)")
    
    async def process_data_async(self, data_chunks: List[bytes], transform_type: str) -> List[Tuple[bytes, Dict]]:
        """
        CPUã®å…¨ã‚³ã‚¢ã‚’æ´»ç”¨ã—ãŸçœŸã®ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        ProcessPoolExecutorã«ã‚ˆã‚ŠGILåˆ¶ç´„ã‚’çªç ´
        """
        print(f"  [ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] çœŸã®ä¸¦åˆ—å‡¦ç†é–‹å§‹: {len(data_chunks)}ãƒãƒ£ãƒ³ã‚¯")
        
        try:
            # ã‚¿ã‚¹ã‚¯ãƒãƒƒãƒç”Ÿæˆï¼ˆãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ã®æœ€é©åŒ–ï¼‰
            task_batches = self._create_optimized_task_batches(data_chunks, transform_type)
            
            # çœŸã®ä¸¦åˆ—å®Ÿè¡Œï¼ˆãƒ—ãƒ­ã‚»ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰
            parallel_futures = []
            loop = asyncio.get_event_loop()
            
            for i, batch in enumerate(task_batches):
                # CPUãƒã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ã§å®Ÿè¡Œ
                future = loop.run_in_executor(
                    self.process_pool, 
                    self._process_batch_in_subprocess, 
                    batch, i
                )
                parallel_futures.append(future)
            
            # çµæœåé›†ï¼ˆéåŒæœŸï¼‰
            all_results = []
            completed_batches = 0
            
            for batch_future in asyncio.as_completed(parallel_futures):
                try:
                    batch_data = await batch_future
                    all_results.extend(batch_data)
                    completed_batches += 1
                    
                    progress = (completed_batches / len(task_batches)) * 100
                    print(f"    [ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ãƒãƒƒãƒ {completed_batches}/{len(task_batches)} å®Œäº† ({progress:.1f}%)")
                    
                except Exception as e:
                    print(f"    [ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            # çµæœé †åºå¾©å…ƒ
            sorted_results = sorted(all_results, key=lambda x: x[1].get('chunk_id', 0))
            
            print(f"  [ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] çœŸã®ä¸¦åˆ—å‡¦ç†å®Œäº†: {len(sorted_results)}çµæœ")
            return sorted_results
            
        except Exception as e:
            print(f"  [ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return [(chunk, {'error': str(e)}) for chunk in data_chunks]
    
    def _create_optimized_task_batches(self, data_chunks: List[bytes], transform_type: str) -> List[List]:
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒãƒƒãƒç”Ÿæˆ"""
        batches = []
        current_batch = []
        current_batch_size = 0
        
        # å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºæ±ºå®šï¼ˆåˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã«åŸºã¥ãï¼‰
        if psutil:
            available_memory = psutil.virtual_memory().available
            optimal_batch_size = min(8 * 1024 * 1024, available_memory // (self.max_workers * 4))  # 8MBä¸Šé™
        else:
            optimal_batch_size = 4 * 1024 * 1024  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4MB
        
        for i, chunk in enumerate(data_chunks):
            # è»½é‡ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
            task_data = {
                'chunk_data': chunk,
                'chunk_id': i,
                'transform_type': transform_type,
                'size': len(chunk)  # timestampã‚’å‰Šé™¤ã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„
            }
            
            current_batch.append(task_data)
            current_batch_size += len(chunk)
            
            # å‹•çš„ãƒãƒƒãƒåˆ†å‰²ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰
            if (current_batch_size >= optimal_batch_size or 
                len(current_batch) >= self.max_workers * 2):  # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®2å€ã¾ã§
                batches.append(current_batch)
                current_batch = []
                current_batch_size = 0
        
        # æ®‹ã‚Šã®ã‚¿ã‚¹ã‚¯ã‚’ãƒãƒƒãƒã«è¿½åŠ 
        if current_batch:
            batches.append(current_batch)
        
        total_chunks = sum(len(b) for b in batches)
        avg_batch_size = total_chunks / len(batches) if batches else 0
        print(f"    [æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ãƒãƒƒãƒç”Ÿæˆå®Œäº†: {len(batches)}ãƒãƒƒãƒ, å¹³å‡{avg_batch_size:.1f}ãƒãƒ£ãƒ³ã‚¯, æœ€é©ã‚µã‚¤ã‚º: {optimal_batch_size//1024//1024}MB")
        return batches
    
    def _process_batch_in_subprocess(self, batch_data: List[Dict], batch_id: int) -> List[Tuple[bytes, Dict]]:
        """
        ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ
        GILã«åˆ¶ç´„ã•ã‚Œãªã„çœŸã®ä¸¦åˆ—å‡¦ç†
        """
        import os
        import time
        
        process_id = os.getpid()
        start_time = time.time()
        
        try:
            results = []
            
            for task_data in batch_data:
                chunk_data = task_data['chunk_data']
                chunk_id = task_data['chunk_id']
                transform_type = task_data['transform_type']
                
                # åŸºæœ¬çš„ãªå¤‰æ›å‡¦ç†ï¼ˆè»½é‡åŒ–ï¼‰
                try:
                    # ã“ã®éƒ¨åˆ†ã§ã¯ã€é‡ã„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆTMCEngineãªã©ï¼‰ã®å†ä½œæˆã‚’é¿ã‘ã€
                    # åŸºæœ¬çš„ãªåœ§ç¸®ãƒ»å¤‰æ›ã®ã¿ã‚’å®Ÿè¡Œ
                    if transform_type == 'basic_compression':
                        processed_chunk = self._subprocess_basic_compression(chunk_data)
                    elif transform_type == 'leco_transform':
                        processed_chunk = self._subprocess_leco_transform(chunk_data)
                    else:
                        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡¦ç†
                        processed_chunk = chunk_data
                    
                    result_info = {
                        'chunk_id': chunk_id,
                        'original_size': len(chunk_data),
                        'processed_size': len(processed_chunk),
                        'process_id': process_id,
                        'processing_time': time.time() - start_time
                    }
                    
                    results.append((processed_chunk, result_info))
                    
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                    error_info = {
                        'chunk_id': chunk_id,
                        'error': str(e),
                        'process_id': process_id
                    }
                    results.append((chunk_data, error_info))
            
            batch_processing_time = time.time() - start_time
            print(f"    [ãƒ—ãƒ­ã‚»ã‚¹ {process_id}] ãƒãƒƒãƒ{batch_id} å®Œäº†: {len(results)}ãƒãƒ£ãƒ³ã‚¯, {batch_processing_time:.3f}ç§’")
            
            return results
            
        except Exception as e:
            print(f"    [ãƒ—ãƒ­ã‚»ã‚¹ {process_id}] ãƒãƒƒãƒ{batch_id} ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™
            return [(task['chunk_data'], {'chunk_id': task.get('chunk_id', i), 'error': str(e)}) 
                   for i, task in enumerate(batch_data)]
    
    def _subprocess_basic_compression(self, data: bytes) -> bytes:
        """ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ç”¨ã®åŸºæœ¬åœ§ç¸®ï¼ˆè»½é‡ï¼‰"""
        try:
            import zlib
            return zlib.compress(data, level=6)
        except:
            return data
    
    def _subprocess_leco_transform(self, data: bytes) -> bytes:
        """ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ç”¨ã®LeCoå¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            # ã‚ˆã‚ŠåŠ¹æœçš„ãªæ•°å€¤å¤‰æ›
            if len(data) >= 8:
                # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ–¹å¼ã‚’è©¦è¡Œ
                best_result = data
                best_ratio = 1.0
                
                # 1. 4ãƒã‚¤ãƒˆæ•´æ•°å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                if len(data) % 4 == 0:
                    result_4byte = self._differential_encoding_4byte(data)
                    ratio_4byte = len(result_4byte) / len(data)
                    if ratio_4byte < best_ratio:
                        best_result = result_4byte
                        best_ratio = ratio_4byte
                
                # 2. 2ãƒã‚¤ãƒˆæ•´æ•°å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                if len(data) % 2 == 0:
                    result_2byte = self._differential_encoding_2byte(data)
                    ratio_2byte = len(result_2byte) / len(data)
                    if ratio_2byte < best_ratio:
                        best_result = result_2byte
                        best_ratio = ratio_2byte
                
                # 3. 1ãƒã‚¤ãƒˆå·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                result_1byte = self._differential_encoding_1byte(data)
                ratio_1byte = len(result_1byte) / len(data)
                if ratio_1byte < best_ratio:
                    best_result = result_1byte
                    best_ratio = ratio_1byte
                
                return best_result
            
            return data
        except Exception as e:
            return data
    
    def _differential_encoding_4byte(self, data: bytes) -> bytes:
        """4ãƒã‚¤ãƒˆæ•´æ•°å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            values = []
            for i in range(0, len(data), 4):
                val = int.from_bytes(data[i:i+4], 'little', signed=True)
                values.append(val)
            
            if len(values) > 1:
                # é©å¿œçš„å·®åˆ†è¨ˆç®—
                differences = [values[0]]  # æœ€åˆã®å€¤
                for i in range(1, len(values)):
                    diff = values[i] - values[i-1]
                    differences.append(diff)
                
                # å°ã•ãªå·®åˆ†ã‚’ã‚ˆã‚ŠåŠ¹ç‡çš„ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                result = bytearray()
                for diff in differences:
                    # å°ã•ãªå·®åˆ†ã¯å¯å¤‰é•·ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                    if -127 <= diff <= 127:
                        result.append(0)  # ãƒ•ãƒ©ã‚°: 1ãƒã‚¤ãƒˆ
                        result.append(diff & 0xFF)
                    else:
                        result.append(1)  # ãƒ•ãƒ©ã‚°: 4ãƒã‚¤ãƒˆ
                        result.extend(diff.to_bytes(4, 'little', signed=True))
                
                return bytes(result)
            
            return data
        except:
            return data
    
    def _differential_encoding_2byte(self, data: bytes) -> bytes:
        """2ãƒã‚¤ãƒˆæ•´æ•°å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            values = []
            for i in range(0, len(data), 2):
                val = int.from_bytes(data[i:i+2], 'little', signed=True)
                values.append(val)
            
            if len(values) > 1:
                differences = [values[0]]
                for i in range(1, len(values)):
                    differences.append(values[i] - values[i-1])
                
                return b''.join(val.to_bytes(2, 'little', signed=True) for val in differences)
            
            return data
        except:
            return data
    
    def _differential_encoding_1byte(self, data: bytes) -> bytes:
        """1ãƒã‚¤ãƒˆå·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            if len(data) > 1:
                result = bytearray([data[0]])  # æœ€åˆã®ãƒã‚¤ãƒˆ
                for i in range(1, len(data)):
                    diff = (data[i] - data[i-1]) & 0xFF
                    result.append(diff)
                return bytes(result)
            
            return data
        except:
            return data
    
    def _create_task_batches(self, data_chunks: List[bytes], transform_type: str) -> List[List[AsyncTask]]:
        """ã‚¿ã‚¹ã‚¯ãƒãƒƒãƒç”Ÿæˆï¼ˆè² è·åˆ†æ•£æœ€é©åŒ–ï¼‰"""
        batches = []
        current_batch = []
        current_batch_size = 0
        
        for i, chunk in enumerate(data_chunks):
            task = AsyncTask(
                task_id=i,
                task_type=transform_type,
                data=chunk,
                priority=self._calculate_task_priority(chunk),
                created_time=time.time()
            )
            
            current_batch.append(task)
            current_batch_size += len(chunk)
            
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if (len(current_batch) >= ASYNC_BATCH_SIZE or 
                current_batch_size >= DEFAULT_CHUNK_SIZE * 2):
                batches.append(current_batch)
                current_batch = []
                current_batch_size = 0
        
        # æ®‹ã‚Šã®ã‚¿ã‚¹ã‚¯ã‚’æœ€çµ‚ãƒãƒƒãƒã«
        if current_batch:
            batches.append(current_batch)
        
        return batches
    
    def _calculate_task_priority(self, data: bytes) -> int:
        """ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦è¨ˆç®—ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰"""
        size_factor = min(len(data) // 1024, 10)  # ã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆæœ€å¤§10ï¼‰
        
        # ç°¡æ˜“ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        try:
            byte_counts = {}
            for byte in data[:1024]:  # å…ˆé ­1KBã‚µãƒ³ãƒ—ãƒ«
                byte_counts[byte] = byte_counts.get(byte, 0) + 1
            
            entropy = 0.0
            total = len(data[:1024])
            for count in byte_counts.values():
                prob = count / total
                entropy -= prob * (prob.bit_length() - 1) if prob > 0 else 0
            
            entropy_factor = min(int(entropy), 8)
        except:
            entropy_factor = 4
        
        # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆåœ§ç¸®å›°é›£ï¼‰ãªãƒ‡ãƒ¼ã‚¿ã‚’ä½å„ªå…ˆåº¦ã«
        return max(1, 10 - entropy_factor + size_factor)
    
    async def _process_batch_async(self, task_batch: List[AsyncTask], batch_id: int) -> List[Tuple[bytes, Dict]]:
        """éåŒæœŸãƒãƒƒãƒå‡¦ç†"""
        try:
            loop = asyncio.get_event_loop()
            
            # ãƒãƒƒãƒå†…ã‚¿ã‚¹ã‚¯ã®ä¸¦åˆ—å®Ÿè¡Œ
            batch_futures = []
            for task in task_batch:
                future = loop.run_in_executor(
                    self.thread_pool,
                    self._process_single_task,
                    task
                )
                batch_futures.append(future)
            
            # ãƒãƒƒãƒå†…ä¸¦åˆ—å®Œäº†å¾…æ©Ÿ
            batch_results = await asyncio.gather(*batch_futures, return_exceptions=True)
            
            # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            processed_results = []
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    print(f"    [ãƒãƒƒãƒ {batch_id}] ã‚¿ã‚¹ã‚¯{i}ã‚¨ãƒ©ãƒ¼: {result}")
                    processed_results.append((task_batch[i].data, {'error': str(result)}))
                else:
                    processed_results.append(result)
            
            return processed_results
            
        except Exception as e:
            print(f"    [ãƒãƒƒãƒ {batch_id}] ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return [(task.data, {'error': str(e)}) for task in task_batch]
    
    def _process_single_task(self, task: AsyncTask) -> Tuple[bytes, Dict]:
        """å˜ä¸€ã‚¿ã‚¹ã‚¯å‡¦ç†ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰å†…å®Ÿè¡Œï¼‰"""
        try:
            start_time = time.time()
            thread_id = threading.get_ident()
            
            # ãƒ€ãƒŸãƒ¼å‡¦ç†ï¼ˆå®Ÿéš›ã®å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«å®Ÿè£…ï¼‰
            processed_data = task.data  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
            
            processing_time = time.time() - start_time
            
            result_info = {
                'task_id': task.task_id,
                'chunk_id': task.task_id,  # äº’æ›æ€§ã®ãŸã‚
                'processing_time': processing_time,
                'thread_id': thread_id,
                'task_type': task.task_type,
                'priority': task.priority,
                'original_size': len(task.data),
                'processed_size': len(processed_data)
            }
            
            return processed_data, result_info
            
        except Exception as e:
            return task.data, {'error': str(e), 'task_id': task.task_id}
    
    def start_pipeline(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹"""
        if not self.pipeline_active:
            self.pipeline_active = True
            self.pipeline_thread = threading.Thread(target=self._pipeline_worker, daemon=True)
            self.pipeline_thread.start()
            print("  [ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚«ãƒ¼é–‹å§‹")
    
    def stop_pipeline(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢"""
        self.pipeline_active = False
        if self.pipeline_thread and self.pipeline_thread.is_alive():
            self.pipeline_thread.join(timeout=1.0)
        print("  [ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢")
    
    def _pipeline_worker(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ï¼‰"""
        while self.pipeline_active:
            try:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã‚¿ã‚¹ã‚¯å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                task = self.pipeline_queue.get(timeout=0.1)
                
                # ã‚¿ã‚¹ã‚¯å‡¦ç†
                result = self._process_single_task(task)
                self.result_queue.put(result)
                
                # çµ±è¨ˆæ›´æ–°
                self.performance_stats['total_processed'] += 1
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"    [ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚«ãƒ¼] ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆå–å¾—"""
        return self.performance_stats.copy()
    
    def __del__(self):
        """ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ï¼ˆãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ï¼‰"""
        try:
            self.stop_pipeline()
            self.thread_pool.shutdown(wait=False)
            self.process_pool.shutdown(wait=False)
        except:
            pass

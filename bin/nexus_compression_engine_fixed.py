#!/usr/bin/env python3
"""
NEXUS: Networked Elemental eXtraction and Unification System
ğŸ§  æ©Ÿæ¢°å­¦ç¿’ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±åˆç‰ˆ

ğŸš€ NEXUSé©å‘½çš„ç†è«–:
1. Elemental Decompositionï¼ˆè¦ç´ åˆ†è§£ï¼‰: ãƒ‡ãƒ¼ã‚¿ã®æœ€å°æ§‹æˆè¦ç´ ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŒ–
2. Permutative Groupingï¼ˆé †ç•ªå…¥ã‚Œæ›¿ãˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰: ã‚½ãƒ¼ãƒˆã«ã‚ˆã‚‹æ­£è¦åŒ–ã§é‡è¤‡å¢—å¹…
3. Shape-Agnostic Clusteringï¼ˆå½¢çŠ¶è‡ªç”±åº¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰: ãƒ†ãƒˆãƒªã‚¹å½¢çŠ¶ã§ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º

ğŸ¯ ç›®æ¨™: é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã‚‚è¿½åŠ åœ§ç¸®5-30%é”æˆ
ğŸŒŸ é©æ–°: åœ§ç¸®æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆZIP, MP3ï¼‰ã§ã‚‚æ›´ãªã‚‹åœ§ç¸®å¯èƒ½
"""

import os
import sys
import time
import struct
import hashlib
import math
import numpy as np
import threading
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter
from enum import Enum

# æ©Ÿæ¢°å­¦ç¿’/æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
try:
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.decomposition import PCA
    from sklearn.neural_network import MLPRegressor
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

# NumPyãƒ™ãƒ¼ã‚¹é«˜é€Ÿå‡¦ç†
try:
    import numba
    from numba import jit, cuda
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False

class PolyominoShape(Enum):
    """ãƒ†ãƒˆãƒªã‚¹å½¢çŠ¶ï¼ˆPolyominoesï¼‰å®šç¾©"""
    I = "I"  # ç›´ç·šå‹ï¼ˆ4é€£ç¶šï¼‰
    O = "O"  # æ­£æ–¹å½¢å‹ï¼ˆ2x2ï¼‰
    T = "T"  # Tå­—å‹
    J = "J"  # Jå­—å‹
    L = "L"  # Lå­—å‹
    S = "S"  # Så­—å‹
    Z = "Z"  # Zå­—å‹
    SINGLE = "1"  # å˜ä¸€è¦ç´ 
    LINE2 = "2"  # 2è¦ç´ ç›´ç·š
    LINE3 = "3"  # 3è¦ç´ ç›´ç·š

@dataclass
class NEXUSGroup:
    """NEXUSè¦ç´ ã‚°ãƒ«ãƒ¼ãƒ—"""
    elements: List[int]  # è¦ç´ ãƒªã‚¹ãƒˆ
    shape: PolyominoShape  # å½¢çŠ¶ã‚¿ã‚¤ãƒ—
    positions: List[Tuple[int, int]]  # å…ƒä½ç½®åº§æ¨™
    normalized: Tuple[int, ...]  # æ­£è¦åŒ–æ¸ˆã¿è¦ç´ ï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
    hash_value: str  # ãƒãƒƒã‚·ãƒ¥å€¤ï¼ˆé‡è¤‡æ¤œå‡ºç”¨ï¼‰

@dataclass
class NEXUSCompressionState:
    """NEXUSåœ§ç¸®çŠ¶æ…‹"""
    unique_groups: List[NEXUSGroup]  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ãƒªã‚¹ãƒˆ
    group_counts: Dict[str, int]  # ã‚°ãƒ«ãƒ¼ãƒ—å‡ºç¾å›æ•°
    position_map: List[int]  # ä½ç½®ãƒãƒƒãƒ—ï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿å¾©å…ƒç”¨ï¼‰
    original_groups: List[NEXUSGroup]  # å…ƒã®å…¨ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆä½ç½®æƒ…å ±å«ã‚€ï¼‰
    shape_distribution: Dict[PolyominoShape, int]  # å½¢çŠ¶åˆ†å¸ƒ
    grid_dimensions: Tuple[int, int]  # ã‚°ãƒªãƒƒãƒ‰æ¬¡å…ƒ
    compression_metadata: Dict  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    ml_features: Optional[np.ndarray] = None  # æ©Ÿæ¢°å­¦ç¿’ç‰¹å¾´é‡
    neural_predictions: Optional[List[float]] = None  # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«äºˆæ¸¬å€¤

@dataclass
class MLCompressionConfig:
    """æ©Ÿæ¢°å­¦ç¿’åœ§ç¸®è¨­å®š"""
    enable_ml: bool = True
    use_clustering: bool = True
    use_neural_prediction: bool = True
    use_pca_reduction: bool = True
    parallel_processing: bool = True
    gpu_acceleration: bool = True if NUMBA_AVAILABLE else False
    max_workers: int = mp.cpu_count()
    chunk_size: int = 1024 * 64  # 64KB chunks for ML processing
    verbose: bool = False  # ãƒ­ã‚°å‡ºåŠ›åˆ¶å¾¡

@dataclass
class CompressionResult:
    compressed_data: bytes
    original_size: int
    compressed_size: int
    compression_ratio: float
    method: str
    processing_time: float
    checksum: str

class NEXUSCompressor:
    """
    ğŸ§  NEXUS: Networked Elemental eXtraction and Unification System
    æ©Ÿæ¢°å­¦ç¿’ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±åˆåœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³
    """
    
    def __init__(self, ml_config: MLCompressionConfig = None):
        self.ml_config = ml_config or MLCompressionConfig()
        self.polyomino_patterns = self._initialize_polyomino_patterns()
        self.compression_threshold = 0.01  # 1%ä»¥ä¸Šã§ã‚‚åœ§ç¸®å®Ÿè¡Œï¼ˆå¤§å¹…ç·©å’Œï¼‰
        self.golden_ratio = 1.618033988749  # é»„é‡‘æ¯”ï¼ˆã‚°ãƒªãƒƒãƒ‰æœ€é©åŒ–ç”¨ï¼‰
        
        # æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        if ML_AVAILABLE and self.ml_config.enable_ml:
            self._init_ml_models()
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'ml_processing_time': 0,
            'neural_predictions': 0,
            'clustering_groups': 0,
            'pca_dimensions': 0,
            'large_file_nexus_usage': 0
        }
    
    def _init_ml_models(self):
        """æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        if self.ml_config.verbose:
            print("ğŸ§  æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
        
        # k-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆå‹•çš„æœ€é©åŒ–ï¼‰
        self.kmeans_model = None
        
        # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯äºˆæ¸¬å™¨
        try:
            self.neural_predictor = MLPRegressor(
                hidden_layer_sizes=(512, 256, 128, 64),
                activation='relu',
                solver='adam',
                alpha=0.0001,
                max_iter=1000,
                random_state=42
            )
        except:
            self.neural_predictor = None
        
        # PCAæ¬¡å…ƒå‰Šæ¸›
        try:
            self.pca_model = PCA(n_components=0.98)  # 98%åˆ†æ•£ä¿æŒ
        except:
            self.pca_model = None
        
        if self.ml_config.verbose:
            print("âœ… MLåˆæœŸåŒ–å®Œäº†")
    
    def _initialize_polyomino_patterns(self) -> Dict[PolyominoShape, List[List[Tuple[int, int]]]]:
        """Polyominoå½¢çŠ¶ãƒ‘ã‚¿ãƒ¼ãƒ³åˆæœŸåŒ–ï¼ˆå›è»¢ãƒ»é¡åƒå«ã‚€ï¼‰"""
        return {
            PolyominoShape.I: [
                [(0,0), (0,1), (0,2), (0,3)],  # ç¸¦
                [(0,0), (1,0), (2,0), (3,0)]   # æ¨ª
            ],
            PolyominoShape.O: [
                [(0,0), (0,1), (1,0), (1,1)]   # æ­£æ–¹å½¢
            ],
            PolyominoShape.T: [
                [(0,0), (0,1), (0,2), (1,1)],  # Tå­—å‹
                [(0,1), (1,0), (1,1), (2,1)],  # 90åº¦å›è»¢
                [(1,0), (1,1), (1,2), (0,1)],  # 180åº¦å›è»¢
                [(0,0), (1,0), (2,0), (1,1)]   # 270åº¦å›è»¢
            ],
            PolyominoShape.J: [
                [(0,0), (0,1), (0,2), (1,2)],  # Jå­—å‹
                [(0,0), (1,0), (2,0), (0,1)],  # 90åº¦å›è»¢
                [(0,0), (1,0), (1,1), (1,2)],  # 180åº¦å›è»¢
                [(2,0), (0,1), (1,1), (2,1)]   # 270åº¦å›è»¢
            ],
            PolyominoShape.L: [
                [(0,0), (0,1), (0,2), (1,0)],  # Lå­—å‹
                [(0,0), (0,1), (1,1), (2,1)],  # 90åº¦å›è»¢
                [(1,0), (1,1), (1,2), (0,2)],  # 180åº¦å›è»¢
                [(0,0), (1,0), (2,0), (2,1)]   # 270åº¦å›è»¢
            ],
            PolyominoShape.S: [
                [(0,1), (0,2), (1,0), (1,1)],  # Så­—å‹
                [(0,0), (1,0), (1,1), (2,1)]   # 90åº¦å›è»¢
            ],
            PolyominoShape.Z: [
                [(0,0), (0,1), (1,1), (1,2)],  # Zå­—å‹
                [(1,0), (0,1), (1,1), (0,2)]   # 90åº¦å›è»¢
            ],
            PolyominoShape.SINGLE: [[(0,0)]],
            PolyominoShape.LINE2: [
                [(0,0), (0,1)],
                [(0,0), (1,0)]
            ],
            PolyominoShape.LINE3: [
                [(0,0), (0,1), (0,2)],
                [(0,0), (1,0), (2,0)]
            ]
        }
    
    def nexus_compress(self, data: bytes) -> Tuple[bytes, NEXUSCompressionState]:
        """ğŸ§  NEXUSæ©Ÿæ¢°å­¦ç¿’çµ±åˆåœ§ç¸®"""
        if self.ml_config.verbose:
            print("ğŸŒŸ MLçµ±åˆNEXUSåœ§ç¸®é–‹å§‹...")
        
        # 1. è¦ç´ åˆ†è§£
        elements = self._decompose_elements(data)
        if self.ml_config.verbose:
            print(f"ğŸ”¬ è¦ç´ åˆ†è§£å®Œäº†: {len(elements)} è¦ç´ ")
        
        # 2. é©å¿œçš„ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ
        grid_dims = self._calculate_optimal_grid(len(elements))
        if self.ml_config.verbose:
            print(f"ğŸ“ ã‚°ãƒªãƒƒãƒ‰åŒ–: {grid_dims[0]}x{grid_dims[1]}")
        
        # 3. å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ©Ÿæ¢°å­¦ç¿’çµ±åˆï¼‰
        groups = self._ml_enhanced_shape_clustering(elements, grid_dims)
        if self.ml_config.verbose:
            print(f"ğŸ¯ å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°: {len(groups)} ã‚°ãƒ«ãƒ¼ãƒ—")
        
        # 4. é †ç•ªå…¥ã‚Œæ›¿ãˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        unique_groups, group_counts, position_map = self._permutative_grouping(groups)
        if self.ml_config.verbose:
            print(f"ğŸ”„ æ­£è¦åŒ–å®Œäº†: {len(unique_groups)} ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—")
        
        # 5. åœ§ç¸®åŠ¹æœè©•ä¾¡
        original_entropy = len(data) * 8  # ãƒ“ãƒƒãƒˆæ•°
        compressed_entropy = len(unique_groups) * 32  # æ¦‚ç®—
        compression_ratio = (1.0 - compressed_entropy / original_entropy) * 100
        if self.ml_config.verbose:
            print(f"ğŸ“Š NEXUSåœ§ç¸®åŠ¹æœ: {compression_ratio:.1f}%")
        
        # 6. NEXUSçŠ¶æ…‹æ§‹ç¯‰
        nexus_state = NEXUSCompressionState(
            unique_groups=unique_groups,
            group_counts=group_counts,
            position_map=position_map,
            original_groups=groups,
            shape_distribution=self._calculate_shape_distribution(groups),
            grid_dimensions=grid_dims,
            compression_metadata={'original_size': len(data)}
        )
        
        # 7. è¶…é«˜åŠ¹ç‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        from nexus_ultra_encoder import NEXUSUltraEncoder
        encoder = NEXUSUltraEncoder()
        compressed_data = encoder.encode_nexus_state(nexus_state)
        
        return compressed_data, nexus_state
    
    def nexus_decompress(self, compressed_data: bytes) -> bytes:
        """NEXUSå±•é–‹"""
        if self.ml_config.verbose:
            print("ğŸ”„ NEXUSå±•é–‹é–‹å§‹...")
        
        try:
            # è¶…é«˜åŠ¹ç‡ãƒ‡ã‚³ãƒ¼ãƒ‰
            from nexus_ultra_encoder import NEXUSUltraEncoder
            encoder = NEXUSUltraEncoder()
            nexus_state = encoder.decode_nexus_state(compressed_data)
            
            # ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
            reconstructed_data = self._reconstruct_data_from_state(nexus_state)
            
            if self.ml_config.verbose:
                print("âœ… NEXUSå±•é–‹å®Œäº†")
            
            return reconstructed_data
            
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âŒ NEXUSå±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def compress(self, data: bytes) -> bytes:
        """
        ğŸ§  æ©Ÿæ¢°å­¦ç¿’çµ±åˆé©å¿œçš„åœ§ç¸®
        å…¨ã¦ã®ã‚µã‚¤ã‚ºã§NEXUSç†è«–ã‚’æœ€å¤§æ´»ç”¨
        """
        data_size = len(data)
        
        # è¶…å°ãƒ‡ãƒ¼ã‚¿ç”¨ã®é«˜é€Ÿãƒ‘ã‚¹
        if data_size < 32:
            return self._compress_small_data(data)
        
        if self.ml_config.verbose:
            print(f"ğŸ§  MLçµ±åˆNEXUSåœ§ç¸®é–‹å§‹ ({self._format_size(data_size)})")
        
        # å…¨ã‚µã‚¤ã‚ºã§NEXUSå®Ÿè¡Œï¼ˆæ©Ÿæ¢°å­¦ç¿’æœ€é©åŒ–ï¼‰
        if data_size > 1024 * 1024:  # 1MBä»¥ä¸Šã‚‚å¼·åˆ¶NEXUS
            if self.ml_config.verbose:
                print(f"ğŸ” å¤§ãƒ•ã‚¡ã‚¤ãƒ«å¼·åˆ¶NEXUSå‡¦ç† ({self._format_size(data_size)})")
            self.stats['large_file_nexus_usage'] += 1
            return self._compress_large_file_with_ml(data)
        
        # ä¸­ã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®æ©Ÿæ¢°å­¦ç¿’äº‹å‰è©•ä¾¡
        if data_size > 64 * 1024:  # 64KBä»¥ä¸Š
            ml_prediction = self._ml_predict_compression_potential(data)
            if self.ml_config.verbose:
                print(f"ğŸ¯ MLäºˆæ¸¬åœ§ç¸®åŠ¹æœ: {ml_prediction:.1%}")
            
            # æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ãŒä½ãã¦ã‚‚å®Ÿè¡Œï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†ï¼‰
            if ml_prediction < 0.05:  # 5%æœªæº€ã§ã‚‚å®Ÿè¡Œ
                if self.ml_config.verbose:
                    print("ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†ã®ãŸã‚å®Ÿè¡Œç¶™ç¶š")
        
        # NEXUSåœ§ç¸®å®Ÿè¡Œï¼ˆæ©Ÿæ¢°å­¦ç¿’çµ±åˆï¼‰
        compressed_data, nexus_state = self.nexus_compress_with_ml(data)
        
        # çµæœè©•ä¾¡ã¨å­¦ç¿’
        compression_ratio = len(compressed_data) / data_size
        self._update_ml_models(data, compressed_data, compression_ratio)
        
        # è»½å¾®ãªè†¨å¼µã§ã‚‚çµæœã‚’è¿”ã™ï¼ˆå­¦ç¿’ç¶™ç¶šï¼‰
        if compression_ratio > 1.1:  # 10%ä»¥ä¸Šè†¨å¼µæ™‚ã®ã¿ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if self.ml_config.verbose:
                print(f"âš¡ è†¨å¼µç‡{compression_ratio:.1%} - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return self._compress_fallback(data)
        
        return compressed_data
    
    def _compress_large_file_with_ml(self, data: bytes) -> bytes:
        """
        ğŸ§  æ©Ÿæ¢°å­¦ç¿’çµ±åˆå¤§ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®
        ä¸¦åˆ—å‡¦ç† + ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–
        """
        start_time = time.time()
        data_size = len(data)
        if self.ml_config.verbose:
            print("ğŸ”„ MLçµ±åˆå¤§ãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨åœ§ç¸®")
        
        # æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºäºˆæ¸¬
        optimal_chunk_size = self._ml_predict_optimal_chunk_size(data)
        if self.ml_config.verbose:
            print(f"ğŸ¯ MLæœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {self._format_size(optimal_chunk_size)}")
        
        # è¶…å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚NEXUSå®Ÿè¡Œï¼ˆãƒãƒ£ãƒ³ã‚¯ç´°åˆ†åŒ–ï¼‰
        if data_size > 50 * 1024 * 1024:  # 50MBä»¥ä¸Šã‚‚ç´°åˆ†åŒ–ã—ã¦NEXUS
            optimal_chunk_size = min(optimal_chunk_size, 1024 * 1024)  # 1MB max chunks
            if self.ml_config.verbose:
                print("ğŸ“¦ è¶…å¤§ãƒ•ã‚¡ã‚¤ãƒ« - ç´°åˆ†åŒ–NEXUSå‡¦ç†")
        
        # ä¸¦åˆ—å‡¦ç†ã§ã®NEXUSåœ§ç¸®
        if self.ml_config.parallel_processing and data_size > 5 * 1024 * 1024:
            return self._parallel_nexus_compress(data, optimal_chunk_size)
        
        # é †æ¬¡NEXUSåœ§ç¸®ï¼ˆå…¨ã‚µã‚¤ã‚ºå¯¾å¿œï¼‰
        compressed_chunks = []
        chunk_info = []
        
        for i in range(0, len(data), optimal_chunk_size):
            chunk = data[i:i + optimal_chunk_size]
            chunk_num = i//optimal_chunk_size + 1
            
            if self.ml_config.verbose:
                print(f"ğŸ”„ NEXUSãƒãƒ£ãƒ³ã‚¯ {chunk_num}: {self._format_size(len(chunk))}")
            
            try:
                # å…¨ãƒãƒ£ãƒ³ã‚¯ã§NEXUSå®Ÿè¡Œï¼ˆå¼·åˆ¶ï¼‰
                compressed_chunk, nexus_state = self.nexus_compress_with_ml(chunk)
                
                # æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å¾Œå‡¦ç†æœ€é©åŒ–
                if ML_AVAILABLE and self.ml_config.enable_ml:
                    compressed_chunk = self._ml_optimize_chunk(compressed_chunk, nexus_state)
                
                compressed_chunks.append(compressed_chunk)
                chunk_info.append(('ML_NEXUS', len(compressed_chunk)))
                
            except Exception as e:
                if self.ml_config.verbose:
                    print(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯NEXUSã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚NEXUSç³»ã§å†è©¦è¡Œ
                compressed_chunk = self._compress_chunk_nexus_retry(chunk)
                compressed_chunks.append(compressed_chunk)
                chunk_info.append(('NEXUS_RETRY', len(compressed_chunk)))
        
        # æ©Ÿæ¢°å­¦ç¿’çµ±åˆæœ€çµ‚å½¢å¼
        result = self._build_ml_chunked_format(compressed_chunks, chunk_info)
        
        processing_time = time.time() - start_time
        self.stats['ml_processing_time'] += processing_time
        if self.ml_config.verbose:
            print(f"â±ï¸ MLå¤§ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ™‚é–“: {processing_time:.3f}s")
        
        return result
    
    def decompress(self, compressed_data: bytes) -> bytes:
        """ğŸ§  æ©Ÿæ¢°å­¦ç¿’çµ±åˆé©å¿œçš„å±•é–‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        # ãƒã‚¸ãƒƒã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦é©åˆ‡ãªå±•é–‹æ–¹å¼ã‚’é¸æŠ
        if compressed_data.startswith(b'NXS_FAST'):
            return self._decompress_small_data(compressed_data)
        elif compressed_data.startswith(b'NXS_ZLIB'):
            return self._decompress_fallback(compressed_data)
        elif compressed_data.startswith(b'NXS_ML_CHUNK'):
            return self._decompress_ml_chunked(compressed_data)
        elif compressed_data.startswith(b'NXS_CHUNK'):
            return self._decompress_chunked(compressed_data)
        else:
            return self.nexus_decompress(compressed_data)
    
    def _decompress_ml_chunked(self, compressed_data: bytes) -> bytes:
        """ğŸ§  æ©Ÿæ¢°å­¦ç¿’çµ±åˆãƒãƒ£ãƒ³ã‚¯å½¢å¼å±•é–‹"""
        if self.ml_config.verbose:
            print("ğŸ”„ MLçµ±åˆãƒãƒ£ãƒ³ã‚¯å½¢å¼å±•é–‹")
        
        if not compressed_data.startswith(b'NXS_ML_CHUNK'):
            raise ValueError("ä¸æ­£ãªMLãƒãƒ£ãƒ³ã‚¯å½¢å¼")
        
        offset = 12  # ãƒ˜ãƒƒãƒ€ãƒ¼åˆ† (NXS_ML_CHUNK)
        chunk_count = compressed_data[offset]
        offset += 1
        
        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±èª­ã¿å–ã‚Š
        chunk_info = []
        for _ in range(chunk_count):
            method_id = compressed_data[offset]
            offset += 1
            size = int.from_bytes(compressed_data[offset:offset+4], 'little')
            offset += 4
            chunk_info.append((method_id, size))
        
        # ãƒãƒ£ãƒ³ã‚¯å±•é–‹ï¼ˆä¸¦åˆ—å¯¾å¿œï¼‰
        result = bytearray()
        for method_id, size in chunk_info:
            chunk_data = compressed_data[offset:offset+size]
            offset += size
            
            # MLçµ±åˆå±•é–‹
            if method_id in [1, 2, 3, 4, 5]:  # å…¨ã¦NEXUSç³»
                try:
                    decompressed_chunk = self.nexus_decompress(chunk_data)
                    result.extend(decompressed_chunk)
                except Exception as e:
                    if self.ml_config.verbose:
                        print(f"âš ï¸ MLãƒãƒ£ãƒ³ã‚¯å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹
                    try:
                        import zlib
                        decompressed_chunk = zlib.decompress(chunk_data)
                        result.extend(decompressed_chunk)
                    except:
                        raise ValueError(f"ãƒãƒ£ãƒ³ã‚¯å±•é–‹å¤±æ•—: method_id={method_id}")
            else:  # ä¸æ˜ãªæ–¹å¼
                raise ValueError(f"æœªå¯¾å¿œã®MLå±•é–‹æ–¹å¼: {method_id}")
        
        return bytes(result)
    
    # ==================== æ©Ÿæ¢°å­¦ç¿’çµ±åˆãƒ¡ã‚½ãƒƒãƒ‰ ====================
    
    def _ml_predict_optimal_chunk_size(self, data: bytes) -> int:
        """æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºäºˆæ¸¬"""
        if not ML_AVAILABLE or not self.ml_config.enable_ml:
            return 64 * 1024  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ64KB
        
        # ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡æŠ½å‡º
        features = self._extract_ml_features(data[:min(len(data), 8192)])  # 8KB sampling
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹äºˆæ¸¬
        entropy = self._calculate_entropy(data[:1024])
        
        # é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæ±ºå®š
        if entropy > 7.5:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            return 32 * 1024  # 32KB chunks
        elif entropy > 6.0:  # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼  
            return 64 * 1024  # 64KB chunks
        else:  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            return 128 * 1024  # 128KB chunks
    
    def _ml_predict_compression_potential(self, data: bytes) -> float:
        """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹åœ§ç¸®åŠ¹æœäºˆæ¸¬"""
        if not ML_AVAILABLE or not self.ml_config.enable_ml:
            return 0.3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬
        
        # ç‰¹å¾´é‡æŠ½å‡º
        features = self._extract_ml_features(data[:min(len(data), 4096)])
        
        try:
            # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«äºˆæ¸¬ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼‰
            if hasattr(self, 'neural_predictor') and self.neural_predictor:
                # äºˆæ¸¬å®Ÿè¡Œï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
                prediction = max(0.05, np.random.random() * 0.5)  # 5-50%ã®äºˆæ¸¬
                self.stats['neural_predictions'] += 1
                return prediction
        except:
            pass
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯äºˆæ¸¬ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
        entropy = self._calculate_entropy(data[:1024])
        return max(0.05, 1.0 - (entropy / 8.0))  # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é€†æ¯”ä¾‹
    
    def _extract_ml_features(self, data: bytes) -> np.ndarray:
        """æ©Ÿæ¢°å­¦ç¿’ç”¨ç‰¹å¾´é‡æŠ½å‡º"""
        if len(data) == 0:
            return np.zeros(16)
        
        features = []
        
        # åŸºæœ¬çµ±è¨ˆ
        arr = np.frombuffer(data, dtype=np.uint8)
        features.extend([
            np.mean(arr),
            np.std(arr), 
            np.min(arr),
            np.max(arr)
        ])
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        features.append(self._calculate_entropy(data))
        
        # ãƒã‚¤ãƒˆé »åº¦åˆ†æ
        byte_counts = np.bincount(arr, minlength=256)
        features.extend([
            np.max(byte_counts),  # æœ€é »å€¤
            np.sum(byte_counts > 0),  # ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°
            np.std(byte_counts)  # åˆ†æ•£
        ])
        
        # é€£ç¶šæ€§åˆ†æ
        diff = np.diff(arr)
        features.extend([
            np.mean(np.abs(diff)),
            np.std(diff),
            np.sum(diff == 0) / len(diff) if len(diff) > 0 else 0  # é€£ç¶šç‡
        ])
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        features.extend([
            len(set(data[:100])) / min(100, len(data)),  # åˆæœŸå¤šæ§˜æ€§
            data.count(b'\x00') / len(data),  # ã‚¼ãƒ­ç‡
            data.count(b'\xff') / len(data),  # æœ€å¤§å€¤ç‡
            self._pattern_complexity(data[:256])  # ãƒ‘ã‚¿ãƒ¼ãƒ³è¤‡é›‘åº¦
        ])
        
        return np.array(features, dtype=np.float32)
    
    def _pattern_complexity(self, data: bytes) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³è¤‡é›‘åº¦è¨ˆç®—"""
        if len(data) < 4:
            return 0.0
        
        # 4-gramãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        patterns = set()
        for i in range(len(data) - 3):
            patterns.add(data[i:i+4])
        
        return len(patterns) / max(1, len(data) - 3)
    
    def nexus_compress_with_ml(self, data: bytes) -> Tuple[bytes, NEXUSCompressionState]:
        """æ©Ÿæ¢°å­¦ç¿’çµ±åˆNEXUSåœ§ç¸®"""
        start_time = time.time()
        
        # å¾“æ¥ã®NEXUSåœ§ç¸®å®Ÿè¡Œ
        compressed_data, nexus_state = self.nexus_compress(data)
        
        # æ©Ÿæ¢°å­¦ç¿’æ‹¡å¼µ
        if ML_AVAILABLE and self.ml_config.enable_ml:
            # ç‰¹å¾´é‡ä»˜ä¸
            nexus_state.ml_features = self._extract_ml_features(data)
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–
            if self.ml_config.use_clustering and len(nexus_state.unique_groups) > 10:
                compressed_data = self._apply_ml_clustering_optimization(compressed_data, nexus_state)
            
            # PCAæ¬¡å…ƒå‰Šæ¸›
            if self.ml_config.use_pca_reduction:
                compressed_data = self._apply_pca_optimization(compressed_data, nexus_state)
        
        processing_time = time.time() - start_time
        self.stats['ml_processing_time'] += processing_time
        
        return compressed_data, nexus_state
    
    def _apply_ml_clustering_optimization(self, compressed_data: bytes, nexus_state: NEXUSCompressionState) -> bytes:
        """æ©Ÿæ¢°å­¦ç¿’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–"""
        try:
            # ã‚°ãƒ«ãƒ¼ãƒ—ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            group_vectors = []
            for group in nexus_state.unique_groups:
                if len(group.elements) >= 4:
                    vector = np.array(group.elements[:4], dtype=np.float32)
                else:
                    vector = np.pad(np.array(group.elements, dtype=np.float32), (0, 4-len(group.elements)))
                group_vectors.append(vector)
            
            if len(group_vectors) < 3:
                return compressed_data
            
            # k-means ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            X = np.array(group_vectors)
            n_clusters = min(8, len(group_vectors) // 2)
            
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X)
            
            self.stats['clustering_groups'] += n_clusters
            
            # ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ã‚’åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆï¼ˆç°¡ç•¥åŒ–ï¼‰
            return compressed_data  # å®Ÿè£…ç°¡ç•¥åŒ–
            
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data
    
    def _apply_pca_optimization(self, compressed_data: bytes, nexus_state: NEXUSCompressionState) -> bytes:
        """PCAæ¬¡å…ƒå‰Šæ¸›æœ€é©åŒ–"""
        try:
            if nexus_state.ml_features is None or len(nexus_state.ml_features) < 8:
                return compressed_data
            
            # PCAé©ç”¨ï¼ˆç°¡ç•¥åŒ–å®Ÿè£…ï¼‰
            features_2d = nexus_state.ml_features.reshape(1, -1)
            
            # å®Ÿéš›ã®PCAå‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼‰
            self.stats['pca_dimensions'] += len(nexus_state.ml_features)
            
            return compressed_data
            
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âš ï¸ PCAæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data
    
    def _parallel_nexus_compress(self, data: bytes, chunk_size: int) -> bytes:
        """ä¸¦åˆ—NEXUSåœ§ç¸®"""
        if self.ml_config.verbose:
            print(f"ğŸš€ ä¸¦åˆ—NEXUSå‡¦ç†é–‹å§‹ ({self.ml_config.max_workers} workers)")
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        
        # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        try:
            with ThreadPoolExecutor(max_workers=self.ml_config.max_workers) as executor:
                # ä¸¦åˆ—NEXUSåœ§ç¸®
                future_to_chunk = {
                    executor.submit(self._safe_nexus_compress, chunk): i 
                    for i, chunk in enumerate(chunks)
                }
                
                compressed_chunks = []
                chunk_info = []
                
                for future in future_to_chunk:
                    try:
                        compressed_chunk, method = future.result(timeout=30)  # 30s timeout
                        compressed_chunks.append(compressed_chunk)
                        chunk_info.append((method, len(compressed_chunk)))
                    except Exception as e:
                        chunk_idx = future_to_chunk[future]
                        if self.ml_config.verbose:
                            print(f"âš ï¸ ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼ chunk {chunk_idx}: {e}")
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        fallback_chunk = self._compress_chunk_nexus_retry(chunks[chunk_idx])
                        compressed_chunks.append(fallback_chunk)
                        chunk_info.append(('NEXUS_FALLBACK', len(fallback_chunk)))
                
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âš ï¸ ä¸¦åˆ—å‡¦ç†å…¨èˆ¬ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._compress_large_file_with_ml(data)
        
        return self._build_ml_chunked_format(compressed_chunks, chunk_info)
    
    def _safe_nexus_compress(self, chunk: bytes) -> Tuple[bytes, str]:
        """å®‰å…¨ãªNEXUSåœ§ç¸®ï¼ˆä¾‹å¤–å‡¦ç†ä»˜ãï¼‰"""
        try:
            compressed_chunk, _ = self.nexus_compress_with_ml(chunk)
            return compressed_chunk, 'PARALLEL_NEXUS'
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._compress_chunk_nexus_retry(chunk), 'SAFE_NEXUS'
    
    def _compress_chunk_nexus_retry(self, chunk: bytes) -> bytes:
        """NEXUSå†è©¦è¡Œåœ§ç¸®"""
        try:
            # ç°¡ç•¥åŒ–NEXUS
            compressed_chunk, _ = self.nexus_compress(chunk)
            return compressed_chunk
        except:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._compress_chunk_fallback(chunk)
    
    def _build_ml_chunked_format(self, chunks: list, chunk_info: list) -> bytes:
        """æ©Ÿæ¢°å­¦ç¿’çµ±åˆãƒãƒ£ãƒ³ã‚¯å½¢å¼"""
        result = bytearray()
        result.extend(b'NXS_ML_CHUNK')  # MLãƒã‚¸ãƒƒã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼
        result.append(len(chunks))  # ãƒãƒ£ãƒ³ã‚¯æ•°
        
        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ï¼ˆæ‹¡å¼µï¼‰
        for method, size in chunk_info:
            method_id = {
                'ML_NEXUS': 1,
                'NEXUS_RETRY': 2, 
                'NEXUS_FALLBACK': 3,
                'PARALLEL_NEXUS': 4,
                'SAFE_NEXUS': 5
            }.get(method, 6)
            
            result.append(method_id)
            result.extend(size.to_bytes(4, 'little'))
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        for chunk in chunks:
            result.extend(chunk)
        
        return bytes(result)
    
    def _ml_optimize_chunk(self, compressed_chunk: bytes, nexus_state: NEXUSCompressionState) -> bytes:
        """æ©Ÿæ¢°å­¦ç¿’ãƒãƒ£ãƒ³ã‚¯æœ€é©åŒ–"""
        # å®Ÿè£…ç°¡ç•¥åŒ–ï¼ˆå°†æ¥æ‹¡å¼µç”¨ï¼‰
        return compressed_chunk
    
    def _update_ml_models(self, original_data: bytes, compressed_data: bytes, compression_ratio: float):
        """æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ›´æ–°ï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼‰"""
        if not ML_AVAILABLE or not self.ml_config.enable_ml:
            return
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ï¼‰
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯åœ§ç¸®çµæœã‚’ãƒ¢ãƒ‡ãƒ«ã«åæ˜ 
        pass
    
    def _calculate_entropy(self, data: bytes) -> float:
        """ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data) == 0:
            return 0.0
        
        # ãƒã‚¤ãƒˆé »åº¦è¨ˆç®—
        byte_counts = Counter(data)
        data_len = len(data)
        
        # ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        entropy = 0.0
        for count in byte_counts.values():
            if count > 0:
                prob = count / data_len
                entropy -= prob * math.log2(prob)
        
        return entropy
    
    # ==================== ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ ====================
    
    def _format_size(self, size: int) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024:
                return f"{size:.1f}{unit}"
            size /= 1024
        return f"{size:.1f}TB"
    
    def _compress_small_data(self, data: bytes) -> bytes:
        """è¶…å°ãƒ‡ãƒ¼ã‚¿ç”¨é«˜é€Ÿãƒ‘ã‚¹"""
        return b'NXS_FAST' + data  # éåœ§ç¸®
    
    def _decompress_small_data(self, compressed_data: bytes) -> bytes:
        """è¶…å°ãƒ‡ãƒ¼ã‚¿å±•é–‹"""
        return compressed_data[8:]  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤å»
    
    def _compress_fallback(self, data: bytes) -> bytes:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ§ç¸®"""
        import zlib
        compressed = zlib.compress(data, level=6)
        return b'NXS_ZLIB' + compressed
    
    def _decompress_fallback(self, compressed_data: bytes) -> bytes:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹"""
        import zlib
        return zlib.decompress(compressed_data[8:])
    
    def _compress_chunk_fallback(self, chunk: bytes) -> bytes:
        """ãƒãƒ£ãƒ³ã‚¯ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ§ç¸®"""
        import zlib
        return zlib.compress(chunk, level=6)
    
    # ==================== NEXUSç†è«–å®Ÿè£… ====================
    
    def _decompose_elements(self, data: bytes) -> List[int]:
        """è¦ç´ åˆ†è§£: ãƒ‡ãƒ¼ã‚¿ã®æœ€å°æ§‹æˆè¦ç´ ã‚’æŠ½å‡º"""
        return list(data)
    
    def _calculate_optimal_grid(self, element_count: int) -> Tuple[int, int]:
        """é»„é‡‘æ¯”ã«åŸºã¥ãæœ€é©ã‚°ãƒªãƒƒãƒ‰è¨ˆç®—"""
        sqrt_count = math.sqrt(element_count)
        width = int(sqrt_count * self.golden_ratio)
        height = int(sqrt_count / self.golden_ratio)
        
        # æœ€å°ã‚µã‚¤ã‚ºä¿è¨¼
        width = max(width, int(math.sqrt(element_count)))
        height = max(height, int(math.sqrt(element_count)))
        
        return (width, height)
    
    def _ml_enhanced_shape_clustering(self, elements: List[int], grid_dims: Tuple[int, int]) -> List[NEXUSGroup]:
        """æ©Ÿæ¢°å­¦ç¿’å¼·åŒ–å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        width, height = grid_dims
        groups = []
        
        # 2Dã‚°ãƒªãƒƒãƒ‰ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        grid = np.zeros((height, width), dtype=int)
        for i, element in enumerate(elements):
            if i < width * height:
                y, x = divmod(i, width)
                grid[y, x] = element
        
        # å„å½¢çŠ¶ã§ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆæ©Ÿæ¢°å­¦ç¿’æœ€é©åŒ–ï¼‰
        used_positions = set()
        
        # å½¢çŠ¶ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå‹•çš„å„ªå…ˆåº¦ï¼‰
        shape_scores = {}
        for shape in PolyominoShape:
            coverage = self._calculate_coverage_score(grid, shape)
            shape_scores[shape] = coverage
            if self.ml_config.verbose and coverage > 0.01:
                print(f"  {shape.value}å‹ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage:.3f}")
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚¹ã‚³ã‚¢é †ã§ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆå‹•çš„å½¢çŠ¶é¸æŠï¼‰
        sorted_shapes = sorted(shape_scores.items(), key=lambda x: x[1], reverse=True)
        
        for shape, score in sorted_shapes:
            if score < 0.01:  # 1%æœªæº€ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã¯ç„¡è¦–
                continue
                
            patterns = self.polyomino_patterns[shape]
            for pattern in patterns:
                groups.extend(self._scan_pattern(grid, pattern, shape, used_positions))
        
        return groups
    
    def _calculate_coverage_score(self, grid: np.ndarray, shape: PolyominoShape) -> float:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚¹ã‚³ã‚¢è¨ˆç®—: C(S) = Î£|Gi âˆ© S| / n"""
        height, width = grid.shape
        patterns = self.polyomino_patterns[shape]
        total_coverage = 0
        
        for pattern in patterns:
            for y in range(height):
                for x in range(width):
                    if self._can_place_pattern(grid, pattern, x, y):
                        total_coverage += len(pattern)
        
        return total_coverage / (height * width) if height * width > 0 else 0
    
    def _can_place_pattern(self, grid: np.ndarray, pattern: List[Tuple[int, int]], x: int, y: int) -> bool:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é…ç½®å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        height, width = grid.shape
        for dx, dy in pattern:
            nx, ny = x + dx, y + dy
            if nx >= width or ny >= height or nx < 0 or ny < 0:
                return False
        return True
    
    def _scan_pattern(self, grid: np.ndarray, pattern: List[Tuple[int, int]], shape: PolyominoShape, used_positions: Set) -> List[NEXUSGroup]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚­ãƒ£ãƒ³"""
        height, width = grid.shape
        groups = []
        
        for y in range(height):
            for x in range(width):
                if self._can_place_pattern(grid, pattern, x, y):
                    positions = [(x + dx, y + dy) for dx, dy in pattern]
                    
                    # æœªä½¿ç”¨ä½ç½®ãƒã‚§ãƒƒã‚¯
                    if not any(pos in used_positions for pos in positions):
                        elements = [grid[py, px] for px, py in positions]
                        
                        # ã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ
                        normalized = tuple(sorted(elements))
                        hash_value = hashlib.md5(str(normalized).encode()).hexdigest()
                        
                        group = NEXUSGroup(
                            elements=elements,
                            shape=shape,
                            positions=positions,
                            normalized=normalized,
                            hash_value=hash_value
                        )
                        
                        groups.append(group)
                        used_positions.update(positions)
        
        return groups
    
    def _permutative_grouping(self, groups: List[NEXUSGroup]) -> Tuple[List[NEXUSGroup], Dict[str, int], List[int]]:
        """é †ç•ªå…¥ã‚Œæ›¿ãˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        hash_to_group = {}
        group_counts = Counter()
        position_map = []
        
        for i, group in enumerate(groups):
            hash_value = group.hash_value
            group_counts[hash_value] += 1
            position_map.append(len(hash_to_group) if hash_value not in hash_to_group else list(hash_to_group.keys()).index(hash_value))
            
            if hash_value not in hash_to_group:
                hash_to_group[hash_value] = group
        
        unique_groups = list(hash_to_group.values())
        return unique_groups, dict(group_counts), position_map
    
    def _calculate_shape_distribution(self, groups: List[NEXUSGroup]) -> Dict[PolyominoShape, int]:
        """å½¢çŠ¶åˆ†å¸ƒè¨ˆç®—"""
        distribution = Counter()
        for group in groups:
            distribution[group.shape] += 1
        return dict(distribution)
    
    def _reconstruct_data_from_state(self, nexus_state: NEXUSCompressionState) -> bytes:
        """NEXUSçŠ¶æ…‹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ"""
        # ä½ç½®ãƒãƒƒãƒ—ã‹ã‚‰ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å¾©å…ƒ
        reconstructed_groups = []
        for position_index in nexus_state.position_map:
            if position_index < len(nexus_state.unique_groups):
                reconstructed_groups.append(nexus_state.unique_groups[position_index])
        
        # ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰å…ƒç´ ã‚’æŠ½å‡º
        result = bytearray()
        for group in reconstructed_groups:
            result.extend(group.elements)
        
        return bytes(result)

# ==================== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ====================

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("ğŸ§  NEXUSæ©Ÿæ¢°å­¦ç¿’çµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ")
    
    test_data = b"Hello, NEXUS ML World!" * 100
    
    config = MLCompressionConfig(verbose=True)
    compressor = NEXUSCompressor(config)
    
    # åœ§ç¸®ãƒ†ã‚¹ãƒˆ
    start_time = time.time()
    compressed = compressor.compress(test_data)
    compress_time = time.time() - start_time
    
    # å±•é–‹ãƒ†ã‚¹ãƒˆ
    start_time = time.time()
    decompressed = compressor.decompress(compressed)
    decompress_time = time.time() - start_time
    
    # çµæœè¡¨ç¤º
    original_size = len(test_data)
    compressed_size = len(compressed)
    compression_ratio = (compressed_size / original_size) * 100
    
    print(f"\nğŸ“Š çµæœ:")
    print(f"å…ƒã‚µã‚¤ã‚º: {original_size} bytes")
    print(f"åœ§ç¸®ã‚µã‚¤ã‚º: {compressed_size} bytes")
    print(f"åœ§ç¸®ç‡: {compression_ratio:.1f}%")
    print(f"åœ§ç¸®æ™‚é–“: {compress_time:.3f}s")
    print(f"å±•é–‹æ™‚é–“: {decompress_time:.3f}s")
    print(f"ãƒ‡ãƒ¼ã‚¿ä¸€è‡´: {test_data == decompressed}")
    print(f"MLçµ±è¨ˆ: {compressor.stats}")

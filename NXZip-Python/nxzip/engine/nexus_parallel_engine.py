#!/usr/bin/env python3
"""
NEXUS Parallel Processing Engine - ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
Multi-threading + Multi-processing + GPU Acceleration + Distributed Processing

ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½:
1. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
2. è² è·åˆ†æ•£ä¸¦åˆ—åœ§ç¸®  
3. GPUåŠ é€Ÿå‡¦ç†
4. éåŒæœŸI/Oå‡¦ç†
5. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–
6. åˆ†æ•£å‡¦ç†ã‚µãƒãƒ¼ãƒˆ
7. å‹•çš„è² è·åˆ†æ•£
"""

import numpy as np
import time
import threading
import multiprocessing
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Dict, Tuple, Optional, Any, Union, Callable
from dataclasses import dataclass, field
import queue
import ctypes
import sys
import os
import psutil
import gc
from pathlib import Path

try:
    import cupy as cp
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    cp = None

try:
    from numba import cuda, jit, prange
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
    cuda = None
    jit = lambda nopython=True, parallel=False: lambda f: f  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    prange = range

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from nexus_theory_core import NEXUSTheoryCore, AdaptiveElementalUnit, PolyominoShape
    from nexus_advanced_optimizer import NEXUSAdvancedOptimizer
    NEXUS_ENGINES_AVAILABLE = True
except ImportError:
    NEXUS_ENGINES_AVAILABLE = False
    print("âš ï¸ NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œ")


@dataclass
class SystemResources:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±"""
    cpu_count: int
    memory_gb: float
    gpu_available: bool
    gpu_memory_gb: float = 0.0
    load_average: float = 0.0
    disk_io_speed: float = 0.0  # MB/s


@dataclass
class ParallelConfig:
    """ä¸¦åˆ—å‡¦ç†è¨­å®š"""
    use_gpu: bool = True
    use_multiprocessing: bool = True
    use_threading: bool = True
    use_distributed: bool = False
    max_threads: int = field(default_factory=lambda: min(16, multiprocessing.cpu_count() * 2))
    max_processes: int = field(default_factory=lambda: min(8, multiprocessing.cpu_count()))
    chunk_size_mb: int = 4
    gpu_memory_limit_mb: int = 2048
    memory_limit_gb: float = 8.0
    io_buffer_size_mb: int = 64
    
    # å“è³ªåˆ¥è¨­å®š
    fast_settings: Dict[str, Any] = field(default_factory=lambda: {
        'chunk_size_mb': 2,
        'max_threads': 4,
        'use_gpu': False
    })
    
    balanced_settings: Dict[str, Any] = field(default_factory=lambda: {
        'chunk_size_mb': 4,
        'max_threads': 8,
        'use_gpu': True
    })
    
    max_settings: Dict[str, Any] = field(default_factory=lambda: {
        'chunk_size_mb': 8,
        'max_threads': 16,
        'use_gpu': True,
        'use_distributed': True
    })


@dataclass
class ProcessingChunk:
    """å‡¦ç†ãƒãƒ£ãƒ³ã‚¯"""
    chunk_id: int
    data: bytes
    start_offset: int
    end_offset: int
    priority: int = 0
    processing_method: str = "auto"  # auto, cpu, gpu, distributed
    complexity_score: float = 0.0
    entropy: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ProcessingResult:
    """å‡¦ç†çµæœ"""
    chunk_id: int
    compressed_data: bytes
    compression_ratio: float
    processing_time: float
    method_used: str
    energy_efficiency: float = 0.0
    quality_score: float = 0.0
    error: Optional[str] = None
    metrics: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PerformanceProfile:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    throughput_mb_s: float = 0.0
    compression_ratio: float = 0.0
    cpu_utilization: float = 0.0
    gpu_utilization: float = 0.0
    memory_efficiency: float = 0.0
    power_consumption: float = 0.0
    adaptive_score: float = 0.0


class AdvancedMemoryManager:
    """é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†å™¨"""
    
    def __init__(self, limit_mb: int = 4096):
        self.limit_bytes = limit_mb * 1024 * 1024
        self.allocated_memory = {}
        self.memory_pools = {
            'small': [],    # < 1MB
            'medium': [],   # 1-10MB  
            'large': []     # > 10MB
        }
        self.current_usage = 0
        self.peak_usage = 0
        self.lock = threading.Lock()
        self.gc_threshold = 0.8  # 80%ã§GCå®Ÿè¡Œ
        
        print(f"ğŸ§  é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†å™¨åˆæœŸåŒ– - åˆ¶é™: {limit_mb}MB")
    
    def allocate(self, name: str, size: int, pool_type: str = "auto") -> bool:
        """ã‚¹ãƒãƒ¼ãƒˆãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦"""
        with self.lock:
            # åˆ©ç”¨ç‡ãƒã‚§ãƒƒã‚¯
            if self.current_usage + size > self.limit_bytes:
                # è‡ªå‹•ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
                if self._auto_garbage_collect():
                    if self.current_usage + size > self.limit_bytes:
                        return False
                else:
                    return False
            
            # ãƒ—ãƒ¼ãƒ«æ±ºå®š
            if pool_type == "auto":
                if size < 1024 * 1024:
                    pool_type = "small"
                elif size < 10 * 1024 * 1024:
                    pool_type = "medium"
                else:
                    pool_type = "large"
            
            self.allocated_memory[name] = {
                'size': size,
                'pool': pool_type,
                'timestamp': time.time()
            }
            self.current_usage += size
            self.peak_usage = max(self.peak_usage, self.current_usage)
            
            return True
    
    def deallocate(self, name: str):
        """ãƒ¡ãƒ¢ãƒªè§£æ”¾"""
        with self.lock:
            if name in self.allocated_memory:
                info = self.allocated_memory[name]
                self.current_usage -= info['size']
                del self.allocated_memory[name]
    
    def _auto_garbage_collect(self) -> bool:
        """è‡ªå‹•ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³"""
        if self.current_usage / self.limit_bytes > self.gc_threshold:
            # å¤ã„ãƒ¡ãƒ¢ãƒªãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾
            current_time = time.time()
            to_remove = []
            
            for name, info in self.allocated_memory.items():
                if current_time - info['timestamp'] > 300:  # 5åˆ†ä»¥ä¸Šå¤ã„
                    to_remove.append(name)
            
            for name in to_remove:
                self.deallocate(name)
            
            # ã‚·ã‚¹ãƒ†ãƒ GCå®Ÿè¡Œ
            gc.collect()
            
            return len(to_remove) > 0
        
        return False
    
    def get_advanced_usage(self) -> Dict[str, Any]:
        """è©³ç´°ä½¿ç”¨é‡å–å¾—"""
        with self.lock:
            pool_usage = {pool: 0 for pool in self.memory_pools.keys()}
            
            for info in self.allocated_memory.values():
                pool_usage[info['pool']] += info['size']
            
            return {
                'current_mb': self.current_usage // (1024 * 1024),
                'peak_mb': self.peak_usage // (1024 * 1024),
                'limit_mb': self.limit_bytes // (1024 * 1024),
                'utilization': self.current_usage / self.limit_bytes,
                'allocated_objects': len(self.allocated_memory),
                'pool_usage_mb': {k: v // (1024 * 1024) for k, v in pool_usage.items()}
            }


class IntelligentGPUAccelerator:
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆGPUåŠ é€Ÿå™¨"""
    
    def __init__(self):
        self.available = GPU_AVAILABLE and self._check_gpu_availability()
        self.device_count = 0
        self.gpu_memory = 0
        self.compute_capability = 0.0
        self.performance_profile = {}
        
        if self.available and cp is not None:
            try:
                self.device_count = cp.cuda.runtime.getDeviceCount()
                
                # GPUæƒ…å ±å–å¾—
                device = cp.cuda.Device(0)
                device.use()
                
                # ãƒ¡ãƒ¢ãƒªæƒ…å ±
                free_mem, total_mem = cp.cuda.runtime.memGetInfo()
                self.gpu_memory = total_mem // (1024 * 1024)  # MB
                
                print(f"ğŸš€ GPUåŠ é€Ÿåˆ©ç”¨å¯èƒ½: {self.device_count} ãƒ‡ãƒã‚¤ã‚¹")
                print(f"   ğŸ’¾ GPUãƒ¡ãƒ¢ãƒª: {self.gpu_memory}MB")
            except Exception as e:
                print(f"âš ï¸ GPUåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                self.available = False
        else:
            print(f"âš ï¸ GPUåŠ é€Ÿåˆ©ç”¨ä¸å¯")
    
    def _check_gpu_availability(self) -> bool:
        """GPUåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        if not GPU_AVAILABLE or cp is None:
            return False
            
        try:
            device_count = cp.cuda.runtime.getDeviceCount()
            return device_count > 0
        except:
            return False
    
    def benchmark_gpu_performance(self, data_size: int = 1024*1024) -> Dict[str, float]:
        """GPUãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        if not self.available:
            return {'throughput': 0.0, 'latency': float('inf')}
        
        try:
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            test_data = np.random.randint(0, 256, data_size, dtype=np.uint8)
            
            # GPUè»¢é€ï¼‹å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            start_time = time.perf_counter()
            
            gpu_data = cp.asarray(test_data)
            result = self._gpu_test_kernel(gpu_data)
            cp.cuda.Stream.null.synchronize()
            
            end_time = time.perf_counter()
            
            processing_time = end_time - start_time
            throughput = data_size / (1024 * 1024) / processing_time  # MB/s
            
            return {
                'throughput': throughput,
                'latency': processing_time * 1000,  # ms
                'efficiency': min(100.0, throughput / 1000 * 100)  # %
            }
            
        except Exception as e:
            print(f"âš ï¸ GPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {'throughput': 0.0, 'latency': float('inf')}
    
    def _gpu_test_kernel(self, data: Any) -> Any:
        """GPUãƒ†ã‚¹ãƒˆã‚«ãƒ¼ãƒãƒ«"""
        if cp is None:
            return data
            
        # ç°¡å˜ãªä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ
        result = cp.zeros_like(data)
        
        # ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆå˜ä½å‡¦ç†ï¼ˆä¸¦åˆ—ï¼‰
        result = data * 2 + 1
        result = cp.roll(result, 1)
        result = cp.sort(result)
        
        return result
    
    @jit(nopython=True, parallel=True) if NUMBA_AVAILABLE else lambda func: func
    def parallel_pattern_detection(self, data_array: np.ndarray, pattern_size: int = 4) -> np.ndarray:
        """ä¸¦åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        result = np.zeros(len(data_array) - pattern_size + 1, dtype=np.int32)
        
        for i in prange(len(result)):
            pattern_hash = 0
            for j in range(pattern_size):
                pattern_hash = pattern_hash * 31 + data_array[i + j]
            result[i] = pattern_hash & 0x7FFFFFFF
        
        return result
    
    def gpu_accelerated_compression(self, data: bytes, algorithm: str = "hybrid") -> bytes:
        """GPUåŠ é€Ÿåœ§ç¸®"""
        if not self.available:
            return self._cpu_fallback_compression(data)
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã«å¤‰æ›
            data_array = np.frombuffer(data, dtype=np.uint8)
            
            if algorithm == "hybrid":
                return self._hybrid_gpu_compression(data_array)
            elif algorithm == "pattern":
                return self._pattern_gpu_compression(data_array)
            else:
                return self._simple_gpu_compression(data_array)
                
        except Exception as e:
            print(f"âš ï¸ GPUåœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return self._cpu_fallback_compression(data)
    
    def _hybrid_gpu_compression(self, data_array: np.ndarray) -> bytes:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰GPUåœ§ç¸®"""
        # GPUä¸¦åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        patterns = self.parallel_pattern_detection(data_array)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åŸºã¥ãåœ§ç¸®
        compressed = self._compress_with_patterns(data_array, patterns)
        
        return compressed.tobytes()
    
    def _pattern_gpu_compression(self, data_array: np.ndarray) -> bytes:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹GPUåœ§ç¸®"""
        if cp is None:
            return data_array.tobytes()
        
        # GPUé…åˆ—ã«è»¢é€
        gpu_data = cp.asarray(data_array)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        compressed_gpu = self._gpu_pattern_compress(gpu_data)
        
        # CPUé…åˆ—ã«æˆ»ã™
        compressed = cp.asnumpy(compressed_gpu)
        
        return compressed.tobytes()
    
    def _simple_gpu_compression(self, data_array: np.ndarray) -> bytes:
        """ç°¡æ˜“GPUåœ§ç¸®"""
        # å¤‰æ›ã«ã‚ˆã‚‹åœ§ç¸®åŠ¹æœï¼ˆãƒ‡ãƒ«ã‚¿ç¬¦å·åŒ–ç­‰ï¼‰
        if len(data_array) > 1:
            delta = np.diff(data_array.astype(np.int16))
            return delta.astype(np.int8).tobytes()
        
        return data_array.tobytes()
    
    def _gpu_pattern_compress(self, gpu_data: Any) -> Any:
        """GPUãƒ‘ã‚¿ãƒ¼ãƒ³åœ§ç¸®ã‚«ãƒ¼ãƒãƒ«"""
        if cp is None:
            return gpu_data
        
        # å·®åˆ†è¨ˆç®—ï¼ˆGPUä¸¦åˆ—ï¼‰
        diff = cp.diff(gpu_data.astype(cp.int16))
        
        # é–¾å€¤å‡¦ç†
        compressed = cp.where(cp.abs(diff) < 5, 0, diff)
        
        return compressed.astype(cp.int8)
    
    def _compress_with_patterns(self, data_array: np.ndarray, patterns: np.ndarray) -> np.ndarray:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹åœ§ç¸®"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦åˆ†æ
        unique_patterns, counts = np.unique(patterns, return_counts=True)
        
        # é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çŸ­ã„ç¬¦å·ã«ç½®æ›
        result = np.copy(data_array)
        
        # ä¸Šä½10ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åœ§ç¸®
        top_patterns = unique_patterns[np.argsort(counts)[-10:]]
        
        for i, pattern in enumerate(top_patterns):
            mask = patterns == pattern
            if np.sum(mask) > 5:  # ååˆ†ãªé »åº¦
                # ç°¡æ˜“åœ§ç¸®: ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¸€æ„å€¤ã«ç½®æ›
                result[mask] = 200 + i  # ç‰¹åˆ¥ãªå€¤åŸŸã‚’ä½¿ç”¨
        
        return result
    
    def _cpu_fallback_compression(self, data: bytes) -> bytes:
        """CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ§ç¸®"""
        import lzma
        return lzma.compress(data, preset=1)


class AdaptiveLoadBalancer:
    """é©å¿œçš„è² è·åˆ†æ•£å™¨"""
    
    def __init__(self, system_resources: SystemResources):
        self.resources = system_resources
        self.worker_performance = {}  # ãƒ¯ãƒ¼ã‚«ãƒ¼åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´
        self.current_loads = {}       # ç¾åœ¨ã®è² è·
        self.balancing_history = []   # åˆ†æ•£å±¥æ­´
        self.lock = threading.Lock()
        
        print(f"âš–ï¸ é©å¿œçš„è² è·åˆ†æ•£å™¨åˆæœŸåŒ–")
    
    def calculate_optimal_chunks(self, data_size: int, quality: str) -> Dict[str, Any]:
        """æœ€é©ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²è¨ˆç®—"""
        base_chunk_size = 4 * 1024 * 1024  # 4MBåŸºæº–
        
        # å“è³ªåˆ¥èª¿æ•´
        quality_multipliers = {
            'fast': 0.5,
            'balanced': 1.0,
            'max': 2.0
        }
        
        chunk_size = int(base_chunk_size * quality_multipliers.get(quality, 1.0))
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹åˆ¥èª¿æ•´
        if self.resources.memory_gb < 4:
            chunk_size = min(chunk_size, 2 * 1024 * 1024)  # 2MBåˆ¶é™
        elif self.resources.memory_gb > 16:
            chunk_size = max(chunk_size, 8 * 1024 * 1024)  # 8MBæœ€å°
        
        chunk_count = max(1, (data_size + chunk_size - 1) // chunk_size)
        
        # ä¸¦åˆ—åº¦æ±ºå®š
        optimal_threads = min(
            self.resources.cpu_count,
            chunk_count,
            8 if quality == 'fast' else 16
        )
        
        optimal_processes = min(
            self.resources.cpu_count // 2,
            chunk_count // 2,
            4
        )
        
        return {
            'chunk_size': chunk_size,
            'chunk_count': chunk_count,
            'optimal_threads': optimal_threads,
            'optimal_processes': optimal_processes,
            'memory_per_worker': max(512, self.resources.memory_gb * 1024 // optimal_threads),
            'use_gpu': self.resources.gpu_available and data_size > 10 * 1024 * 1024
        }
    
    def distribute_chunks(self, chunks: List[ProcessingChunk], workers: int) -> List[List[ProcessingChunk]]:
        """ãƒãƒ£ãƒ³ã‚¯åˆ†æ•£"""
        if not chunks:
            return []
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´
        actual_workers = min(workers, len(chunks))
        
        # è¤‡é›‘åº¦ãƒ™ãƒ¼ã‚¹åˆ†æ•£
        chunks_sorted = sorted(chunks, key=lambda c: c.complexity_score, reverse=True)
        
        # Round-robin with complexity balancing
        worker_assignments = [[] for _ in range(actual_workers)]
        worker_loads = [0.0] * actual_workers
        
        for chunk in chunks_sorted:
            # æœ€ã‚‚è² è·ã®å°‘ãªã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é¸æŠ
            min_load_worker = min(range(actual_workers), key=lambda i: worker_loads[i])
            
            worker_assignments[min_load_worker].append(chunk)
            worker_loads[min_load_worker] += chunk.complexity_score
        
        return worker_assignments
    
    def update_worker_performance(self, worker_id: str, processing_time: float, data_size: int):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ›´æ–°"""
        with self.lock:
            if worker_id not in self.worker_performance:
                self.worker_performance[worker_id] = {
                    'total_time': 0.0,
                    'total_data': 0,
                    'task_count': 0,
                    'avg_throughput': 0.0
                }
            
            perf = self.worker_performance[worker_id]
            perf['total_time'] += processing_time
            perf['total_data'] += data_size
            perf['task_count'] += 1
            
            # ç§»å‹•å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
            current_throughput = data_size / max(processing_time, 0.001)
            alpha = 0.3
            perf['avg_throughput'] = (
                alpha * current_throughput + 
                (1 - alpha) * perf['avg_throughput']
            )
    
    def get_load_balancing_report(self) -> Dict[str, Any]:
        """è² è·åˆ†æ•£ãƒ¬ãƒãƒ¼ãƒˆ"""
        with self.lock:
            total_throughput = sum(
                perf['avg_throughput'] 
                for perf in self.worker_performance.values()
            )
            
            worker_efficiency = {}
            for worker_id, perf in self.worker_performance.items():
                if total_throughput > 0:
                    efficiency = perf['avg_throughput'] / total_throughput * 100
                else:
                    efficiency = 0.0
                
                worker_efficiency[worker_id] = {
                    'efficiency_percent': efficiency,
                    'total_tasks': perf['task_count'],
                    'avg_throughput_mb_s': perf['avg_throughput'] / (1024 * 1024)
                }
            
            return {
                'total_workers': len(self.worker_performance),
                'total_throughput_mb_s': total_throughput / (1024 * 1024),
                'worker_efficiency': worker_efficiency,
                'balancing_history_count': len(self.balancing_history)
            }


class NEXUSParallelEngine:
    """
    NEXUSä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ - æ¬¡ä¸–ä»£ä¸¦åˆ—åœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ 
    
    æ©Ÿèƒ½:
    1. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆè² è·åˆ†æ•£
    2. é©å¿œçš„ä¸¦åˆ—æˆ¦ç•¥
    3. GPU/CPU ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†
    4. é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†
    5. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–
    6. åˆ†æ•£å‡¦ç†ã‚µãƒãƒ¼ãƒˆ
    """
    
    def __init__(self, config: Optional[ParallelConfig] = None):
        """åˆæœŸåŒ–"""
        self.config = config or ParallelConfig()
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹åˆ†æ
        self.system_resources = self._analyze_system_resources()
        
        # é«˜åº¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.memory_manager = AdvancedMemoryManager(
            int(self.config.memory_limit_gb * 1024)
        )
        self.gpu_accelerator = IntelligentGPUAccelerator()
        self.load_balancer = AdaptiveLoadBalancer(self.system_resources)
        
        # ä¸¦åˆ—å‡¦ç†ãƒ—ãƒ¼ãƒ«
        self.thread_pool = ThreadPoolExecutor(
            max_workers=self.config.max_threads,
            thread_name_prefix="NEXUS-Thread"
        )
        self.process_pool = ProcessPoolExecutor(
            max_workers=self.config.max_processes
        )
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        self.performance_profile = PerformanceProfile()
        
        # ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if NEXUS_ENGINES_AVAILABLE:
            try:
                self.base_engine = NEXUSTheoryCore()
                self.optimizer = NEXUSAdvancedOptimizer()
                print(f"âœ… NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆå®Œäº†")
            except Exception as e:
                print(f"âš ï¸ NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆå¤±æ•—: {e}")
                self.base_engine = None
                self.optimizer = None
        else:
            self.base_engine = None
            self.optimizer = None
        
        # çµ±è¨ˆ
        self.processing_stats = {
            'total_chunks_processed': 0,
            'total_data_processed': 0,
            'average_chunk_time': 0.0,
            'peak_throughput': 0.0,
            'energy_efficiency': 0.0
        }
        
        print(f"ğŸš€ NEXUSä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        print(f"   ğŸ’» CPU: {self.system_resources.cpu_count} ã‚³ã‚¢")
        print(f"   ğŸ§  ãƒ¡ãƒ¢ãƒª: {self.system_resources.memory_gb:.1f}GB")
        print(f"   ğŸš€ GPU: {'æœ‰åŠ¹' if self.gpu_accelerator.available else 'ç„¡åŠ¹'}")
        print(f"   ğŸ§µ æœ€å¤§ã‚¹ãƒ¬ãƒƒãƒ‰: {self.config.max_threads}")
        print(f"   âš¡ æœ€å¤§ãƒ—ãƒ­ã‚»ã‚¹: {self.config.max_processes}")
    
    def _analyze_system_resources(self) -> SystemResources:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹åˆ†æ"""
        # CPUæƒ…å ±
        cpu_count = multiprocessing.cpu_count()
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        memory_info = psutil.virtual_memory()
        memory_gb = memory_info.total / (1024**3)
        
        # GPUæƒ…å ±
        gpu_available = GPU_AVAILABLE
        gpu_memory_gb = 0.0
        
        if gpu_available and cp is not None:
            try:
                free_mem, total_mem = cp.cuda.runtime.memGetInfo()
                gpu_memory_gb = total_mem / (1024**3)
            except:
                gpu_available = False
        
        # è² è·å¹³å‡
        try:
            load_average = psutil.getloadavg()[0]
        except:
            load_average = psutil.cpu_percent() / 100.0
        
        # ãƒ‡ã‚£ã‚¹ã‚¯I/Oé€Ÿåº¦ï¼ˆç°¡æ˜“æ¸¬å®šï¼‰
        disk_io_speed = self._measure_disk_speed()
        
        return SystemResources(
            cpu_count=cpu_count,
            memory_gb=memory_gb,
            gpu_available=gpu_available,
            gpu_memory_gb=gpu_memory_gb,
            load_average=load_average,
            disk_io_speed=disk_io_speed
        )
    
    def _measure_disk_speed(self) -> float:
        """ãƒ‡ã‚£ã‚¹ã‚¯I/Oé€Ÿåº¦æ¸¬å®š"""
        try:
            test_data = b"0" * (1024 * 1024)  # 1MB
            
            import tempfile
            with tempfile.NamedTemporaryFile() as tmp:
                start_time = time.perf_counter()
                tmp.write(test_data)
                tmp.flush()
                os.fsync(tmp.fileno())
                write_time = time.perf_counter() - start_time
                
                tmp.seek(0)
                start_time = time.perf_counter()
                _ = tmp.read()
                read_time = time.perf_counter() - start_time
            
            # æ›¸ãè¾¼ã¿é€Ÿåº¦ã‚’ãƒ™ãƒ¼ã‚¹ã«è¨ˆç®—
            speed = len(test_data) / (1024 * 1024) / max(write_time, 0.001)
            return speed
            
        except:
            return 100.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def parallel_compress(self, data: bytes, quality: str = 'balanced') -> bytes:
        """
        ãƒ¡ã‚¤ãƒ³ä¸¦åˆ—åœ§ç¸®é–¢æ•°
        
        Args:
            data: åœ§ç¸®å¯¾è±¡ãƒ‡ãƒ¼ã‚¿
            quality: åœ§ç¸®å“è³ª ('fast', 'balanced', 'max')
            
        Returns:
            åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        """
        print(f"ğŸ”„ NEXUSä¸¦åˆ—åœ§ç¸®é–‹å§‹")
        print(f"   ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(data) / 1024 / 1024:.2f}MB")
        print(f"   ğŸ¯ å“è³ªãƒ¢ãƒ¼ãƒ‰: {quality}")
        
        compression_start = time.perf_counter()
        
        try:
            # 1. å‹•çš„è¨­å®šæœ€é©åŒ–
            self._optimize_config_for_quality(quality)
            
            # 2. æœ€é©ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²è¨ˆç®—
            chunk_config = self.load_balancer.calculate_optimal_chunks(
                len(data), quality
            )
            print(f"   ğŸ”· æœ€é©ãƒãƒ£ãƒ³ã‚¯: {chunk_config['chunk_count']} å€‹")
            
            # 3. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            chunks = self._intelligent_data_chunking(data, chunk_config)
            
            # 4. ä¸¦åˆ—å‡¦ç†æˆ¦ç•¥æ±ºå®š
            strategy = self._determine_processing_strategy(chunks, quality)
            print(f"   ğŸ“‹ å‡¦ç†æˆ¦ç•¥: {strategy}")
            
            # 5. ä¸¦åˆ—åœ§ç¸®å®Ÿè¡Œ
            if strategy == 'gpu_hybrid':
                results = self._gpu_hybrid_compression(chunks, quality)
            elif strategy == 'multiprocess_advanced':
                results = self._advanced_multiprocess_compression(chunks, quality)
            elif strategy == 'multithread_optimized':
                results = self._optimized_multithread_compression(chunks, quality)
            elif strategy == 'distributed':
                results = self._distributed_compression(chunks, quality)
            else:
                results = self._sequential_compression(chunks, quality)
            
            # 6. é«˜åº¦çµæœçµ±åˆ
            compressed_data = self._advanced_merge_results(results, data, quality)
            
            # 7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
            total_time = time.perf_counter() - compression_start
            self._update_performance_profile(data, compressed_data, total_time, strategy)
            
            compression_ratio = (1 - len(compressed_data) / len(data)) * 100
            throughput = len(data) / 1024 / 1024 / total_time
            
            print(f"âœ… ä¸¦åˆ—åœ§ç¸®å®Œäº†!")
            print(f"   ğŸ“ˆ åœ§ç¸®ç‡: {compression_ratio:.2f}%")
            print(f"   âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f}MB/s")
            print(f"   â±ï¸ å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
            
            return compressed_data
            
        except Exception as e:
            print(f"âŒ ä¸¦åˆ—åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._fallback_compression(data)
    
    def _optimize_config_for_quality(self, quality: str):
        """å“è³ªåˆ¥è¨­å®šæœ€é©åŒ–"""
        quality_configs = {
            'fast': self.config.fast_settings,
            'balanced': self.config.balanced_settings,
            'max': self.config.max_settings
        }
        
        if quality in quality_configs:
            optimization = quality_configs[quality]
            
            # å‹•çš„è¨­å®šé©ç”¨
            for key, value in optimization.items():
                if hasattr(self.config, key):
                    setattr(self.config, key, value)
    
    def _intelligent_data_chunking(self, data: bytes, config: Dict[str, Any]) -> List[ProcessingChunk]:
        """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²"""
        chunks = []
        chunk_size = config['chunk_size']
        
        # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æ
        entropy_profile = self._analyze_data_characteristics(data)
        
        current_pos = 0
        chunk_id = 0
        
        while current_pos < len(data):
            # é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæ±ºå®š
            local_entropy = entropy_profile.get('local_entropy', [5.0])[
                min(chunk_id, len(entropy_profile.get('local_entropy', [])) - 1)
            ]
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹èª¿æ•´
            if local_entropy > 7.0:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                adaptive_size = int(chunk_size * 0.7)
            elif local_entropy < 3.0:  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                adaptive_size = int(chunk_size * 1.3)
            else:
                adaptive_size = chunk_size
            
            end_pos = min(current_pos + adaptive_size, len(data))
            
            # ã‚¹ãƒãƒ¼ãƒˆå¢ƒç•Œèª¿æ•´
            end_pos = self._smart_boundary_adjustment(data, current_pos, end_pos)
            
            chunk_data = data[current_pos:end_pos]
            
            # ãƒãƒ£ãƒ³ã‚¯è¤‡é›‘åº¦è¨ˆç®—
            complexity = self._calculate_chunk_complexity(chunk_data, local_entropy)
            
            chunk = ProcessingChunk(
                chunk_id=chunk_id,
                data=chunk_data,
                start_offset=current_pos,
                end_offset=end_pos,
                complexity_score=complexity,
                entropy=local_entropy,
                processing_method="auto",
                metadata={
                    'size_kb': len(chunk_data) // 1024,
                    'entropy': local_entropy,
                    'complexity': complexity
                }
            )
            
            chunks.append(chunk)
            current_pos = end_pos
            chunk_id += 1
        
        return chunks
    
    def _analyze_data_characteristics(self, data: bytes) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æ"""
        sample_size = min(len(data), 64 * 1024)  # 64KB ã‚µãƒ³ãƒ—ãƒ«
        sample = data[:sample_size]
        
        # åŸºæœ¬çµ±è¨ˆ
        byte_array = np.frombuffer(sample, dtype=np.uint8)
        
        characteristics = {
            'size': len(data),
            'entropy': self._calculate_entropy(sample),
            'compression_estimate': self._estimate_compression_ratio(sample),
            'pattern_density': self._analyze_pattern_density(sample),
            'local_entropy': self._calculate_local_entropy_profile(data)
        }
        
        return characteristics
    
    def _calculate_entropy(self, data: bytes) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if not data:
            return 0.0
        
        byte_counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)
        probabilities = byte_counts / len(data)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆ0ã«ã‚ˆã‚‹é™¤ç®—å›é¿ï¼‰
        entropy = 0.0
        for p in probabilities:
            if p > 0:
                entropy -= p * np.log2(p)
        
        return entropy
    
    def _estimate_compression_ratio(self, sample: bytes) -> float:
        """åœ§ç¸®ç‡æ¨å®š"""
        try:
            import lzma
            compressed = lzma.compress(sample, preset=1)
            return len(compressed) / len(sample)
        except:
            return 0.8  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨å®šå€¤
    
    def _analyze_pattern_density(self, data: bytes) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³å¯†åº¦åˆ†æ"""
        if len(data) < 4:
            return 0.0
        
        # 4ãƒã‚¤ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡è¤‡ç‡
        patterns = {}
        for i in range(len(data) - 3):
            pattern = data[i:i+4]
            patterns[pattern] = patterns.get(pattern, 0) + 1
        
        total_patterns = len(data) - 3
        unique_patterns = len(patterns)
        
        if total_patterns == 0:
            return 0.0
        
        return 1 - (unique_patterns / total_patterns)
    
    def _calculate_local_entropy_profile(self, data: bytes) -> List[float]:
        """å±€æ‰€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
        window_size = 4096  # 4KB ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        profile = []
        
        for i in range(0, len(data), window_size):
            window = data[i:i + window_size]
            if window:
                entropy = self._calculate_entropy(window)
                profile.append(entropy)
        
        return profile
    
    def _calculate_chunk_complexity(self, chunk_data: bytes, entropy: float) -> float:
        """ãƒãƒ£ãƒ³ã‚¯è¤‡é›‘åº¦è¨ˆç®—"""
        # ã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        size_factor = min(1.0, len(chunk_data) / (4 * 1024 * 1024))
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        entropy_factor = entropy / 8.0
        
        # çµ„ã¿åˆã‚ã›è¤‡é›‘åº¦
        complexity = (size_factor * 0.3 + entropy_factor * 0.7) * 100
        
        return complexity
    
    def _smart_boundary_adjustment(self, data: bytes, start: int, end: int) -> int:
        """ã‚¹ãƒãƒ¼ãƒˆå¢ƒç•Œèª¿æ•´"""
        if end >= len(data):
            return len(data)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å¢ƒç•Œæ¤œå‡ºç¯„å›²
        search_range = min(256, len(data) - end)
        
        # åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³çµ‚äº†æ¤œå‡º
        for i in range(search_range):
            pos = end + i
            if pos < len(data) - 4:
                # 4ãƒã‚¤ãƒˆå¢ƒç•Œã§ã®åˆ‡æ–­æœ€é©åŒ–
                if (data[pos:pos+2] == data[start:start+2] and 
                    data[pos+2:pos+4] != data[start+2:start+4]):
                    return pos + 2
        
        return end
    
    def _determine_processing_strategy(self, chunks: List[ProcessingChunk], quality: str) -> str:
        """å‡¦ç†æˆ¦ç•¥æ±ºå®š"""
        total_size = sum(len(chunk.data) for chunk in chunks)
        chunk_count = len(chunks)
        avg_complexity = sum(chunk.complexity_score for chunk in chunks) / max(chunk_count, 1)
        
        # GPU ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥
        if (self.gpu_accelerator.available and 
            total_size > 20 * 1024 * 1024 and  # 20MBä»¥ä¸Š
            avg_complexity > 50 and
            quality in ['balanced', 'max']):
            return 'gpu_hybrid'
        
        # é«˜åº¦ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹æˆ¦ç•¥
        elif (chunk_count >= 4 and 
              total_size > 50 * 1024 * 1024 and  # 50MBä»¥ä¸Š
              self.system_resources.memory_gb > 8 and
              quality == 'max'):
            return 'multiprocess_advanced'
        
        # æœ€é©åŒ–ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰æˆ¦ç•¥
        elif (chunk_count >= 2 and 
              total_size > 5 * 1024 * 1024):  # 5MBä»¥ä¸Š
            return 'multithread_optimized'
        
        # åˆ†æ•£å‡¦ç†æˆ¦ç•¥
        elif (self.config.use_distributed and
              total_size > 100 * 1024 * 1024 and  # 100MBä»¥ä¸Š
              chunk_count >= 8):
            return 'distributed'
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«æˆ¦ç•¥
        else:
            return 'sequential'
    
    def _fallback_compression(self, data: bytes) -> bytes:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ§ç¸®"""
        try:
            import lzma
            return lzma.compress(data, preset=1)
        except:
            return data  # æœ€å¾Œã®æ‰‹æ®µï¼šéåœ§ç¸®
    
    def _update_performance_profile(self, original_data: bytes, compressed_data: bytes, 
                                  processing_time: float, strategy: str):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°"""
        data_size_mb = len(original_data) / 1024 / 1024
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.performance_profile.throughput_mb_s = data_size_mb / processing_time
        self.performance_profile.compression_ratio = len(compressed_data) / len(original_data)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.performance_profile.cpu_utilization = psutil.cpu_percent()
        memory_info = psutil.virtual_memory()
        self.performance_profile.memory_efficiency = (1 - memory_info.percent / 100) * 100
        
        # çµ±è¨ˆæ›´æ–°
        self.processing_stats['total_chunks_processed'] += 1
        self.processing_stats['total_data_processed'] += len(original_data)
        
        # ç§»å‹•å¹³å‡æ›´æ–°
        alpha = 0.2
        self.processing_stats['average_chunk_time'] = (
            alpha * processing_time + 
            (1 - alpha) * self.processing_stats.get('average_chunk_time', processing_time)
        )
        
        # ãƒ”ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ›´æ–°
        self.processing_stats['peak_throughput'] = max(
            self.processing_stats.get('peak_throughput', 0),
            self.performance_profile.throughput_mb_s
        )
    
    # ç°¡æ˜“å®Ÿè£…ç‰ˆã®ä¸¦åˆ—å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰
    def _gpu_hybrid_compression(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        print(f"ğŸš€ GPU ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åœ§ç¸®å®Ÿè¡Œ")
        return self._optimized_multithread_compression(chunks, quality)
    
    def _advanced_multiprocess_compression(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        print(f"âš¡ é«˜åº¦ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹åœ§ç¸®å®Ÿè¡Œ")
        return self._optimized_multithread_compression(chunks, quality)
    
    def _optimized_multithread_compression(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        print(f"ğŸ§µ æœ€é©åŒ–ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰åœ§ç¸®å®Ÿè¡Œ")
        
        results = []
        futures = []
        
        # è² è·åˆ†æ•£ãƒãƒ£ãƒ³ã‚¯é…å¸ƒ
        worker_assignments = self.load_balancer.distribute_chunks(
            chunks, self.config.max_threads
        )
        
        with self.thread_pool as executor:
            for worker_id, worker_chunks in enumerate(worker_assignments):
                if worker_chunks:  # ç©ºã§ãªã„å ´åˆã®ã¿
                    future = executor.submit(
                        self._process_worker_chunks, 
                        worker_chunks, 
                        f"worker_{worker_id}",
                        quality
                    )
                    futures.append(future)
            
            # çµæœåé›†
            for future in as_completed(futures, timeout=600):
                try:
                    worker_results = future.result()
                    results.extend(worker_results)
                except Exception as e:
                    print(f"âš ï¸ ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def _distributed_compression(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        print(f"ğŸŒ åˆ†æ•£å‡¦ç†åœ§ç¸®å®Ÿè¡Œï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
        return self._optimized_multithread_compression(chunks, quality)
    
    def _sequential_compression(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        print(f"ğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«åœ§ç¸®å®Ÿè¡Œ")
        
        results = []
        for chunk in chunks:
            result = self._compress_single_chunk(chunk, "sequential", quality)
            results.append(result)
        
        return results
    
    def _process_worker_chunks(self, chunks: List[ProcessingChunk], worker_id: str, quality: str) -> List[ProcessingResult]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
        results = []
        
        for chunk in chunks:
            start_time = time.perf_counter()
            result = self._compress_single_chunk(chunk, worker_id, quality)
            processing_time = time.perf_counter() - start_time
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
            self.load_balancer.update_worker_performance(
                worker_id, processing_time, len(chunk.data)
            )
            
            results.append(result)
        
        return results
    
    def _compress_single_chunk(self, chunk: ProcessingChunk, method: str, quality: str) -> ProcessingResult:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®"""
        start_time = time.perf_counter()
        
        try:
            # ãƒ¡ãƒ¢ãƒªç¢ºä¿
            memory_name = f"chunk_{chunk.chunk_id}_{method}"
            memory_needed = len(chunk.data) * 2  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³
            
            if not self.memory_manager.allocate(memory_name, memory_needed):
                raise MemoryError(f"ãƒ¡ãƒ¢ãƒªä¸è¶³: {memory_needed // 1024 // 1024}MB å¿…è¦")
            
            # åœ§ç¸®å®Ÿè¡Œ
            if self.base_engine and quality == 'max':
                # NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³ä½¿ç”¨
                compressed_data = self.base_engine.compress(chunk.data)
            elif self.gpu_accelerator.available and len(chunk.data) > 1024*1024:
                # GPUåŠ é€Ÿåœ§ç¸®
                compressed_data = self.gpu_accelerator.gpu_accelerated_compression(
                    chunk.data, "hybrid"
                )
            else:
                # æ¨™æº–åœ§ç¸®
                compressed_data = self._standard_compression(chunk.data, quality)
            
            processing_time = time.perf_counter() - start_time
            compression_ratio = len(compressed_data) / len(chunk.data)
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            self.memory_manager.deallocate(memory_name)
            
            return ProcessingResult(
                chunk_id=chunk.chunk_id,
                compressed_data=compressed_data,
                compression_ratio=compression_ratio,
                processing_time=processing_time,
                method_used=method,
                quality_score=self._calculate_quality_score(
                    compression_ratio, processing_time
                ),
                metrics={
                    'original_size': len(chunk.data),
                    'compressed_size': len(compressed_data),
                    'complexity': chunk.complexity_score,
                    'entropy': chunk.entropy
                }
            )
            
        except Exception as e:
            processing_time = time.perf_counter() - start_time
            
            return ProcessingResult(
                chunk_id=chunk.chunk_id,
                compressed_data=chunk.data,  # éåœ§ç¸®
                compression_ratio=1.0,
                processing_time=processing_time,
                method_used=method,
                error=str(e)
            )
    
    def _standard_compression(self, data: bytes, quality: str) -> bytes:
        """æ¨™æº–åœ§ç¸®"""
        try:
            import lzma
            
            # å“è³ªåˆ¥ãƒ—ãƒªã‚»ãƒƒãƒˆ
            presets = {
                'fast': 0,
                'balanced': 3,
                'max': 6
            }
            
            preset = presets.get(quality, 3)
            return lzma.compress(data, preset=preset)
            
        except Exception:
            return data
    
    def _calculate_quality_score(self, compression_ratio: float, processing_time: float) -> float:
        """å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # åœ§ç¸®ç‡ã¨å‡¦ç†æ™‚é–“ã®ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢
        compression_score = (1 - compression_ratio) * 100  # åœ§ç¸®ç‡ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
        speed_score = min(100, 10 / max(processing_time, 0.001))  # é€Ÿåº¦ï¼ˆé€Ÿã„ã»ã©è‰¯ã„ï¼‰
        
        # é‡ã¿ä»˜ã‘å¹³å‡
        quality_score = compression_score * 0.7 + speed_score * 0.3
        
        return min(100, max(0, quality_score))
    
    def _advanced_merge_results(self, results: List[ProcessingResult], 
                              original_data: bytes, quality: str) -> bytes:
        """é«˜åº¦çµæœçµ±åˆ"""
        # ãƒãƒ£ãƒ³ã‚¯IDã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda r: r.chunk_id)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        header = self._create_advanced_header(results, original_data, quality)
        
        # ãƒ‡ãƒ¼ã‚¿çµåˆ
        merged_data = header
        for result in results:
            chunk_header = self._create_enhanced_chunk_header(result)
            merged_data += chunk_header + result.compressed_data
        
        return merged_data
    
    def _create_advanced_header(self, results: List[ProcessingResult], 
                              original_data: bytes, quality: str) -> bytes:
        """é«˜åº¦ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        import struct
        
        header = bytearray(128)  # æ‹¡å¼µãƒ˜ãƒƒãƒ€ãƒ¼
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
        header[0:8] = b'NXPAR002'  # NEXUS Parallel v2
        
        # åŸºæœ¬æƒ…å ±
        header[8:16] = struct.pack('<Q', len(original_data))
        header[16:20] = struct.pack('<I', len(results))
        
        # å“è³ªè¨­å®š
        quality_code = {'fast': 1, 'balanced': 2, 'max': 3}.get(quality, 2)
        header[20:24] = struct.pack('<I', quality_code)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        total_time = sum(r.processing_time for r in results)
        avg_compression_ratio = sum(r.compression_ratio for r in results) / len(results)
        avg_quality_score = sum(r.quality_score for r in results) / len(results)
        
        header[24:32] = struct.pack('<d', total_time)
        header[32:40] = struct.pack('<d', avg_compression_ratio)
        header[40:48] = struct.pack('<d', avg_quality_score)
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        header[48:52] = struct.pack('<I', self.system_resources.cpu_count)
        header[52:56] = struct.pack('<f', self.system_resources.memory_gb)
        header[56:60] = struct.pack('<I', 1 if self.gpu_accelerator.available else 0)
        
        # åœ§ç¸®çµ±è¨ˆ
        total_original = sum(r.metrics.get('original_size', 0) for r in results)
        total_compressed = sum(r.metrics.get('compressed_size', 0) for r in results)
        
        header[60:68] = struct.pack('<Q', total_original)
        header[68:76] = struct.pack('<Q', total_compressed)
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
        import hashlib
        checksum = hashlib.blake2b(header[8:76], digest_size=16).digest()
        header[76:92] = checksum
        
        return bytes(header)
    
    def _create_enhanced_chunk_header(self, result: ProcessingResult) -> bytes:
        """æ‹¡å¼µãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        import struct
        
        header = bytearray(64)  # æ‹¡å¼µãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼
        
        # åŸºæœ¬æƒ…å ±
        header[0:4] = struct.pack('<I', result.chunk_id)
        header[4:8] = struct.pack('<I', len(result.compressed_data))
        header[8:16] = struct.pack('<d', result.compression_ratio)
        header[16:24] = struct.pack('<d', result.processing_time)
        header[24:32] = struct.pack('<d', result.quality_score)
        
        # ãƒ¡ã‚½ãƒƒãƒ‰æƒ…å ±
        method_bytes = result.method_used.encode('ascii')[:16]
        header[32:32+len(method_bytes)] = method_bytes
        
        # ã‚¨ãƒ©ãƒ¼æƒ…å ±
        if result.error:
            header[48:52] = struct.pack('<I', 1)  # ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ©ã‚°
        
        return bytes(header)
    
    def get_comprehensive_report(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        return {
            'system_resources': {
                'cpu_count': self.system_resources.cpu_count,
                'memory_gb': self.system_resources.memory_gb,
                'gpu_available': self.system_resources.gpu_available,
                'gpu_memory_gb': self.system_resources.gpu_memory_gb,
                'load_average': self.system_resources.load_average,
                'disk_io_speed': self.system_resources.disk_io_speed
            },
            'performance_profile': {
                'throughput_mb_s': self.performance_profile.throughput_mb_s,
                'compression_ratio': self.performance_profile.compression_ratio,
                'cpu_utilization': self.performance_profile.cpu_utilization,
                'gpu_utilization': self.performance_profile.gpu_utilization,
                'memory_efficiency': self.performance_profile.memory_efficiency
            },
            'processing_stats': self.processing_stats,
            'memory_usage': self.memory_manager.get_advanced_usage(),
            'load_balancing': self.load_balancer.get_load_balancing_report(),
            'gpu_performance': self.gpu_accelerator.benchmark_gpu_performance() if self.gpu_accelerator.available else {},
            'engine_integration': {
                'nexus_theory_available': NEXUS_ENGINES_AVAILABLE,
                'base_engine_active': self.base_engine is not None,
                'optimizer_active': self.optimizer is not None
            }
        }
    
    def __del__(self):
        """ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿"""
        try:
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=False)
            if hasattr(self, 'process_pool'):
                self.process_pool.shutdown(wait=False)
        except:
            pass
    
    def __init__(self, config: Optional[ParallelConfig] = None):
        self.config = config or ParallelConfig()
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹åˆ†æ
        self.system_resources = self._analyze_system_resources()
        
        # é«˜åº¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.memory_manager = AdvancedMemoryManager(
            int(self.config.memory_limit_gb * 1024)
        )
        self.gpu_accelerator = IntelligentGPUAccelerator()
        self.load_balancer = AdaptiveLoadBalancer(self.system_resources)
        
        # ä¸¦åˆ—å‡¦ç†ãƒ—ãƒ¼ãƒ«
        self.thread_pool = ThreadPoolExecutor(
            max_workers=self.config.max_threads,
            thread_name_prefix="NEXUS-Thread"
        )
        self.process_pool = ProcessPoolExecutor(
            max_workers=self.config.max_processes
        )
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        self.performance_profile = PerformanceProfile()
        
        # ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if NEXUS_ENGINES_AVAILABLE:
            try:
                self.base_engine = NEXUSTheoryCore()
                self.optimizer = NEXUSAdvancedOptimizer()
                print(f"âœ… NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆå®Œäº†")
            except Exception as e:
                print(f"âš ï¸ NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆå¤±æ•—: {e}")
                self.base_engine = None
                self.optimizer = None
        else:
            self.base_engine = None
            self.optimizer = None
        
        # çµ±è¨ˆ
        self.processing_stats = {
            'total_chunks_processed': 0,
            'total_data_processed': 0,
            'average_chunk_time': 0.0,
            'peak_throughput': 0.0,
            'energy_efficiency': 0.0
        }
        
        print(f"ğŸš€ NEXUSä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        print(f"   ğŸ’» CPU: {self.system_resources.cpu_count} ã‚³ã‚¢")
        print(f"   ğŸ§  ãƒ¡ãƒ¢ãƒª: {self.system_resources.memory_gb:.1f}GB")
        print(f"   ğŸš€ GPU: {'æœ‰åŠ¹' if self.gpu_accelerator.available else 'ç„¡åŠ¹'}")
        print(f"   ğŸ§µ æœ€å¤§ã‚¹ãƒ¬ãƒƒãƒ‰: {self.config.max_threads}")
        print(f"   âš¡ æœ€å¤§ãƒ—ãƒ­ã‚»ã‚¹: {self.config.max_processes}")
    
    def parallel_compress(self, data: bytes, quality: str = 'balanced') -> bytes:
        """ä¸¦åˆ—åœ§ç¸®"""
        print(f"ğŸ”§ NEXUSä¸¦åˆ—åœ§ç¸®é–‹å§‹ - ã‚µã‚¤ã‚º: {len(data)//1024//1024:.1f}MB")
        
        start_time = time.perf_counter()
        
        # 1. ãƒ‡ãƒ¼ã‚¿åˆ†æã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunks = self._split_data_intelligently(data)
        print(f"ğŸ“Š ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²: {len(chunks)} ãƒãƒ£ãƒ³ã‚¯")
        
        # 2. ä¸¦åˆ—å‡¦ç†æˆ¦ç•¥æ±ºå®š
        strategy = self._determine_parallel_strategy(chunks, quality)
        print(f"ğŸ¯ ä¸¦åˆ—æˆ¦ç•¥: {strategy}")
        
        # 3. ä¸¦åˆ—åœ§ç¸®å®Ÿè¡Œ
        if strategy == 'gpu_accelerated':
            results = self._gpu_parallel_compress(chunks, quality)
        elif strategy == 'multiprocess':
            results = self._multiprocess_compress(chunks, quality)
        elif strategy == 'multithread':
            results = self._multithread_compress(chunks, quality)
        else:
            results = self._sequential_compress(chunks, quality)
        
        # 4. çµæœçµ±åˆ
        final_result = self._merge_compression_results(results, data)
        
        total_time = time.perf_counter() - start_time
        compression_ratio = (1 - len(final_result) / len(data)) * 100
        
        print(f"âœ… ä¸¦åˆ—åœ§ç¸®å®Œäº†: {compression_ratio:.2f}% ({total_time:.2f}s)")
        self._update_performance_stats(len(chunks), total_time)
        
        return final_result
    
    def parallel_decompress(self, compressed_data: bytes) -> bytes:
        """ä¸¦åˆ—å±•é–‹"""
        print(f"ğŸ”“ NEXUSä¸¦åˆ—å±•é–‹é–‹å§‹")
        
        start_time = time.perf_counter()
        
        # 1. åœ§ç¸®ãƒ‡ãƒ¼ã‚¿è§£æ
        chunk_info = self._analyze_compressed_data(compressed_data)
        
        # 2. ä¸¦åˆ—å±•é–‹æˆ¦ç•¥æ±ºå®š
        strategy = self._determine_decompression_strategy(chunk_info)
        
        # 3. ä¸¦åˆ—å±•é–‹å®Ÿè¡Œ
        if strategy == 'multiprocess':
            results = self._multiprocess_decompress(chunk_info)
        elif strategy == 'multithread':
            results = self._multithread_decompress(chunk_info)
        else:
            results = self._sequential_decompress(chunk_info)
        
        # 4. çµæœçµ±åˆ
        final_result = self._merge_decompression_results(results)
        
        total_time = time.perf_counter() - start_time
        print(f"âœ… ä¸¦åˆ—å±•é–‹å®Œäº†: {len(final_result)//1024//1024:.1f}MB ({total_time:.2f}s)")
        
        return final_result
    
    def _split_data_intelligently(self, data: bytes) -> List[ProcessingChunk]:
        """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²"""
        chunks = []
        chunk_size = self.config.chunk_size_mb * 1024 * 1024
        
        # ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´åˆ†æ
        entropy_profile = self._analyze_entropy_profile(data)
        
        # é©å¿œçš„åˆ†å‰²
        current_pos = 0
        chunk_id = 0
        
        while current_pos < len(data):
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«åŸºã¥ãå‹•çš„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæ±ºå®š
            local_entropy = entropy_profile[min(chunk_id, len(entropy_profile) - 1)]
            
            if local_entropy > 7.0:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                adaptive_size = int(chunk_size * 0.8)  # å°ã•ãªãƒãƒ£ãƒ³ã‚¯
            elif local_entropy < 3.0:  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                adaptive_size = int(chunk_size * 1.5)  # å¤§ããªãƒãƒ£ãƒ³ã‚¯
            else:
                adaptive_size = chunk_size
            
            end_pos = min(current_pos + adaptive_size, len(data))
            
            # å¢ƒç•Œèª¿æ•´ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³å¢ƒç•Œã‚’è€ƒæ…®ï¼‰
            end_pos = self._adjust_chunk_boundary(data, current_pos, end_pos)
            
            chunk_data = data[current_pos:end_pos]
            
            chunk = ProcessingChunk(
                chunk_id=chunk_id,
                data=chunk_data,
                start_offset=current_pos,
                end_offset=end_pos,
                metadata={'entropy': local_entropy}
            )
            chunks.append(chunk)
            
            current_pos = end_pos
            chunk_id += 1
        
        return chunks
    
    def _analyze_entropy_profile(self, data: bytes, window_size: int = 1024) -> List[float]:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ"""
        profile = []
        
        for i in range(0, len(data), window_size):
            window = data[i:i + window_size]
            if window:
                entropy = self._calculate_local_entropy(window)
                profile.append(entropy)
        
        return profile
    
    def _calculate_local_entropy(self, window: bytes) -> float:
        """å±€æ‰€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if not window:
            return 0.0
        
        byte_counts = [0] * 256
        for byte in window:
            byte_counts[byte] += 1
        
        entropy = 0.0
        total = len(window)
        
        for count in byte_counts:
            if count > 0:
                prob = count / total
                entropy -= prob * np.log2(prob)
        
        return entropy
    
    def _adjust_chunk_boundary(self, data: bytes, start: int, end: int) -> int:
        """ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œèª¿æ•´"""
        if end >= len(data):
            return len(data)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³å¢ƒç•Œæ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        search_range = min(100, len(data) - end)
        
        for i in range(search_range):
            # åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ‚äº†ã‚’æ¤œå‡º
            if end + i < len(data) - 1:
                if data[end + i] == data[start] and data[end + i + 1] != data[start + 1]:
                    return end + i + 1
        
        return end
    
    def _determine_parallel_strategy(self, chunks: List[ProcessingChunk], quality: str) -> str:
        """ä¸¦åˆ—æˆ¦ç•¥æ±ºå®š"""
        total_size = sum(len(chunk.data) for chunk in chunks)
        chunk_count = len(chunks)
        
        # GPUæˆ¦ç•¥
        if (self.gpu_accelerator.available and 
            total_size > 50 * 1024 * 1024 and  # 50MBä»¥ä¸Š
            quality in ['balanced', 'max']):
            return 'gpu_accelerated'
        
        # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹æˆ¦ç•¥
        if (chunk_count >= 4 and 
            total_size > 10 * 1024 * 1024 and  # 10MBä»¥ä¸Š
            quality != 'fast'):
            return 'multiprocess'
        
        # ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰æˆ¦ç•¥
        if chunk_count >= 2:
            return 'multithread'
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«æˆ¦ç•¥
        return 'sequential'
    
    def _gpu_parallel_compress(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        """GPUä¸¦åˆ—åœ§ç¸®"""
        print(f"ğŸ”¥ GPUä¸¦åˆ—åœ§ç¸®å®Ÿè¡Œ")
        
        results = []
        
        # GPUç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
        gpu_tasks = self._prepare_gpu_tasks(chunks)
        
        # ãƒãƒƒãƒå‡¦ç†
        batch_size = min(4, len(chunks))
        
        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i + batch_size]
            batch_results = self._process_gpu_batch(batch_chunks, quality)
            results.extend(batch_results)
        
        return results
    
    def _prepare_gpu_tasks(self, chunks: List[ProcessingChunk]) -> List[Dict]:
        """GPU ã‚¿ã‚¹ã‚¯æº–å‚™"""
        tasks = []
        
        for chunk in chunks:
            # ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã«å¤‰æ›
            data_array = np.frombuffer(chunk.data, dtype=np.uint8)
            
            task = {
                'chunk_id': chunk.chunk_id,
                'data_array': data_array,
                'metadata': chunk.metadata
            }
            tasks.append(task)
        
        return tasks
    
    def _process_gpu_batch(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        """GPUãƒãƒƒãƒå‡¦ç†"""
        results = []
        
        for chunk in chunks:
            start_time = time.perf_counter()
            
            try:
                # GPUåŠ é€Ÿåœ§ç¸®
                data_array = np.frombuffer(chunk.data, dtype=np.uint8)
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆGPUåŠ é€Ÿï¼‰
                patterns = self._gpu_detect_patterns(data_array)
                
                # åœ§ç¸®å®Ÿè¡Œ
                compressed = self._compress_with_gpu_patterns(chunk.data, patterns)
                
                processing_time = time.perf_counter() - start_time
                compression_ratio = len(compressed) / len(chunk.data)
                
                result = ProcessingResult(
                    chunk_id=chunk.chunk_id,
                    compressed_data=compressed,
                    compression_ratio=compression_ratio,
                    processing_time=processing_time
                )
                results.append(result)
                
            except Exception as e:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                result = ProcessingResult(
                    chunk_id=chunk.chunk_id,
                    compressed_data=chunk.data,
                    compression_ratio=1.0,
                    processing_time=0.0,
                    error=str(e)
                )
                results.append(result)
        
        return results
    
    def _gpu_detect_patterns(self, data_array: np.ndarray) -> List[np.ndarray]:
        """GPU ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        if not self.gpu_accelerator.available:
            return []
        
        # ç°¡æ˜“ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        patterns = []
        
        # é•·ã•åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        for pattern_len in [2, 4, 8, 16]:
            if len(data_array) >= pattern_len * 2:
                detected = self._detect_patterns_of_length(data_array, pattern_len)
                patterns.extend(detected)
        
        return patterns
    
    def _detect_patterns_of_length(self, data_array: np.ndarray, length: int) -> List[np.ndarray]:
        """æŒ‡å®šé•·ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        patterns = []
        pattern_counts = {}
        
        for i in range(len(data_array) - length + 1):
            pattern = data_array[i:i + length]
            pattern_key = pattern.tobytes()
            
            pattern_counts[pattern_key] = pattern_counts.get(pattern_key, 0) + 1
        
        # é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸æŠ
        for pattern_bytes, count in pattern_counts.items():
            if count >= 3:  # æœ€ä½3å›å‡ºç¾
                pattern = np.frombuffer(pattern_bytes, dtype=np.uint8)
                patterns.append(pattern)
        
        return patterns
    
    def _compress_with_gpu_patterns(self, data: bytes, patterns: List[np.ndarray]) -> bytes:
        """GPUãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹åœ§ç¸®"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±ã‚’æ´»ç”¨ã—ãŸæœ€é©åŒ–åœ§ç¸®
        # å®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
        return self.base_engine.compress(data)
    
    def _multiprocess_compress(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        """ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹åœ§ç¸®"""
        print(f"ğŸ”„ ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹åœ§ç¸®å®Ÿè¡Œ")
        
        futures = []
        results = []
        
        with self.process_pool as executor:
            # ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†ã«æŠ•å…¥
            for chunk in chunks:
                future = executor.submit(
                    _compress_chunk_worker, 
                    chunk.data, 
                    chunk.chunk_id, 
                    quality
                )
                futures.append(future)
            
            # çµæœåé›†
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    results.append(result)
                except Exception as e:
                    print(f"âŒ ãƒ—ãƒ­ã‚»ã‚¹åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def _multithread_compress(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        """ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰åœ§ç¸®"""
        print(f"ğŸ§µ ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰åœ§ç¸®å®Ÿè¡Œ")
        
        futures = []
        results = []
        
        with self.thread_pool as executor:
            # ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†ã«æŠ•å…¥
            for chunk in chunks:
                future = executor.submit(
                    self._compress_chunk_thread, 
                    chunk, 
                    quality
                )
                futures.append(future)
            
            # çµæœåé›†
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=120)  # 2åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    results.append(result)
                except Exception as e:
                    print(f"âŒ ã‚¹ãƒ¬ãƒƒãƒ‰åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def _compress_chunk_thread(self, chunk: ProcessingChunk, quality: str) -> ProcessingResult:
        """ã‚¹ãƒ¬ãƒƒãƒ‰ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®"""
        start_time = time.perf_counter()
        
        try:
            # ãƒ¡ãƒ¢ãƒªç¢ºä¿
            memory_name = f"chunk_{chunk.chunk_id}"
            if not self.memory_manager.allocate(memory_name, len(chunk.data) * 2):
                raise MemoryError("Memory allocation failed")
            
            # åœ§ç¸®å®Ÿè¡Œ
            compressed = self.optimizer.optimize_compression(chunk.data, quality)
            
            processing_time = time.perf_counter() - start_time
            compression_ratio = len(compressed) / len(chunk.data)
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            self.memory_manager.deallocate(memory_name)
            
            return ProcessingResult(
                chunk_id=chunk.chunk_id,
                compressed_data=compressed,
                compression_ratio=compression_ratio,
                processing_time=processing_time
            )
            
        except Exception as e:
            return ProcessingResult(
                chunk_id=chunk.chunk_id,
                compressed_data=chunk.data,
                compression_ratio=1.0,
                processing_time=0.0,
                error=str(e)
            )
    
    def _sequential_compress(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        """ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«åœ§ç¸®"""
        print(f"ğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«åœ§ç¸®å®Ÿè¡Œ")
        
        results = []
        
        for chunk in chunks:
            result = self._compress_chunk_thread(chunk, quality)
            results.append(result)
        
        return results
    
    def _merge_compression_results(self, results: List[ProcessingResult], original_data: bytes) -> bytes:
        """åœ§ç¸®çµæœçµ±åˆ"""
        # ãƒãƒ£ãƒ³ã‚¯IDã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda r: r.chunk_id)
        
        # çµ±åˆãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        header = self._create_parallel_header(results, len(original_data))
        
        # ãƒ‡ãƒ¼ã‚¿çµåˆ
        combined_data = header
        for result in results:
            # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±
            chunk_header = self._create_chunk_header(result)
            combined_data += chunk_header + result.compressed_data
        
        return combined_data
    
    def _create_parallel_header(self, results: List[ProcessingResult], original_size: int) -> bytes:
        """ä¸¦åˆ—ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        header = bytearray(64)
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
        header[0:8] = b'NXPAR001'  # NEXUS Parallel v1
        
        # åŸºæœ¬æƒ…å ±
        header[8:16] = original_size.to_bytes(8, 'little')
        header[16:20] = len(results).to_bytes(4, 'little')
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        total_time = sum(r.processing_time for r in results)
        avg_ratio = sum(r.compression_ratio for r in results) / len(results)
        
        header[20:24] = int(total_time * 1000).to_bytes(4, 'little')  # ms
        header[24:28] = int(avg_ratio * 10000).to_bytes(4, 'little')  # 0.01%å˜ä½
        
        return bytes(header)
    
    def _create_chunk_header(self, result: ProcessingResult) -> bytes:
        """ãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        header = bytearray(16)
        
        header[0:4] = result.chunk_id.to_bytes(4, 'little')
        header[4:8] = len(result.compressed_data).to_bytes(4, 'little')
        header[8:12] = int(result.compression_ratio * 10000).to_bytes(4, 'little')
        header[12:16] = int(result.processing_time * 1000).to_bytes(4, 'little')
        
        return bytes(header)
    
    def _analyze_compressed_data(self, compressed_data: bytes) -> Dict:
        """åœ§ç¸®ãƒ‡ãƒ¼ã‚¿è§£æ"""
        if len(compressed_data) < 64:
            raise ValueError("Invalid compressed data")
        
        header = compressed_data[:64]
        
        if header[0:8] != b'NXPAR001':
            raise ValueError("Invalid parallel format")
        
        original_size = int.from_bytes(header[8:16], 'little')
        chunk_count = int.from_bytes(header[16:20], 'little')
        
        return {
            'original_size': original_size,
            'chunk_count': chunk_count,
            'data_offset': 64
        }
    
    def _determine_decompression_strategy(self, chunk_info: Dict) -> str:
        """å±•é–‹æˆ¦ç•¥æ±ºå®š"""
        chunk_count = chunk_info['chunk_count']
        
        if chunk_count >= 4:
            return 'multiprocess'
        elif chunk_count >= 2:
            return 'multithread'
        else:
            return 'sequential'
    
    def _multiprocess_decompress(self, chunk_info: Dict) -> List[bytes]:
        """ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å±•é–‹"""
        print(f"ğŸ”„ ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹å±•é–‹å®Ÿè¡Œ")
        return []  # å®Ÿè£…ç°¡ç•¥åŒ–
    
    def _multithread_decompress(self, chunk_info: Dict) -> List[bytes]:
        """ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å±•é–‹"""
        print(f"ğŸ§µ ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å±•é–‹å®Ÿè¡Œ")
        return []  # å®Ÿè£…ç°¡ç•¥åŒ–
    
    def _sequential_decompress(self, chunk_info: Dict) -> List[bytes]:
        """ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å±•é–‹"""
        print(f"ğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å±•é–‹å®Ÿè¡Œ")
        return []  # å®Ÿè£…ç°¡ç•¥åŒ–
    
    def _merge_decompression_results(self, results: List[bytes]) -> bytes:
        """å±•é–‹çµæœçµ±åˆ"""
        return b"".join(results)
    
    def _update_performance_stats(self, chunk_count: int, total_time: float):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆæ›´æ–°"""
        self.performance_stats['total_chunks_processed'] += chunk_count
        
        if chunk_count > 0:
            avg_time = total_time / chunk_count
            current_avg = self.performance_stats['average_chunk_time']
            total_chunks = self.performance_stats['total_chunks_processed']
            
            # æŒ‡æ•°å¹³æ»‘ç§»å‹•å¹³å‡
            alpha = 0.1
            self.performance_stats['average_chunk_time'] = (
                alpha * avg_time + (1 - alpha) * current_avg
            )
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        memory_usage = self.memory_manager.get_usage()
        
        return {
            'parallel_stats': self.performance_stats,
            'memory_usage': memory_usage,
            'gpu_available': self.gpu_accelerator.available,
            'gpu_devices': self.gpu_accelerator.device_count,
            'config': self.config
        }
    
    def __del__(self):
        """ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿"""
        try:
            self.thread_pool.shutdown(wait=False)
            self.process_pool.shutdown(wait=False)
        except:
            pass


def _compress_chunk_worker(data: bytes, chunk_id: int, quality: str) -> ProcessingResult:
    """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ç”¨ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®é–¢æ•°"""
    start_time = time.perf_counter()
    
    try:
        # æ¨™æº–åœ§ç¸®ã‚’ä½¿ç”¨ï¼ˆãƒ—ãƒ­ã‚»ã‚¹é–“ã§ã¯å…±æœ‰ã§ããªã„ãŸã‚ï¼‰
        import lzma
        
        # å“è³ªåˆ¥ãƒ—ãƒªã‚»ãƒƒãƒˆ
        presets = {
            'fast': 0,
            'balanced': 3,
            'max': 6
        }
        
        preset = presets.get(quality, 3)
        compressed = lzma.compress(data, preset=preset)
        
        processing_time = time.perf_counter() - start_time
        compression_ratio = len(compressed) / len(data)
        
        return ProcessingResult(
            chunk_id=chunk_id,
            compressed_data=compressed,
            compression_ratio=compression_ratio,
            processing_time=processing_time,
            method_used="process_worker",
            quality_score=(1 - compression_ratio) * 100,
            metrics={
                'original_size': len(data),
                'compressed_size': len(compressed)
            }
        )
        
    except Exception as e:
        return ProcessingResult(
            chunk_id=chunk_id,
            compressed_data=data,
            compression_ratio=1.0,
            processing_time=0.0,
            method_used="fallback",
            error=str(e)
        )


def test_nexus_parallel_engine():
    """NEXUSä¸¦åˆ—ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ"""
    print("âš¡ NEXUSæ¬¡ä¸–ä»£ä¸¦åˆ—ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    # ä¸¦åˆ—è¨­å®š
    config = ParallelConfig(
        use_gpu=True,
        use_multiprocessing=True,
        use_threading=True,
        max_threads=6,
        max_processes=3,
        chunk_size_mb=2,
        memory_limit_gb=4.0
    )
    
    # ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
    engine = NEXUSParallelEngine(config)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆã‚ˆã‚Šãƒªã‚¢ãƒ«ãªãƒ‡ãƒ¼ã‚¿ï¼‰
    test_datasets = [
        {
            'name': 'å°ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰',
            'data': (
                b"NEXUS Parallel Engine Test " * 100 +
                b"Compression Performance Evaluation " * 200 +
                b"Multi-threading and GPU Acceleration " * 150
            ),
            'quality': 'fast'
        },
        {
            'name': 'ä¸­ãƒ‡ãƒ¼ã‚¿ï¼ˆæ··åˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰',
            'data': (
                b"Pattern-123-ABC" * 2000 +
                b"\x00\x01\x02\x03\x04\x05" * 3000 +
                b"Repetitive-Data-Sequence" * 1500 +
                bytes(range(256)) * 100
            ),
            'quality': 'balanced'
        },
        {
            'name': 'å¤§ãƒ‡ãƒ¼ã‚¿ï¼ˆé«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰',
            'data': (
                np.random.randint(0, 256, 500000, dtype=np.uint8).tobytes() +
                b"Structured-Section" * 5000 +
                np.random.randint(0, 256, 300000, dtype=np.uint8).tobytes()
            ),
            'quality': 'max'
        }
    ]
    
    print(f"ğŸ”¬ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(test_datasets)} ç¨®é¡")
    
    # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆ
    total_results = []
    
    for i, dataset in enumerate(test_datasets):
        print(f"\n{'='*60}")
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i+1}: {dataset['name']}")
        print(f"   ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(dataset['data']) / 1024:.1f}KB")
        print(f"   ğŸ¯ å“è³ªãƒ¢ãƒ¼ãƒ‰: {dataset['quality']}")
        
        try:
            # ä¸¦åˆ—åœ§ç¸®å®Ÿè¡Œ
            start_time = time.perf_counter()
            compressed = engine.parallel_compress(dataset['data'], dataset['quality'])
            total_time = time.perf_counter() - start_time
            
            # çµæœåˆ†æ
            original_size = len(dataset['data'])
            compressed_size = len(compressed)
            compression_ratio = (1 - compressed_size / original_size) * 100
            throughput = original_size / 1024 / 1024 / total_time  # MB/s
            
            result = {
                'name': dataset['name'],
                'quality': dataset['quality'],
                'original_size': original_size,
                'compressed_size': compressed_size,
                'compression_ratio': compression_ratio,
                'processing_time': total_time,
                'throughput': throughput
            }
            
            total_results.append(result)
            
            print(f"   âœ… åœ§ç¸®æˆåŠŸ!")
            print(f"      ğŸ“ˆ åœ§ç¸®ç‡: {compression_ratio:.2f}%")
            print(f"      âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.2f}MB/s")
            print(f"      â±ï¸ å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
            print(f"      ğŸ’¾ åœ§ç¸®å‰: {original_size:,} bytes")
            print(f"      ğŸ’¾ åœ§ç¸®å¾Œ: {compressed_size:,} bytes")
            
        except Exception as e:
            print(f"   âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ")
    print(f"{'='*80}")
    
    if total_results:
        avg_compression_ratio = sum(r['compression_ratio'] for r in total_results) / len(total_results)
        avg_throughput = sum(r['throughput'] for r in total_results) / len(total_results)
        total_data_processed = sum(r['original_size'] for r in total_results)
        total_processing_time = sum(r['processing_time'] for r in total_results)
        
        print(f"ğŸ¯ ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
        print(f"   ğŸ“Š ãƒ†ã‚¹ãƒˆæ•°: {len(total_results)}")
        print(f"   ğŸ“ˆ å¹³å‡åœ§ç¸®ç‡: {avg_compression_ratio:.2f}%")
        print(f"   âš¡ å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {avg_throughput:.2f}MB/s")
        print(f"   ğŸ’¾ ç·å‡¦ç†ãƒ‡ãƒ¼ã‚¿: {total_data_processed / 1024 / 1024:.2f}MB")
        print(f"   â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_processing_time:.3f}ç§’")
        
        # å“è³ªåˆ¥åˆ†æ
        print(f"\nğŸ¯ å“è³ªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        for quality in ['fast', 'balanced', 'max']:
            quality_results = [r for r in total_results if r['quality'] == quality]
            if quality_results:
                q_avg_ratio = sum(r['compression_ratio'] for r in quality_results) / len(quality_results)
                q_avg_throughput = sum(r['throughput'] for r in quality_results) / len(quality_results)
                print(f"   {quality:>8}: åœ§ç¸®ç‡ {q_avg_ratio:.2f}%, ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ {q_avg_throughput:.2f}MB/s")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\nğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ:")
    report = engine.get_comprehensive_report()
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹
    sys_res = report['system_resources']
    print(f"   ğŸ’» CPU: {sys_res['cpu_count']} ã‚³ã‚¢, è² è· {sys_res['load_average']:.2f}")
    print(f"   ğŸ§  ãƒ¡ãƒ¢ãƒª: {sys_res['memory_gb']:.1f}GB")
    print(f"   ğŸš€ GPU: {'æœ‰åŠ¹' if sys_res['gpu_available'] else 'ç„¡åŠ¹'}")
    if sys_res['gpu_available']:
        print(f"       GPU ãƒ¡ãƒ¢ãƒª: {sys_res['gpu_memory_gb']:.1f}GB")
    print(f"   ğŸ’½ ãƒ‡ã‚£ã‚¹ã‚¯I/O: {sys_res['disk_io_speed']:.1f}MB/s")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    perf = report['performance_profile']
    print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"   âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {perf['throughput_mb_s']:.2f}MB/s")
    print(f"   ğŸ“ˆ åœ§ç¸®ç‡: {perf['compression_ratio']:.3f}")
    print(f"   ğŸ’» CPUåˆ©ç”¨ç‡: {perf['cpu_utilization']:.1f}%")
    print(f"   ğŸ§  ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: {perf['memory_efficiency']:.1f}%")
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    memory = report['memory_usage']
    print(f"\nğŸ§  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
    print(f"   ğŸ“Š ç¾åœ¨: {memory['current_mb']}MB / {memory['limit_mb']}MB")
    print(f"   ğŸ“ˆ ãƒ”ãƒ¼ã‚¯: {memory['peak_mb']}MB")
    print(f"   ğŸ“Š åˆ©ç”¨ç‡: {memory['utilization']*100:.1f}%")
    print(f"   ğŸ—‚ï¸ å‰²ã‚Šå½“ã¦ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: {memory['allocated_objects']}")
    
    # è² è·åˆ†æ•£
    load_bal = report['load_balancing']
    print(f"\nâš–ï¸ è² è·åˆ†æ•£:")
    print(f"   ğŸ‘¥ ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {load_bal['total_workers']}")
    print(f"   âš¡ ç·ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {load_bal['total_throughput_mb_s']:.2f}MB/s")
    
    # GPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    if report['gpu_performance']:
        gpu_perf = report['gpu_performance']
        print(f"\nğŸš€ GPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        print(f"   âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {gpu_perf.get('throughput', 0):.2f}MB/s")
        print(f"   â±ï¸ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {gpu_perf.get('latency', 0):.2f}ms")
        print(f"   ğŸ¯ åŠ¹ç‡: {gpu_perf.get('efficiency', 0):.1f}%")
    
    # ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆ
    engine_int = report['engine_integration']
    print(f"\nğŸ”§ ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆ:")
    print(f"   ğŸ§  NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³: {'åˆ©ç”¨å¯èƒ½' if engine_int['nexus_theory_available'] else 'åˆ©ç”¨ä¸å¯'}")
    print(f"   âš™ï¸ ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ³: {'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–' if engine_int['base_engine_active'] else 'éã‚¢ã‚¯ãƒ†ã‚£ãƒ–'}")
    print(f"   ğŸ¯ ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼: {'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–' if engine_int['optimizer_active'] else 'éã‚¢ã‚¯ãƒ†ã‚£ãƒ–'}")
    
    print(f"\nğŸ‰ NEXUSæ¬¡ä¸–ä»£ä¸¦åˆ—ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print(f"=" * 80)


if __name__ == "__main__":
    test_nexus_parallel_engine()

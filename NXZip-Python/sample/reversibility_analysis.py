#!/usr/bin/env python3
"""NEXUS å¯é€†æ€§ã®æ ¹æœ¬åŽŸå› åˆ†æž"""
import json
import hashlib
from nexus_advanced_engine import NexusAdvancedCompressor

def analyze_reversibility():
    """åœ§ç¸®ãƒ»è§£å‡ãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°åˆ†æž"""
    engine = NexusAdvancedCompressor()
    
    # å˜ç´”ã§æ˜Žç¢ºãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_data = b"ABCDEFGH"  # 8ãƒã‚¤ãƒˆã€å…¨ã¦ç•°ãªã‚‹å€¤
    print(f"ðŸ” Original data: {test_data}")
    print(f"ðŸ” Original bytes: {list(test_data)}")
    print(f"ðŸ” Original hash: {hashlib.md5(test_data).hexdigest()}")
    
    # åœ§ç¸®ãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°ç›£è¦–
    print("\n" + "="*60)
    print("ðŸ“Š COMPRESSION PROCESS ANALYSIS")
    print("="*60)
    
    # ä¸€æ™‚çš„ã«ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
    compressed = engine.compress(test_data, level=0)
    
    # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰è§£æž
    print("\n" + "="*60)
    print("ðŸ“¦ PAYLOAD STRUCTURE ANALYSIS")
    print("="*60)
    
    try:
        import lzma
        decompressed_payload = lzma.decompress(compressed)
        payload = json.loads(decompressed_payload.decode('utf-8'))
        
        print(f"Header: {payload['header']}")
        print(f"Unique groups count: {len(payload['unique_groups'])}")
        print(f"Unique groups: {payload['unique_groups']}")
        print(f"Perm map dict: {payload['perm_map_dict']}")
        
        # Huffmanå¾©å·
        group_huff_tree = payload['huffman_trees']['group_ids']
        group_ids = engine.huffman_encoder.decode(payload['encoded_streams']['group_ids'], group_huff_tree)
        
        if 'perm_ids' in payload['encoded_streams'] and payload['encoded_streams']['perm_ids']:
            perm_huff_tree = payload['huffman_trees']['perm_ids']
            perm_ids = engine.huffman_encoder.decode(payload['encoded_streams']['perm_ids'], perm_huff_tree)
        else:
            perm_ids = [0] * len(group_ids)
        
        print(f"Group ID stream: {group_ids}")
        print(f"Perm ID stream: {perm_ids}")
        
        print("\n" + "="*60)
        print("ðŸ”„ DECOMPRESSION STEP-BY-STEP")
        print("="*60)
        
        # æ‰‹å‹•ã§ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å¾©å…ƒ
        unique_groups = [tuple(g) for g in payload['unique_groups']]
        perm_map_dict = {int(k): tuple(v) for k, v in payload['perm_map_dict'].items()}
        
        reconstructed_blocks = []
        for i, (group_id, perm_id) in enumerate(zip(group_ids, perm_ids)):
            print(f"\nBlock {i}:")
            print(f"  Group ID: {group_id} -> {unique_groups[group_id] if group_id < len(unique_groups) else 'INVALID'}")
            print(f"  Perm ID: {perm_id} -> {perm_map_dict.get(perm_id, 'INVALID')}")
            
            if group_id < len(unique_groups) and perm_id in perm_map_dict:
                group = unique_groups[group_id]
                perm_map = perm_map_dict[perm_id]
                
                print(f"  Before inverse permutation: {group}")
                print(f"  Permutation map: {perm_map}")
                
                # é€†å¤‰æ›ã®è©³ç´°ãƒ­ã‚°
                if len(group) == len(perm_map):
                    result = [0] * len(group)
                    for original_pos, shuffled_pos in enumerate(perm_map):
                        if 0 <= shuffled_pos < len(result):
                            result[original_pos] = group[shuffled_pos]
                            print(f"    result[{original_pos}] = group[{shuffled_pos}] = {group[shuffled_pos]}")
                    
                    reconstructed_block = tuple(result)
                    print(f"  After inverse permutation: {reconstructed_block}")
                    reconstructed_blocks.append(reconstructed_block)
                else:
                    print(f"  âš ï¸ Length mismatch: group={len(group)}, perm_map={len(perm_map)}")
                    reconstructed_blocks.append(group)
            else:
                print(f"  âŒ Invalid indices")
                reconstructed_blocks.append((0,))
        
        # ãƒ•ãƒ©ãƒƒãƒˆåŒ–
        print(f"\nReconstructed blocks: {reconstructed_blocks}")
        flat_data = []
        for block in reconstructed_blocks:
            flat_data.extend(block)
        
        original_length = payload['header']['original_length']
        result_bytes = bytes(flat_data[:original_length])
        
        print(f"Flat data: {flat_data}")
        print(f"Result bytes: {list(result_bytes)}")
        print(f"Result string: {result_bytes}")
        print(f"Result hash: {hashlib.md5(result_bytes).hexdigest()}")
        print(f"Match: {test_data == result_bytes}")
        
    except Exception as e:
        print(f"âŒ Payload analysis error: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*60)
    print("ðŸ OFFICIAL DECOMPRESSION")
    print("="*60)
    
    try:
        official_result = engine.decompress(compressed)
        print(f"Official result: {official_result}")
        print(f"Official bytes: {list(official_result)}")
        print(f"Official hash: {hashlib.md5(official_result).hexdigest()}")
        print(f"Official match: {test_data == official_result}")
    except Exception as e:
        print(f"âŒ Official decompression error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    analyze_reversibility()

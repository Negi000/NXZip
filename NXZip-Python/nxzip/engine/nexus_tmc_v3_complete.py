#!/usr/bin/env python3
"""
NEXUS TMC Engine v3.0 - è¶…é«˜æ€§èƒ½Transform-Model-Codeåœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
å®Œå…¨å®Ÿè£…ç‰ˆ - å…¨æ©Ÿèƒ½çµ±åˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
"""

import os
import sys
import time
import struct
import zlib
import lzma
import bz2
from typing import Tuple, Dict, Any, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor
from enum import Enum
import numpy as np


class DataType(Enum):
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡"""
    STRUCTURED_NUMERIC = "structured_numeric"
    TEXT_LIKE = "text_like" 
    TIME_SERIES = "time_series"
    REPETITIVE_BINARY = "repetitive_binary"
    COMPRESSED_LIKE = "compressed_like"
    GENERIC_BINARY = "generic_binary"


class NEXUSTMCEngine:
    """NEXUS TMC Engine v3.0 - å®Œå…¨çµ±åˆé«˜æ€§èƒ½ç‰ˆ"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'reversibility_tests_passed': 0,
            'reversibility_tests_total': 0,
            'performance_metrics': []
        }
    
    def compress_tmc(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC v3.0 çµ±åˆåœ§ç¸®å‡¦ç†"""
        compression_start = time.perf_counter()
        
        try:
            # ç©ºãƒ‡ãƒ¼ã‚¿å‡¦ç†
            if len(data) == 0:
                return self._create_empty_tmc(), self._create_empty_info(compression_start)
            
            # é«˜é€Ÿãƒ‡ãƒ¼ã‚¿åˆ†æ
            analysis = self._ultra_fast_analysis(data)
            
            # é©å¿œçš„å‰å‡¦ç†
            preprocessed = self._adaptive_preprocessing(data, analysis)
            
            # ãƒãƒ«ãƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä¸¦åˆ—åœ§ç¸®
            compression_results = self._parallel_compression_suite(preprocessed, analysis)
            
            # æœ€é©çµæœé¸æŠ
            best_result = self._select_optimal_result(compression_results, data)
            
            # TMCãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰
            tmc_data = self._build_tmc_v3_format(best_result, analysis)
            
            total_time = time.perf_counter() - compression_start
            
            return tmc_data, self._build_compression_info(data, tmc_data, analysis, best_result, total_time)
            
        except Exception as e:
            return self._fallback_compression(data, compression_start, str(e))
    
    def decompress_tmc(self, compressed_data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC v3.0 çµ±åˆå±•é–‹å‡¦ç†"""
        decompression_start = time.perf_counter()
        
        try:
            # TMC v3.0 ãƒ˜ãƒƒãƒ€ãƒ¼ãƒã‚§ãƒƒã‚¯
            if not self._is_valid_tmc_v3(compressed_data):
                return self._fallback_decompression(compressed_data, decompression_start)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            header = self._parse_tmc_v3_header(compressed_data)
            
            # ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã¨å±•é–‹
            payload = compressed_data[header['header_size']:]
            decompressed = self._decompress_payload(payload, header)
            
            # é€†å‰å‡¦ç†
            original_data = self._reverse_preprocessing(decompressed, header)
            
            total_time = time.perf_counter() - decompression_start
            
            return original_data, self._build_decompression_info(original_data, total_time, header)
            
        except Exception as e:
            return self._fallback_decompression(compressed_data, decompression_start, str(e))
    
    def _ultra_fast_analysis(self, data: bytes) -> Dict[str, Any]:
        """è¶…é«˜é€Ÿãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰"""
        try:
            analysis = {
                'size': len(data),
                'data_type': DataType.GENERIC_BINARY,
                'entropy': 8.0,
                'repetition_score': 0.0,
                'structure_score': 0.0,
                'compression_potential': 0.5,
                'optimal_strategy': 'general'
            }
            
            if len(data) == 0:
                return analysis
            
            # é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åˆ†æ
            sample_size = min(8192, len(data))
            sample = np.frombuffer(data[:sample_size], dtype=np.uint8)
            
            # ãƒã‚¤ãƒˆåˆ†å¸ƒã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            byte_counts = np.bincount(sample, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(sample)
            analysis['entropy'] = float(-np.sum(probabilities * np.log2(probabilities)))
            
            # åå¾©æ€§ã‚¹ã‚³ã‚¢
            unique_ratio = len(np.unique(sample)) / 256
            analysis['repetition_score'] = 1.0 - unique_ratio
            
            # æ§‹é€ æ€§ã‚¹ã‚³ã‚¢ï¼ˆå·®åˆ†ã®å®‰å®šæ€§ï¼‰
            if len(sample) > 1:
                diffs = np.abs(np.diff(sample.astype(np.int16)))
                diff_variance = np.var(diffs)
                analysis['structure_score'] = 1.0 / (1.0 + diff_variance / 100.0)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—æ¨å®š
            analysis['data_type'] = self._classify_data_type_fast(analysis, sample)
            
            # åœ§ç¸®æˆ¦ç•¥æ±ºå®š
            analysis['optimal_strategy'] = self._determine_optimal_strategy(analysis)
            
            # åœ§ç¸®å¯èƒ½æ€§äºˆæ¸¬
            analysis['compression_potential'] = self._predict_compression_potential(analysis)
            
            return analysis
            
        except Exception:
            return {
                'size': len(data),
                'data_type': DataType.GENERIC_BINARY,
                'entropy': 8.0,
                'optimal_strategy': 'general'
            }
    
    def _classify_data_type_fast(self, analysis: Dict[str, Any], sample: np.ndarray) -> DataType:
        """é«˜é€Ÿãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        try:
            entropy = analysis['entropy']
            repetition = analysis['repetition_score']
            structure = analysis['structure_score']
            
            # ASCIIæ–‡å­—ã®å‰²åˆ
            ascii_ratio = np.sum((sample >= 32) & (sample <= 126)) / len(sample)
            
            # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
            if entropy < 2.0 or repetition > 0.8:
                return DataType.REPETITIVE_BINARY
            elif ascii_ratio > 0.7 and entropy < 6.0:
                return DataType.TEXT_LIKE
            elif structure > 0.7:
                return DataType.TIME_SERIES
            elif entropy > 7.5:
                return DataType.COMPRESSED_LIKE
            elif repetition < 0.3 and structure > 0.5:
                return DataType.STRUCTURED_NUMERIC
            else:
                return DataType.GENERIC_BINARY
                
        except Exception:
            return DataType.GENERIC_BINARY
    
    def _determine_optimal_strategy(self, analysis: Dict[str, Any]) -> str:
        """æœ€é©åœ§ç¸®æˆ¦ç•¥æ±ºå®š"""
        data_type = analysis['data_type']
        entropy = analysis['entropy']
        repetition = analysis['repetition_score']
        
        if data_type == DataType.REPETITIVE_BINARY or repetition > 0.7:
            return 'rle_heavy'
        elif data_type == DataType.TEXT_LIKE:
            return 'text_optimized'
        elif data_type == DataType.TIME_SERIES:
            return 'delta_compression'
        elif data_type == DataType.STRUCTURED_NUMERIC:
            return 'structure_aware'
        elif entropy > 7.0:
            return 'lightweight'
        else:
            return 'balanced'
    
    def _predict_compression_potential(self, analysis: Dict[str, Any]) -> float:
        """åœ§ç¸®å¯èƒ½æ€§äºˆæ¸¬"""
        try:
            entropy = analysis['entropy']
            repetition = analysis['repetition_score']
            structure = analysis['structure_score']
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹äºˆæ¸¬
            entropy_factor = max(0.0, (8.0 - entropy) / 8.0)
            
            # åå¾©æ€§ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
            repetition_factor = repetition
            
            # æ§‹é€ æ€§ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
            structure_factor = structure * 0.5
            
            # ç·åˆã‚¹ã‚³ã‚¢
            potential = (entropy_factor * 0.5 + repetition_factor * 0.3 + structure_factor * 0.2)
            
            return min(1.0, max(0.0, potential))
            
        except Exception:
            return 0.5
    
    def _adaptive_preprocessing(self, data: bytes, analysis: Dict[str, Any]) -> bytes:
        """é©å¿œçš„å‰å‡¦ç†"""
        try:
            strategy = analysis['optimal_strategy']
            
            if strategy == 'text_optimized':
                return self._text_preprocessing(data)
            elif strategy == 'delta_compression':
                return self._delta_preprocessing(data)
            elif strategy == 'structure_aware':
                return self._structure_preprocessing(data)
            elif strategy == 'rle_heavy':
                return self._rle_preprocessing(data)
            else:
                return data
                
        except Exception:
            return data
    
    def _text_preprocessing(self, data: bytes) -> bytes:
        """ãƒ†ã‚­ã‚¹ãƒˆç‰¹åŒ–å‰å‡¦ç†"""
        try:
            text = data.decode('utf-8', errors='ignore')
            
            # é«˜é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¾æ›¸ç½®æ›
            replacements = [
                ('  ', '\x01'),     # é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹
                ('\t', '\x02'),     # ã‚¿ãƒ–
                ('\r\n', '\x03'),   # Windowsæ”¹è¡Œ
                ('\n', '\x04'),     # Unixæ”¹è¡Œ
                ('the ', '\x05'),
                ('and ', '\x06'),
                ('that ', '\x07'),
                ('with ', '\x08'),
                ('for ', '\x09'),
                ('ing ', '\x0A'),
            ]
            
            processed = text
            used_replacements = []
            
            for original, replacement in replacements:
                if original in processed and len(original) >= 2:
                    count = processed.count(original)
                    if count >= 3:  # 3å›ä»¥ä¸Šå‡ºç¾ã§åŠ¹æœçš„
                        processed = processed.replace(original, replacement)
                        used_replacements.append((original, replacement))
            
            # è¾æ›¸ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
            if used_replacements:
                header = f"TMC_DICT:{len(used_replacements)}:"
                for orig, repl in used_replacements:
                    header += f"{orig.encode('unicode_escape').decode()}:{repl.encode('unicode_escape').decode()}:"
                header += "DATA:"
                result = header + processed
                return result.encode('utf-8')
            
            return data
            
        except Exception:
            return data
    
    def _delta_preprocessing(self, data: bytes) -> bytes:
        """å·®åˆ†å‰å‡¦ç†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            if len(data) < 4:
                return data
            
            # æœ€é©ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰æ¤œå‡º
            best_stride = 1
            min_variance = float('inf')
            
            for stride in [1, 2, 4]:
                if len(data) >= stride * 8:
                    values = []
                    for i in range(0, len(data) - stride + 1, stride):
                        if stride == 1:
                            values.append(data[i])
                        elif stride == 2:
                            values.append(struct.unpack('<H', data[i:i+2])[0])
                        elif stride == 4:
                            values.append(struct.unpack('<I', data[i:i+4])[0])
                    
                    if len(values) > 1:
                        diffs = [values[i+1] - values[i] for i in range(len(values)-1)]
                        variance = np.var(diffs) if diffs else float('inf')
                        
                        if variance < min_variance:
                            min_variance = variance
                            best_stride = stride
            
            # å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ
            if min_variance < 1000:  # åŠ¹æœçš„ãªå ´åˆã®ã¿
                return self._encode_delta_optimized(data, best_stride)
            
            return data
            
        except Exception:
            return data
    
    def _encode_delta_optimized(self, data: bytes, stride: int) -> bytes:
        """æœ€é©åŒ–å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            result = bytearray()
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            result.extend(b'DELTA')
            result.extend(struct.pack('<II', len(data), stride))
            
            if stride == 1:
                if len(data) > 0:
                    result.append(data[0])
                    for i in range(1, len(data)):
                        diff = (data[i] - data[i-1] + 256) % 256
                        result.append(diff)
                        
            elif stride == 2:
                for i in range(0, len(data) - 1, 2):
                    if i == 0:
                        result.extend(data[i:i+2])
                    else:
                        prev_val = struct.unpack('<H', data[i-2:i])[0]
                        curr_val = struct.unpack('<H', data[i:i+2])[0]
                        diff = (curr_val - prev_val + 65536) % 65536
                        result.extend(struct.pack('<H', diff))
                        
            elif stride == 4:
                for i in range(0, len(data) - 3, 4):
                    if i == 0:
                        result.extend(data[i:i+4])
                    else:
                        prev_val = struct.unpack('<I', data[i-4:i])[0]
                        curr_val = struct.unpack('<I', data[i:i+4])[0]
                        diff = (curr_val - prev_val) & 0xFFFFFFFF
                        result.extend(struct.pack('<I', diff))
            
            return bytes(result) if len(result) < len(data) else data
            
        except Exception:
            return data
    
    def _structure_preprocessing(self, data: bytes) -> bytes:
        """æ§‹é€ çš„ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        try:
            if len(data) < 16:
                return data
            
            # æœ€é©åˆ†é›¢ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            best_separation = None
            best_score = 0.0
            
            for type_size in [2, 4, 8]:
                if len(data) % type_size == 0:
                    separated = self._separate_by_structure(data, type_size)
                    score = self._evaluate_separation_quality(separated)
                    
                    if score > best_score:
                        best_score = score
                        best_separation = separated
            
            if best_score > 0.3:  # åŠ¹æœçš„ãªå ´åˆ
                return self._encode_separated_structure(best_separation)
            
            return data
            
        except Exception:
            return data
    
    def _separate_by_structure(self, data: bytes, type_size: int) -> List[bytes]:
        """æ§‹é€ çš„åˆ†é›¢"""
        streams = []
        data_array = np.frombuffer(data, dtype=np.uint8)
        reshaped = data_array.reshape(-1, type_size)
        
        for i in range(type_size):
            stream = reshaped[:, i].tobytes()
            streams.append(stream)
        
        return streams
    
    def _evaluate_separation_quality(self, streams: List[bytes]) -> float:
        """åˆ†é›¢å“è³ªè©•ä¾¡"""
        try:
            total_score = 0.0
            total_weight = 0
            
            for stream in streams:
                if len(stream) > 0:
                    stream_array = np.frombuffer(stream, dtype=np.uint8)
                    
                    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è©•ä¾¡
                    byte_counts = np.bincount(stream_array, minlength=256)
                    probs = byte_counts[byte_counts > 0] / len(stream_array)
                    entropy = -np.sum(probs * np.log2(probs))
                    
                    # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã»ã©é«˜ã‚¹ã‚³ã‚¢
                    score = max(0.0, (8.0 - entropy) / 8.0)
                    
                    total_score += score * len(stream)
                    total_weight += len(stream)
            
            return total_score / total_weight if total_weight > 0 else 0.0
            
        except Exception:
            return 0.0
    
    def _encode_separated_structure(self, streams: List[bytes]) -> bytes:
        """åˆ†é›¢æ§‹é€ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            result = bytearray()
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            result.extend(b'STRUCT')
            result.extend(struct.pack('<I', len(streams)))
            
            # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚º
            for stream in streams:
                result.extend(struct.pack('<I', len(stream)))
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿
            for stream in streams:
                result.extend(stream)
            
            return bytes(result)
            
        except Exception:
            return b''.join(streams)
    
    def _rle_preprocessing(self, data: bytes) -> bytes:
        """RLEå‰å‡¦ç†ï¼ˆé«˜åŠ¹ç‡ç‰ˆï¼‰"""
        try:
            if len(data) == 0:
                return data
            
            result = bytearray()
            result.extend(b'RLE_V2')
            
            i = 0
            while i < len(data):
                current_byte = data[i]
                count = 1
                
                # é€£ç¶šã‚«ã‚¦ãƒ³ãƒˆ
                while i + count < len(data) and data[i + count] == current_byte and count < 255:
                    count += 1
                
                # RLEåŠ¹ç‡åˆ¤å®š
                if count >= 3 or (count >= 2 and current_byte in [0, 255]):
                    # RLEã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                    result.append(0xFF)  # RLEãƒãƒ¼ã‚«ãƒ¼
                    result.append(current_byte)
                    result.append(count)
                else:
                    # ç”Ÿãƒ‡ãƒ¼ã‚¿
                    for _ in range(count):
                        if current_byte == 0xFF:
                            result.extend([0xFF, 0xFF])  # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
                        else:
                            result.append(current_byte)
                
                i += count
            
            return bytes(result) if len(result) < len(data) else data
            
        except Exception:
            return data
    
    def _parallel_compression_suite(self, data: bytes, analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä¸¦åˆ—åœ§ç¸®ã‚¹ã‚¤ãƒ¼ãƒˆ"""
        compression_methods = []
        
        # æˆ¦ç•¥ã«å¿œã˜ãŸãƒ¡ã‚½ãƒƒãƒ‰é¸æŠ
        strategy = analysis['optimal_strategy']
        
        if strategy == 'lightweight':
            compression_methods = [
                ('zlib_fast', lambda d: zlib.compress(d, level=1)),
                ('zlib_balanced', lambda d: zlib.compress(d, level=6))
            ]
        elif strategy == 'rle_heavy':
            compression_methods = [
                ('bz2_high', lambda d: bz2.compress(d, compresslevel=9)),
                ('zlib_high', lambda d: zlib.compress(d, level=9))
            ]
        else:
            compression_methods = [
                ('zlib_balanced', lambda d: zlib.compress(d, level=6)),
                ('lzma_balanced', lambda d: lzma.compress(d, preset=6)),
                ('bz2_balanced', lambda d: bz2.compress(d, compresslevel=6))
            ]
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            for method_name, compress_func in compression_methods:
                future = executor.submit(self._safe_compress, data, method_name, compress_func)
                futures.append(future)
            
            for future in futures:
                result = future.result()
                if result:
                    results.append(result)
        
        return results
    
    def _safe_compress(self, data: bytes, method_name: str, compress_func) -> Optional[Dict[str, Any]]:
        """å®‰å…¨ãªåœ§ç¸®å®Ÿè¡Œ"""
        try:
            start_time = time.perf_counter()
            compressed = compress_func(data)
            compression_time = time.perf_counter() - start_time
            
            return {
                'method': method_name,
                'data': compressed,
                'original_size': len(data),
                'compressed_size': len(compressed),
                'compression_ratio': (1 - len(compressed) / len(data)) * 100 if len(data) > 0 else 0,
                'compression_time': compression_time,
                'throughput_mb_s': (len(data) / 1024 / 1024) / compression_time if compression_time > 0 else 0
            }
            
        except Exception:
            return None
    
    def _select_optimal_result(self, results: List[Dict[str, Any]], original_data: bytes) -> Dict[str, Any]:
        """æœ€é©çµæœé¸æŠ"""
        if not results:
            return {
                'method': 'none',
                'data': original_data,
                'compressed_size': len(original_data),
                'compression_ratio': 0.0
            }
        
        # åœ§ç¸®ç‡é‡è¦–ã®é¸æŠ
        best_result = min(results, key=lambda x: x['compressed_size'])
        
        # è†¨å¼µé˜²æ­¢ãƒã‚§ãƒƒã‚¯
        if best_result['compressed_size'] > len(original_data) * 1.05:  # 5%ä»¥ä¸Šã®è†¨å¼µã¯é¿ã‘ã‚‹
            return {
                'method': 'store',
                'data': original_data,
                'compressed_size': len(original_data),
                'compression_ratio': 0.0
            }
        
        return best_result
    
    def _build_tmc_v3_format(self, compression_result: Dict[str, Any], analysis: Dict[str, Any]) -> bytes:
        """TMC v3.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰"""
        try:
            header = bytearray()
            
            # TMC v3.0 ç½²å
            header.extend(b'TMC3')
            
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
            header.extend(struct.pack('<I', 300))  # v3.0
            
            # åœ§ç¸®ãƒ¡ã‚½ãƒƒãƒ‰
            method_bytes = compression_result['method'].encode('utf-8')[:32].ljust(32, b'\x00')
            header.extend(method_bytes)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæƒ…å ±
            header.extend(struct.pack('<II', 
                                    analysis['size'],  # å…ƒã‚µã‚¤ã‚º
                                    compression_result['compressed_size']))  # åœ§ç¸®ã‚µã‚¤ã‚º
            
            # åˆ†ææƒ…å ±ï¼ˆç°¡ç´„ç‰ˆï¼‰
            header.append(analysis['data_type'].value.encode('utf-8')[:16].ljust(16, b'\x00')[:16][0])
            header.extend(struct.pack('<f', analysis['entropy']))
            header.extend(struct.pack('<f', analysis['compression_potential']))
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
            payload = compression_result['data']
            checksum = zlib.crc32(payload) & 0xffffffff
            header.extend(struct.pack('<I', checksum))
            
            return bytes(header) + payload
            
        except Exception:
            return compression_result['data']
    
    def _is_valid_tmc_v3(self, data: bytes) -> bool:
        """TMC v3.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œè¨¼"""
        return len(data) >= 64 and data[:4] == b'TMC3'
    
    def _parse_tmc_v3_header(self, data: bytes) -> Dict[str, Any]:
        """TMC v3.0 ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ"""
        try:
            offset = 4  # TMC3 skip
            
            version = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            method = data[offset:offset+32].rstrip(b'\x00').decode('utf-8')
            offset += 32
            
            original_size, compressed_size = struct.unpack('<II', data[offset:offset+8])
            offset += 8
            
            data_type_code = data[offset]
            offset += 1
            
            entropy, compression_potential = struct.unpack('<ff', data[offset:offset+8])
            offset += 8
            
            checksum = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            return {
                'version': version,
                'method': method,
                'original_size': original_size,
                'compressed_size': compressed_size,
                'data_type_code': data_type_code,
                'entropy': entropy,
                'compression_potential': compression_potential,
                'checksum': checksum,
                'header_size': offset
            }
            
        except Exception:
            return {'header_size': 64}
    
    def _decompress_payload(self, payload: bytes, header: Dict[str, Any]) -> bytes:
        """ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰å±•é–‹"""
        try:
            method = header.get('method', 'unknown')
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¤œè¨¼
            expected_checksum = header.get('checksum', 0)
            actual_checksum = zlib.crc32(payload) & 0xffffffff
            
            if expected_checksum != actual_checksum:
                raise ValueError("Checksum mismatch")
            
            # ãƒ¡ã‚½ãƒƒãƒ‰åˆ¥å±•é–‹
            if method.startswith('zlib'):
                return zlib.decompress(payload)
            elif method.startswith('lzma'):
                return lzma.decompress(payload)
            elif method.startswith('bz2'):
                return bz2.decompress(payload)
            elif method == 'store':
                return payload
            else:
                return payload
                
        except Exception:
            return payload
    
    def _reverse_preprocessing(self, data: bytes, header: Dict[str, Any]) -> bytes:
        """é€†å‰å‡¦ç†"""
        try:
            # å‰å‡¦ç†ãƒãƒ¼ã‚«ãƒ¼ãƒã‚§ãƒƒã‚¯
            if data.startswith(b'TMC_DICT:'):
                return self._reverse_text_preprocessing(data)
            elif data.startswith(b'DELTA'):
                return self._reverse_delta_preprocessing(data)
            elif data.startswith(b'STRUCT'):
                return self._reverse_structure_preprocessing(data)
            elif data.startswith(b'RLE_V2'):
                return self._reverse_rle_preprocessing(data)
            else:
                return data
                
        except Exception:
            return data
    
    def _reverse_text_preprocessing(self, data: bytes) -> bytes:
        """ãƒ†ã‚­ã‚¹ãƒˆé€†å‰å‡¦ç†"""
        try:
            text = data.decode('utf-8', errors='ignore')
            
            if not text.startswith('TMC_DICT:'):
                return data
            
            parts = text.split('DATA:', 1)
            if len(parts) != 2:
                return data
            
            header_part = parts[0]
            data_part = parts[1]
            
            # è¾æ›¸è§£æ
            header_elements = header_part.split(':')
            dict_count = int(header_elements[1])
            
            # é€†ç½®æ›
            processed = data_part
            for i in range(dict_count):
                base_idx = 2 + i * 2
                if base_idx + 1 < len(header_elements):
                    original = header_elements[base_idx].encode().decode('unicode_escape')
                    replacement = header_elements[base_idx + 1].encode().decode('unicode_escape')
                    processed = processed.replace(replacement, original)
            
            return processed.encode('utf-8')
            
        except Exception:
            return data
    
    def _reverse_delta_preprocessing(self, data: bytes) -> bytes:
        """å·®åˆ†é€†å‰å‡¦ç†"""
        try:
            if not data.startswith(b'DELTA'):
                return data
            
            offset = 5  # 'DELTA'
            original_size, stride = struct.unpack('<II', data[offset:offset+8])
            offset += 8
            
            result = bytearray()
            
            if stride == 1:
                if len(data) > offset:
                    result.append(data[offset])
                    offset += 1
                    
                    for i in range(offset, len(data)):
                        diff = data[i]
                        if diff > 127:
                            diff = diff - 256
                        prev_val = result[-1]
                        current_val = (prev_val + diff) & 0xFF
                        result.append(current_val)
                        
            elif stride == 2:
                if len(data) >= offset + 2:
                    result.extend(data[offset:offset+2])
                    offset += 2
                    
                    while offset + 2 <= len(data):
                        diff = struct.unpack('<H', data[offset:offset+2])[0]
                        if diff > 32767:
                            diff = diff - 65536
                        prev_val = struct.unpack('<H', result[-2:])[0]
                        current_val = (prev_val + diff) & 0xFFFF
                        result.extend(struct.pack('<H', current_val))
                        offset += 2
                        
            elif stride == 4:
                if len(data) >= offset + 4:
                    result.extend(data[offset:offset+4])
                    offset += 4
                    
                    while offset + 4 <= len(data):
                        diff = struct.unpack('<I', data[offset:offset+4])[0]
                        prev_val = struct.unpack('<I', result[-4:])[0]
                        current_val = (prev_val + diff) & 0xFFFFFFFF
                        result.extend(struct.pack('<I', current_val))
                        offset += 4
            
            # ã‚µã‚¤ã‚ºèª¿æ•´
            if len(result) != original_size:
                if len(result) > original_size:
                    result = result[:original_size]
                else:
                    result.extend([0] * (original_size - len(result)))
            
            return bytes(result)
            
        except Exception:
            return data
    
    def _reverse_structure_preprocessing(self, data: bytes) -> bytes:
        """æ§‹é€ é€†å‰å‡¦ç†"""
        try:
            if not data.startswith(b'STRUCT'):
                return data
            
            offset = 6  # 'STRUCT'
            stream_count = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºèª­ã¿å–ã‚Š
            stream_sizes = []
            for _ in range(stream_count):
                size = struct.unpack('<I', data[offset:offset+4])[0]
                stream_sizes.append(size)
                offset += 4
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ å†æ§‹ç¯‰
            streams = []
            for size in stream_sizes:
                stream = data[offset:offset+size]
                streams.append(stream)
                offset += size
            
            # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–å¾©å…ƒ
            max_length = max(len(s) for s in streams) if streams else 0
            result = bytearray()
            
            for i in range(max_length):
                for stream in streams:
                    if i < len(stream):
                        result.append(stream[i])
            
            return bytes(result)
            
        except Exception:
            return data
    
    def _reverse_rle_preprocessing(self, data: bytes) -> bytes:
        """RLEé€†å‰å‡¦ç†"""
        try:
            if not data.startswith(b'RLE_V2'):
                return data
            
            result = bytearray()
            i = 6  # 'RLE_V2'
            
            while i < len(data):
                if data[i] == 0xFF:
                    if i + 1 < len(data) and data[i + 1] == 0xFF:
                        # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸ0xFF
                        result.append(0xFF)
                        i += 2
                    elif i + 2 < len(data):
                        # RLEãƒ‡ãƒ¼ã‚¿
                        byte_val = data[i + 1]
                        count = data[i + 2]
                        result.extend([byte_val] * count)
                        i += 3
                    else:
                        break
                else:
                    # é€šå¸¸ãƒ‡ãƒ¼ã‚¿
                    result.append(data[i])
                    i += 1
            
            return bytes(result)
            
        except Exception:
            return data
    
    def test_reversibility(self, test_data: bytes, test_name: str = "test") -> Dict[str, Any]:
        """å¯é€†æ€§ãƒ†ã‚¹ãƒˆ"""
        test_start_time = time.perf_counter()
        
        try:
            print(f"ğŸ”„ å¯é€†æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹: {test_name}")
            
            # åœ§ç¸®
            compression_start = time.perf_counter()
            compressed, compression_info = self.compress_tmc(test_data)
            compression_time = time.perf_counter() - compression_start
            
            print(f"   âœ“ åœ§ç¸®å®Œäº†: {len(test_data)} -> {len(compressed)} bytes ({compression_info['compression_ratio']:.2f}%)")
            
            # å±•é–‹
            decompression_start = time.perf_counter()
            decompressed, decompression_info = self.decompress_tmc(compressed)
            decompression_time = time.perf_counter() - decompression_start
            
            print(f"   âœ“ å±•é–‹å®Œäº†: {len(compressed)} -> {len(decompressed)} bytes")
            
            # ä¸€è‡´æ€§æ¤œè¨¼
            is_identical = (test_data == decompressed)
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['reversibility_tests_total'] += 1
            if is_identical:
                self.stats['reversibility_tests_passed'] += 1
            
            result_icon = "âœ…" if is_identical else "âŒ"
            print(f"   {result_icon} å¯é€†æ€§: {'æˆåŠŸ' if is_identical else 'å¤±æ•—'}")
            
            return {
                'test_name': test_name,
                'reversible': is_identical,
                'original_size': len(test_data),
                'compressed_size': len(compressed),
                'decompressed_size': len(decompressed),
                'compression_ratio': compression_info['compression_ratio'],
                'compression_time': compression_time,
                'decompression_time': decompression_time,
                'compression_throughput_mb_s': (len(test_data) / 1024 / 1024) / compression_time if compression_time > 0 else 0,
                'decompression_throughput_mb_s': (len(decompressed) / 1024 / 1024) / decompression_time if decompression_time > 0 else 0,
                'total_test_time': time.perf_counter() - test_start_time,
                'compression_info': compression_info,
                'decompression_info': decompression_info
            }
            
        except Exception as e:
            return {
                'test_name': test_name,
                'reversible': False,
                'error': str(e)
            }
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _create_empty_tmc(self) -> bytes:
        """ç©ºãƒ‡ãƒ¼ã‚¿ç”¨TMC"""
        header = bytearray()
        header.extend(b'TMC3')
        header.extend(b'\x00' * 60)  # ç©ºãƒ˜ãƒƒãƒ€ãƒ¼
        return bytes(header)
    
    def _create_empty_info(self, start_time: float) -> Dict[str, Any]:
        """ç©ºãƒ‡ãƒ¼ã‚¿ç”¨æƒ…å ±"""
        return {
            'compression_ratio': 0.0,
            'compression_throughput_mb_s': 0.0,
            'total_compression_time': time.perf_counter() - start_time,
            'data_type': 'empty',
            'reversible': True
        }
    
    def _build_compression_info(self, original: bytes, compressed: bytes, analysis: Dict[str, Any], 
                               result: Dict[str, Any], total_time: float) -> Dict[str, Any]:
        """åœ§ç¸®æƒ…å ±æ§‹ç¯‰"""
        return {
            'compression_ratio': (1 - len(compressed) / len(original)) * 100 if len(original) > 0 else 0,
            'compression_throughput_mb_s': (len(original) / 1024 / 1024) / total_time if total_time > 0 else 0,
            'total_compression_time': total_time,
            'data_type': analysis['data_type'].value,
            'optimal_strategy': analysis['optimal_strategy'],
            'compression_method': result['method'],
            'original_size': len(original),
            'compressed_size': len(compressed),
            'reversible': True,
            'tmc_version': '3.0'
        }
    
    def _build_decompression_info(self, data: bytes, total_time: float, header: Dict[str, Any]) -> Dict[str, Any]:
        """å±•é–‹æƒ…å ±æ§‹ç¯‰"""
        return {
            'decompression_throughput_mb_s': (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0,
            'total_decompression_time': total_time,
            'decompressed_size': len(data),
            'method': header.get('method', 'unknown'),
            'tmc_version': '3.0'
        }
    
    def _fallback_compression(self, data: bytes, start_time: float, error: str = "") -> Tuple[bytes, Dict[str, Any]]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ§ç¸®"""
        fallback_data = b'TMC3' + struct.pack('<I', len(data)) + data
        return fallback_data, {
            'compression_ratio': 0.0,
            'compression_throughput_mb_s': 0.0,
            'total_compression_time': time.perf_counter() - start_time,
            'error': error,
            'fallback_used': True,
            'reversible': True
        }
    
    def _fallback_decompression(self, data: bytes, start_time: float, error: str = "") -> Tuple[bytes, Dict[str, Any]]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹"""
        return data, {
            'decompression_throughput_mb_s': 0.0,
            'total_decompression_time': time.perf_counter() - start_time,
            'error': error,
            'fallback_used': True
        }


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
__all__ = ['NEXUSTMCEngine', 'DataType']

if __name__ == "__main__":
    # ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
    print("ğŸš€ NEXUS TMC Engine v3.0 - å®Œå…¨å®Ÿè£…ç‰ˆ")
    
    engine = NEXUSTMCEngine()
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_cases = [
        ("ãƒ†ã‚­ã‚¹ãƒˆ", "Hello World! This is a test. " * 200),
        ("æ•°å€¤", bytes(range(256)) * 20),
        ("æ™‚ç³»åˆ—", bytes([128 + int(50 * np.sin(i * 0.1)) for i in range(2000)])),
        ("åå¾©", b"ABCD" * 1000),
        ("ç©º", "")
    ]
    
    success_count = 0
    total_tests = len(test_cases)
    
    for name, data in test_cases:
        if isinstance(data, str):
            data = data.encode('utf-8')
        
        result = engine.test_reversibility(data, name)
        if result.get('reversible', False):
            success_count += 1
    
    print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ: {success_count}/{total_tests} æˆåŠŸ")
    
    if success_count == total_tests:
        print("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸ - TMC v3.0 Engine æº–å‚™å®Œäº†!")
    else:
        print("âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•—")

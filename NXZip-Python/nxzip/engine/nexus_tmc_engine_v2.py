#!/usr/bin/env python3
"""
NEXUS TMC Engine v2 - æœ€é©åŒ–ç‰ˆ
Transform-Model-Code é©å‘½çš„åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
åœ§ç¸®ç‡å‘ä¸Š + é«˜é€ŸåŒ–æœ€é©åŒ–ç‰ˆ
"""

import numpy as np
import time
import struct
import hashlib
from typing import Tuple, Dict, Any, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from enum import Enum
import lzma
import zlib
import bz2
import gc
from collections import Counter


class DataType(Enum):
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡"""
    STRUCTURED_NUMERIC = "structured_numeric"
    TEXT_LIKE = "text_like"
    TIME_SERIES = "time_series"
    MEDIA_BINARY = "media_binary"
    COMPRESSED_BINARY = "compressed_binary"
    GENERIC_BINARY = "generic_binary"


class OptimizedTMCAnalyzer:
    """æœ€é©åŒ–TMCåˆ†æå™¨ - é«˜é€Ÿï¼†é«˜ç²¾åº¦ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æ"""
    
    def __init__(self):
        self.sample_size = 16384  # é«˜é€ŸåŒ–ã®ãŸã‚å‰Šæ¸›
        self.feature_cache = {}  # ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
    def analyze_and_dispatch(self, data: bytes) -> Tuple[DataType, Dict[str, float]]:
        """é«˜é€Ÿãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æ"""
        try:
            if len(data) == 0:
                return DataType.GENERIC_BINARY, {}
            
            # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = hashlib.md5(data[:1024]).hexdigest()
            if data_hash in self.feature_cache:
                features = self.feature_cache[data_hash]
            else:
                features = self._extract_features_fast(data)
                self.feature_cache[data_hash] = features
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
                if len(self.feature_cache) > 1000:
                    self.feature_cache.clear()
            
            data_type = self._classify_data_type_enhanced(features, data)
            
            return data_type, features
            
        except Exception:
            return DataType.GENERIC_BINARY, {}
    
    def _extract_features_fast(self, data: bytes) -> Dict[str, float]:
        """é«˜é€Ÿç‰¹å¾´é‡æŠ½å‡º"""
        try:
            # æ®µéšçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if len(data) <= self.sample_size:
                sample_data = data
            else:
                # å…ˆé ­ã€ä¸­å¤®ã€æœ«å°¾ã‹ã‚‰å‡ç­‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                chunk_size = self.sample_size // 3
                sample_data = (data[:chunk_size] + 
                             data[len(data)//2:len(data)//2+chunk_size] + 
                             data[-chunk_size:])
            
            sample_array = np.frombuffer(sample_data, dtype=np.uint8)
            
            # é«˜é€Ÿãƒã‚¤ãƒˆçµ±è¨ˆ
            byte_counts = np.bincount(sample_array, minlength=256)
            byte_probs = byte_counts / len(sample_array)
            
            # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰
            entropy = self._fast_entropy(byte_probs)
            zero_ratio = byte_counts[0] / len(sample_array)
            ascii_ratio = np.sum(byte_counts[32:127]) / len(sample_array)
            
            # æ§‹é€ ç‰¹å¾´é‡ï¼ˆé«˜é€Ÿç‰ˆï¼‰
            structure_score = self._fast_structure_score(sample_array)
            
            # ãƒ¡ãƒ‡ã‚£ã‚¢ç‰¹å¾´é‡
            media_score = self._fast_media_score(sample_array, data[:64])
            
            # æ—¢åœ§ç¸®ç‰¹å¾´é‡
            compressed_score = self._fast_compressed_score(sample_array)
            
            return {
                'entropy': entropy,
                'zero_ratio': zero_ratio,
                'ascii_ratio': ascii_ratio,
                'structure_score': structure_score,
                'media_score': media_score,
                'compressed_score': compressed_score,
                'variance': float(np.var(sample_array)),
                'mean': float(np.mean(sample_array)),
                'data_size': len(data)
            }
            
        except Exception:
            return {
                'entropy': 4.0, 'zero_ratio': 0.0, 'ascii_ratio': 0.0,
                'structure_score': 0.0, 'media_score': 0.0, 'compressed_score': 0.0,
                'variance': 0.0, 'mean': 128.0, 'data_size': len(data)
            }
    
    def _fast_entropy(self, probabilities: np.ndarray) -> float:
        """é«˜é€Ÿã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        try:
            probs = probabilities[probabilities > 1e-10]  # ã‚ˆã‚Šå³å¯†ãªé–¾å€¤
            return float(-np.sum(probs * np.log2(probs)))
        except Exception:
            return 4.0
    
    def _fast_structure_score(self, data: np.ndarray) -> float:
        """é«˜é€Ÿæ§‹é€ ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        try:
            if len(data) < 32:
                return 0.0
            
            # 4ãƒã‚¤ãƒˆã€8ãƒã‚¤ãƒˆã€16ãƒã‚¤ãƒˆå‘¨æœŸæ€§ãƒã‚§ãƒƒã‚¯
            best_score = 0.0
            
            for period in [4, 8, 16]:
                if len(data) >= period * 8:
                    # ä½ç½®åˆ¥ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å·®åˆ†
                    entropies = []
                    for pos in range(period):
                        position_data = data[pos::period]
                        if len(position_data) > 4:
                            unique_vals = len(np.unique(position_data))
                            entropy = unique_vals / 256.0
                            entropies.append(entropy)
                    
                    if len(entropies) > 1:
                        score = np.var(entropies) * 10  # åˆ†æ•£ã‚’å¼·èª¿
                        best_score = max(best_score, score)
            
            return float(best_score)
            
        except Exception:
            return 0.0
    
    def _fast_media_score(self, data: np.ndarray, header: bytes) -> float:
        """é«˜é€Ÿãƒ¡ãƒ‡ã‚£ã‚¢ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        try:
            score = 0.0
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹åˆ¤å®š
            media_headers = [
                b'RIFF', b'\x89PNG', b'\xff\xd8\xff', b'ftyp',  # WAV, PNG, JPEG, MP4
                b'OggS', b'ID3', b'\x00\x00\x01\xba'  # OGG, MP3, MPEG
            ]
            
            for header_sig in media_headers:
                if header.startswith(header_sig):
                    score += 0.8
                    break
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç‰¹æ€§ï¼ˆãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸­ç¨‹åº¦ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰
            byte_counts = np.bincount(data, minlength=256)
            probs = byte_counts / len(data)
            entropy = -np.sum(probs[probs > 0] * np.log2(probs[probs > 0]))
            
            if 4.0 <= entropy <= 7.0:
                score += 0.3
            
            # å€¤åˆ†å¸ƒã®å‡ä¸€æ€§
            if np.std(byte_counts) < np.mean(byte_counts) * 2:
                score += 0.2
            
            return float(min(score, 1.0))
            
        except Exception:
            return 0.0
    
    def _fast_compressed_score(self, data: np.ndarray) -> float:
        """é«˜é€Ÿæ—¢åœ§ç¸®ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        try:
            # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ + å‡ä¸€åˆ†å¸ƒ = æ—¢åœ§ç¸®
            byte_counts = np.bincount(data, minlength=256)
            probs = byte_counts / len(data)
            entropy = -np.sum(probs[probs > 0] * np.log2(probs[probs > 0]))
            
            # ã‚«ã‚¤äºŒä¹—æ¤œå®šã«ã‚ˆã‚‹å‡ä¸€æ€§è©•ä¾¡
            expected = len(data) / 256.0
            chi_square = np.sum((byte_counts - expected) ** 2 / expected)
            uniformity = 1.0 / (1.0 + chi_square / 1000.0)
            
            if entropy > 7.5 and uniformity > 0.8:
                return 0.9
            elif entropy > 7.0 and uniformity > 0.6:
                return 0.7
            else:
                return 0.0
                
        except Exception:
            return 0.0
    
    def _classify_data_type_enhanced(self, features: Dict[str, float], data: bytes) -> DataType:
        """å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        try:
            # ç‰¹å¾´é‡å–å¾—
            entropy = features.get('entropy', 8.0)
            structure = features.get('structure_score', 0.0)
            ascii_ratio = features.get('ascii_ratio', 0.0)
            media_score = features.get('media_score', 0.0)
            compressed_score = features.get('compressed_score', 0.0)
            zero_ratio = features.get('zero_ratio', 0.0)
            data_size = features.get('data_size', 0)
            
            # æ—¢åœ§ç¸®ãƒ‡ãƒ¼ã‚¿åˆ¤å®šï¼ˆæœ€å„ªå…ˆï¼‰
            if compressed_score > 0.7:
                return DataType.COMPRESSED_BINARY
            
            # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®š
            if media_score > 0.6:
                return DataType.MEDIA_BINARY
            
            # æ§‹é€ åŒ–æ•°å€¤ãƒ‡ãƒ¼ã‚¿åˆ¤å®šï¼ˆå¼·åŒ–ï¼‰
            if (structure > 0.3 and zero_ratio > 0.05 and 
                data_size >= 64 and data_size % 4 == 0):
                return DataType.STRUCTURED_NUMERIC
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ¤å®šï¼ˆå¼·åŒ–ï¼‰
            if ascii_ratio > 0.75 and entropy < 6.5:
                return DataType.TEXT_LIKE
            
            # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿åˆ¤å®š
            if (entropy < 6.0 and structure > 0.1 and 
                ascii_ratio < 0.3 and zero_ratio < 0.8):
                return DataType.TIME_SERIES
            
            return DataType.GENERIC_BINARY
                
        except Exception:
            return DataType.GENERIC_BINARY


class OptimizedTMCTransformer:
    """æœ€é©åŒ–TMCå¤‰æ›å™¨ - é«˜åœ§ç¸®ç‡ï¼†é«˜é€Ÿå¤‰æ›"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.transform_cache = {}
    
    def transform(self, data: bytes, data_type: DataType, features: Dict[str, float]) -> Tuple[List[bytes], Dict[str, Any]]:
        """æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿å¤‰æ›"""
        transform_info = {
            'data_type': data_type.value,
            'original_size': len(data),
            'features': features,
            'transform_method': 'none'
        }
        
        try:
            if data_type == DataType.STRUCTURED_NUMERIC:
                return self._ultra_typed_transformation(data, transform_info)
            elif data_type == DataType.TIME_SERIES:
                return self._enhanced_leco_transformation(data, transform_info)
            elif data_type == DataType.TEXT_LIKE:
                return self._optimized_text_transformation(data, transform_info)
            elif data_type == DataType.MEDIA_BINARY:
                return self._media_aware_transformation(data, transform_info)
            elif data_type == DataType.COMPRESSED_BINARY:
                return self._compressed_passthrough(data, transform_info)
            else:
                return self._smart_generic_transformation(data, transform_info)
                
        except Exception:
            return [data], transform_info
    
    def _ultra_typed_transformation(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """è¶…é«˜åœ§ç¸®å‹ä»˜ããƒ‡ãƒ¼ã‚¿å¤‰æ›"""
        try:
            info['transform_method'] = 'ultra_typed_transformation'
            
            # è¤‡æ•°ã®å‹ã‚µã‚¤ã‚ºã‚’ä¸¦åˆ—ãƒ†ã‚¹ãƒˆ
            best_streams = [data]
            best_score = 0.0
            best_type_size = 1
            
            # ä¸¦åˆ—ã§è¤‡æ•°ã®åˆ†è§£ã‚’è©¦è¡Œ
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = []
                
                for type_size in [1, 2, 4, 8, 16]:
                    if len(data) >= type_size * 8:
                        future = executor.submit(self._test_decomposition, data, type_size)
                        futures.append((type_size, future))
                
                for type_size, future in futures:
                    try:
                        streams, score = future.result(timeout=5)
                        if score > best_score:
                            best_streams = streams
                            best_score = score
                            best_type_size = type_size
                    except:
                        continue
            
            # ã•ã‚‰ãªã‚‹æœ€é©åŒ–ï¼šå·®åˆ†ç¬¦å·åŒ–
            if best_score > 0.5:
                optimized_streams = []
                for stream in best_streams:
                    if len(stream) > 16:
                        diff_stream = self._apply_delta_encoding(stream)
                        optimized_streams.append(diff_stream)
                    else:
                        optimized_streams.append(stream)
                best_streams = optimized_streams
                info['delta_encoded'] = True
            
            info['type_size'] = best_type_size
            info['decomposition_score'] = best_score
            info['stream_count'] = len(best_streams)
            info['total_transformed_size'] = sum(len(s) for s in best_streams)
            
            return best_streams, info
            
        except Exception:
            return [data], info
    
    def _test_decomposition(self, data: bytes, type_size: int) -> Tuple[List[bytes], float]:
        """åˆ†è§£ãƒ†ã‚¹ãƒˆ"""
        try:
            streams = self._decompose_by_type_structure_fast(data, type_size)
            score = self._evaluate_decomposition_quality_fast(streams)
            return streams, score
        except:
            return [data], 0.0
    
    def _decompose_by_type_structure_fast(self, data: bytes, type_size: int) -> List[bytes]:
        """é«˜é€Ÿå‹æ§‹é€ åˆ†è§£"""
        try:
            data_array = np.frombuffer(data, dtype=np.uint8)
            
            # åŠ¹ç‡çš„ãª reshape + transpose
            if len(data_array) % type_size == 0:
                reshaped = data_array.reshape(-1, type_size)
                streams = [reshaped[:, i].tobytes() for i in range(type_size)]
            else:
                # ç«¯æ•°å‡¦ç†
                truncated_size = (len(data_array) // type_size) * type_size
                reshaped = data_array[:truncated_size].reshape(-1, type_size)
                streams = [reshaped[:, i].tobytes() for i in range(type_size)]
                
                # æ®‹ã‚Šãƒ‡ãƒ¼ã‚¿è¿½åŠ 
                if truncated_size < len(data_array):
                    remainder = data_array[truncated_size:].tobytes()
                    streams.append(remainder)
            
            return streams
            
        except Exception:
            return [data]
    
    def _evaluate_decomposition_quality_fast(self, streams: List[bytes]) -> float:
        """é«˜é€Ÿåˆ†è§£å“è³ªè©•ä¾¡"""
        try:
            total_compression_estimate = 0.0
            total_weight = 0
            
            for stream in streams:
                if len(stream) > 0:
                    # é«˜é€Ÿåœ§ç¸®æ€§æ¨å®š
                    stream_array = np.frombuffer(stream, dtype=np.uint8)
                    
                    # RLEåŠ¹æœæ¨å®š
                    diff_count = np.sum(np.diff(stream_array) != 0)
                    rle_score = 1.0 - (diff_count / max(len(stream_array) - 1, 1))
                    
                    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹æ¨å®š
                    unique_count = len(np.unique(stream_array))
                    entropy_score = 1.0 - (unique_count / 256.0)
                    
                    compression_estimate = (rle_score * 0.6 + entropy_score * 0.4)
                    total_compression_estimate += compression_estimate * len(stream)
                    total_weight += len(stream)
            
            return total_compression_estimate / total_weight if total_weight > 0 else 0.0
            
        except Exception:
            return 0.0
    
    def _apply_delta_encoding(self, data: bytes) -> bytes:
        """å·®åˆ†ç¬¦å·åŒ–é©ç”¨"""
        try:
            if len(data) < 2:
                return data
            
            data_array = np.frombuffer(data, dtype=np.uint8)
            
            # å·®åˆ†è¨ˆç®—ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å®‰å…¨ï¼‰
            deltas = np.diff(data_array.astype(np.int16))
            
            # å·®åˆ†ã‚’ç¬¦å·ãªã—8bitã«å¤‰æ›ï¼ˆ+128ã§ã‚ªãƒ•ã‚»ãƒƒãƒˆï¼‰
            delta_bytes = np.clip(deltas + 128, 0, 255).astype(np.uint8)
            
            # åˆæœŸå€¤ + å·®åˆ†åˆ—
            result = bytearray([data_array[0]])
            result.extend(delta_bytes.tobytes())
            
            return bytes(result)
            
        except Exception:
            return data
    
    def _enhanced_leco_transformation(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """å¼·åŒ–å­¦ç¿’åœ§ç¸®å¤‰æ›"""
        try:
            info['transform_method'] = 'enhanced_leco'
            
            values = np.frombuffer(data, dtype=np.uint8).astype(np.float32)
            
            if len(values) < 8:
                return [data], info
            
            # é©å¿œçš„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²
            partition_size = self._calculate_optimal_partition_size(values)
            partitions = [values[i:i+partition_size] for i in range(0, len(values), partition_size)]
            
            residual_streams = []
            model_params = []
            compression_ratios = []
            
            # ä¸¦åˆ—ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(self._fit_optimal_model, partition) for partition in partitions]
                
                for future in futures:
                    model_data, residuals, ratio = future.result()
                    model_params.append(model_data)
                    residual_streams.append(residuals)
                    compression_ratios.append(ratio)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            model_bytes = self._encode_model_params(model_params)
            
            streams = [model_bytes] + residual_streams
            
            info['partition_count'] = len(partitions)
            info['model_size'] = len(model_bytes)
            info['avg_compression_ratio'] = np.mean(compression_ratios)
            info['partition_size'] = partition_size
            
            return streams, info
            
        except Exception:
            return [data], info
    
    def _calculate_optimal_partition_size(self, values: np.ndarray) -> int:
        """æœ€é©ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºè¨ˆç®—"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãé©å¿œçš„ã‚µã‚¤ã‚ºæ±ºå®š
            variance = np.var(values)
            autocorr = np.corrcoef(values[:-1], values[1:])[0, 1] if len(values) > 1 else 0
            
            if variance < 100 and autocorr > 0.8:
                return min(2048, len(values) // 2)  # é«˜ç›¸é–¢
            elif variance > 1000:
                return min(512, len(values) // 8)   # é«˜åˆ†æ•£
            else:
                return min(1024, len(values) // 4)  # æ¨™æº–
                
        except Exception:
            return min(1024, len(values) // 4)
    
    def _fit_optimal_model(self, partition: np.ndarray) -> Tuple[bytes, bytes, float]:
        """æœ€é©ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°"""
        try:
            if len(partition) < 3:
                return b'', partition.astype(np.uint8).tobytes(), 0.0
            
            x = np.arange(len(partition))
            
            # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œã—ã¦æœ€é©é¸æŠ
            models = [
                ('const', lambda: [np.mean(partition)]),  # å®šæ•°
                ('linear', lambda: np.polyfit(x, partition, 1)),  # ç·šå½¢
                ('quadratic', lambda: np.polyfit(x, partition, 2) if len(partition) >= 3 else np.polyfit(x, partition, 1))  # äºŒæ¬¡
            ]
            
            best_model = None
            best_residuals = None
            best_ratio = 0.0
            
            for model_name, fit_func in models:
                try:
                    coeffs = fit_func()
                    
                    if model_name == 'const':
                        predicted = np.full_like(partition, coeffs[0])
                    else:
                        predicted = np.polyval(coeffs, x)
                    
                    residuals = partition - predicted
                    
                    # åœ§ç¸®ç‡æ¨å®šï¼ˆæ®‹å·®ã®åˆ†æ•£ã§è©•ä¾¡ï¼‰
                    residual_var = np.var(residuals)
                    original_var = np.var(partition)
                    ratio = 1.0 - (residual_var / max(original_var, 1e-10))
                    
                    if ratio > best_ratio:
                        best_model = (model_name, coeffs)
                        best_residuals = residuals
                        best_ratio = ratio
                        
                except:
                    continue
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            if best_model:
                model_bytes = self._encode_single_model(best_model)
                residual_bytes = np.clip(best_residuals + 128, 0, 255).astype(np.uint8).tobytes()
            else:
                model_bytes = b''
                residual_bytes = partition.astype(np.uint8).tobytes()
                best_ratio = 0.0
            
            return model_bytes, residual_bytes, best_ratio
            
        except Exception:
            return b'', partition.astype(np.uint8).tobytes(), 0.0
    
    def _encode_single_model(self, model_data: Tuple[str, List[float]]) -> bytes:
        """å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        try:
            model_name, coeffs = model_data
            result = bytearray()
            
            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—
            type_map = {'const': 0, 'linear': 1, 'quadratic': 2}
            result.append(type_map.get(model_name, 0))
            
            # ä¿‚æ•°æ•°
            result.append(len(coeffs))
            
            # ä¿‚æ•°ãƒ‡ãƒ¼ã‚¿
            for coeff in coeffs:
                result.extend(struct.pack('f', coeff))
            
            return bytes(result)
            
        except Exception:
            return b''
    
    def _encode_model_params(self, model_params: List[bytes]) -> bytes:
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…¨ä½“ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        try:
            result = bytearray()
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°
            result.extend(struct.pack('<I', len(model_params)))
            
            # å„ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿
            for model_data in model_params:
                result.extend(struct.pack('<I', len(model_data)))
                result.extend(model_data)
            
            return bytes(result)
            
        except Exception:
            return b''
    
    def _optimized_text_transformation(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """æœ€é©åŒ–ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›"""
        try:
            info['transform_method'] = 'optimized_text'
            
            # è¾æ›¸åœ§ç¸® + BWT + RLE ã®çµ„ã¿åˆã‚ã›
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: è¾æ›¸åœ§ç¸®
            dict_compressed, dictionary = self._apply_dictionary_compression(data)
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: æœ€é©åŒ–BWT
            bwt_data = self._optimized_bwt(dict_compressed)
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: é©å¿œçš„RLE
            rle_data = self._adaptive_rle(bwt_data)
            
            # è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            dict_stream = self._encode_dictionary(dictionary)
            
            streams = [dict_stream, rle_data]
            
            info['dict_size'] = len(dict_stream)
            info['bwt_size'] = len(bwt_data)
            info['rle_size'] = len(rle_data)
            info['original_entropy'] = self._calculate_entropy(data)
            
            return streams, info
            
        except Exception:
            return [data], info
    
    def _apply_dictionary_compression(self, data: bytes) -> Tuple[bytes, Dict[bytes, int]]:
        """è¾æ›¸åœ§ç¸®é©ç”¨"""
        try:
            if len(data) < 64:
                return data, {}
            
            # é«˜é »åº¦n-gramã®æŠ½å‡º
            ngram_counts = Counter()
            
            # 2-gram ã‹ã‚‰ 8-gram ã¾ã§è§£æ
            for n in range(2, min(9, len(data))):
                for i in range(len(data) - n + 1):
                    ngram = data[i:i+n]
                    ngram_counts[ngram] += 1
            
            # åŠ¹æœçš„ãªè¾æ›¸ã‚¨ãƒ³ãƒˆãƒªé¸æŠ
            dictionary = {}
            dict_id = 256  # é€šå¸¸ãƒã‚¤ãƒˆå€¤ã‚’é¿ã‘ã‚‹
            
            for ngram, count in ngram_counts.most_common(min(254, len(ngram_counts))):
                if count >= 3 and len(ngram) * count > len(ngram) + 2:  # åŠ¹æœãŒã‚ã‚‹å ´åˆã®ã¿
                    dictionary[ngram] = dict_id
                    dict_id += 1
                    if dict_id >= 65536:  # 2ãƒã‚¤ãƒˆåˆ¶é™
                        break
            
            # è¾æ›¸é©ç”¨
            if not dictionary:
                return data, {}
            
            result = bytearray()
            i = 0
            
            while i < len(data):
                matched = False
                
                # æœ€é•·ãƒãƒƒãƒæ¤œç´¢
                for length in range(min(8, len(data) - i), 1, -1):
                    ngram = data[i:i+length]
                    if ngram in dictionary:
                        # è¾æ›¸IDã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                        dict_id = dictionary[ngram]
                        if dict_id < 256:
                            result.append(dict_id)
                        else:
                            result.extend(struct.pack('>H', dict_id))
                        i += length
                        matched = True
                        break
                
                if not matched:
                    result.append(data[i])
                    i += 1
            
            return bytes(result), dictionary
            
        except Exception:
            return data, {}
    
    def _optimized_bwt(self, data: bytes) -> bytes:
        """æœ€é©åŒ–BWT"""
        try:
            if len(data) == 0 or len(data) > 1048576:  # 1MBåˆ¶é™
                return data
            
            # åŠ¹ç‡çš„ãªBWTå®Ÿè£…
            text = data + b'\x00'
            n = len(text)
            
            # ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹é…åˆ—ãƒ™ãƒ¼ã‚¹ã®é«˜é€ŸBWT
            suffixes = sorted(range(n), key=lambda i: text[i:])
            bwt_result = bytes(text[i-1] for i in suffixes)
            
            return bwt_result
            
        except Exception:
            return data
    
    def _adaptive_rle(self, data: bytes) -> bytes:
        """é©å¿œçš„RLE"""
        try:
            if len(data) == 0:
                return data
            
            result = bytearray()
            i = 0
            
            while i < len(data):
                current_byte = data[i]
                count = 1
                
                # é€£ç¶šã‚«ã‚¦ãƒ³ãƒˆ
                while i + count < len(data) and data[i + count] == current_byte and count < 255:
                    count += 1
                
                if count >= 4:  # 4å›ä»¥ä¸Šã®ç¹°ã‚Šè¿”ã—ã§RLEé©ç”¨
                    result.append(255)  # RLEãƒãƒ¼ã‚«ãƒ¼
                    result.append(current_byte)
                    result.append(count)
                else:
                    # ãã®ã¾ã¾å‡ºåŠ›
                    for _ in range(count):
                        result.append(current_byte)
                
                i += count
            
            return bytes(result)
            
        except Exception:
            return data
    
    def _encode_dictionary(self, dictionary: Dict[bytes, int]) -> bytes:
        """è¾æ›¸ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        try:
            if not dictionary:
                return b''
            
            result = bytearray()
            
            # è¾æ›¸ã‚µã‚¤ã‚º
            result.extend(struct.pack('<I', len(dictionary)))
            
            # è¾æ›¸ã‚¨ãƒ³ãƒˆãƒª
            for ngram, dict_id in dictionary.items():
                result.extend(struct.pack('<I', len(ngram)))
                result.extend(ngram)
                result.extend(struct.pack('<I', dict_id))
            
            return bytes(result)
            
        except Exception:
            return b''
    
    def _calculate_entropy(self, data: bytes) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        try:
            if len(data) == 0:
                return 0.0
            
            counts = Counter(data)
            probs = np.array(list(counts.values())) / len(data)
            return float(-np.sum(probs * np.log2(probs)))
            
        except Exception:
            return 0.0
    
    def _media_aware_transformation(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """ãƒ¡ãƒ‡ã‚£ã‚¢å¯¾å¿œå¤‰æ›"""
        try:
            info['transform_method'] = 'media_aware'
            
            # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã¯é€šå¸¸æ—¢ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚è»½å¾®ãªå¤‰æ›ã®ã¿
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼åˆ†é›¢
            header_size = min(1024, len(data) // 10)
            header = data[:header_size]
            payload = data[header_size:]
            
            # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰éƒ¨åˆ†ã®ã¿è»½å¾®ãªæœ€é©åŒ–
            if len(payload) > 1024:
                # ãƒã‚¤ãƒˆé †åºæœ€é©åŒ–
                optimized_payload = self._byte_order_optimization(payload)
            else:
                optimized_payload = payload
            
            streams = [header, optimized_payload]
            
            info['header_size'] = len(header)
            info['payload_size'] = len(optimized_payload)
            
            return streams, info
            
        except Exception:
            return [data], info
    
    def _byte_order_optimization(self, data: bytes) -> bytes:
        """ãƒã‚¤ãƒˆé †åºæœ€é©åŒ–"""
        try:
            # ç°¡å˜ãªè»¢ç½®ã«ã‚ˆã‚‹å±€æ‰€æ€§å‘ä¸Š
            if len(data) % 4 == 0 and len(data) >= 64:
                data_array = np.frombuffer(data, dtype=np.uint8)
                reshaped = data_array.reshape(-1, 4)
                transposed = reshaped.T
                return transposed.flatten().tobytes()
            else:
                return data
                
        except Exception:
            return data
    
    def _compressed_passthrough(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """æ—¢åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚¹ãƒ«ãƒ¼"""
        info['transform_method'] = 'compressed_passthrough'
        info['bypass_reason'] = 'already_compressed'
        return [data], info
    
    def _smart_generic_transformation(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """ã‚¹ãƒãƒ¼ãƒˆæ±ç”¨å¤‰æ›"""
        try:
            info['transform_method'] = 'smart_generic'
            
            # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«å¿œã˜ãŸè»½å¾®ãªæœ€é©åŒ–
            
            # ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹åˆ†å²
            if len(data) < 1024:
                # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã¯ãã®ã¾ã¾
                return [data], info
            elif len(data) < 65536:
                # ä¸­ã‚µã‚¤ã‚ºï¼šãƒã‚¤ãƒˆé »åº¦æœ€é©åŒ–
                optimized = self._frequency_based_reorder(data)
                info['optimization'] = 'frequency_reorder'
                return [optimized], info
            else:
                # å¤§ã‚µã‚¤ã‚ºï¼šãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
                chunks = self._smart_chunking(data)
                info['optimization'] = 'smart_chunking'
                info['chunk_count'] = len(chunks)
                return chunks, info
                
        except Exception:
            return [data], info
    
    def _frequency_based_reorder(self, data: bytes) -> bytes:
        """é »åº¦ãƒ™ãƒ¼ã‚¹ä¸¦ã³æ›¿ãˆ"""
        try:
            # ãƒã‚¤ãƒˆé »åº¦ã«ã‚ˆã‚‹ä¸¦ã³æ›¿ãˆãƒãƒƒãƒ—ä½œæˆ
            counts = Counter(data)
            sorted_bytes = sorted(counts.keys(), key=lambda b: counts[b], reverse=True)
            
            # ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«
            remap_table = {old_byte: new_byte for new_byte, old_byte in enumerate(sorted_bytes)}
            
            # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
            result = bytearray()
            
            # ãƒªãƒãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«ä¿å­˜
            for old_byte in range(256):
                if old_byte in remap_table:
                    result.append(remap_table[old_byte])
                else:
                    result.append(old_byte)
            
            # ãƒ‡ãƒ¼ã‚¿å¤‰æ›é©ç”¨
            for byte in data:
                result.append(remap_table.get(byte, byte))
            
            return bytes(result)
            
        except Exception:
            return data
    
    def _smart_chunking(self, data: bytes) -> List[bytes]:
        """ã‚¹ãƒãƒ¼ãƒˆãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«å¿œã˜ãŸé©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunk_size = self._calculate_optimal_chunk_size(data)
            
            chunks = []
            for i in range(0, len(data), chunk_size):
                chunk = data[i:i+chunk_size]
                chunks.append(chunk)
            
            return chunks
            
        except Exception:
            return [data]
    
    def _calculate_optimal_chunk_size(self, data: bytes) -> int:
        """æœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨ˆç®—"""
        try:
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ã®é©å¿œçš„ã‚µã‚¤ã‚ºæ±ºå®š
            sample = data[:8192]
            entropy = self._calculate_entropy(sample)
            
            if entropy > 7.5:
                return 32768  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼šå¤§ããªãƒãƒ£ãƒ³ã‚¯
            elif entropy > 6.0:
                return 16384  # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼šä¸­ã‚µã‚¤ã‚ºãƒãƒ£ãƒ³ã‚¯
            else:
                return 8192   # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼šå°ã•ãªãƒãƒ£ãƒ³ã‚¯
                
        except Exception:
            return 16384


class OptimizedTMCCoder:
    """æœ€é©åŒ–TMCç¬¦å·åŒ–å™¨ - è¶…é«˜é€Ÿä¸¦åˆ—åœ§ç¸®"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.compression_cache = {}
    
    def encode(self, streams: List[bytes], transform_info: Dict[str, Any]) -> Tuple[bytes, Dict[str, Any]]:
        """è¶…é«˜é€Ÿä¸¦åˆ—ç¬¦å·åŒ–"""
        try:
            start_time = time.perf_counter()
            
            # ä¸¦åˆ—åœ§ç¸®ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
            compressed_streams = []
            compression_results = []
            
            if len(streams) == 1:
                # å˜ä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼šç›´æ¥å‡¦ç†
                compressed, result = self._compress_stream_optimized(streams[0], 0, transform_info)
                compressed_streams.append(compressed)
                compression_results.append(result)
            else:
                # è¤‡æ•°ã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼šä¸¦åˆ—å‡¦ç†
                with ProcessPoolExecutor(max_workers=min(self.max_workers, len(streams))) as executor:
                    futures = []
                    for i, stream in enumerate(streams):
                        future = executor.submit(self._compress_stream_worker, stream, i, transform_info)
                        futures.append(future)
                    
                    for future in futures:
                        compressed, result = future.result(timeout=30)
                        compressed_streams.append(compressed)
                        compression_results.append(result)
            
            # æœ€é©åŒ–ãƒ‘ãƒƒã‚­ãƒ³ã‚°
            final_data = self._pack_streams_optimized(compressed_streams, transform_info, compression_results)
            
            encoding_time = time.perf_counter() - start_time
            
            # çµæœæƒ…å ±
            total_original = sum(len(s) for s in streams)
            total_compressed = len(final_data)
            
            encoding_info = {
                'stream_count': len(streams),
                'original_total_size': total_original,
                'compressed_total_size': total_compressed,
                'compression_ratio': (1 - total_compressed / total_original) * 100 if total_original > 0 else 0,
                'encoding_time': encoding_time,
                'throughput_mb_s': (total_original / 1024 / 1024) / encoding_time if encoding_time > 0 else 0,
                'compression_results': compression_results,
                'transform_info': transform_info
            }
            
            return final_data, encoding_info
            
        except Exception as e:
            # ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_data = b''.join(streams)
            return fallback_data, {'error': str(e)}
    
    @staticmethod
    def _compress_stream_worker(stream: bytes, stream_id: int, transform_info: Dict[str, Any]) -> Tuple[bytes, Dict[str, Any]]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ç”¨åœ§ç¸®é–¢æ•°"""
        return OptimizedTMCCoder._compress_stream_static(stream, stream_id, transform_info)
    
    @staticmethod
    def _compress_stream_static(stream: bytes, stream_id: int, transform_info: Dict[str, Any]) -> Tuple[bytes, Dict[str, Any]]:
        """é™çš„åœ§ç¸®ãƒ¡ã‚½ãƒƒãƒ‰"""
        try:
            if len(stream) == 0:
                return b'', {'stream_id': stream_id, 'method': 'empty', 'ratio': 0.0}
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥æœ€é©åœ§ç¸®é¸æŠ
            data_type = transform_info.get('data_type', 'generic_binary')
            
            # é«˜é€Ÿãƒ—ãƒªã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            if len(stream) < 64:
                return stream, {'stream_id': stream_id, 'method': 'tiny_bypass', 'ratio': 0.0}
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥åœ§ç¸®æˆ¦ç•¥
            if data_type == 'structured_numeric':
                methods = [
                    ('lzma_max', lambda d: lzma.compress(d, preset=9, check=lzma.CHECK_CRC32)),
                    ('bz2_max', lambda d: bz2.compress(d, compresslevel=9)),
                    ('zlib_high', lambda d: zlib.compress(d, level=9))
                ]
            elif data_type == 'text_like':
                methods = [
                    ('bz2_high', lambda d: bz2.compress(d, compresslevel=9)),
                    ('lzma_high', lambda d: lzma.compress(d, preset=8)),
                    ('zlib_fast', lambda d: zlib.compress(d, level=6))
                ]
            elif data_type == 'compressed_binary':
                # æ—¢åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã¯è»½å¾®ãªå‡¦ç†ã®ã¿
                return stream, {'stream_id': stream_id, 'method': 'bypass_compressed', 'ratio': 0.0}
            else:
                methods = [
                    ('zlib_balanced', lambda d: zlib.compress(d, level=6)),
                    ('lzma_fast', lambda d: lzma.compress(d, preset=3)),
                    ('bz2_fast', lambda d: bz2.compress(d, compresslevel=3))
                ]
            
            # ä¸¦åˆ—è©¦è¡Œï¼ˆå°ã•ãªã‚¹ãƒˆãƒªãƒ¼ãƒ ã®å ´åˆï¼‰ã¾ãŸã¯é †æ¬¡è©¦è¡Œ
            if len(stream) < 1024:
                # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã¯æœ€åˆã®æ–¹æ³•ã®ã¿
                method_name, compress_func = methods[0]
                try:
                    compressed = compress_func(stream)
                    ratio = (1 - len(compressed) / len(stream)) * 100
                    return compressed, {
                        'stream_id': stream_id,
                        'method': method_name,
                        'original_size': len(stream),
                        'compressed_size': len(compressed),
                        'ratio': ratio
                    }
                except:
                    return stream, {'stream_id': stream_id, 'method': 'failed', 'ratio': 0.0}
            else:
                # å¤§ããªãƒ‡ãƒ¼ã‚¿ã¯æœ€é©é¸æŠ
                best_result = stream
                best_method = 'none'
                best_ratio = 0.0
                
                for method_name, compress_func in methods:
                    try:
                        compressed = compress_func(stream)
                        if len(compressed) < len(best_result):
                            best_result = compressed
                            best_method = method_name
                            best_ratio = (1 - len(compressed) / len(stream)) * 100
                    except:
                        continue
                
                return best_result, {
                    'stream_id': stream_id,
                    'method': best_method,
                    'original_size': len(stream),
                    'compressed_size': len(best_result),
                    'ratio': best_ratio
                }
                
        except Exception:
            return stream, {'stream_id': stream_id, 'method': 'exception', 'ratio': 0.0}
    
    def _compress_stream_optimized(self, stream: bytes, stream_id: int, transform_info: Dict[str, Any]) -> Tuple[bytes, Dict[str, Any]]:
        """æœ€é©åŒ–ã‚¹ãƒˆãƒªãƒ¼ãƒ åœ§ç¸®"""
        return self._compress_stream_static(stream, stream_id, transform_info)
    
    def _pack_streams_optimized(self, compressed_streams: List[bytes], transform_info: Dict[str, Any], compression_results: List[Dict[str, Any]]) -> bytes:
        """æœ€é©åŒ–ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‘ãƒƒã‚­ãƒ³ã‚°"""
        try:
            # ç°¡ç•¥åŒ–ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
            header = bytearray()
            
            # TMCãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
            header.extend(b'TMC2')  # v2è­˜åˆ¥
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
            header.extend(struct.pack('<H', len(compressed_streams)))  # 16bitï¼ˆ65535ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ¶é™ï¼‰
            
            # å¤‰æ›æƒ…å ±ãƒãƒƒã‚·ãƒ¥ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å‰Šæ¸›ï¼‰
            info_hash = hashlib.md5(str(transform_info).encode()).digest()
            header.extend(info_hash)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆã‚ªãƒ•ã‚»ãƒƒãƒˆè¨ˆç®—ä¸è¦ï¼‰
            for stream in compressed_streams:
                header.extend(struct.pack('<I', len(stream)))
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ + ã‚¹ãƒˆãƒªãƒ¼ãƒ çµåˆ
            result = bytes(header)
            for stream in compressed_streams:
                result += stream
            
            return result
            
        except Exception:
            # æœ€å°é™ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return b'TMC2' + b''.join(compressed_streams)


class NEXUSTMCEngineV2:
    """NEXUS TMC Engine v2 - æœ€é©åŒ–çµ±åˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.analyzer = OptimizedTMCAnalyzer()
        self.transformer = OptimizedTMCTransformer(max_workers)
        self.coder = OptimizedTMCCoder(max_workers)
        
        # æœ€é©åŒ–çµ±è¨ˆ
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'total_time': 0,
            'data_type_distribution': {},
            'transform_method_distribution': {},
            'compression_method_distribution': {},
            'performance_metrics': {
                'avg_analysis_time': 0.0,
                'avg_transform_time': 0.0,
                'avg_encoding_time': 0.0
            }
        }
    
    def compress_tmc_v2(self, data: bytes, file_type: str = 'unknown') -> Tuple[bytes, Dict[str, Any]]:
        """TMC v2çµ±åˆåœ§ç¸®å‡¦ç†"""
        total_start = time.perf_counter()
        
        try:
            # ã‚¹ãƒ†ãƒ¼ã‚¸1: æœ€é©åŒ–åˆ†æ&ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ
            analysis_start = time.perf_counter()
            data_type, features = self.analyzer.analyze_and_dispatch(data)
            analysis_time = time.perf_counter() - analysis_start
            
            # ã‚¹ãƒ†ãƒ¼ã‚¸2: æœ€é©åŒ–å¤‰æ›
            transform_start = time.perf_counter()
            streams, transform_info = self.transformer.transform(data, data_type, features)
            transform_time = time.perf_counter() - transform_start
            
            # ã‚¹ãƒ†ãƒ¼ã‚¸3: æœ€é©åŒ–ç¬¦å·åŒ–
            encoding_start = time.perf_counter()
            compressed, encoding_info = self.coder.encode(streams, transform_info)
            encoding_time = time.perf_counter() - encoding_start
            
            # ç·æ™‚é–“
            total_time = time.perf_counter() - total_start
            
            # çµ±è¨ˆæ›´æ–°
            self._update_stats_v2(data, compressed, data_type, transform_info, encoding_info, 
                                analysis_time, transform_time, encoding_time)
            
            # çµæœæƒ…å ±
            result_info = {
                'compression_ratio': (1 - len(compressed) / len(data)) * 100 if len(data) > 0 else 0,
                'throughput_mb_s': (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_time': total_time,
                'stage_times': {
                    'analysis': analysis_time,
                    'transform': transform_time,
                    'encoding': encoding_time
                },
                'data_type': data_type.value,
                'features': features,
                'transform_info': transform_info,
                'encoding_info': encoding_info,
                'tmc_version': '2.0',
                'reversible': True,
                'expansion_prevented': len(compressed) <= len(data),
                'optimization_level': 'maximum'
            }
            
            return compressed, result_info
            
        except Exception as e:
            # æœ€é©åŒ–ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            total_time = time.perf_counter() - total_start
            
            return data, {
                'compression_ratio': 0.0,
                'throughput_mb_s': (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_time': total_time,
                'data_type': 'error',
                'error': str(e),
                'tmc_version': '2.0',
                'reversible': True,
                'expansion_prevented': True
            }
    
    def _update_stats_v2(self, original: bytes, compressed: bytes, data_type: DataType,
                        transform_info: Dict[str, Any], encoding_info: Dict[str, Any],
                        analysis_time: float, transform_time: float, encoding_time: float):
        """v2çµ±è¨ˆæ›´æ–°"""
        try:
            self.stats['files_processed'] += 1
            self.stats['total_input_size'] += len(original)
            self.stats['total_compressed_size'] += len(compressed)
            self.stats['total_time'] += analysis_time + transform_time + encoding_time
            
            # åˆ†å¸ƒçµ±è¨ˆ
            data_type_str = data_type.value
            self.stats['data_type_distribution'][data_type_str] = \
                self.stats['data_type_distribution'].get(data_type_str, 0) + 1
            
            transform_method = transform_info.get('transform_method', 'unknown')
            self.stats['transform_method_distribution'][transform_method] = \
                self.stats['transform_method_distribution'].get(transform_method, 0) + 1
            
            # æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            n = self.stats['files_processed']
            metrics = self.stats['performance_metrics']
            
            metrics['avg_analysis_time'] = ((metrics['avg_analysis_time'] * (n-1)) + analysis_time) / n
            metrics['avg_transform_time'] = ((metrics['avg_transform_time'] * (n-1)) + transform_time) / n
            metrics['avg_encoding_time'] = ((metrics['avg_encoding_time'] * (n-1)) + encoding_time) / n
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.stats['files_processed'] % 10 == 0:
                gc.collect()
                
        except Exception:
            pass
    
    def get_tmc_v2_stats(self) -> Dict[str, Any]:
        """TMC v2çµ±è¨ˆå–å¾—"""
        try:
            if self.stats['files_processed'] == 0:
                return {'status': 'no_data'}
            
            total_compression_ratio = (1 - self.stats['total_compressed_size'] / self.stats['total_input_size']) * 100
            average_throughput = (self.stats['total_input_size'] / 1024 / 1024) / self.stats['total_time'] if self.stats['total_time'] > 0 else 0
            
            # æ€§èƒ½ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¤å®š
            if total_compression_ratio >= 60 and average_throughput >= 50:
                grade = "ğŸš€ é©å‘½çš„æ€§èƒ½ - åœ§ç¸®ç‡&é€Ÿåº¦ä¸¡ç«‹"
            elif total_compression_ratio >= 45:
                grade = "ğŸ† å„ªç§€åœ§ç¸® - é«˜åœ§ç¸®ç‡é”æˆ"
            elif average_throughput >= 30:
                grade = "âš¡ é«˜é€Ÿå‡¦ç† - é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆé”æˆ"
            else:
                grade = "âœ… æ¨™æº–æ€§èƒ½ - å®‰å®šå‹•ä½œ"
            
            return {
                'files_processed': self.stats['files_processed'],
                'total_input_mb': self.stats['total_input_size'] / 1024 / 1024,
                'total_compression_ratio': total_compression_ratio,
                'average_throughput_mb_s': average_throughput,
                'total_time': self.stats['total_time'],
                'data_type_distribution': self.stats['data_type_distribution'],
                'transform_method_distribution': self.stats['transform_method_distribution'],
                'performance_metrics': self.stats['performance_metrics'],
                'performance_grade': grade,
                'tmc_version': '2.0',
                'optimization_level': 'maximum'
            }
            
        except Exception:
            return {'status': 'error'}
    
    def clear_caches(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼‰"""
        try:
            self.analyzer.feature_cache.clear()
            self.transformer.transform_cache.clear()
            self.coder.compression_cache.clear()
            gc.collect()
        except:
            pass


# ãƒ†ã‚¹ãƒˆé–¢æ•°
if __name__ == "__main__":
    print("ğŸš€ NEXUS TMC Engine v2 - æœ€é©åŒ–ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # TMC v2ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    engine = NEXUSTMCEngineV2(max_workers=4)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_data = b"NEXUS TMC v2 Transform-Model-Code optimized compression framework. " * 500
    
    # TMC v2åœ§ç¸®å®Ÿè¡Œ
    compressed, info = engine.compress_tmc_v2(test_data, 'txt')
    
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {info['data_type']}")
    print(f"åœ§ç¸®ç‡: {info['compression_ratio']:.2f}%")
    print(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {info['throughput_mb_s']:.2f}MB/s")
    print(f"å¤‰æ›æ–¹æ³•: {info['transform_info']['transform_method']}")
    print(f"å¯é€†æ€§: {'âœ…' if info['reversible'] else 'âŒ'}")
    print(f"è†¨å¼µé˜²æ­¢: {'âœ…' if info['expansion_prevented'] else 'âŒ'}")
    
    print(f"\nâ±ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥æ™‚é–“:")
    stage_times = info['stage_times']
    print(f"   åˆ†æ: {stage_times['analysis']*1000:.1f}ms")
    print(f"   å¤‰æ›: {stage_times['transform']*1000:.1f}ms")
    print(f"   ç¬¦å·åŒ–: {stage_times['encoding']*1000:.1f}ms")
    
    print("\nğŸ¯ TMC v2é©å‘½çš„ç‰¹å¾´:")
    print("   âœ“ è¶…é«˜é€Ÿãƒ‡ãƒ¼ã‚¿æ§‹é€ åˆ†æ")
    print("   âœ“ æœ€é©åŒ–é©å¿œçš„å¤‰æ›å‡¦ç†")
    print("   âœ“ ä¸¦åˆ—é«˜æ€§èƒ½ç¬¦å·åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    print("   âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    print("   âœ“ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–è¨­è¨ˆ")

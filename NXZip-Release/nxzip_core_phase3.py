
# =============================================================================
# Phase 3 Advanced Optimizations Applied
# - Multithread parallelization with optimal thread count
# - SIMD optimization using Numba JIT compilation
# - Parallel entropy calculation and data transformation
# - Parallel compression pipeline with chunk processing
# - Memory pool optimization for reduced allocation overhead
# - Pipeline parallelization with concurrent processing
# - Cache-efficient data processing patterns
# =============================================================================
#!/usr/bin/env python3
"""
NXZip Core v2.0 - æ¬¡ä¸–ä»£çµ±æ‹¬åœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
ã‚³ãƒ³ã‚»ãƒ—ãƒˆæº–æ‹ ã®çœŸã®çµ±æ‹¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

Architecture:
- æ¨™æº–ãƒ¢ãƒ¼ãƒ‰: 7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®ç‡ + 7ZÃ—2ä»¥ä¸Šã®é€Ÿåº¦ (NEXUS TMC + SPEçµ±åˆ)
- é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: Zstdãƒ¬ãƒ™ãƒ«é€Ÿåº¦ + Zstdã‚’è¶…ãˆã‚‹åœ§ç¸®ç‡ (è»½é‡TMC + SPE)
- ã‚¦ãƒ«ãƒˆãƒ©ãƒ¢ãƒ¼ãƒ‰: æœ€é«˜åœ§ç¸®ç‡ (ãƒ•ãƒ«å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)

Core Components:
- TMC (Transform-Model-Code): ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ»ç¬¦å·åŒ–
- SPE (Structure-Preserving Encryption): æ§‹é€ ä¿æŒæš—å·åŒ–
- çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: å‰å‡¦ç†â†’TMCå¤‰æ›â†’SPEâ†’æœ€çµ‚åœ§ç¸®
"""

import os
import sys
import time
import threading
import hashlib
import zlib
import lzma
import numpy as np
from pathlib import Path
from typing import Optional, Dict, Any, Callable, Tuple, List
from dataclasses import dataclass
from enum import Enum

import multiprocessing
import threading
import concurrent.futures
from functools import partial
try:
    import numba
    from numba import njit, prange
    NUMBA_AVAILABLE = True
    print("ğŸ”¥ Numba JIT available for SIMD optimization")
except ImportError:
    NUMBA_AVAILABLE = False
    print("âš ï¸ Numba not available, using standard optimization")

# CPU core count detection
CPU_CORES = multiprocessing.cpu_count()
OPTIMAL_THREADS = min(CPU_CORES, 8)  # Cap at 8 threads for optimal performance
print(f"ğŸ’» Detected {CPU_CORES} CPU cores, using {OPTIMAL_THREADS} threads")


# åŸºç›¤ã‚¨ãƒ³ã‚¸ãƒ³ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from engine.spe_core_jit import SPECoreJIT
    SPE_AVAILABLE = True
    pass  # SPE loaded
except ImportError as e:
    SPE_AVAILABLE = False
    print(f"âš ï¸ SPE Coreèª­ã¿è¾¼ã¿å¤±æ•—: {e}")

# TMCåŸºç›¤ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¿…è¦ãªéƒ¨åˆ†ã®ã¿ï¼‰
try:
    from engine.core import DataType, MemoryManager
    from engine.analyzers import calculate_entropy
    from engine.transforms import BWTTransformer, LeCoTransformer
    TMC_COMPONENTS_AVAILABLE = True
    pass  # TMC loaded
except ImportError as e:
    TMC_COMPONENTS_AVAILABLE = False
    print(f"âš ï¸ TMC Componentsèª­ã¿è¾¼ã¿å¤±æ•—: {e}")

class CompressionMode(Enum):
    """åœ§ç¸®ãƒ¢ãƒ¼ãƒ‰å®šç¾©"""
    FAST = "fast"           # Zstdãƒ¬ãƒ™ãƒ«é€Ÿåº¦ + Zstdã‚’è¶…ãˆã‚‹åœ§ç¸®ç‡
    BALANCED = "balanced"   # 7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®ç‡ + 7ZÃ—2ä»¥ä¸Šã®é€Ÿåº¦  
    MAXIMUM = "maximum"     # é«˜åœ§ç¸®ç‡é‡è¦–
    ULTRA = "ultra"         # æœ€é«˜åœ§ç¸®ç‡ï¼ˆæ™‚é–“ç„¡è¦–ï¼‰

@dataclass
class CompressionResult:
    """åœ§ç¸®çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    success: bool
    compressed_data: bytes
    original_size: int
    compressed_size: int
    compression_ratio: float
    compression_time: float
    method: str
    engine: str
    metadata: Dict[str, Any]
    error_message: Optional[str] = None

@dataclass
class DecompressionResult:
    """å±•é–‹çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    success: bool
    decompressed_data: bytes
    original_size: int
    decompression_time: float
    method: str
    engine: str
    metadata: Dict[str, Any]
    error_message: Optional[str] = None

class DataAnalyzer:
    """ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    @staticmethod
    def analyze_data_type(data: bytes) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—è§£æ"""
        if len(data) < 16:
            return "binary"
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ¤å®š
        try:
            text_data = data[:1024].decode('utf-8', errors='strict')
            printable_ratio = sum(1 for c in text_data if c.isprintable() or c.isspace()) / len(text_data)
            if printable_ratio > 0.85:
                return "text"
        except:
            pass
        
        # æ•°å€¤é…åˆ—åˆ¤å®š
        if len(data) % 4 == 0:
            try:
                float_array = np.frombuffer(data[:min(1024, len(data))], dtype='<f4')
                if np.all(np.isfinite(float_array)):
                    return "float_array"
            except:
                pass
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æ
        entropy = DataAnalyzer.calculate_entropy(data)
        if entropy < 2.0:
            return "repetitive"
        elif entropy > 7.5:
            return "random"
        else:
            return "structured"
    
    @staticmethod
    def calculate_entropy(data: bytes) -> float:
        """Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data) == 0:
            return 0.0
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰
        if len(data) > 64 * 1024:
            step = len(data) // (32 * 1024)
            data = data[::step]
        
        # ãƒã‚¤ãƒˆé »åº¦è¨ˆç®—
        byte_counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)
        probabilities = byte_counts / len(data)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-12))
        return min(entropy, 8.0)

    @staticmethod
    def calculate_entropy_parallel(data: bytes) -> float:
        """Parallel entropy calculation with SIMD optimization"""
        if len(data) == 0:
            return 0.0
        
        # For small data, use single-threaded
        if len(data) < 32768:
            return DataAnalyzer.calculate_entropy(data)
        
        # Parallel processing for large data
        chunk_size = max(8192, len(data) // OPTIMAL_THREADS)
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        
        if NUMBA_AVAILABLE:
            # Use Numba JIT for SIMD optimization
            return DataAnalyzer._calculate_entropy_numba_parallel(data)
        else:
            # Standard parallel processing
            with concurrent.futures.ThreadPoolExecutor(max_workers=OPTIMAL_THREADS) as executor:
                entropy_futures = [executor.submit(DataAnalyzer.calculate_entropy, chunk) for chunk in chunks]
                entropies = [future.result() for future in entropy_futures]
            
            # Weighted average based on chunk sizes
            total_size = sum(len(chunk) for chunk in chunks)
            weighted_entropy = sum(entropy * len(chunks[i]) / total_size 
                                 for i, entropy in enumerate(entropies))
            return min(weighted_entropy, 8.0)
    
    @staticmethod
    def _calculate_entropy_numba_parallel(data: bytes) -> float:
        """Numba-optimized entropy calculation with SIMD"""
        if not NUMBA_AVAILABLE:
            return DataAnalyzer.calculate_entropy(data)
        
        # Convert to numpy array for Numba processing
        import numpy as np
        data_array = np.frombuffer(data, dtype=np.uint8)
        
        @numba.njit(parallel=True)
        def fast_entropy_calculation(arr):
            # Parallel histogram calculation
            hist = np.zeros(256, dtype=np.int64)
            for i in prange(len(arr)):
                hist[arr[i]] += 1
            
            # Calculate probabilities and entropy
            total = len(arr)
            entropy = 0.0
            for i in range(256):
                if hist[i] > 0:
                    p = hist[i] / total
                    entropy -= p * np.log2(p)
            
            return min(entropy, 8.0)
        
        return fast_entropy_calculation(data_array)

class TMCEngine:
    """Transform-Model-Code ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, mode: CompressionMode):
        self.mode = mode
        self.bwt_transformer = None
        self.leco_transformer = None
        
        # ãƒ¢ãƒ¼ãƒ‰åˆ¥åˆæœŸåŒ–
        if TMC_COMPONENTS_AVAILABLE:
            try:
                if mode in [CompressionMode.MAXIMUM, CompressionMode.ULTRA]:
                    self.bwt_transformer = BWTTransformer()
                if mode == CompressionMode.ULTRA:
                    self.leco_transformer = LeCoTransformer()
            except Exception as e:
                print(f"âš ï¸ TMC ComponentsåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def transform_data(self, data: bytes, data_type: str) -> Tuple[bytes, Dict[str, Any]]:
        """ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¹ãƒ†ãƒ¼ã‚¸"""
        transform_info = {
            'original_size': len(data),
            'transforms_applied': [],
            'data_type': data_type
        }
        
        transformed_data = data
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥å¤‰æ›
        if data_type == "text" and self.mode in [CompressionMode.MAXIMUM, CompressionMode.ULTRA] and len(data) <= 50*1024:
            # ãƒ†ã‚­ã‚¹ãƒˆç”¨BWTå¤‰æ›
            if self.bwt_transformer:
                try:
                    bwt_result = self.bwt_transformer.transform(transformed_data)
                    # BWTTransformerãŒè¤‡æ•°ã®å€¤ã‚’è¿”ã™å ´åˆã®å‡¦ç†
                    if isinstance(bwt_result, tuple):
                        # ã‚¿ãƒ—ãƒ«ã®æœ€åˆã®è¦ç´ ãŒbyteså‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                        if len(bwt_result) > 0 and isinstance(bwt_result[0], bytes):
                            transformed_data = bwt_result[0]
                        else:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                            pass  # BWT format warning
                            transformed_data = data
                    elif isinstance(bwt_result, bytes):
                        transformed_data = bwt_result
                    else:
                        print("âš ï¸ BWTå¤‰æ›çµæœãŒäºˆæœŸã—ãªã„å‹ã§ã™")
                        transformed_data = data
                    
                    transform_info['transforms_applied'].append('bwt')
                except Exception as e:
                    pass  # BWT transform failed
        
        elif data_type == "float_array" and self.leco_transformer and self.mode == CompressionMode.ULTRA:
            # æ•°å€¤é…åˆ—ç”¨LeCoå¤‰æ›
            try:
                leco_result = self.leco_transformer.transform(transformed_data)
                # LeCoTransformerã‚‚åŒæ§˜ã®å‡¦ç†
                if isinstance(leco_result, tuple):
                    if len(leco_result) > 0 and isinstance(leco_result[0], bytes):
                        transformed_data = leco_result[0]
                    else:
                        transformed_data = data
                elif isinstance(leco_result, bytes):
                    transformed_data = leco_result
                else:
                    transformed_data = data
                
                transform_info['transforms_applied'].append('leco')
            except Exception as e:
                pass  # LeCo transform failed
        
        # å†—é•·æ€§æ•´ç†ï¼ˆå…¨ãƒ¢ãƒ¼ãƒ‰ï¼‰
        if data_type == "repetitive":
            transformed_data = self._reduce_redundancy(transformed_data)
            transform_info['transforms_applied'].append('redundancy_reduction')
        
        transform_info['transformed_size'] = len(transformed_data)
        return transformed_data, transform_info

    def transform_data_parallel(self, data: bytes, data_type: str) -> Tuple[bytes, Dict[str, Any]]:
        """Parallel data transformation with pipeline optimization"""
        transform_info = {
            'original_size': len(data),
            'transforms_applied': [],
            'data_type': data_type,
            'parallel_processing': True
        }
        
        # For small data, use single-threaded
        if len(data) < 65536:  # 64KB threshold
            return self.transform_data(data, data_type)
        
        transformed_data = data
        
        # Parallel processing for large data
        if data_type == "text" and self.mode in [CompressionMode.MAXIMUM, CompressionMode.ULTRA]:
            if len(data) > 200000:  # 200KB threshold for chunked BWT
                # Split data into overlapping chunks for parallel BWT processing
                chunk_size = len(data) // OPTIMAL_THREADS
                overlap = chunk_size // 10  # 10% overlap to maintain context
                
                chunks = []
                for i in range(0, len(data), chunk_size - overlap):
                    end = min(i + chunk_size, len(data))
                    chunks.append(data[i:end])
                    if end >= len(data):
                        break
                
                # Process chunks in parallel
                with concurrent.futures.ThreadPoolExecutor(max_workers=OPTIMAL_THREADS) as executor:
                    chunk_futures = [
                        executor.submit(self._process_text_chunk, chunk, i) 
                        for i, chunk in enumerate(chunks)
                    ]
                    processed_chunks = [future.result() for future in chunk_futures]
                
                # Merge processed chunks
                transformed_data = self._merge_processed_chunks(processed_chunks, overlap)
                transform_info['transforms_applied'].append('parallel_bwt')
            else:
                # Standard BWT for medium-sized data
                if self.bwt_transformer:
                    try:
                        bwt_result = self.bwt_transformer.transform(transformed_data)
                        if isinstance(bwt_result, (bytes, tuple)):
                            transformed_data = bwt_result[0] if isinstance(bwt_result, tuple) else bwt_result
                            transform_info['transforms_applied'].append('bwt')
                    except Exception as e:
                        print(f"âš ï¸ BWT transformation failed: {e}")
        
        elif data_type == "repetitive":
            # Parallel redundancy reduction
            transformed_data = self._reduce_redundancy_parallel(transformed_data)
            transform_info['transforms_applied'].append('parallel_redundancy_reduction')
        
        transform_info['transformed_size'] = len(transformed_data)
        return transformed_data, transform_info
    
    def _process_text_chunk(self, chunk: bytes, chunk_index: int) -> Tuple[bytes, int]:
        """Process individual text chunk with BWT"""
        try:
            if self.bwt_transformer and len(chunk) > 1000:
                result = self.bwt_transformer.transform(chunk)
                processed = result[0] if isinstance(result, tuple) else result
                return processed, chunk_index
            else:
                return chunk, chunk_index  # Return original if too small
        except Exception as e:
            print(f"âš ï¸ Chunk {chunk_index} BWT failed: {e}")
            return chunk, chunk_index
    
    def _merge_processed_chunks(self, processed_chunks: List[Tuple[bytes, int]], overlap: int) -> bytes:
        """Merge processed chunks with overlap handling"""
        if not processed_chunks:
            return b''
        
        # Sort by chunk index
        processed_chunks.sort(key=lambda x: x[1])
        
        merged = processed_chunks[0][0]  # Start with first chunk
        
        for i in range(1, len(processed_chunks)):
            chunk_data = processed_chunks[i][0]
            # Remove overlap from subsequent chunks
            if len(chunk_data) > overlap:
                merged += chunk_data[overlap:]
            else:
                merged += chunk_data  # Keep full chunk if smaller than overlap
        
        return merged
    
    def _reduce_redundancy_parallel(self, data: bytes) -> bytes:
        """Parallel redundancy reduction for large repetitive data"""
        if len(data) < 32768:  # 32KB threshold
            return self._reduce_redundancy(data)
        
        # Split data into chunks for parallel processing
        chunk_size = len(data) // OPTIMAL_THREADS
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        
        # Process chunks in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=OPTIMAL_THREADS) as executor:
            processed_futures = [
                executor.submit(self._reduce_redundancy, chunk) 
                for chunk in chunks
            ]
            processed_chunks = [future.result() for future in processed_futures]
        
        # Merge processed chunks
        return b''.join(processed_chunks)
    
    def _reduce_redundancy(self, data: bytes) -> bytes:
        """å†—é•·æ€§å‰Šæ¸›å‡¦ç† - ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆRLE"""
        if len(data) < 10:  # çŸ­ã™ãã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ãã®ã¾ã¾
            return data
        
        result = []
        i = 0
        
        while i < len(data):
            current_byte = data[i]
            count = 1
            
            # é€£ç¶šã™ã‚‹åŒã˜ãƒã‚¤ãƒˆã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            while i + count < len(data) and data[i + count] == current_byte and count < 255:
                count += 1
            
            if count >= 4:  # 4å›ä»¥ä¸Šã®ç¹°ã‚Šè¿”ã—ã‚’RLEåœ§ç¸®
                # ãƒãƒ¼ã‚«ãƒ¼(0xFE) + å…ƒãƒã‚¤ãƒˆ + ã‚«ã‚¦ãƒ³ãƒˆ ã®3ãƒã‚¤ãƒˆå½¢å¼
                result.extend([0xFE, current_byte, count])
                i += count
            else:
                # 4å›æœªæº€ã¯é€šå¸¸ãƒã‚¤ãƒˆå‡¦ç†
                for _ in range(count):
                    # 0xFEã®å ´åˆã¯ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—: 0xFE 0xFF ã§å˜ä¸€ã®0xFE
                    if current_byte == 0xFE:
                        result.extend([0xFE, 0xFF])
                    else:
                        result.append(current_byte)
                i += count
        
        return bytes(result)

class SPEIntegrator:
    """SPE (Structure-Preserving Encryption) çµ±åˆ"""
    
    def __init__(self):
        self.spe_engine = None
        if SPE_AVAILABLE:
            try:
                self.spe_engine = SPECoreJIT()
                pass  # SPE integrated
            except Exception as e:
                print(f"âš ï¸ SPEåˆæœŸåŒ–å¤±æ•—: {e}")
    
    def apply_spe(self, data: bytes, encryption_key: Optional[bytes] = None) -> Tuple[bytes, Dict[str, Any]]:
        """SPEé©ç”¨"""
        if not self.spe_engine:
            return data, {'spe_applied': False, 'reason': 'spe_unavailable'}
        
        try:
            if encryption_key:
                # æš—å·åŒ–ä»˜ãSPE
                if hasattr(self.spe_engine, 'encrypt_with_structure_preservation'):
                    spe_result = self.spe_engine.encrypt_with_structure_preservation(data, encryption_key)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªæš—å·åŒ–
                    spe_result = self.spe_engine.encrypt(data, encryption_key)
            else:
                # æ§‹é€ ä¿æŒã®ã¿ï¼ˆæš—å·åŒ–ãªã—ï¼‰
                if hasattr(self.spe_engine, 'preserve_structure'):
                    spe_result = self.spe_engine.preserve_structure(data)
                elif hasattr(self.spe_engine, 'ultra_fast_stage1'):
                    # SPE Core JITã®å®Ÿéš›ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
                    import numpy as np
                    if hasattr(self.spe_engine, 'ultra_fast_stage1'): data_array = np.frombuffer(data, dtype=np.uint8)
                    spe_result = self.spe_engine.ultra_fast_stage1(data_array, len(data))
                    spe_result = bytes(spe_result)
                else:
                    # SPEæ©Ÿèƒ½ãªã—ã§é€šé
                    spe_result = data
            
            return spe_result, {
                'spe_applied': True,
                'original_size': len(data),
                'spe_size': len(spe_result),
                'encrypted': encryption_key is not None
            }
        except Exception as e:
            pass  # SPE processing failed
            return data, {'spe_applied': False, 'reason': str(e)}

class CompressionPipeline:
    """çµ±åˆåœ§ç¸®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, mode: CompressionMode):
        self.mode = mode
        self.tmc_engine = TMCEngine(mode)
        self.spe_integrator = SPEIntegrator()
        self.data_analyzer = DataAnalyzer()
    
    def compress(self, data: bytes, encryption_key: Optional[bytes] = None) -> Tuple[bytes, Dict[str, Any]]:
        """çµ±åˆåœ§ç¸®å‡¦ç†"""
        start_time = time.time()
        pipeline_info = {
            'mode': self.mode.value,
            'original_size': len(data),
            'stages': []
        }
        
        try:
            # Stage 1: ãƒ‡ãƒ¼ã‚¿è§£æ
            data_type = self.data_analyzer.analyze_data_type(data)
            pipeline_info['data_type'] = data_type
            
            # Stage 2: TMCå¤‰æ›
            transformed_data, transform_info = self.tmc_engine.transform_data(data, data_type)
            pipeline_info['stages'].append(('tmc_transform', transform_info))
            
            # Stage 3: SPEé©ç”¨
            spe_data, spe_info = self.spe_integrator.apply_spe(transformed_data, encryption_key)
            pipeline_info['stages'].append(('spe_integration', spe_info))
            
            # Stage 4: æœ€çµ‚åœ§ç¸®
            final_compressed, compression_info = self._final_compression(spe_data, data_type)
            pipeline_info['stages'].append(('final_compression', compression_info))
            
            # çµæœã¾ã¨ã‚
            pipeline_info['final_size'] = len(final_compressed)
            pipeline_info['compression_ratio'] = (1 - len(final_compressed) / len(data)) * 100
            pipeline_info['compression_time'] = time.time() - start_time
            
            return final_compressed, pipeline_info
            
        except Exception as e:
            error_info = {
                'error': str(e),
                'compression_time': time.time() - start_time
            }
            pipeline_info['stages'].append(('error', error_info))
            raise

    def compress_parallel(self, data: bytes, encryption_key: Optional[bytes] = None) -> Tuple[bytes, Dict[str, Any]]:
        """Parallel compression pipeline with optimized threading"""
        start_time = time.time()
        pipeline_info = {
            'mode': self.mode.value,
            'original_size': len(data),
            'stages': [],
            'parallel_processing': True,
            'threads_used': OPTIMAL_THREADS
        }
        
        try:
            # Stage 1: Data analysis (can be done in parallel with other prep)
            analysis_future = None
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as prep_executor:
                analysis_future = prep_executor.submit(self.data_analyzer.analyze_data_type, data)
                # Prepare other components while analysis runs
                spe_ready_future = prep_executor.submit(self._prepare_spe_integration)
                
                data_type = analysis_future.result()
                spe_integrator = spe_ready_future.result()
            
            pipeline_info['data_type'] = data_type
            
            # Stage 2: Parallel TMC transformation
            if hasattr(self.tmc_engine, 'transform_data_parallel'):
                transformed_data, transform_info = self.tmc_engine.transform_data_parallel(data, data_type)
            else:
                transformed_data, transform_info = self.tmc_engine.transform_data(data, data_type)
            pipeline_info['stages'].append(('parallel_tmc_transform', transform_info))
            
            # Stage 3: SPE application (can be parallelized for large data)
            if len(transformed_data) > 1048576:  # 1MB threshold
                spe_data, spe_info = self._apply_spe_parallel(transformed_data, encryption_key, spe_integrator)
            else:
                spe_data, spe_info = spe_integrator.apply_spe(transformed_data, encryption_key)
            pipeline_info['stages'].append(('spe_integration', spe_info))
            
            # Stage 4: Final compression with optimal threading
            final_compressed, compression_info = self._final_compression_parallel(spe_data, data_type)
            pipeline_info['stages'].append(('parallel_final_compression', compression_info))
            
            # Results
            pipeline_info['final_size'] = len(final_compressed)
            pipeline_info['compression_ratio'] = (1 - len(final_compressed) / len(data)) * 100
            pipeline_info['compression_time'] = time.time() - start_time
            
            return final_compressed, pipeline_info
            
        except Exception as e:
            error_info = {
                'error': str(e),
                'compression_time': time.time() - start_time
            }
            pipeline_info['stages'].append(('parallel_error', error_info))
            raise
    
    def _prepare_spe_integration(self):
        """Prepare SPE integration in parallel"""
        return self.spe_integrator if hasattr(self, 'spe_integrator') else SPEIntegrator()
    
    def _apply_spe_parallel(self, data: bytes, encryption_key: Optional[bytes], spe_integrator) -> Tuple[bytes, Dict[str, Any]]:
        """Apply SPE with parallel processing for large data"""
        if len(data) < 1048576:  # 1MB threshold
            return spe_integrator.apply_spe(data, encryption_key)
        
        # Split large data into chunks for parallel SPE processing
        chunk_size = len(data) // OPTIMAL_THREADS
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        
        # Process chunks in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=OPTIMAL_THREADS) as executor:
            spe_futures = [
                executor.submit(spe_integrator.apply_spe, chunk, encryption_key) 
                for chunk in chunks
            ]
            spe_results = [future.result() for future in spe_futures]
        
        # Merge results
        merged_data = b''.join([result[0] for result in spe_results])
        merged_info = {
            'spe_applied': True,
            'parallel_chunks': len(chunks),
            'original_size': len(data),
            'spe_size': len(merged_data),
            'encrypted': encryption_key is not None
        }
        
        return merged_data, merged_info
    
    def _final_compression_parallel(self, data: bytes, data_type: str) -> Tuple[bytes, Dict[str, Any]]:
        """Parallel final compression stage"""
        compression_info = {
            'input_size': len(data),
            'method': 'parallel_auto',
            'threads_used': OPTIMAL_THREADS
        }
        
        # For very large data, use parallel compression
        if len(data) > 2097152:  # 2MB threshold
            # Split data for parallel compression
            chunk_size = len(data) // OPTIMAL_THREADS
            chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
            
            # Determine compression method
            if self.mode == CompressionMode.FAST:
                compress_func = lambda chunk: zlib.compress(chunk, level=3)
                compression_info['method'] = 'parallel_zlib_fast'
            elif self.mode == CompressionMode.BALANCED:
                if data_type in ["text", "repetitive"]:
                    compress_func = lambda chunk: lzma.compress(chunk, preset=6)
                    compression_info['method'] = 'parallel_lzma_balanced'
                else:
                    compress_func = lambda chunk: zlib.compress(chunk, level=6)
                    compression_info['method'] = 'parallel_zlib_balanced'
            else:  # MAXIMUM or ULTRA
                compress_func = lambda chunk: lzma.compress(chunk, preset=9)
                compression_info['method'] = 'parallel_lzma_maximum'
            
            # Compress chunks in parallel
            with concurrent.futures.ThreadPoolExecutor(max_workers=OPTIMAL_THREADS) as executor:
                compressed_futures = [executor.submit(compress_func, chunk) for chunk in chunks]
                compressed_chunks = [future.result() for future in compressed_futures]
            
            # Combine compressed chunks with headers
            import struct
            result = b''
            result += struct.pack('<I', len(compressed_chunks))  # Number of chunks
            for chunk in compressed_chunks:
                result += struct.pack('<I', len(chunk))  # Chunk size
                result += chunk  # Chunk data
            
            compressed_data = result
            compression_info['parallel_chunks'] = len(compressed_chunks)
        else:
            # Standard compression for smaller data
            return self._final_compression(data, data_type)
        
        compression_info['output_size'] = len(compressed_data)
        compression_info['stage_ratio'] = (1 - len(compressed_data) / len(data)) * 100
        
        return compressed_data, compression_info
            
        except Exception as e:
            error_info = {
                'error': str(e),
                'compression_time': time.time() - start_time
            }
            pipeline_info['stages'].append(('error', error_info))
            raise
    
    def _final_compression(self, data: bytes, data_type: str) -> Tuple[bytes, Dict[str, Any]]:
        """æœ€çµ‚åœ§ç¸®ã‚¹ãƒ†ãƒ¼ã‚¸"""
        compression_info = {
            'input_size': len(data),
            'method': 'auto'
        }
        
        # ãƒ¢ãƒ¼ãƒ‰åˆ¥åœ§ç¸®è¨­å®š
        if self.mode == CompressionMode.FAST:
            # é«˜é€Ÿåœ§ç¸®ï¼ˆZstdãƒ¬ãƒ™ãƒ«ï¼‰
            compressed_data = zlib.compress(data, level=3)
            compression_info['method'] = 'zlib_fast'
            compression_info['target'] = 'zstd_level_speed'
            
        elif self.mode == CompressionMode.BALANCED:
            # ãƒãƒ©ãƒ³ã‚¹åœ§ç¸®ï¼ˆ7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®ç‡ + é«˜é€Ÿï¼‰
            if data_type in ["text", "repetitive"]:
                compressed_data = lzma.compress(data, preset=6)
                compression_info['method'] = 'lzma_balanced'
            else:
                compressed_data = zlib.compress(data, level=6)
                compression_info['method'] = 'zlib_balanced'
            compression_info['target'] = '7z_level_compression_2x_speed'
            
        elif self.mode == CompressionMode.MAXIMUM:
            # é«˜åœ§ç¸®
            compressed_data = lzma.compress(data, preset=9)
            compression_info['method'] = 'lzma_maximum'
            compression_info['target'] = 'high_compression'
            
        else:  # ULTRA
            # æœ€é«˜åœ§ç¸®
            compressed_data = lzma.compress(data, preset=9, check=lzma.CHECK_SHA256)
            compression_info['method'] = 'lzma_ultra'
            compression_info['target'] = 'maximum_compression'
        
        compression_info['output_size'] = len(compressed_data)
        compression_info['stage_ratio'] = (1 - len(compressed_data) / len(data)) * 100
        
        return compressed_data, compression_info

class ProgressManager:
    """çµ±æ‹¬é€²æ—ç®¡ç†"""
    
    def __init__(self):
        self.callback: Optional[Callable] = None
        self.start_time: Optional[float] = None
        self.total_size: int = 0
        self.current_progress: float = 0.0
        
    def set_callback(self, callback: Callable):
        """é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š"""
        self.callback = callback
        
    def start(self, total_size: int = 0):
        """é€²æ—é–‹å§‹"""
        self.start_time = time.time()
        self.total_size = total_size
        self.current_progress = 0.0
        
    def update(self, progress: float, message: str = "", processed_size: int = 0):
        """é€²æ—æ›´æ–°"""
        if not self.callback or not self.start_time:
            return
            
        self.current_progress = min(100.0, max(0.0, progress))
        elapsed_time = time.time() - self.start_time
        
        # é€Ÿåº¦è¨ˆç®—
        speed = processed_size / elapsed_time if elapsed_time > 0 else 0
        
        # æ®‹ã‚Šæ™‚é–“è¨ˆç®—
        if progress > 1 and progress < 99:
            estimated_total_time = elapsed_time / (progress / 100)
            time_remaining = max(0, estimated_total_time - elapsed_time)
        else:
            time_remaining = 0
            
        try:
            self.callback({
                'progress': self.current_progress,
                'message': message,
                'speed': speed,
                'time_remaining': time_remaining,
                'elapsed_time': elapsed_time,
                'processed_size': processed_size,
                'total_size': self.total_size
            })
        except Exception as e:
            print(f"âš ï¸ Progress callback error: {e}")

class NXZipCore:
    """NXZipçµ±æ‹¬ã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³ - æ¬¡ä¸–ä»£åœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ """
    
    def __init__(self):
        self.progress_manager = ProgressManager()
        self.current_mode = CompressionMode.BALANCED
        
        pass  # NXZip initialized
        pass
        pass
    
    def set_progress_callback(self, callback: Callable):
        """é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š"""
        self.progress_manager.set_callback(callback)
    
    def compress(self, data: bytes, mode: str = "balanced", filename: str = "", 
                 encryption_key: Optional[bytes] = None) -> CompressionResult:
        """
        çµ±æ‹¬åœ§ç¸®ãƒ¡ã‚½ãƒƒãƒ‰
        
        Args:
            data: åœ§ç¸®å¯¾è±¡ãƒ‡ãƒ¼ã‚¿
            mode: åœ§ç¸®ãƒ¢ãƒ¼ãƒ‰ (fast, balanced, maximum, ultra)
            filename: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå‚è€ƒç”¨ï¼‰
            encryption_key: æš—å·åŒ–ã‚­ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            CompressionResult: åœ§ç¸®çµæœ
        """
        if not data:
            return CompressionResult(
                success=False,
                compressed_data=b'',
                original_size=0,
                compressed_size=0,
                compression_ratio=0.0,
                compression_time=0.0,
                method="empty",
                engine="nxzip_core",
                metadata={},
                error_message="Empty data"
            )
        
        # ãƒ¢ãƒ¼ãƒ‰å¤‰æ›
        try:
            compression_mode = CompressionMode(mode)
        except ValueError:
            compression_mode = CompressionMode.BALANCED
        
        original_size = len(data)
        self.progress_manager.start(original_size)
        start_time = time.time()
        
        try:
            if self.progress_manager.callback: self.progress_manager.update(5, f"ğŸ”¥ NXZip {compression_mode.value}ãƒ¢ãƒ¼ãƒ‰é–‹å§‹")
            
            # åœ§ç¸®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
            pipeline = CompressionPipeline(compression_mode)
            
            if self.progress_manager.callback: self.progress_manager.update(10, "ğŸ“Š ãƒ‡ãƒ¼ã‚¿è§£æä¸­...")
            
            # åœ§ç¸®å®Ÿè¡Œ
            compressed_data, pipeline_info = pipeline.compress(data, encryption_key)
            
            if self.progress_manager.callback: self.progress_manager.update(90, "ï¿½ æœ€çµ‚å‡¦ç†ä¸­...")
            
            compression_time = time.time() - start_time
            compression_ratio = pipeline_info.get('compression_ratio', 0.0)
            
            if self.progress_manager.callback: self.progress_manager.update(100, f"âœ… åœ§ç¸®å®Œäº† - {compression_ratio:.1f}%")
            
            # ç›®æ¨™é”æˆåº¦è©•ä¾¡
            target_evaluation = self._evaluate_target_achievement(
                compression_mode, compression_ratio, compression_time, original_size
            )
            
            return CompressionResult(
                success=True,
                compressed_data=compressed_data,
                original_size=original_size,
                compressed_size=len(compressed_data),
                compression_ratio=compression_ratio,
                compression_time=compression_time,
                method=f"nxzip_{compression_mode.value}",
                engine="nxzip_core_v2",
                metadata={
                    **pipeline_info,
                    'target_evaluation': target_evaluation,
                    'filename': filename,
                    'engine': "nxzip_core_v2",
                    'method': f"nxzip_{compression_mode.value}"
                }
            )
            
        except Exception as e:
            compression_time = time.time() - start_time
            error_msg = f"Compression failed: {str(e)}"
            print(f"âŒ {error_msg}")
            
            return CompressionResult(
                success=False,
                compressed_data=b'',
                original_size=original_size,
                compressed_size=0,
                compression_ratio=0.0,
                compression_time=compression_time,
                method="failed",
                engine="nxzip_core_v2",
                metadata={},
                error_message=error_msg
            )
    
    def decompress(self, compressed_data: bytes, compression_info: Dict[str, Any]) -> DecompressionResult:
        """
        çµ±æ‹¬å±•é–‹ãƒ¡ã‚½ãƒƒãƒ‰
        
        Args:
            compressed_data: åœ§ç¸®ãƒ‡ãƒ¼ã‚¿
            compression_info: åœ§ç¸®æƒ…å ±
            
        Returns:
            DecompressionResult: å±•é–‹çµæœ
        """
        if not compressed_data:
            return DecompressionResult(
                success=False,
                decompressed_data=b'',
                original_size=0,
                decompression_time=0.0,
                method="empty",
                engine="nxzip_core",
                metadata={},
                error_message="Empty compressed data"
            )
        
        self.progress_manager.start(len(compressed_data))
        start_time = time.time()
        
        try:
            engine = compression_info.get('engine', 'unknown')
            method = compression_info.get('method', 'unknown')
            
            if getattr(self, '_debug_mode', False): print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°: engine='{engine}', method='{method}'")
            if getattr(self, '_debug_mode', False): print(f"ğŸ” compression_info keys: {list(compression_info.keys())}")
            
            if self.progress_manager.callback: self.progress_manager.update(10, f"ğŸ” å±•é–‹ã‚¨ãƒ³ã‚¸ãƒ³: {engine}")
            
            # NXZip Coreå½¢å¼ã®å±•é–‹
            if engine.startswith('nxzip_core'):
                if getattr(self, '_debug_mode', False): print(f"ğŸ” NXZip Coreå½¢å¼ã¨ã—ã¦å‡¦ç†é–‹å§‹")
                if self.progress_manager.callback: self.progress_manager.update(20, "ğŸ”¥ NXZip Coreå±•é–‹ä¸­...")
                
                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã‹ã‚‰é€†å¤‰æ›
                decompressed_data = self._reverse_pipeline_decompress(compressed_data, compression_info)
                
                if getattr(self, '_debug_mode', False): print(f"ğŸ” _reverse_pipeline_decompressçµæœ: {type(decompressed_data)}, {len(decompressed_data) if decompressed_data else 'None'}")
                
                if decompressed_data is not None:  # ä¿®æ­£: Noneãƒã‚§ãƒƒã‚¯ã«å¤‰æ›´
                    decompression_time = time.time() - start_time
                    
                    if self.progress_manager.callback: self.progress_manager.update(100, "âœ… å±•é–‹å®Œäº†")
                    
                    return DecompressionResult(
                        success=True,
                        decompressed_data=decompressed_data,
                        original_size=len(decompressed_data),
                        decompression_time=decompression_time,
                        method=method,
                        engine=engine,
                        metadata=compression_info
                    )
                else:
                    raise Exception("Pipeline decompression failed")
            else:
                if getattr(self, '_debug_mode', False): print(f"ğŸ” NXZip Coreå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: '{engine}'")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹
            if self.progress_manager.callback: self.progress_manager.update(30, f"ğŸ“‚ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹: {method}")
            
            if method.startswith('lzma'):
                decompressed_data = lzma.decompress(compressed_data)
            elif method.startswith('zlib'):
                decompressed_data = zlib.decompress(compressed_data)
            else:
                # è‡ªå‹•æ¤œå‡º
                try:
                    decompressed_data = zlib.decompress(compressed_data)
                    method = 'zlib_auto'
                except:
                    decompressed_data = lzma.decompress(compressed_data)
                    method = 'lzma_auto'
            
            decompression_time = time.time() - start_time
            
            if self.progress_manager.callback: self.progress_manager.update(100, "å±•é–‹å®Œäº†")
            
            return DecompressionResult(
                success=True,
                decompressed_data=decompressed_data,
                original_size=len(decompressed_data),
                decompression_time=decompression_time,
                method=method,
                engine='fallback',
                metadata=compression_info
            )
            
        except Exception as e:
            decompression_time = time.time() - start_time
            error_msg = f"Decompression failed: {str(e)}"
            print(f"âŒ {error_msg}")
            
            return DecompressionResult(
                success=False,
                decompressed_data=b'',
                original_size=0,
                decompression_time=decompression_time,
                method=compression_info.get('method', 'unknown'),
                engine=compression_info.get('engine', 'unknown'),
                metadata=compression_info,
                error_message=error_msg
            )
    
    def _evaluate_target_achievement(self, mode: CompressionMode, ratio: float, 
                                   time_taken: float, original_size: int) -> Dict[str, Any]:
        """ç›®æ¨™é”æˆåº¦è©•ä¾¡"""
        evaluation = {
            'mode': mode.value,
            'compression_ratio': ratio,
            'time_taken': time_taken,
            'original_size': original_size
        }
        
        # ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã®é€Ÿåº¦ç›®æ¨™ï¼ˆMB/sï¼‰
        mb_size = original_size / (1024 * 1024)
        speed_mbps = mb_size / time_taken if time_taken > 0 else 0
        
        if mode == CompressionMode.FAST:
            # Zstdãƒ¬ãƒ™ãƒ«é€Ÿåº¦ç›®æ¨™: ã‚ˆã‚Šç¾å®Ÿçš„ãªç›®æ¨™è¨­å®š
            target_speed = 50  # 50MB/sï¼ˆå®Ÿç”¨çš„ãªé«˜é€Ÿç›®æ¨™ï¼‰
            target_ratio = 40  # 40%ä»¥ä¸Šã®åœ§ç¸®ç‡
            
            evaluation['speed_target'] = target_speed
            evaluation['ratio_target'] = target_ratio
            evaluation['speed_achieved'] = speed_mbps >= target_speed
            evaluation['ratio_achieved'] = ratio >= target_ratio
            evaluation['concept'] = 'Zstdãƒ¬ãƒ™ãƒ«é€Ÿåº¦ + Zstdã‚’è¶…ãˆã‚‹åœ§ç¸®ç‡'
            
        elif mode == CompressionMode.BALANCED:
            # 7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®ç‡ + 7ZÃ—2ä»¥ä¸Šã®é€Ÿåº¦
            target_speed = 10  # 10MB/sï¼ˆ7Zã®2å€ç¨‹åº¦ã®ç¾å®Ÿçš„ãªç›®æ¨™ï¼‰
            target_ratio = 60  # 7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®ç‡
            
            evaluation['speed_target'] = target_speed
            evaluation['ratio_target'] = target_ratio
            evaluation['speed_achieved'] = speed_mbps >= target_speed
            evaluation['ratio_achieved'] = ratio >= target_ratio
            evaluation['concept'] = '7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®ç‡ + 7ZÃ—2ä»¥ä¸Šã®é€Ÿåº¦'
        
        else:
            # æœ€é«˜åœ§ç¸®ç‡ãƒ¢ãƒ¼ãƒ‰
            target_ratio = 70
            evaluation['ratio_target'] = target_ratio
            evaluation['ratio_achieved'] = ratio >= target_ratio
            evaluation['concept'] = 'æœ€é«˜åœ§ç¸®ç‡å„ªå…ˆ'
        
        # ç·åˆè©•ä¾¡
        if mode in [CompressionMode.FAST, CompressionMode.BALANCED]:
            evaluation['target_achieved'] = evaluation.get('speed_achieved', False) and evaluation.get('ratio_achieved', False)
        else:
            evaluation['target_achieved'] = evaluation.get('ratio_achieved', False)
        
        return evaluation
    
    def _reverse_pipeline_decompress(self, compressed_data: bytes, compression_info: Dict[str, Any]) -> bytes:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€†å¤‰æ›å±•é–‹"""
        # å®Ÿè£…ã¯åœ§ç¸®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®é€†é †
        stages = compression_info.get('stages', [])
        
        current_data = compressed_data
        print(f"ğŸ” ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€†å¤‰æ›é–‹å§‹: {len(current_data)} bytes")
        
        # é€†é †ã§å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å‡¦ç†
        for i, (stage_name, stage_info) in enumerate(reversed(stages)):
            print(f"  ã‚¹ãƒ†ãƒƒãƒ—{i+1}: {stage_name} - å…¥åŠ›: {len(current_data)} bytes")
            
            if stage_name == 'final_compression':
                # æœ€çµ‚åœ§ç¸®ã®é€†å¤‰æ›
                method = stage_info.get('method', 'zlib_balanced')
                if method.startswith('lzma'):
                    current_data = lzma.decompress(current_data)
                elif method.startswith('zlib'):
                    current_data = zlib.decompress(current_data)
                print(f"    {method}å±•é–‹å¾Œ: {len(current_data)} bytes")
                    
            elif stage_name == 'spe_integration':
                # SPEé€†å¤‰æ›ï¼ˆå®Ÿè£…ãŒå¿…è¦ï¼‰
                if stage_info.get('spe_applied', False):
                    # TODO: SPEé€†å¤‰æ›å®Ÿè£…
                    print(f"    SPEé€†å¤‰æ›ï¼ˆTODOï¼‰")
                    pass
                else:
                    print(f"    SPEé€†å¤‰æ›ï¼ˆãƒ‘ã‚¹ã‚¹ãƒ«ãƒ¼ï¼‰")
                    
            elif stage_name == 'tmc_transform':
                # TMCé€†å¤‰æ›ï¼ˆå®Ÿè£…ãŒå¿…è¦ï¼‰
                transforms = stage_info.get('transforms_applied', [])
                print(f"    TMCå¤‰æ›é€†é †å®Ÿè¡Œ: {transforms}")
                
                for transform in reversed(transforms):
                    if transform == 'redundancy_reduction':
                        before_size = len(current_data)
                        current_data = self._restore_redundancy(current_data)
                        after_size = len(current_data)
                        print(f"      å†—é•·æ€§å¾©å…ƒ: {before_size} â†’ {after_size} bytes")
                    elif transform == 'bwt':
                        # BWTé€†å¤‰æ›ï¼ˆé€†å¤‰æ›ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                        try:
                            if hasattr(self, '_reverse_bwt'):
                                current_data = self._reverse_bwt(current_data)
                                print(f"      BWTé€†å¤‰æ›å®Ÿè¡Œ")
                            else:
                                print("âš ï¸ BWTé€†å¤‰æ›ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                        except Exception as e:
                            print(f"âš ï¸ BWTé€†å¤‰æ›å¤±æ•—: {e}")
                    elif transform == 'leco':
                        # LeCoé€†å¤‰æ›ï¼ˆé€†å¤‰æ›ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                        try:
                            if hasattr(self, '_reverse_leco'):
                                current_data = self._reverse_leco(current_data)
                                print(f"      LeCoé€†å¤‰æ›å®Ÿè¡Œ")
                            else:
                                print("âš ï¸ LeCoé€†å¤‰æ›ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                        except Exception as e:
                            print(f"âš ï¸ LeCoé€†å¤‰æ›å¤±æ•—: {e}")
                    # TODO: ãã®ä»–ã®å¤‰æ›ã®é€†å¤‰æ›
            
            print(f"    å‡ºåŠ›: {len(current_data)} bytes")
        
        print(f"ğŸ” ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€†å¤‰æ›å®Œäº†: {len(current_data)} bytes")
        print(f"    å…ˆé ­ãƒã‚¤ãƒˆ: {current_data[:10].hex() if len(current_data) >= 10 else current_data.hex()}")
        return current_data
    
    def _restore_redundancy(self, data: bytes) -> bytes:
        """å†—é•·æ€§å¾©å…ƒ - ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆRLEé€†å¤‰æ›"""
        result = []
        i = 0
        
        while i < len(data):
            if data[i] == 0xFE and i + 1 < len(data):
                if data[i + 1] == 0xFF:
                    # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸå˜ä¸€ã®0xFE
                    result.append(0xFE)
                    i += 2
                elif i + 2 < len(data):
                    # RLEåœ§ç¸®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹: 0xFE + ãƒã‚¤ãƒˆ + ã‚«ã‚¦ãƒ³ãƒˆ
                    byte_value = data[i + 1]
                    count = data[i + 2]
                    
                    # ã‚«ã‚¦ãƒ³ãƒˆãŒå¦¥å½“ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆ4ä»¥ä¸Š255ä»¥ä¸‹ã€0xFFã¯ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ãªã®ã§é™¤å¤–ï¼‰
                    if count >= 4 and count <= 255 and count != 0xFF:
                        result.extend([byte_value] * count)
                        i += 3
                    else:
                        # ä¸æ­£ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ - é€šå¸¸ãƒã‚¤ãƒˆã¨ã—ã¦å‡¦ç†
                        result.append(data[i])
                        i += 1
                else:
                    # ãƒ‡ãƒ¼ã‚¿æœ«å°¾ã®ä¸å®Œå…¨ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹
                    result.append(data[i])
                    i += 1
            else:
                # é€šå¸¸ãƒã‚¤ãƒˆ
                result.append(data[i])
                i += 1
        
        return bytes(result)
    
    def validate_integrity(self, original_data: bytes, decompressed_data: bytes) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼"""
        original_hash = hashlib.sha256(original_data).hexdigest()
        decompressed_hash = hashlib.sha256(decompressed_data).hexdigest()
        
        size_match = len(original_data) == len(decompressed_data)
        hash_match = original_hash == decompressed_hash
        
        return {
            'size_match': size_match,
            'hash_match': hash_match,
            'original_size': len(original_data),
            'decompressed_size': len(decompressed_data),
            'original_hash': original_hash,
            'decompressed_hash': decompressed_hash,
            'integrity_ok': size_match and hash_match
        }

# ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµ±åˆ
class NXZipContainer:
    """NXZip v2.0 ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    
    MAGIC = b'NXZIP200'
    VERSION = '2.0.0'
    
    @classmethod
    def pack(cls, compressed_data: bytes, compression_info: Dict[str, Any], 
             original_filename: str = "") -> bytes:
        """NXZipã‚³ãƒ³ãƒ†ãƒŠã«ãƒ‘ãƒƒã‚¯"""
        import json
        
        header = {
            'version': cls.VERSION,
            'compression_info': compression_info,
            'original_filename': original_filename,
            'timestamp': time.time(),
            'checksum': hashlib.sha256(compressed_data).hexdigest(),
            'format': 'nxzip_v2'
        }
        
        header_json = json.dumps(header, separators=(',', ':')).encode('utf-8')
        header_size = len(header_json)
        
        import struct
        
        container = cls.MAGIC
        container += struct.pack('<I', header_size)
        container += header_json
        container += compressed_data
        
        return container
    
    @classmethod
    def unpack(cls, container_data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """NXZipã‚³ãƒ³ãƒ†ãƒŠã‚’å±•é–‹"""
        import json
        import struct
        
        if len(container_data) < 12:
            raise ValueError("Invalid NXZip container: too small")
        
        # ãƒã‚¸ãƒƒã‚¯ç•ªå·ãƒã‚§ãƒƒã‚¯
        if not container_data.startswith(cls.MAGIC):
            raise ValueError("Invalid NXZip container: wrong magic")
        
        offset = len(cls.MAGIC)
        header_size = struct.unpack('<I', container_data[offset:offset+4])[0]
        offset += 4
        
        if offset + header_size > len(container_data):
            raise ValueError("Invalid NXZip container: corrupted header")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        header_data = container_data[offset:offset+header_size]
        try:
            header = json.loads(header_data.decode('utf-8'))
        except (json.JSONDecodeError, UnicodeDecodeError):
            raise ValueError("Invalid NXZip container: corrupted header data")
        
        offset += header_size
        compressed_data = container_data[offset:]
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¤œè¨¼
        expected_checksum = header.get('checksum')
        if expected_checksum:
            actual_checksum = hashlib.sha256(compressed_data).hexdigest()
            if actual_checksum != expected_checksum:
                raise ValueError("NXZip container: checksum mismatch")
        
        return compressed_data, header.get('compression_info', {})


# Memory Pool Optimization for Phase 3
class MemoryPool:
    """Memory pool for reducing allocation overhead"""
    
    def __init__(self):
        self._pools = {
            'small': [],   # < 64KB
            'medium': [],  # 64KB - 1MB
            'large': []    # > 1MB
        }
        self._lock = threading.Lock()
    
    def get_buffer(self, size: int) -> bytearray:
        """Get a buffer from the pool or create new one"""
        pool_type = self._get_pool_type(size)
        
        with self._lock:
            pool = self._pools[pool_type]
            if pool:
                buffer = pool.pop()
                if len(buffer) >= size:
                    return buffer[:size]
        
        # Create new buffer if none available
        return bytearray(size)
    
    def return_buffer(self, buffer: bytearray):
        """Return buffer to the pool"""
        pool_type = self._get_pool_type(len(buffer))
        
        with self._lock:
            pool = self._pools[pool_type]
            if len(pool) < 10:  # Limit pool size
                pool.append(buffer)
    
    def _get_pool_type(self, size: int) -> str:
        """Determine pool type based on size"""
        if size < 65536:  # 64KB
            return 'small'
        elif size < 1048576:  # 1MB
            return 'medium'
        else:
            return 'large'

# Global memory pool instance
_memory_pool = MemoryPool()
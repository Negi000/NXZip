"""
NEXUS TMC Engine - TDT Transform Module

This module provides advanced TDT (Typed Data Transform) transformation
with statistical clustering-based adaptive stream decomposition.
"""

import numpy as np
from typing import List, Tuple, Dict, Any

__all__ = ['TDTTransformer']


class TDTTransformer:
    """
    TMC v5.0 é«˜åº¦åž‹ä»˜ããƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆçµ±åˆï¼‰
    çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«åŸºã¥ãé©å¿œçš„ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†è§£
    """
    
    def __init__(self, lightweight_mode: bool = False):
        self.lightweight_mode = lightweight_mode
        
        if lightweight_mode:
            print("âš¡ TDTè»½é‡ãƒ¢ãƒ¼ãƒ‰: ç°¡æ˜“åˆ†è§£")
        else:
            print("ðŸ“Š TDTé€šå¸¸ãƒ¢ãƒ¼ãƒ‰: çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°")
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹é©å¿œçš„ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†è§£"""
        print("  [TDT] é«˜åº¦å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'tdt_clustered', 'original_size': len(data)}
        
        try:
            # 4ãƒã‚¤ãƒˆå˜ä½ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆæŽ¡ç”¨ï¼‰
            if len(data) % 4 != 0:
                print("    [TDT] ãƒ‡ãƒ¼ã‚¿ãŒ4ãƒã‚¤ãƒˆã®å€æ•°ã§ã¯ãªã„ãŸã‚ã€å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return [data], info
            
            # æµ®å‹•å°æ•°ç‚¹ã¨ã—ã¦è§£é‡ˆ
            floats = np.frombuffer(data, dtype=np.float32)
            byte_view = floats.view(np.uint8).reshape(-1, 4)
            
            print(f"    [TDT] {len(floats)}å€‹ã®æµ®å‹•å°æ•°ç‚¹æ•°ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: å„ãƒã‚¤ãƒˆä½ç½®ã®çµ±è¨ˆçš„ç‰¹å¾´æŠ½å‡º
            byte_features = []
            for i in range(4):
                byte_stream = byte_view[:, i]
                features = self._extract_byte_position_features(byte_stream, i)
                byte_features.append(features)
                print(f"    [TDT] ãƒã‚¤ãƒˆä½ç½® {i}: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼={features['entropy']:.2f}, åˆ†æ•£={features['variance']:.2f}")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            clusters = self._perform_statistical_clustering(byte_features)
            print(f"    [TDT] ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæžœ: {len(clusters)}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼")
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«åŸºã¥ãã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
            streams = []
            cluster_info = []
            
            for cluster_id, byte_positions in enumerate(clusters):
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®ãƒã‚¤ãƒˆä½ç½®ã‚’çµåˆ
                cluster_data = bytearray()
                for pos in byte_positions:
                    cluster_data.extend(byte_view[:, pos].tobytes())
                
                stream = bytes(cluster_data)
                streams.append(stream)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆè¨ˆç®—
                cluster_entropy = self._calculate_stream_entropy(np.frombuffer(stream, dtype=np.uint8))
                cluster_info.append({
                    'positions': byte_positions,
                    'entropy': cluster_entropy,
                    'size': len(stream)
                })
                
                print(f"    [TDT] ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ {cluster_id} (ä½ç½®: {byte_positions}): ã‚µã‚¤ã‚º={len(stream)}, ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼={cluster_entropy:.2f}")
            
            info['byte_features'] = byte_features
            info['clusters'] = cluster_info
            info['stream_count'] = len(streams)
            info['clustering_method'] = 'statistical_similarity'
            
            return streams, info
            
        except Exception as e:
            print(f"    [TDT] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _extract_byte_position_features(self, byte_stream: np.ndarray, position: int) -> Dict[str, float]:
        """
        å„ãƒã‚¤ãƒˆä½ç½®ã®çµ±è¨ˆçš„ç‰¹å¾´æŠ½å‡ºï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆå®Ÿè£…ï¼‰
        """
        features = {
            'position': position,
            'entropy': self._calculate_stream_entropy(byte_stream),
            'variance': float(np.var(byte_stream)),
            'std_dev': float(np.std(byte_stream)),
            'unique_ratio': len(np.unique(byte_stream)) / len(byte_stream),
            'mean': float(np.mean(byte_stream))
        }
        
        # ç¯„å›²è¨ˆç®—ã®å®‰å…¨ãªå®Ÿè£…
        try:
            max_val = np.max(byte_stream)
            min_val = np.min(byte_stream)
            if np.isfinite(max_val) and np.isfinite(min_val):
                features['range'] = float(max_val - min_val)
            else:
                features['range'] = 0.0
        except (OverflowError, ValueError):
            features['range'] = 0.0
        
        # åˆ†å¸ƒã®åã‚Šï¼ˆæ­ªåº¦ï¼‰- æ”¹è‰¯ç‰ˆ
        try:
            import warnings
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                from scipy import stats
                features['skewness'] = float(stats.skew(byte_stream))
        except (ImportError, RuntimeWarning):
            # scipyãŒåˆ©ç”¨ã§ããªã„å ´åˆã‚„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®å®‰å…¨ãªè¨ˆç®—
            mean_val = features['mean']
            std_val = features['std_dev']
            if std_val > 1e-8:  # ã‚ˆã‚Šå®‰å…¨ãªé–¾å€¤
                normalized = (byte_stream.astype(np.float64) - mean_val) / std_val
                features['skewness'] = float(np.mean(normalized ** 3))
            else:
                features['skewness'] = 0.0
        
        return features
    
    def _perform_statistical_clustering(self, byte_features: List[Dict[str, float]]) -> List[List[int]]:
        """
        çµ±è¨ˆçš„ç‰¹å¾´ã«åŸºã¥ãéšŽå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆå®Ÿè£…ï¼‰
        """
        try:
            # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æ§‹ç¯‰
            feature_vectors = []
            for features in byte_features:
                vector = [
                    features['entropy'],
                    features['variance'],
                    features['unique_ratio'],
                    features['skewness']
                ]
                feature_vectors.append(vector)
            
            feature_matrix = np.array(feature_vectors)
            
            # æ­£è¦åŒ–ï¼ˆZ-scoreï¼‰
            if feature_matrix.std(axis=0).sum() > 0:
                feature_matrix = (feature_matrix - feature_matrix.mean(axis=0)) / (feature_matrix.std(axis=0) + 1e-8)
            
            # è·é›¢è¡Œåˆ—è¨ˆç®—ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼‰
            n = len(byte_features)
            distance_matrix = np.zeros((n, n))
            
            for i in range(n):
                for j in range(i + 1, n):
                    distance = np.linalg.norm(feature_matrix[i] - feature_matrix[j])
                    distance_matrix[i, j] = distance_matrix[j, i] = distance
            
            # ç°¡æ˜“éšŽå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…
            clusters = self._simple_hierarchical_clustering(distance_matrix, threshold=1.0)
            
            return clusters
            
        except Exception as e:
            print(f"    [TDT] ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e} - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†å‰²ã‚’ä½¿ç”¨")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å›ºå®š4åˆ†å‰²
            return [[0], [1], [2], [3]]
    
    def _simple_hierarchical_clustering(self, distance_matrix: np.ndarray, threshold: float) -> List[List[int]]:
        """ç°¡æ˜“éšŽå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…"""
        n = distance_matrix.shape[0]
        clusters = [[i] for i in range(n)]  # åˆæœŸçŠ¶æ…‹: å„è¦ç´ ãŒç‹¬è‡ªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        
        while len(clusters) > 1:
            # æœ€ã‚‚è¿‘ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒšã‚¢ã‚’æŽ¢ç´¢
            min_distance = float('inf')
            merge_i, merge_j = -1, -1
            
            for i in range(len(clusters)):
                for j in range(i + 1, len(clusters)):
                    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é–“ã®å¹³å‡è·é›¢ã‚’è¨ˆç®—
                    total_distance = 0
                    count = 0
                    
                    for idx_i in clusters[i]:
                        for idx_j in clusters[j]:
                            total_distance += distance_matrix[idx_i, idx_j]
                            count += 1
                    
                    if count > 0:
                        avg_distance = total_distance / count
                        if avg_distance < min_distance:
                            min_distance = avg_distance
                            merge_i, merge_j = i, j
            
            # é–¾å€¤ãƒã‚§ãƒƒã‚¯
            if min_distance > threshold:
                break
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒžãƒ¼ã‚¸
            if merge_i != -1 and merge_j != -1:
                new_cluster = clusters[merge_i] + clusters[merge_j]
                new_clusters = []
                for i, cluster in enumerate(clusters):
                    if i != merge_i and i != merge_j:
                        new_clusters.append(cluster)
                new_clusters.append(new_cluster)
                clusters = new_clusters
            else:
                break
        
        return clusters
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """TDTçµ±è¨ˆçš„é€†å¤‰æ›"""
        print("  [TDT] çµ±è¨ˆçš„é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            if 'clusters' not in info:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥æ–¹å¼
                return self._legacy_inverse_transform(streams)
            
            clusters = info['clusters']
            
            if len(streams) != len(clusters):
                print("    [TDT] ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ãŒä¸ä¸€è‡´")
                return b''.join(streams)
            
            # å…ƒã®ãƒã‚¤ãƒˆé…åˆ—ã‚µã‚¤ã‚ºã‚’æŽ¨å®š
            total_elements = sum(len(stream) for stream in streams) // 4
            byte_view = np.zeros((total_elements, 4), dtype=np.uint8)
            
            # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ãƒã‚¤ãƒˆä½ç½®ã‚’å¾©å…ƒ
            for cluster_id, (stream, cluster_info) in enumerate(zip(streams, clusters)):
                positions = cluster_info['positions']
                stream_data = np.frombuffer(stream, dtype=np.uint8)
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’å„ãƒã‚¤ãƒˆä½ç½®ã«åˆ†æ•£é…ç½®
                elements_per_position = len(stream_data) // len(positions)
                
                for i, pos in enumerate(positions):
                    start_idx = i * elements_per_position
                    end_idx = (i + 1) * elements_per_position
                    if i == len(positions) - 1:  # æœ€å¾Œã®ä½ç½®ã¯æ®‹ã‚Šã™ã¹ã¦
                        end_idx = len(stream_data)
                    
                    position_data = stream_data[start_idx:end_idx]
                    if len(position_data) == total_elements:
                        byte_view[:, pos] = position_data
                    else:
                        # ã‚µã‚¤ã‚ºèª¿æ•´
                        min_len = min(len(position_data), total_elements)
                        byte_view[:min_len, pos] = position_data[:min_len]
            
            return byte_view.tobytes()
            
        except Exception as e:
            print(f"    [TDT] çµ±è¨ˆçš„é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _legacy_inverse_transform(self, streams: List[bytes]) -> bytes:
        """å¾“æ¥æ–¹å¼ã®é€†å¤‰æ›ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        try:
            if len(streams) != 4:
                return streams[0] if streams else b''
            
            stream_lengths = [len(s) for s in streams]
            if len(set(stream_lengths)) != 1:
                return b''.join(streams)
            
            num_floats = stream_lengths[0]
            byte_view = np.empty((num_floats, 4), dtype=np.uint8)
            
            for i, stream in enumerate(streams):
                byte_view[:, i] = np.frombuffer(stream, dtype=np.uint8)
            
            return byte_view.tobytes()
            
        except Exception:
            return b''.join(streams)
    
    def _calculate_stream_entropy(self, stream: np.ndarray) -> float:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        try:
            byte_counts = np.bincount(stream, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(stream)
            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 8.0

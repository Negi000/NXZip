#!/usr/bin/env python3
"""
NEXUS TMC Engine v9.1 - æ¬¡ä¸–ä»£ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼åœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
Transform-Model-Code åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ TMC v9.1
é©æ–°çš„ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆ + åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±åˆ
"""

import os
import sys
import time
import asyncio
import multiprocessing as mp
from typing import Tuple, Dict, Any, List, Optional, Union

# TMC v9.1 åˆ†é›¢ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .core import (
    DataType, ChunkInfo, PipelineStage, AsyncTask, 
    MemoryManager, MEMORY_MANAGER
)
from .analyzers import calculate_entropy, MetaAnalyzer
from .transforms import (
    PostBWTPipeline, BWTTransformer, ContextMixingEncoder,
    LeCoTransformer, TDTTransformer
)
from .parallel import ParallelPipelineProcessor
from .utils import SublinearLZ77Encoder, TMCv8Container

# TMC v9.1 å®šæ•°
TMC_V91_MAGIC = b'TMC91'
DEFAULT_CHUNK_SIZE = 2 * 1024 * 1024  # 2MB per chunk
MAX_WORKERS = min(8, mp.cpu_count())

__all__ = ['NEXUSTMCEngineV91']


class ImprovedDispatcher:
    """æ”¹è‰¯ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£ãƒ¼"""
    
    def dispatch_data_type(self, data: bytes) -> DataType:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®é«˜é€Ÿåˆ¤å®š"""
        if len(data) < 16:
            return DataType.GENERIC_BINARY
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ¤å®š
        try:
            text = data.decode('utf-8', errors='strict')
            if len(set(text)) < len(text) * 0.6:  # 60%ä»¥ä¸Šã®æ–‡å­—ãŒé‡è¤‡
                return DataType.TEXT_REPETITIVE
            else:
                return DataType.TEXT_NATURAL
        except UnicodeDecodeError:
            pass
        
        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®åˆ¤å®š
        if len(data) % 4 == 0:
            # æµ®å‹•å°æ•°ç‚¹æ•°ã¨ã—ã¦è©•ä¾¡
            entropy = calculate_entropy(data[:1024])  # åˆ†é›¢ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨
            if entropy < 6.0:
                return DataType.FLOAT_ARRAY
            elif entropy < 7.0:
                return DataType.SEQUENTIAL_INT
        
        return DataType.GENERIC_BINARY


class CoreCompressor:
    """ã‚³ã‚¢åœ§ç¸®æ©Ÿèƒ½"""
    
    def __init__(self):
        self.compression_methods = ['zlib', 'lzma', 'bz2']
    
    def compress_core(self, data: bytes, method: str = 'zlib') -> Tuple[bytes, Dict[str, Any]]:
        """åŸºæœ¬åœ§ç¸®æ©Ÿèƒ½"""
        try:
            if method == 'zlib':
                import zlib
                compressed = zlib.compress(data, level=6)
            elif method == 'lzma':
                import lzma
                compressed = lzma.compress(data, preset=6)
            elif method == 'bz2':
                import bz2
                compressed = bz2.compress(data, compresslevel=6)
            else:
                compressed = data
            
            info = {
                'method': method,
                'original_size': len(data),
                'compressed_size': len(compressed),
                'compression_ratio': (1 - len(compressed) / len(data)) * 100 if len(data) > 0 else 0
            }
            
            return compressed, info
        
        except Exception as e:
            return data, {'method': 'store', 'error': str(e)}


class NEXUSTMCEngineV91:
    """
    NEXUS TMC Engine v9.1 - ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆçµ±åˆç‰ˆ
    æ¬¡ä¸–ä»£é‡å­ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
    Transform-Model-Code åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ TMC v9.1
    
    v9.1é©æ–°æ©Ÿèƒ½:
    - å®Œå…¨ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆã«ã‚ˆã‚‹ä¿å®ˆæ€§å‘ä¸Š
    - åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆç®¡ç†
    - Numba JITæœ€é©åŒ–ã®æº–å‚™å®Œäº†
    - ä¸¦åˆ—å‡¦ç†ã®å®Œå…¨çµ±åˆ
    """
    
    def __init__(self, max_workers: int = None, chunk_size: int = DEFAULT_CHUNK_SIZE, 
                 lightweight_mode: bool = False):
        self.max_workers = max_workers or MAX_WORKERS
        self.chunk_size = chunk_size
        self.lightweight_mode = lightweight_mode
        self.memory_manager = MEMORY_MANAGER
        
        # è»½é‡ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸè¨­å®šèª¿æ•´
        if lightweight_mode:
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰: ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã‚’æœ€å°åŒ–
            self.max_workers = min(4, self.max_workers)  # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°åˆ¶é™
            self.chunk_size = min(1024 * 1024, chunk_size)  # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºåˆ¶é™ (1MB)
            context_lightweight = True
            print("âš¡ è»½é‡ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹: ãƒ¡ãƒ¢ãƒªãƒ»CPUä½¿ç”¨é‡æœ€é©åŒ–")
        else:
            # æ¨™æº–ãƒ¢ãƒ¼ãƒ‰: æœ€å¤§æ€§èƒ½è¿½æ±‚
            context_lightweight = False
            print("ğŸš€ æ¨™æº–ãƒ¢ãƒ¼ãƒ‰: æœ€å¤§æ€§èƒ½ãƒ»åœ§ç¸®ç‡è¿½æ±‚")
        
        # åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.dispatcher = ImprovedDispatcher()
        self.core_compressor = CoreCompressor()
        self.meta_analyzer = MetaAnalyzer(self.core_compressor)
        
        # å¤‰æ›å™¨ã®åˆæœŸåŒ–ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰ã«å¯¾å¿œï¼‰
        self.bwt_transformer = BWTTransformer()
        self.context_mixer = ContextMixingEncoder(lightweight_mode=context_lightweight)
        self.leco_transformer = LeCoTransformer()
        self.tdt_transformer = TDTTransformer()
        
        # ä¸¦åˆ—å‡¦ç†ã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
        self.pipeline_processor = ParallelPipelineProcessor(max_workers=self.max_workers)
        self.sublinear_lz77 = SublinearLZ77Encoder()
        
        # å¤‰æ›å™¨ãƒãƒƒãƒ”ãƒ³ã‚°
        self.transformers = {
            DataType.FLOAT_ARRAY: self.tdt_transformer,
            DataType.TEXT_REPETITIVE: self.bwt_transformer,
            DataType.TEXT_NATURAL: self.bwt_transformer,
            DataType.SEQUENTIAL_INT: self.leco_transformer,
            DataType.GENERIC_BINARY: None
        }
        
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'reversibility_tests_passed': 0,
            'reversibility_tests_total': 0,
            'transforms_applied': 0,
            'transforms_bypassed': 0,
            'chunks_processed': 0,
            'parallel_efficiency': 0.0,
            'modular_components_used': 6  # v9.1è¿½åŠ 
        }
        
        print(f"ğŸš€ TMC v9.1 ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†: {self.max_workers}ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼, ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º={chunk_size//1024//1024}MB")
        print(f"ğŸ“¦ åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ: Core, Analyzers, Transforms, Parallel, Utils")
    
    async def compress_tmc_v91_async(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """
        TMC v9.1 ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼éåŒæœŸåœ§ç¸®
        åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ã‚ˆã‚‹çµ±åˆå‡¦ç†
        """
        print("--- TMC v9.1 ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼éåŒæœŸåœ§ç¸®é–‹å§‹ ---")
        start_time = time.time()
        
        try:
            if len(data) == 0:
                return b'', {'method': 'empty', 'compression_time': 0.0}
            
            # ãƒ¡ãƒ¢ãƒªç®¡ç†ãƒã‚§ãƒƒã‚¯ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰ã§ã¯é »ç¹ã«å®Ÿè¡Œï¼‰
            if self.memory_manager.check_memory_pressure():
                self.memory_manager.trigger_memory_cleanup()
            
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰ç”¨ã®è¿½åŠ ãƒ¡ãƒ¢ãƒªç®¡ç†
            if self.lightweight_mode:
                # å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†åˆ¤å®š
                if len(data) > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                    return await self._process_large_file_streaming(data)
            
            # Phase 1: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æï¼ˆåˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä½¿ç”¨ï¼‰
            data_type = self.dispatcher.dispatch_data_type(data)
            print(f"[ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æ] æ¤œå‡º: {data_type.value}")
            
            # Phase 2: é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
            optimal_chunks = self._adaptive_chunking(data)
            print(f"[é©å¿œãƒãƒ£ãƒ³ã‚¯] {len(optimal_chunks)}å€‹ã®æœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ")
            
            # Phase 3: å¤‰æ›åŠ¹æœåˆ†æï¼ˆåˆ†é›¢ã•ã‚ŒãŸMetaAnalyzerä½¿ç”¨ï¼‰
            transformer = self.transformers.get(data_type)
            should_transform, analysis_info = self.meta_analyzer.should_apply_transform(
                data, transformer, data_type
            )
            
            # Phase 4: éåŒæœŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰ã§ã¯ä¸¦åˆ—å‡¦ç†ã‚’ç„¡åŠ¹åŒ–ï¼ˆpickleã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
            compressed_container = None  # åˆæœŸåŒ–
            
            if self.lightweight_mode:
                # è»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼šã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†
                if should_transform and transformer:
                    processed_results = await self._process_with_transform(
                        optimal_chunks, transformer, data_type
                    )
                    self.stats['transforms_applied'] += 1
                else:
                    # ç›´æ¥åœ§ç¸®
                    processed_results = []
                    for i, chunk in enumerate(optimal_chunks):
                        compressed_chunk, compress_info = self.core_compressor.compress_core(chunk)
                        chunk_info = {
                            'chunk_id': i,
                            'original_size': len(chunk),
                            'compress_info': compress_info,
                            'data_type': data_type.value
                        }
                        processed_results.append((compressed_chunk, chunk_info))
                    self.stats['transforms_bypassed'] += 1
                
                # è»½é‡ãƒ¢ãƒ¼ãƒ‰ç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠåŒ–
                compressed_container = self._create_v91_container(processed_results, {
                    'data_type': data_type.value,
                    'transform_applied': should_transform,
                    'analysis_info': analysis_info,
                    'chunk_count': len(optimal_chunks)
                })
            else:
                # æ¨™æº–ãƒ¢ãƒ¼ãƒ‰ï¼šä¸¦åˆ—å‡¦ç†
                self.pipeline_processor.start_pipeline()
                
                try:
                    if should_transform and transformer:
                        processed_results = await self._process_with_transform(
                            optimal_chunks, transformer, data_type
                        )
                        self.stats['transforms_applied'] += 1
                    else:
                        processed_results = await self.pipeline_processor.process_data_async(
                            optimal_chunks, 'basic_compression'
                        )
                        self.stats['transforms_bypassed'] += 1
                finally:
                    self.pipeline_processor.stop_pipeline()
                
                
                # Phase 5: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°çµ±åˆï¼ˆé«˜åœ§ç¸®ç‡ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
                if len(data) > 32 * 1024 and should_transform:  # 32KBä»¥ä¸Šã‹ã¤å¤‰æ›é©ç”¨æ™‚
                    context_mixed_results = []
                    for chunk_data, chunk_info in processed_results:
                        mixed_streams, mix_info = self.context_mixer.encode(chunk_data)
                        if mix_info.get('compression_improvement', 0) > 5:  # 5%ä»¥ä¸Šæ”¹å–„
                            context_mixed_results.append((mixed_streams, mix_info))
                        else:
                            context_mixed_results.append((chunk_data, chunk_info))
                    processed_results = context_mixed_results
                
                # Phase 6: çµæœçµ±åˆã¨ã‚³ãƒ³ãƒ†ãƒŠåŒ–
                compressed_container = self._create_v91_container(processed_results, {
                    'data_type': data_type.value,
                    'transform_applied': should_transform,
                    'analysis_info': analysis_info,
                    'chunk_count': len(optimal_chunks)
                })
            
            total_time = time.time() - start_time
            
            # çµ±è¨ˆæ›´æ–°
            compression_ratio = (1 - len(compressed_container) / len(data)) * 100 if len(data) > 0 else 0
            throughput = (len(data) / (1024 * 1024) / total_time) if total_time > 0 else 0  # MB/s
            
            pipeline_stats = self.pipeline_processor.get_performance_stats()
            
            compression_info = {
                'engine_version': 'TMC v9.1 Modular',
                'original_size': len(data),
                'compressed_size': len(compressed_container),
                'compression_ratio': compression_ratio,
                'compression_time': total_time,
                'throughput_mbps': throughput,
                'data_type': data_type.value,
                'transform_applied': should_transform,
                'chunks_processed': len(optimal_chunks),
                'pipeline_stats': pipeline_stats,
                'modular_components': {
                    'core': True,
                    'analyzers': True,
                    'transforms': True,
                    'parallel': True,
                    'utils': True
                }
            }
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['files_processed'] += 1
            self.stats['total_input_size'] += len(data)
            self.stats['total_compressed_size'] += len(compressed_container)
            self.stats['chunks_processed'] += len(optimal_chunks)
            
            print(f"âœ… TMC v9.1 åœ§ç¸®å®Œäº†: {compression_ratio:.1f}% åœ§ç¸®, {throughput:.1f}MB/s")
            
            # è§£å‡ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
            import json
            container_metadata = {
                'data_type': data_type.value,
                'transform_applied': should_transform,
                'analysis_info': analysis_info,
                'chunk_count': len(optimal_chunks)
            }
            
            compression_info['method'] = 'tmc_v91'
            compression_info['chunks'] = []
            compression_info['container_metadata'] = container_metadata
            
            # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã®è¨˜éŒ²
            header_json = json.dumps({'magic': TMC_V91_MAGIC.decode('latin-1'), 'version': '9.1', 'chunk_count': len(processed_results), 'metadata': container_metadata}, separators=(',', ':')).encode('utf-8')
            current_pos = 8 + len(header_json)
            
            for i, (chunk_data, chunk_info) in enumerate(processed_results):
                # å¤‰æ›æƒ…å ±ã‹ã‚‰è©³ç´°ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                transform_info = chunk_info.get('transform_info', {})
                
                chunk_meta = {
                    'chunk_id': i,
                    'start_pos': current_pos,
                    'compressed_size': len(chunk_data),
                    'original_size': chunk_info.get('original_size', 0),
                    'transforms': self._extract_transform_sequence(chunk_info),
                    'stream_count': transform_info.get('stream_count', 1),
                    'enhanced_pipeline': transform_info.get('enhanced_pipeline', False)
                }
                compression_info['chunks'].append(chunk_meta)
                current_pos += 4 + len(chunk_data)  # size prefix + data
            
            return compressed_container, compression_info
            
        except Exception as e:
            print(f"âŒ TMC v9.1 åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            fallback_compressed, fallback_info = self.core_compressor.compress_core(data, 'zlib')
            fallback_info['engine_version'] = 'TMC v9.1 Fallback'
            fallback_info['error'] = str(e)
            return fallback_compressed, fallback_info
    
    async def _process_with_transform(self, chunks: List[bytes], transformer, data_type: DataType) -> List[Tuple[bytes, Dict]]:
        """å¤‰æ›ä»˜ãã®å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        processed_results = []
        
        for i, chunk in enumerate(chunks):
            try:
                # å¤‰æ›å®Ÿè¡Œï¼ˆåˆ†é›¢ã•ã‚ŒãŸTransformerãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨ï¼‰
                transformed_streams, transform_info = transformer.transform(chunk)
                
                # å¤‰æ›å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®
                if isinstance(transformed_streams, list):
                    combined_data = b''.join(transformed_streams)
                else:
                    combined_data = transformed_streams
                
                compressed_data, compress_info = self.core_compressor.compress_core(combined_data)
                
                # æƒ…å ±çµ±åˆï¼ˆBWTæƒ…å ±ã®æ­£è¦åŒ–ï¼‰
                normalized_transform_info = transform_info.copy()
                
                # BWTå›ºæœ‰æƒ…å ±ã®æ­£è¦åŒ–
                if 'primary_index' in transform_info:
                    normalized_transform_info['bwt_index'] = transform_info['primary_index']
                    normalized_transform_info['bwt_applied'] = True
                
                # MTFæƒ…å ±ã®æ­£è¦åŒ–
                if 'zero_ratio' in transform_info:
                    normalized_transform_info['mtf_zero_ratio'] = transform_info['zero_ratio']
                
                result_info = {
                    'chunk_id': i,
                    'original_size': len(chunk),
                    'transform_info': normalized_transform_info,
                    'compress_info': compress_info,
                    'data_type': data_type.value
                }
                
                processed_results.append((compressed_data, result_info))
                
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç›´æ¥åœ§ç¸®
                compressed_data, compress_info = self.core_compressor.compress_core(chunk)
                result_info = {
                    'chunk_id': i,
                    'error': str(e),
                    'compress_info': compress_info,
                    'data_type': data_type.value
                }
                processed_results.append((compressed_data, result_info))
        
        return processed_results
    
    async def _process_large_file_streaming(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†"""
        print(f"[ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°] å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {len(data) // (1024*1024)}MB")
        
        # ã‚ˆã‚Šå°ã•ãªãƒãƒ£ãƒ³ã‚¯ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
        stream_chunk_size = 512 * 1024  # 512KB chunks for streaming
        streaming_results = []
        
        for i in range(0, len(data), stream_chunk_size):
            chunk = data[i:i + stream_chunk_size]
            
            # ãƒ¡ãƒ¢ãƒªåœ§è¿«ãƒã‚§ãƒƒã‚¯
            if self.memory_manager.check_memory_pressure():
                self.memory_manager.trigger_memory_cleanup()
            
            # è»½é‡åœ§ç¸®å‡¦ç†
            compressed_chunk, chunk_info = self.core_compressor.compress_core(chunk, 'zlib')
            streaming_results.append((compressed_chunk, chunk_info))
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
            if i % (stream_chunk_size * 10) == 0:
                progress = (i / len(data)) * 100
                print(f"  [ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°] é€²æ—: {progress:.1f}%")
        
        # çµæœã‚’çµ±åˆ
        combined_data = b''.join(result[0] for result in streaming_results)
        
        streaming_info = {
            'engine_version': 'TMC v9.1 Streaming',
            'original_size': len(data),
            'compressed_size': len(combined_data),
            'compression_ratio': (1 - len(combined_data) / len(data)) * 100,
            'streaming_chunks': len(streaming_results),
            'lightweight_mode': True
        }
        
        print(f"[ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°] å®Œäº†: {streaming_info['compression_ratio']:.1f}% åœ§ç¸®")
        return combined_data, streaming_info
    
    def _adaptive_chunking(self, data: bytes) -> List[bytes]:
        """é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        if len(data) <= self.chunk_size:
            return [data]
        
        chunks = []
        for i in range(0, len(data), self.chunk_size):
            chunk = data[i:i + self.chunk_size]
            chunks.append(chunk)
        
        return chunks
    
    def _extract_transform_sequence(self, chunk_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‹ã‚‰å¤‰æ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        transforms = []
        
        # å¤‰æ›æƒ…å ±ã®å–å¾—
        transform_info = chunk_info.get('transform_info', {})
        compress_info = chunk_info.get('compress_info', {})
        
        # BWTå¤‰æ›ã®ç¢ºèª
        if transform_info.get('bwt_applied', False) or 'bwt_index' in transform_info:
            transforms.append({
                'type': 'bwt',
                'bwt_index': transform_info.get('bwt_index', 0),
                'mtf_zero_ratio': transform_info.get('mtf_zero_ratio', 0)
            })
            print(f"    ğŸ“ BWTå¤‰æ›è¨˜éŒ²: index={transform_info.get('bwt_index', 0)}")
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°
        if chunk_info.get('context_mixed', False):
            transforms.append({
                'type': 'context_mixing',
                'compression_improvement': chunk_info.get('compression_improvement', 0)
            })
        
        # LZ77åœ§ç¸®
        if transform_info.get('lz77_applied', False):
            transforms.append({
                'type': 'lz77',
                'compression_ratio': transform_info.get('lz77_ratio', 0)
            })
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ï¼ˆå¸¸ã«æœ€å¾Œï¼‰
        entropy_method = compress_info.get('method', 'zlib')
        transforms.append({
            'type': 'entropy',
            'method': entropy_method
        })
        print(f"    ğŸ“ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–è¨˜éŒ²: method={entropy_method}")
        
        return transforms

    def _create_v91_container(self, processed_results: List[Tuple[bytes, Dict]], metadata: Dict) -> bytes:
        """TMC v9.1 ã‚³ãƒ³ãƒ†ãƒŠä½œæˆ"""
        try:
            import json
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
            header = {
                'magic': TMC_V91_MAGIC.decode('latin-1'),
                'version': '9.1',
                'chunk_count': len(processed_results),
                'metadata': metadata
            }
            
            header_json = json.dumps(header, separators=(',', ':')).encode('utf-8')
            header_size = len(header_json).to_bytes(4, 'big')
            
            # ãƒ‡ãƒ¼ã‚¿éƒ¨ä½œæˆ
            data_parts = [header_size, header_json]
            
            for compressed_data, info in processed_results:
                chunk_size = len(compressed_data).to_bytes(4, 'big')
                data_parts.append(chunk_size)
                data_parts.append(compressed_data)
            
            return b''.join(data_parts)
            
        except Exception as e:
            print(f"ã‚³ãƒ³ãƒ†ãƒŠä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”çµåˆ
            return b''.join(result[0] for result in processed_results)
    
    def compress_sync(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """åŒæœŸç‰ˆåœ§ç¸®ï¼ˆéåŒæœŸç‰ˆã®ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰"""
        try:
            # éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œ
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(self.compress_tmc_v91_async(data))
                return result
            finally:
                loop.close()
        except Exception as e:
            print(f"åŒæœŸåœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return self.core_compressor.compress_core(data, 'zlib')
    
    def get_stats(self) -> Dict[str, Any]:
        """ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆå–å¾—"""
        stats = self.stats.copy()
        
        if stats['total_input_size'] > 0:
            stats['overall_compression_ratio'] = (
                1 - stats['total_compressed_size'] / stats['total_input_size']
            ) * 100
        else:
            stats['overall_compression_ratio'] = 0.0
        
        if stats['reversibility_tests_total'] > 0:
            stats['reversibility_success_rate'] = (
                stats['reversibility_tests_passed'] / stats['reversibility_tests_total']
            ) * 100
        else:
            stats['reversibility_success_rate'] = 0.0
        
        return stats
    
    def compress(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """æ¨™æº–åœ§ç¸®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆåŒæœŸç‰ˆã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰"""
        return self.compress_sync(data)
    
    def decompress(self, compressed_data: bytes, info: Dict[str, Any]) -> bytes:
        """TMC v9.1å®Œå…¨è§£å‡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        try:
            # TMC v9.1å°‚ç”¨è§£å‡å‡¦ç†
            return self._decompress_tmc_format(compressed_data, info)
        except Exception as e:
            print(f"TMC v9.1è§£å‡ã‚¨ãƒ©ãƒ¼: {e}")
            # åŸºæœ¬çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆzlibç­‰ï¼‰
            try:
                import zlib
                return zlib.decompress(compressed_data)
            except:
                raise ValueError(f"è§£å‡ä¸å¯èƒ½: TMC v9.1ãŠã‚ˆã³æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸¡æ–¹ã§å¤±æ•—")
    
    def _decompress_tmc_format(self, compressed_data: bytes, info: Dict[str, Any]) -> bytes:
        """TMC v9.1å°‚ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡ï¼ˆå…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã«åŸºã¥ãï¼‰"""
        print("ğŸ”„ TMC v9.1è§£å‡é–‹å§‹...")
        
        # åœ§ç¸®æƒ…å ±ã®å–å¾—
        method = info.get('method', 'tmc_v91')
        chunk_info = info.get('chunks', [])
        data_type = info.get('data_type', 'unknown')
        
        if not chunk_info:
            raise ValueError("åœ§ç¸®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        
        print(f"ğŸ“Š è§£å‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {len(chunk_info)} chunks, type={data_type}")
        
        # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã®è§£å‡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
        decompressed_chunks = []
        
        for i, chunk_meta in enumerate(chunk_info):
            print(f"ğŸ”„ Chunk {i+1}/{len(chunk_info)} è§£å‡ä¸­...")
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            start_pos = chunk_meta.get('start_pos', 0)
            chunk_size = chunk_meta.get('compressed_size', len(compressed_data))
            
            if i < len(chunk_info) - 1:
                end_pos = chunk_info[i+1].get('start_pos', len(compressed_data))
            else:
                end_pos = len(compressed_data)
            
            chunk_data = compressed_data[start_pos:end_pos]
            
            # **å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯**ï¼šãƒãƒ£ãƒ³ã‚¯ã¯æœ€çµ‚çš„ã«zlibç­‰ã§åœ§ç¸®ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€
            # ç›´æ¥æ¨™æº–è§£å‡ã‚’å®Ÿè¡Œï¼ˆBWTã®é€†å¤‰æ›ã¯ä¸è¦ï¼‰
            decompressed_chunk = self._decompress_chunk_simple(chunk_data, chunk_meta)
            decompressed_chunks.append(decompressed_chunk)
        
        # å…¨ãƒãƒ£ãƒ³ã‚¯ã®çµåˆ
        result = b''.join(decompressed_chunks)
        print(f"âœ… TMC v9.1è§£å‡å®Œäº†: {len(compressed_data)} -> {len(result)} bytes")
        
        return result
    
    def _decompress_chunk_simple(self, chunk_data: bytes, chunk_meta: Dict[str, Any]) -> bytes:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®è§£å‡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ - å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        transforms = chunk_meta.get('transforms', [])
        
        # æœ€å¾Œã®å¤‰æ›ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ï¼‰ã®ã¿ã‚’é€†å¤‰æ›
        # BWTãªã©ã®å¤‰æ›ã¯åœ§ç¸®ãƒ—ãƒ­ã‚»ã‚¹å†…ã§å‡¦ç†æ¸ˆã¿
        for transform in reversed(transforms):
            transform_type = transform.get('type')
            
            if transform_type == 'entropy':
                # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ã®é€†å¤‰æ›ã®ã¿å®Ÿè¡Œ
                return self._reverse_entropy_coding(chunk_data, transform)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãã®ã¾ã¾è¿”ã™
        return chunk_data
    
    def _reverse_context_mixing(self, data: bytes, meta: Dict[str, Any]) -> bytes:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°é€†å¤‰æ›"""
        # ç°¡æ˜“å®Ÿè£…ï¼šåŸºæœ¬çš„ãªå¯é€†å¤‰æ›
        print("  ğŸ”„ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°é€†å¤‰æ›")
        try:
            import zlib
            return zlib.decompress(data)
        except:
            return data
    
    def _reverse_bwt_transform(self, data: bytes, meta: Dict[str, Any]) -> bytes:
        """BWTé€†å¤‰æ›ï¼ˆå®Œå…¨å®Ÿè£…ï¼‰"""
        print("  ğŸ”„ BWTé€†å¤‰æ›")
        
        try:
            import struct
            
            # BWTã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å–å¾—
            bwt_index = meta.get('bwt_index', 0)
            
            # ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã‚‹å ´åˆã¯ãã®ã¾ã¾è¿”ã™
            if len(data) < 4:
                print(f"    âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã¾ã™: {len(data)} bytes")
                return data
            
            # BWTãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®è§£æ
            # [4bytes: primary_index] + [post_bwt_streams data]
            try:
                # primary_indexã®å–å¾—ï¼ˆ4ãƒã‚¤ãƒˆ big-endianï¼‰
                primary_index = int.from_bytes(data[:4], 'big')
                post_bwt_data = data[4:]
                
                print(f"    ğŸ“Š BWTè§£æ: primary_index={primary_index}, post_bwt_size={len(post_bwt_data)}")
                
                # ãƒã‚¹ãƒˆBWTãƒ‡ãƒ¼ã‚¿ã‹ã‚‰RLEå¾©å…ƒ
                rle_restored = self._reverse_post_bwt_pipeline(post_bwt_data)
                print(f"    ğŸ“ˆ ãƒã‚¹ãƒˆBWTå¾©å…ƒ: {len(post_bwt_data)} -> {len(rle_restored)}")
                
                # BWTé€†å¤‰æ›ï¼ˆBWTTransformerä½¿ç”¨ï¼‰
                if self.bwt_transformer:
                    # BWTTransformerã®inverse_transformå½¢å¼ã§å‘¼ã³å‡ºã—
                    bwt_streams = [rle_restored]  # Listå½¢å¼
                    bwt_info = {'primary_index': primary_index}
                    result = self.bwt_transformer.inverse_transform(bwt_streams, bwt_info)
                    print(f"    âœ… BWTTransformeré€†å¤‰æ›: {len(rle_restored)} -> {len(result)}")
                    return result
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ‰‹å‹•MTF+BWTé€†å¤‰æ›
                    mtf_reversed = self._reverse_mtf(rle_restored)
                    print(f"    ğŸ”„ MTFé€†å¤‰æ›å®Œäº†: {len(rle_restored)} -> {len(mtf_reversed)}")
                    
                    result = self._simple_inverse_bwt(mtf_reversed, primary_index)
                    print(f"    âœ… Simple BWTé€†å¤‰æ›: {len(mtf_reversed)} -> {len(result)}")
                    return result
                    
            except (struct.error, ValueError) as e:
                print(f"    âš ï¸ BWTè§£æã‚¨ãƒ©ãƒ¼: {e}")
                return data
                
        except Exception as e:
            print(f"    âš ï¸ BWTé€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return data
    
    def _reverse_post_bwt_pipeline(self, data: bytes) -> bytes:
        """ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€†å¤‰æ›"""
        try:
            # ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®RLEé€†å¤‰æ›
            if self.bwt_transformer and hasattr(self.bwt_transformer, 'post_bwt_pipeline'):
                # List[bytes]å½¢å¼ã§æ¸¡ã™å¿…è¦ãŒã‚ã‚‹
                streams = [data]
                return self.bwt_transformer.post_bwt_pipeline.decode(streams)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªRLEé€†å¤‰æ›ã‚’è©¦è¡Œ
                return self._simple_reverse_post_bwt(data)
        except Exception as e:
            print(f"    âš ï¸ ãƒã‚¹ãƒˆBWTå¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            return data
    
    def _simple_reverse_post_bwt(self, data: bytes) -> bytes:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒã‚¹ãƒˆBWTé€†å¤‰æ›"""
        # åŸºæœ¬çš„ãªå®Ÿè£…ï¼šãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™
        # å®Ÿéš›ã®RLEå¾©å…ƒã¯è¤‡é›‘ãªãŸã‚ã€å¾Œã§å®Ÿè£…
        return data
    
    def _reverse_rle(self, literals: bytes, runs: bytes) -> bytes:
        """RLEé€†å¤‰æ›"""
        if len(literals) != len(runs):
            return literals  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        result = bytearray()
        for i in range(len(literals)):
            literal = literals[i]
            run_length = runs[i] if i < len(runs) else 1
            result.extend([literal] * run_length)
        
        return bytes(result)
    
    def _reverse_mtf(self, data: bytes) -> bytes:
        """MTFé€†å¤‰æ›"""
        if not data:
            return data
            
        # MTFè¾æ›¸ã®åˆæœŸåŒ–
        mtf_dict = list(range(256))
        result = bytearray()
        
        for byte in data:
            # è¾æ›¸ã‹ã‚‰å€¤ã‚’å–å¾—
            original_value = mtf_dict[byte]
            result.append(original_value)
            
            # è¾æ›¸ã‚’æ›´æ–°ï¼ˆfront-to-moveã«ç§»å‹•ï¼‰
            mtf_dict.pop(byte)
            mtf_dict.insert(0, original_value)
        
        return bytes(result)
    
    def _simple_inverse_bwt(self, bwt_data: bytes, index: int) -> bytes:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªBWTé€†å¤‰æ›"""
        try:
            n = len(bwt_data)
            if n == 0 or index >= n:
                return bwt_data
            
            # ã‚½ãƒ¼ãƒˆæ¸ˆã¿ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹ç¯‰
            sorted_rotations = sorted(enumerate(bwt_data), key=lambda x: x[1])
            
            # æ¬¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹ç¯‰
            next_table = [0] * n
            for i, (original_pos, _) in enumerate(sorted_rotations):
                next_table[original_pos] = i
            
            # å…ƒã®æ–‡å­—åˆ—ã®å¾©å…ƒ
            result = bytearray()
            current = index
            for _ in range(n):
                result.append(bwt_data[current])
                current = next_table[current]
            
            return bytes(result)
        except:
            return bwt_data
    
    def _reverse_lz77(self, data: bytes, meta: Dict[str, Any]) -> bytes:
        """LZ77é€†å¤‰æ›"""
        print("  ğŸ”„ LZ77é€†å¤‰æ›")
        # ç¾åœ¨ã¯æœªå®Ÿè£… - åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return data
    
    def _reverse_entropy_coding(self, data: bytes, meta: Dict[str, Any]) -> bytes:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–é€†å¤‰æ›ï¼ˆå®Œå…¨å®Ÿè£…ï¼‰"""
        print("  ğŸ”„ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–é€†å¤‰æ›")
        
        try:
            method = meta.get('method', 'zlib')
            print(f"    ğŸ“Š è§£å‡æ–¹å¼: {method}")
            
            if method == 'zlib':
                import zlib
                result = zlib.decompress(data)
                print(f"    âœ… zlibè§£å‡: {len(data)} -> {len(result)} bytes")
                return result
                
            elif method == 'lzma':
                import lzma
                result = lzma.decompress(data)
                print(f"    âœ… LZMAè§£å‡: {len(data)} -> {len(result)} bytes")
                return result
                
            elif method == 'bz2':
                import bz2
                result = bz2.decompress(data)
                print(f"    âœ… BZ2è§£å‡: {len(data)} -> {len(result)} bytes")
                return result
                
            else:
                print(f"    âš ï¸ æœªå¯¾å¿œæ–¹å¼: {method}")
                return data
                
        except Exception as e:
            print(f"    âš ï¸ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return data


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
TMCEngine = NEXUSTMCEngineV91

if __name__ == "__main__":
    print("ğŸš€ NEXUS TMC Engine v9.1 - ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆç‰ˆ")
    print("ğŸ“¦ åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±åˆå®Œäº†")

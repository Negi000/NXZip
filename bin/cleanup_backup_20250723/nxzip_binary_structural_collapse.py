#!/usr/bin/env python3
"""
NXZip Binary-Level Structural Collapse Engine
ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«æ§‹é€ å´©å£Šã‚¨ãƒ³ã‚¸ãƒ³ - å®Œå…¨å¯é€†æ€§ä¿è¨¼

ç‰¹å¾´:
- ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«è©³ç´°è§£æ
- å…ƒçŠ¶æ…‹ã®å®Œå…¨ä¿å­˜
- ãƒ‡ãƒ¼ã‚¿æ§‹é€ å´©å£Šã«ã‚ˆã‚‹æ¥µé™åœ§ç¸®
- å…ƒçŠ¶æ…‹ãƒ™ãƒ¼ã‚¹å®Œå…¨å¾©å…ƒ
- 100%å¯é€†æ€§ä¿è¨¼
"""

import struct
import time
import hashlib
import os
import sys
import zlib
import pickle
from typing import List, Tuple, Dict, Optional
from collections import defaultdict
import math

class BinaryStructuralCollapseEngine:
    def __init__(self):
        self.magic = b'NXBSC'  # NXZip Binary Structural Collapse
        self.version = 1
        
    def deep_binary_analysis(self, data: bytes) -> Dict:
        """ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«æ·±å±¤è§£æ"""
        analysis = {
            'size': len(data),
            'md5_hash': hashlib.md5(data).hexdigest(),
            'byte_frequency': [0] * 256,
            'entropy_regions': [],
            'pattern_map': {},
            'structural_markers': [],
            'correlation_matrix': {},
            'sequence_patterns': {}
        }
        
        # ãƒã‚¤ãƒˆé »åº¦è§£æ
        for byte in data:
            analysis['byte_frequency'][byte] += 1
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åœ°åŸŸè§£æï¼ˆ1KBå˜ä½ï¼‰
        chunk_size = 1024
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i+chunk_size]
            if chunk:
                entropy = self.calculate_entropy(chunk)
                analysis['entropy_regions'].append({
                    'offset': i,
                    'size': len(chunk),
                    'entropy': entropy,
                    'dominant_bytes': self.get_dominant_bytes(chunk)
                })
        
        # æ§‹é€ ãƒãƒ¼ã‚«ãƒ¼æ¤œå‡º
        analysis['structural_markers'] = self.detect_structural_markers(data)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ—æ§‹ç¯‰
        analysis['pattern_map'] = self.build_pattern_map(data)
        
        # ãƒã‚¤ãƒˆç›¸é–¢è§£æ
        analysis['correlation_matrix'] = self.analyze_byte_correlations(data)
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
        analysis['sequence_patterns'] = self.analyze_sequence_patterns(data)
        
        print(f"ğŸ”¬ ãƒã‚¤ãƒŠãƒªæ·±å±¤è§£æå®Œäº†: {len(data):,} bytes analyzed")
        print(f"ğŸ“Š ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åœ°åŸŸ: {len(analysis['entropy_regions'])} regions")
        print(f"ğŸ—ï¸  æ§‹é€ ãƒãƒ¼ã‚«ãƒ¼: {len(analysis['structural_markers'])} markers")
        print(f"ğŸ§© ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(analysis['pattern_map'])} unique patterns")
        
        return analysis
    
    def calculate_entropy(self, data: bytes) -> float:
        """ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if not data:
            return 0.0
        
        freq = defaultdict(int)
        for byte in data:
            freq[byte] += 1
        
        entropy = 0.0
        total = len(data)
        for count in freq.values():
            if count > 0:
                prob = count / total
                entropy -= prob * math.log2(prob)
        
        return entropy
    
    def get_dominant_bytes(self, data: bytes) -> List[int]:
        """æ”¯é…çš„ãƒã‚¤ãƒˆæ¤œå‡º"""
        freq = defaultdict(int)
        for byte in data:
            freq[byte] += 1
        
        # é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_bytes = sorted(freq.items(), key=lambda x: x[1], reverse=True)
        return [byte for byte, count in sorted_bytes[:5]]  # ä¸Šä½5ã¤
    
    def detect_structural_markers(self, data: bytes) -> List[Dict]:
        """æ§‹é€ ãƒãƒ¼ã‚«ãƒ¼æ¤œå‡º"""
        markers = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ç½²åæ¤œå‡º
        signatures = [
            (b'\x89PNG\r\n\x1a\n', 'PNG_SIGNATURE'),
            (b'\xff\xd8\xff', 'JPEG_SOI'),
            (b'PK\x03\x04', 'ZIP_LOCAL_HEADER'),
            (b'%PDF', 'PDF_HEADER'),
            (b'\x1f\x8b', 'GZIP_HEADER'),
            (b'RIFF', 'RIFF_HEADER'),
            (b'\x00\x00\x01', 'MPEG_START_CODE'),
        ]
        
        for sig, name in signatures:
            pos = data.find(sig)
            if pos != -1:
                markers.append({
                    'type': name,
                    'offset': pos,
                    'size': len(sig),
                    'data': sig
                })
        
        # ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        for pattern_len in [2, 3, 4, 8, 16]:
            self.detect_repeating_patterns(data, pattern_len, markers)
        
        # ã‚¼ãƒ­å¡«å……é ˜åŸŸæ¤œå‡º
        self.detect_zero_regions(data, markers)
        
        return markers
    
    def detect_repeating_patterns(self, data: bytes, pattern_len: int, markers: List[Dict]):
        """ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        pattern_positions = defaultdict(list)
        
        for i in range(len(data) - pattern_len + 1):
            pattern = data[i:i+pattern_len]
            pattern_positions[pattern].append(i)
        
        # 3å›ä»¥ä¸Šç¹°ã‚Šè¿”ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒãƒ¼ã‚«ãƒ¼ã¨ã—ã¦è¿½åŠ 
        for pattern, positions in pattern_positions.items():
            if len(positions) >= 3:
                markers.append({
                    'type': f'REPEAT_PATTERN_{pattern_len}',
                    'pattern': pattern,
                    'positions': positions,
                    'count': len(positions)
                })
    
    def detect_zero_regions(self, data: bytes, markers: List[Dict]):
        """ã‚¼ãƒ­å¡«å……é ˜åŸŸæ¤œå‡º"""
        in_zero_region = False
        start_pos = 0
        
        for i, byte in enumerate(data):
            if byte == 0:
                if not in_zero_region:
                    in_zero_region = True
                    start_pos = i
            else:
                if in_zero_region:
                    length = i - start_pos
                    if length >= 8:  # 8ãƒã‚¤ãƒˆä»¥ä¸Šã®ã‚¼ãƒ­é ˜åŸŸ
                        markers.append({
                            'type': 'ZERO_REGION',
                            'offset': start_pos,
                            'size': length
                        })
                    in_zero_region = False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã®ã‚¼ãƒ­é ˜åŸŸ
        if in_zero_region:
            length = len(data) - start_pos
            if length >= 8:
                markers.append({
                    'type': 'ZERO_REGION',
                    'offset': start_pos,
                    'size': length
                })
    
    def build_pattern_map(self, data: bytes) -> Dict:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ—æ§‹ç¯‰"""
        pattern_map = {}
        
        # 2-8ãƒã‚¤ãƒˆã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è§£æ
        for pattern_len in range(2, 9):
            if len(data) < pattern_len:
                continue
                
            patterns = defaultdict(int)
            for i in range(len(data) - pattern_len + 1):
                pattern = data[i:i+pattern_len]
                patterns[pattern] += 1
            
            # é »åº¦ã®é«˜ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿å­˜
            frequent_patterns = {p: count for p, count in patterns.items() if count >= 3}
            if frequent_patterns:
                pattern_map[pattern_len] = frequent_patterns
        
        return pattern_map
    
    def analyze_byte_correlations(self, data: bytes) -> Dict:
        """ãƒã‚¤ãƒˆç›¸é–¢è§£æ"""
        correlations = {}
        
        # éš£æ¥ãƒã‚¤ãƒˆç›¸é–¢
        if len(data) > 1:
            adjacent_pairs = defaultdict(int)
            for i in range(len(data) - 1):
                pair = (data[i], data[i+1])
                adjacent_pairs[pair] += 1
            correlations['adjacent'] = dict(adjacent_pairs)
        
        # è·é›¢åˆ¥ç›¸é–¢
        for distance in [2, 4, 8, 16]:
            if len(data) > distance:
                distant_pairs = defaultdict(int)
                for i in range(len(data) - distance):
                    pair = (data[i], data[i+distance])
                    distant_pairs[pair] += 1
                correlations[f'distance_{distance}'] = dict(distant_pairs)
        
        return correlations
    
    def analyze_sequence_patterns(self, data: bytes) -> Dict:
        """ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ"""
        patterns = {}
        
        # å¢—åŠ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ¤œå‡º
        inc_sequences = []
        current_seq = [data[0]] if data else []
        
        for i in range(1, len(data)):
            if data[i] == current_seq[-1] + 1:
                current_seq.append(data[i])
            else:
                if len(current_seq) >= 4:
                    inc_sequences.append({
                        'start': i - len(current_seq),
                        'length': len(current_seq),
                        'type': 'INCREASING'
                    })
                current_seq = [data[i]]
        
        patterns['increasing_sequences'] = inc_sequences
        
        # æ¸›å°‘ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ¤œå‡º
        dec_sequences = []
        current_seq = [data[0]] if data else []
        
        for i in range(1, len(data)):
            if data[i] == current_seq[-1] - 1:
                current_seq.append(data[i])
            else:
                if len(current_seq) >= 4:
                    dec_sequences.append({
                        'start': i - len(current_seq),
                        'length': len(current_seq),
                        'type': 'DECREASING'
                    })
                current_seq = [data[i]]
        
        patterns['decreasing_sequences'] = dec_sequences
        
        return patterns
    
    def structural_collapse(self, data: bytes, analysis: Dict) -> Tuple[bytes, Dict]:
        """ãƒ‡ãƒ¼ã‚¿æ§‹é€ å´©å£Šå‡¦ç†"""
        print(f"ğŸ’¥ æ§‹é€ å´©å£Šé–‹å§‹: {len(data):,} bytes")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: é »åº¦é †ãƒã‚¤ãƒˆå†ãƒãƒƒãƒ”ãƒ³ã‚°
        freq_sorted = sorted(range(256), key=lambda x: analysis['byte_frequency'][x], reverse=True)
        byte_remap = {}
        reverse_remap = {}
        
        for new_val, original_val in enumerate(freq_sorted):
            if analysis['byte_frequency'][original_val] > 0:
                byte_remap[original_val] = new_val
                reverse_remap[new_val] = original_val
        
        remapped_data = bytearray()
        for byte in data:
            remapped_data.append(byte_remap[byte])
        
        print(f"ğŸ”„ ãƒã‚¤ãƒˆå†ãƒãƒƒãƒ”ãƒ³ã‚°: {len(data):,} â†’ {len(remapped_data):,} bytes")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å»
        pattern_removed, pattern_info = self.remove_patterns(bytes(remapped_data), analysis['pattern_map'])
        print(f"ğŸ§© ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å»: {len(remapped_data):,} â†’ {len(pattern_removed):,} bytes")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åœ§ç¸®
        sequence_compressed, sequence_info = self.compress_sequences(pattern_removed, analysis['sequence_patterns'])
        print(f"ğŸ“ˆ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åœ§ç¸®: {len(pattern_removed):,} â†’ {len(sequence_compressed):,} bytes")
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¼ãƒ­é ˜åŸŸåœ§ç¸®
        zero_compressed, zero_info = self.compress_zero_regions(sequence_compressed)
        print(f"âš« ã‚¼ãƒ­åœ§ç¸®: {len(sequence_compressed):,} â†’ {len(zero_compressed):,} bytes")
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: æœ€çµ‚å·®åˆ†å¤‰æ›
        final_data = self.final_differential_transform(zero_compressed)
        print(f"ğŸ”§ å·®åˆ†å¤‰æ›: {len(zero_compressed):,} â†’ {len(final_data):,} bytes")
        
        collapse_info = {
            'byte_remap': reverse_remap,
            'pattern_info': pattern_info,
            'sequence_info': sequence_info,
            'zero_info': zero_info,
            'original_analysis': analysis
        }
        
        print(f"ğŸ’¥ æ§‹é€ å´©å£Šå®Œäº†: {len(data):,} â†’ {len(final_data):,} bytes ({(1-len(final_data)/len(data))*100:.1f}%æ¸›å°‘)")
        return final_data, collapse_info
    
    def remove_patterns(self, data: bytes, pattern_map: Dict) -> Tuple[bytes, Dict]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å»"""
        removed_data = bytearray(data)
        pattern_info = {}
        removed_positions = set()
        
        # é•·ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å‡¦ç†
        for pattern_len in sorted(pattern_map.keys(), reverse=True):
            patterns = pattern_map[pattern_len]
            
            for pattern, count in patterns.items():
                if count < 3:
                    continue
                
                positions = []
                pos = 0
                while pos <= len(removed_data) - pattern_len:
                    if pos not in removed_positions:
                        segment = bytes(removed_data[pos:pos+pattern_len])
                        if segment == pattern:
                            positions.append(pos)
                            # æœ€åˆã®å‡ºç¾ä»¥å¤–ã‚’é™¤å»
                            if len(positions) > 1:
                                for i in range(pattern_len):
                                    removed_positions.add(pos + i)
                    pos += 1
                
                if len(positions) > 1:
                    pattern_info[pattern.hex()] = {
                        'pattern': pattern,
                        'positions': positions,
                        'length': pattern_len
                    }
        
        # é™¤å»ã•ã‚Œã¦ã„ãªã„ãƒã‚¤ãƒˆã®ã¿æ®‹ã™
        final_data = bytearray()
        for i, byte in enumerate(removed_data):
            if i not in removed_positions:
                final_data.append(byte)
        
        return bytes(final_data), pattern_info
    
    def compress_sequences(self, data: bytes, sequence_patterns: Dict) -> Tuple[bytes, Dict]:
        """ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åœ§ç¸®"""
        compressed = bytearray()
        sequence_info = {'compressed_sequences': []}
        i = 0
        
        while i < len(data):
            # å¢—åŠ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ¤œå‡º
            if i < len(data) - 3:
                seq_len = 1
                while (i + seq_len < len(data) and 
                       seq_len < 255 and
                       data[i + seq_len] == (data[i] + seq_len) & 0xFF):
                    seq_len += 1
                
                if seq_len >= 4:
                    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åœ§ç¸®: [0xFE, length, start_value]
                    compressed.extend([0xFE, seq_len, data[i]])
                    sequence_info['compressed_sequences'].append({
                        'type': 'increasing',
                        'start': data[i],
                        'length': seq_len,
                        'original_pos': i
                    })
                    i += seq_len
                    continue
            
            # é€šå¸¸ã®ãƒã‚¤ãƒˆ
            if data[i] == 0xFE:
                compressed.extend([0xFD, 0xFE])  # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
            else:
                compressed.append(data[i])
            i += 1
        
        return bytes(compressed), sequence_info
    
    def compress_zero_regions(self, data: bytes) -> Tuple[bytes, Dict]:
        """ã‚¼ãƒ­é ˜åŸŸåœ§ç¸®"""
        compressed = bytearray()
        zero_info = {'zero_regions': []}
        i = 0
        
        while i < len(data):
            if data[i] == 0:
                # ã‚¼ãƒ­ã®é€£ç¶šã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                zero_count = 0
                j = i
                while j < len(data) and data[j] == 0 and zero_count < 255:
                    zero_count += 1
                    j += 1
                
                if zero_count >= 3:
                    # ã‚¼ãƒ­åœ§ç¸®: [0xFF, count]
                    compressed.extend([0xFF, zero_count])
                    zero_info['zero_regions'].append({
                        'start': i,
                        'length': zero_count
                    })
                    i += zero_count
                else:
                    compressed.append(data[i])
                    i += 1
            else:
                if data[i] == 0xFF:
                    compressed.extend([0xFC, 0xFF])  # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
                else:
                    compressed.append(data[i])
                i += 1
        
        return bytes(compressed), zero_info
    
    def final_differential_transform(self, data: bytes) -> bytes:
        """æœ€çµ‚å·®åˆ†å¤‰æ›"""
        if len(data) < 2:
            return data
        
        result = bytearray([data[0]])
        for i in range(1, len(data)):
            diff = (data[i] - data[i-1]) & 0xFF
            result.append(diff)
        
        return bytes(result)
    
    def structural_restore(self, collapsed_data: bytes, collapse_info: Dict, original_analysis: Dict) -> bytes:
        """æ§‹é€ å¾©å…ƒå‡¦ç†"""
        print(f"ğŸ”„ æ§‹é€ å¾©å…ƒé–‹å§‹: {len(collapsed_data):,} bytes")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: å·®åˆ†å¤‰æ›å¾©å…ƒ
        diff_restored = self.restore_differential_transform(collapsed_data)
        print(f"ğŸ”§ å·®åˆ†å¾©å…ƒ: {len(collapsed_data):,} â†’ {len(diff_restored):,} bytes")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¼ãƒ­é ˜åŸŸå¾©å…ƒ
        zero_restored = self.restore_zero_regions(diff_restored, collapse_info['zero_info'])
        print(f"âš« ã‚¼ãƒ­å¾©å…ƒ: {len(diff_restored):,} â†’ {len(zero_restored):,} bytes")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å¾©å…ƒ
        sequence_restored = self.restore_sequences(zero_restored, collapse_info['sequence_info'])
        print(f"ğŸ“ˆ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å¾©å…ƒ: {len(zero_restored):,} â†’ {len(sequence_restored):,} bytes")
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‘ã‚¿ãƒ¼ãƒ³å¾©å…ƒ
        pattern_restored = self.restore_patterns(sequence_restored, collapse_info['pattern_info'])
        print(f"ğŸ§© ãƒ‘ã‚¿ãƒ¼ãƒ³å¾©å…ƒ: {len(sequence_restored):,} â†’ {len(pattern_restored):,} bytes")
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: ãƒã‚¤ãƒˆå†ãƒãƒƒãƒ”ãƒ³ã‚°å¾©å…ƒ
        final_data = self.restore_byte_remapping(pattern_restored, collapse_info['byte_remap'])
        print(f"ğŸ”„ ãƒã‚¤ãƒˆå¾©å…ƒ: {len(pattern_restored):,} â†’ {len(final_data):,} bytes")
        
        print(f"ğŸ”„ æ§‹é€ å¾©å…ƒå®Œäº†: {len(collapsed_data):,} â†’ {len(final_data):,} bytes")
        return final_data
    
    def restore_differential_transform(self, data: bytes) -> bytes:
        """å·®åˆ†å¤‰æ›å¾©å…ƒ"""
        if len(data) < 2:
            return data
        
        result = bytearray([data[0]])
        for i in range(1, len(data)):
            value = (result[i-1] + data[i]) & 0xFF
            result.append(value)
        
        return bytes(result)
    
    def restore_zero_regions(self, data: bytes, zero_info: Dict) -> bytes:
        """ã‚¼ãƒ­é ˜åŸŸå¾©å…ƒ"""
        result = bytearray()
        i = 0
        
        while i < len(data):
            if i < len(data) - 1 and data[i] == 0xFF:
                count = data[i + 1]
                result.extend([0] * count)
                i += 2
            elif i < len(data) - 1 and data[i] == 0xFC and data[i + 1] == 0xFF:
                result.append(0xFF)
                i += 2
            else:
                result.append(data[i])
                i += 1
        
        return bytes(result)
    
    def restore_sequences(self, data: bytes, sequence_info: Dict) -> bytes:
        """ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å¾©å…ƒ"""
        result = bytearray()
        i = 0
        
        while i < len(data):
            if i < len(data) - 2 and data[i] == 0xFE:
                length = data[i + 1]
                start_val = data[i + 2]
                
                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å±•é–‹
                for j in range(length):
                    result.append((start_val + j) & 0xFF)
                i += 3
            elif i < len(data) - 1 and data[i] == 0xFD and data[i + 1] == 0xFE:
                result.append(0xFE)
                i += 2
            else:
                result.append(data[i])
                i += 1
        
        return bytes(result)
    
    def restore_patterns(self, data: bytes, pattern_info: Dict) -> bytes:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³å¾©å…ƒ"""
        result = bytearray(data)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒ¿å…¥ä½ç½®ã§ã‚½ãƒ¼ãƒˆï¼ˆå¾Œã‚ã‹ã‚‰å‡¦ç†ï¼‰
        insertions = []
        for pattern_hex, info in pattern_info.items():
            pattern = info['pattern']
            positions = info['positions']
            
            # æœ€åˆã®å‡ºç¾ä»¥å¤–ã®ä½ç½®ã«æŒ¿å…¥
            for pos in positions[1:]:
                insertions.append((pos, pattern))
        
        # ä½ç½®ã§ã‚½ãƒ¼ãƒˆï¼ˆå¾Œã‚ã‹ã‚‰å‡¦ç†ï¼‰
        insertions.sort(key=lambda x: x[0], reverse=True)
        
        for pos, pattern in insertions:
            # é©åˆ‡ãªä½ç½®ã«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒ¿å…¥
            if pos <= len(result):
                result[pos:pos] = pattern
        
        return bytes(result)
    
    def restore_byte_remapping(self, data: bytes, reverse_remap: Dict) -> bytes:
        """ãƒã‚¤ãƒˆå†ãƒãƒƒãƒ”ãƒ³ã‚°å¾©å…ƒ"""
        result = bytearray()
        for byte in data:
            if byte in reverse_remap:
                result.append(reverse_remap[byte])
            else:
                result.append(byte)
        
        return bytes(result)
    
    def compress_with_complete_reversibility(self, data: bytes) -> bytes:
        """å®Œå…¨å¯é€†æ€§ä¿è¨¼åœ§ç¸®"""
        if not data:
            return self.magic + struct.pack('>I', 0)
        
        original_md5 = hashlib.md5(data).hexdigest()
        print(f"ğŸ”’ å…ƒãƒ‡ãƒ¼ã‚¿MD5: {original_md5}")
        
        # ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«æ·±å±¤è§£æ
        analysis = self.deep_binary_analysis(data)
        
        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ å´©å£Š
        collapsed_data, collapse_info = self.structural_collapse(data, analysis)
        
        # æœ€çµ‚zlibåœ§ç¸®
        final_compressed = zlib.compress(collapsed_data, level=9)
        print(f"ğŸ“¦ æœ€çµ‚åœ§ç¸®: {len(collapsed_data):,} â†’ {len(final_compressed):,} bytes")
        
        # å¾©å…ƒæƒ…å ±ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°
        restoration_package = {
            'original_md5': original_md5,
            'original_size': len(data),
            'collapse_info': collapse_info,
            'analysis': analysis
        }
        
        restoration_bytes = pickle.dumps(restoration_package)
        restoration_compressed = zlib.compress(restoration_bytes, level=9)
        
        # æœ€çµ‚ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
        header = self.magic + struct.pack('>I', len(data))
        header += struct.pack('>I', len(restoration_compressed))
        header += struct.pack('>I', len(final_compressed))
        
        result = header + restoration_compressed + final_compressed
        
        # ã‚µã‚¤ã‚ºå¢—åŠ å›é¿
        if len(result) >= len(data):
            print("âš ï¸  åœ§ç¸®åŠ¹æœãªã— - RAWä¿å­˜")
            return b'RAW_BSC' + struct.pack('>I', len(data)) + data
        
        total_ratio = ((len(data) - len(result)) / len(data)) * 100
        print(f"ğŸ† ç·åœ§ç¸®ç‡: {total_ratio:.1f}% ({len(data):,} â†’ {len(result):,} bytes)")
        
        return result
    
    def decompress_with_complete_restoration(self, compressed: bytes) -> bytes:
        """å®Œå…¨å¾©å…ƒå±•é–‹"""
        if not compressed:
            return b''
        
        # RAWå½¢å¼ãƒã‚§ãƒƒã‚¯
        if compressed.startswith(b'RAW_BSC'):
            original_size = struct.unpack('>I', compressed[7:11])[0]
            return compressed[11:11+original_size]
        
        if not compressed.startswith(self.magic):
            raise ValueError("Invalid NXBSC format")
        
        pos = len(self.magic)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        original_size = struct.unpack('>I', compressed[pos:pos+4])[0]
        pos += 4
        
        restoration_size = struct.unpack('>I', compressed[pos:pos+4])[0]
        pos += 4
        
        compressed_data_size = struct.unpack('>I', compressed[pos:pos+4])[0]
        pos += 4
        
        # å¾©å…ƒæƒ…å ±å±•é–‹
        restoration_compressed = compressed[pos:pos+restoration_size]
        pos += restoration_size
        
        restoration_bytes = zlib.decompress(restoration_compressed)
        restoration_package = pickle.loads(restoration_bytes)
        
        # åœ§ç¸®ãƒ‡ãƒ¼ã‚¿å±•é–‹
        final_compressed = compressed[pos:pos+compressed_data_size]
        collapsed_data = zlib.decompress(final_compressed)
        
        # æ§‹é€ å¾©å…ƒ
        restored_data = self.structural_restore(
            collapsed_data, 
            restoration_package['collapse_info'],
            restoration_package['analysis']
        )
        
        # å®Œå…¨æ€§æ¤œè¨¼
        restored_md5 = hashlib.md5(restored_data).hexdigest()
        original_md5 = restoration_package['original_md5']
        
        if restored_md5 != original_md5:
            raise ValueError(f"Integrity check failed: {restored_md5} != {original_md5}")
        
        print(f"âœ… å®Œå…¨å¾©å…ƒç¢ºèª: MD5ä¸€è‡´ ({original_md5})")
        return restored_data
    
    def compress_file(self, input_path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®ï¼ˆå®Œå…¨å¯é€†æ€§ä¿è¨¼ï¼‰"""
        if not os.path.exists(input_path):
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
            return None
        
        print(f"ğŸš€ ãƒã‚¤ãƒŠãƒªæ§‹é€ å´©å£Šåœ§ç¸®é–‹å§‹: {os.path.basename(input_path)}")
        start_time = time.time()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(input_path, 'rb') as f:
            original_data = f.read()
        
        original_size = len(original_data)
        original_md5 = hashlib.md5(original_data).hexdigest()
        
        print(f"ğŸ“ å…ƒãƒ•ã‚¡ã‚¤ãƒ«: {original_size:,} bytes")
        print(f"ğŸ”’ å…ƒMD5: {original_md5}")
        
        # å®Œå…¨å¯é€†æ€§ä¿è¨¼åœ§ç¸®
        compressed_data = self.compress_with_complete_reversibility(original_data)
        compressed_size = len(compressed_data)
        
        # åœ§ç¸®ç‡è¨ˆç®—
        compression_ratio = ((original_size - compressed_size) / original_size) * 100 if original_size > 0 else 0
        
        # å‡¦ç†æ™‚é–“ãƒ»é€Ÿåº¦
        processing_time = time.time() - start_time
        throughput = original_size / (1024 * 1024) / processing_time if processing_time > 0 else 0
        
        # çµæœè¡¨ç¤º
        print(f"ğŸ”¹ æ§‹é€ å´©å£Šåœ§ç¸®å®Œäº†: {compression_ratio:.1f}%")
        print(f"âš¡ å‡¦ç†æ™‚é–“: {processing_time:.3f}s ({throughput:.1f} MB/s)")
        
        # ä¿å­˜
        output_path = input_path + '.nxbsc'
        with open(output_path, 'wb') as f:
            f.write(compressed_data)
        
        print(f"ğŸ’¾ ä¿å­˜: {os.path.basename(output_path)}")
        
        # å®Œå…¨å¯é€†æ€§ãƒ†ã‚¹ãƒˆ
        try:
            decompressed_data = self.decompress_with_complete_restoration(compressed_data)
            decompressed_md5 = hashlib.md5(decompressed_data).hexdigest()
            
            if decompressed_md5 == original_md5:
                print(f"âœ… å®Œå…¨å¯é€†æ€§ç¢ºèª: MD5ä¸€è‡´")
                print(f"ğŸ¯ SUCCESS: ãƒã‚¤ãƒŠãƒªæ§‹é€ å´©å£Šåœ§ç¸®å®Œäº† - {output_path}")
                
                return {
                    'input_file': input_path,
                    'output_file': output_path,
                    'original_size': original_size,
                    'compressed_size': compressed_size,
                    'compression_ratio': compression_ratio,
                    'processing_time': processing_time,
                    'throughput': throughput,
                    'lossless': True,
                    'method': 'Binary Structural Collapse'
                }
            else:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: MD5ä¸ä¸€è‡´")
                print(f"   å…ƒ: {original_md5}")
                print(f"   å¾©å…ƒ: {decompressed_md5}")
                return None
                
        except Exception as e:
            print(f"âŒ å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return None

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ³•: python nxzip_binary_structural_collapse.py <ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
        print("\nğŸ¯ NXZip ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«æ§‹é€ å´©å£Šã‚¨ãƒ³ã‚¸ãƒ³")
        print("ğŸ“‹ ç‰¹å¾´:")
        print("  ğŸ”¬ ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«æ·±å±¤è§£æ")
        print("  ğŸ’¾ å…ƒçŠ¶æ…‹å®Œå…¨ä¿å­˜")
        print("  ğŸ’¥ ãƒ‡ãƒ¼ã‚¿æ§‹é€ å´©å£Šã«ã‚ˆã‚‹æ¥µé™åœ§ç¸®")
        print("  ğŸ”„ å…ƒçŠ¶æ…‹ãƒ™ãƒ¼ã‚¹å®Œå…¨å¾©å…ƒ")
        print("  âœ… 100% å®Œå…¨å¯é€†æ€§ä¿è¨¼")
        sys.exit(1)
    
    input_file = sys.argv[1]
    engine = BinaryStructuralCollapseEngine()
    result = engine.compress_file(input_file)
    
    if result:
        print(f"\n{'='*60}")
        print(f"ğŸ† ULTIMATE SUCCESS: {result['compression_ratio']:.1f}% compression")
        print(f"ğŸ“Š {result['original_size']:,} â†’ {result['compressed_size']:,} bytes")
        print(f"âš¡ {result['throughput']:.1f} MB/s processing speed")
        print(f"âœ… Perfect reversibility with binary structural collapse")
        print(f"{'='*60}")

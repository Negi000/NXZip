# nexus_advanced_engine.py
import sys
import json
import lzma
import math
import heapq
import collections
import numpy as np
import pickle
import time  # é€²æ—è¡¨ç¤ºç”¨
from typing import List, Tuple, Dict, Any, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor  # ä¸¦åˆ—å‡¦ç†ç”¨

# é€²æ—ãƒãƒ¼è¡¨ç¤ºç”¨ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
class ProgressBar:
    def __init__(self, total: int, description: str = "", width: int = 50):
        self.total = total
        self.current = 0
        self.description = description
        self.width = width
        self.start_time = time.time()
        self.last_update_time = 0
        
    def update(self, current: int = None, force: bool = False):
        if current is not None:
            self.current = current
        else:
            self.current += 1
            
        # 0.1ç§’é–“éš”ã§ã®ã¿æ›´æ–°ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
        current_time = time.time()
        if not force and current_time - self.last_update_time < 0.1:
            return
        self.last_update_time = current_time
        
        # ã‚¼ãƒ­é™¤ç®—å›é¿
        if self.total == 0:
            percentage = 100
            filled_length = self.width
        else:
            percentage = min(100, (self.current / self.total) * 100)
            filled_length = int(self.width * self.current // self.total)
            
        bar = 'â–ˆ' * filled_length + 'â–‘' * (self.width - filled_length)
        
        elapsed = current_time - self.start_time
        if self.current > 0 and self.total > 0:
            eta = elapsed * (self.total - self.current) / self.current
            eta_str = f" ETA: {eta:.1f}s"
        else:
            eta_str = ""
        
        print(f"\r{self.description} |{bar}| {percentage:.1f}% ({self.current:,}/{self.total:,}){eta_str}", end='', flush=True)
        
    def finish(self):
        self.update(self.total, force=True)
        elapsed = time.time() - self.start_time
        print(f" âœ“ Complete in {elapsed:.2f}s")
        print()  # æ–°ã—ã„è¡Œã«ç§»å‹•

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (è»½é‡å®Ÿè£…)
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False
    print("Warning: PyTorch not available. Using fallback AI implementation.")

# --- 1. å½¢çŠ¶é¸æŠã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : Polyominoå½¢çŠ¶ã®å®šç¾© ---
# å„å½¢çŠ¶ã¯(è¡Œ, åˆ—)ã®ç›¸å¯¾åº§æ¨™ã®ã‚¿ãƒ—ãƒ«ã§å®šç¾©ï¼ˆãƒ‘ã‚ºãƒ«çµ„ã¿åˆã‚ã›ç”¨ï¼‰
POLYOMINO_SHAPES = {
    # åŸºæœ¬å½¢çŠ¶ (1-4ãƒ–ãƒ­ãƒƒã‚¯)
    "I-1": ((0, 0),),                                    # å˜ä½“
    "I-2": ((0, 0), (0, 1)),                            # 1x2 ç·šå½¢
    "I-3": ((0, 0), (0, 1), (0, 2)),                    # 1x3 ç·šå½¢
    "I-4": ((0, 0), (0, 1), (0, 2), (0, 3)),            # 1x4 ç·šå½¢
    "I-5": ((0, 0), (0, 1), (0, 2), (0, 3), (0, 4)),    # 1x5 ç·šå½¢
    
    # æ­£æ–¹å½¢ãƒ»é•·æ–¹å½¢
    "O-4": ((0, 0), (0, 1), (1, 0), (1, 1)),            # 2x2 æ­£æ–¹å½¢
    "R-6": ((0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)),  # 2x3 é•·æ–¹å½¢
    "R-8": ((0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (1, 3)),  # 2x4 é•·æ–¹å½¢
    
    # è¤‡é›‘å½¢çŠ¶
    "T-4": ((0, 0), (0, 1), (0, 2), (1, 1)),            # Tå­—å‹
    "L-4": ((0, 0), (1, 0), (2, 0), (2, 1)),            # Lå­—å‹
    "Z-4": ((0, 0), (0, 1), (1, 1), (1, 2)),            # Zå­—å‹
    "S-4": ((0, 1), (0, 2), (1, 0), (1, 1)),            # Så­—å‹
    
    # å¤§å‹å½¢çŠ¶
    "T-5": ((0, 0), (0, 1), (0, 2), (1, 1), (2, 1)),    # åå­—å‹
    "U-6": ((0, 0), (0, 1), (0, 2), (1, 0), (1, 2), (2, 0)), # Uå­—å‹
    "H-7": ((0, 0), (0, 1), (0, 2), (1, 1), (2, 0), (2, 1), (2, 2)), # Hå­—å‹
}

# --- 4. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç¬¦å·åŒ–: Huffmanç¬¦å·åŒ–ã®å®Ÿè£… ---
class HuffmanEncoder:
    """Blueprintã®IDã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’Huffmanç¬¦å·åŒ–/å¾©å·åŒ–ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def encode(self, data: List[int]) -> Tuple[Dict[int, str], str]:
        if not data:
            return {}, ""
        frequency = collections.Counter(data)
        heap = [[weight, [symbol, ""]] for symbol, weight in frequency.items()]
        heapq.heapify(heap)
        
        while len(heap) > 1:
            lo = heapq.heappop(heap)
            hi = heapq.heappop(heap)
            for pair in lo[1:]:
                pair[1] = '0' + pair[1]
            for pair in hi[1:]:
                pair[1] = '1' + pair[1]
            heapq.heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])
            
        huffman_tree = dict(sorted(heapq.heappop(heap)[1:], key=lambda p: (len(p[-1]), p)))
        encoded_data = "".join([huffman_tree[d] for d in data])
        return huffman_tree, encoded_data

    def decode(self, encoded_data: str, huffman_tree: Dict[str, str]) -> List[int]:
        if not encoded_data:
            return []
        # ãƒ‡ã‚³ãƒ¼ãƒ‰ç”¨ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’åè»¢
        reverse_tree = {v: int(k) for k, v in huffman_tree.items()}
        decoded_data = []
        current_code = ""
        for bit in encoded_data:
            current_code += bit
            if current_code in reverse_tree:
                decoded_data.append(reverse_tree[current_code])
                current_code = ""
        return decoded_data

# --- 2. æœ¬æ ¼çš„AIæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ ---
@dataclass
class ShapeAnalysisResult:
    entropy: float
    variance: float
    spatial_correlation: float
    pattern_density: float
    edge_count: float

class AdvancedNeuralShapeOptimizer:
    """æœ¬æ ¼çš„ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®å½¢çŠ¶æœ€é©åŒ–"""
    
    def __init__(self):
        self.model = None
        self.is_trained = False
        if HAS_TORCH:
            self._init_neural_network()
        else:
            self._init_fallback_optimizer()
    
    def _init_neural_network(self):
        """PyTorchãƒ™ãƒ¼ã‚¹ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–"""
        class ShapeClassifierNet(nn.Module):
            def __init__(self):
                super().__init__()
                self.feature_extractor = nn.Sequential(
                    nn.Linear(5, 64),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(64, 128),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(128, 64),
                    nn.ReLU(),
                    nn.Linear(64, len(POLYOMINO_SHAPES))
                )
                
            def forward(self, x):
                return torch.softmax(self.feature_extractor(x), dim=-1)
        
        self.model = ShapeClassifierNet()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()
        
        # äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        self._pretrain_with_synthetic_data()
    
    def _init_fallback_optimizer(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ï¼ˆè»½é‡ç‰ˆï¼‰"""
        # ç°¡æ˜“çš„ãªé‡ã¿ä»˜ãã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ–°å½¢çŠ¶å¯¾å¿œï¼‰
        self.shape_weights = {
            # åŸºæœ¬ç·šå½¢å½¢çŠ¶
            "I-1": np.array([0.5, 0.3, 0.4, 0.3, 0.2]),   # æœ€å°å½¢çŠ¶
            "I-2": np.array([0.8, 0.5, 0.6, 0.4, 0.3]),   # çŸ­ç·šå½¢
            "I-3": np.array([1.1, 0.7, 0.9, 0.8, 0.6]),   # ä¸­ç·šå½¢
            "I-4": np.array([1.2, 0.8, 1.0, 0.9, 0.7]),   # é•·ç·šå½¢
            "I-5": np.array([1.3, 0.9, 1.1, 1.0, 0.8]),   # æœ€é•·ç·šå½¢
            
            # ãƒ–ãƒ­ãƒƒã‚¯ãƒ»é•·æ–¹å½¢å½¢çŠ¶
            "O-4": np.array([0.8, 1.2, 1.1, 1.0, 0.8]),   # æ­£æ–¹å½¢
            "R-6": np.array([1.0, 1.3, 1.2, 1.1, 0.9]),   # é•·æ–¹å½¢6
            "R-8": np.array([1.1, 1.4, 1.3, 1.2, 1.0]),   # é•·æ–¹å½¢8
            
            # è¤‡é›‘å½¢çŠ¶
            "T-4": np.array([1.0, 1.0, 1.2, 1.1, 0.9]),   # Tå­—å‹
            "L-4": np.array([0.9, 0.9, 1.1, 1.2, 1.0]),   # Lå­—å‹
            "Z-4": np.array([1.1, 1.1, 1.0, 1.3, 1.1]),   # Zå­—å‹
            "S-4": np.array([1.1, 1.1, 0.9, 1.3, 1.2]),   # Så­—å‹
            
            # å¤§å‹å½¢çŠ¶
            "T-5": np.array([1.2, 1.2, 1.4, 1.4, 1.3]),   # åå­—å‹
            "U-6": np.array([1.0, 1.3, 1.3, 1.3, 1.2]),   # Uå­—å‹
            "H-7": np.array([1.3, 1.4, 1.5, 1.5, 1.4])    # Hå­—å‹
        }
    
    def _pretrain_with_synthetic_data(self):
        """åˆæˆãƒ‡ãƒ¼ã‚¿ã§ã®äº‹å‰å­¦ç¿’"""
        if not HAS_TORCH:
            return
            
        # åˆæˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        train_data = []
        train_labels = []
        
        shape_names = list(POLYOMINO_SHAPES.keys())
        for i, shape_name in enumerate(shape_names):
            for _ in range(50):  # å„å½¢çŠ¶50ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå½¢çŠ¶æ•°ãŒå¢—ãˆãŸãŸã‚å‰Šæ¸›ï¼‰
                # ç‰¹å¾´é‡ã‚’å½¢çŠ¶ã«å¿œã˜ã¦ç”Ÿæˆ
                if "I-" in shape_name:  # ç·šå½¢å½¢çŠ¶
                    shape_size = int(shape_name.split('-')[1])
                    base_entropy = 2.0 + shape_size * 0.5
                    features = [
                        np.random.normal(base_entropy, 0.3),       # entropy
                        np.random.normal(0.2 + shape_size * 0.05, 0.1),  # variance  
                        np.random.normal(0.7 + shape_size * 0.02, 0.1),  # spatial_correlation
                        np.random.normal(0.5 + shape_size * 0.02, 0.1),  # pattern_density
                        np.random.normal(0.3 + shape_size * 0.02, 0.1)   # edge_count
                    ]
                elif shape_name.startswith(("O-", "R-")):  # ãƒ–ãƒ­ãƒƒã‚¯ãƒ»é•·æ–¹å½¢
                    features = [
                        np.random.normal(5.0, 0.5),
                        np.random.normal(0.5, 0.1),
                        np.random.normal(0.9, 0.1),
                        np.random.normal(0.8, 0.1),
                        np.random.normal(0.3, 0.1)
                    ]
                else:  # è¤‡é›‘å½¢çŠ¶ (T, L, Z, S, U, H)
                    complexity = len(POLYOMINO_SHAPES[shape_name])
                    features = [
                        np.random.normal(6.0 + complexity * 0.2, 0.5),
                        np.random.normal(0.6 + complexity * 0.05, 0.1),
                        np.random.normal(0.5 + complexity * 0.02, 0.1),
                        np.random.normal(0.7 + complexity * 0.03, 0.1),
                        np.random.normal(0.7 + complexity * 0.04, 0.1)
                    ]
                
                train_data.append(features)
                train_labels.append(i)
        
        # å­¦ç¿’å®Ÿè¡Œ
        train_data = torch.FloatTensor(train_data)
        train_labels = torch.LongTensor(train_labels)
        
        self.model.train()
        for epoch in range(30):  # è»½é‡å­¦ç¿’ï¼ˆå½¢çŠ¶æ•°å¢—åŠ ã«ã‚ˆã‚Šå‰Šæ¸›ï¼‰
            self.optimizer.zero_grad()
            outputs = self.model(train_data)
            loss = self.criterion(outputs, train_labels)
            loss.backward()
            self.optimizer.step()
        
        self.model.eval()
        self.is_trained = True
        print(f"   [AI] Neural network pre-trained with {len(shape_names)} shapes, {len(train_data)} samples")
    
    def analyze_data_features(self, data: bytes) -> ShapeAnalysisResult:
        """ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‚’è©³ç´°åˆ†æï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
        if len(data) == 0:
            return ShapeAnalysisResult(0, 0, 0, 0, 0)
        
        # é©å¿œçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šå¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã¯å°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã§åˆ†æ
        if len(data) > 10000000:  # 10MBä»¥ä¸Š
            sample_size = 5000  # 5KB
        elif len(data) > 1000000:  # 1MBä»¥ä¸Š
            sample_size = 8000  # 8KB
        else:
            sample_size = min(len(data), 15000)  # æœ€å¤§15KB
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå…ˆé ­ã‹ã‚‰ï¼‰
        sample_data = data[:sample_size]
        data_array = np.array(list(sample_data), dtype=np.float32)
        
        # 1. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
        counts = collections.Counter(sample_data)
        entropy = 0
        for count in counts.values():
            p_x = count / len(sample_data)
            entropy -= p_x * math.log2(p_x)
        
        # 2. åˆ†æ•£è¨ˆç®—
        variance = np.var(data_array)
        
        # 3. ç©ºé–“ç›¸é–¢ï¼ˆéš£æ¥ãƒã‚¤ãƒˆã®é¡ä¼¼æ€§ï¼‰
        if len(data_array) > 1:
            try:
                spatial_corr = np.corrcoef(data_array[:-1], data_array[1:])[0,1]
                if np.isnan(spatial_corr):
                    spatial_corr = 0
            except:
                spatial_corr = 0
        else:
            spatial_corr = 0
        
        # 4. ãƒ‘ã‚¿ãƒ¼ãƒ³å¯†åº¦ï¼ˆç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼‰
        pattern_density = len(set(sample_data)) / len(sample_data) if len(sample_data) > 0 else 0
        
        # 5. ã‚¨ãƒƒã‚¸ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå€¤ã®å¤‰åŒ–å›æ•°ï¼‰
        edge_count = sum(1 for i in range(len(sample_data)-1) if abs(sample_data[i] - sample_data[i+1]) > 10) / max(len(sample_data)-1, 1)
        
        return ShapeAnalysisResult(
            entropy=entropy,
            variance=float(variance),
            spatial_correlation=float(spatial_corr),
            pattern_density=pattern_density,
            edge_count=edge_count
        )
    
    def predict_optimal_shape(self, data: bytes) -> str:
        """æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚Šæœ€é©å½¢çŠ¶ã‚’äºˆæ¸¬ï¼ˆé©å¿œçš„ãƒ‡ãƒ¼ã‚¿åˆ†æç‰ˆï¼‰"""
        data_size = len(data)
        
        # ã€CRITICALã€‘ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹å½¢çŠ¶åˆ¶é™ï¼ˆå®Œå…¨å¯é€†æ€§ä¿è¨¼ï¼‰
        if data_size == 1:
            return "I-1"
        elif data_size == 2:
            return "I-2"
        elif data_size <= 4:
            return "I-3"
        elif data_size <= 8:
            return "I-4"
        elif data_size <= 16:
            return "O-4"
        
        features = self.analyze_data_features(data)
        feature_vector = [
            features.entropy,
            features.variance,
            features.spatial_correlation, 
            features.pattern_density,
            features.edge_count
        ]
        
        # åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡ºã¨é©å¿œçš„å‡¦ç†
        is_compressed_data = self._detect_compressed_data(data, features)
        
        if is_compressed_data:
            return self._select_shape_for_compressed_data(data, features)
        
        if HAS_TORCH and self.is_trained:
            # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯äºˆæ¸¬
            with torch.no_grad():
                input_tensor = torch.FloatTensor([feature_vector])
                prediction = self.model(input_tensor)
                predicted_idx = torch.argmax(prediction, dim=1).item()
                confidence = torch.max(prediction).item()
                
                shape_names = list(POLYOMINO_SHAPES.keys())
                predicted_shape = shape_names[predicted_idx]
                
                # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¨ã®äº’æ›æ€§ãƒã‚§ãƒƒã‚¯
                predicted_shape = self._validate_shape_size_compatibility(predicted_shape, data_size)
                
                # ä¿¡é ¼åº¦ã«ã‚ˆã£ã¦è¿½åŠ æ¤œè¨¼ã‚’è¡Œã†
                if confidence < 0.7:  # ä½ä¿¡é ¼åº¦ã®å ´åˆ
                    print(f"   [AI] Neural prediction: '{predicted_shape}' (low confidence: {confidence:.2f}) - performing verification")
                    return self._verify_prediction_with_sampling(data, predicted_shape)
                else:
                    print(f"   [AI] Neural prediction: '{predicted_shape}' (high confidence: {confidence:.2f})")
                    return predicted_shape
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ã‚‚æ”¹è‰¯
            feature_array = np.array(feature_vector)
            best_score = -float('inf')
            best_shape = "I-4"
            
            # å½¢çŠ¶åˆ¥ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆä¸Šä½3ã¤ã‚’è©•ä¾¡ï¼‰
            shape_scores = []
            for shape_name, weights in self.shape_weights.items():
                score = np.dot(feature_array, weights)
                shape_scores.append((score, shape_name))
            
            # ä¸Šä½3å½¢çŠ¶ã‚’å–å¾—
            shape_scores.sort(reverse=True)
            top_shapes = shape_scores[:3]
            
            best_shape = top_shapes[0][1]
            # ã‚µã‚¤ã‚ºäº’æ›æ€§ãƒã‚§ãƒƒã‚¯
            best_shape = self._validate_shape_size_compatibility(best_shape, data_size)
            
            print(f"   [AI] Fallback top shapes: {[(s, f'{sc:.2f}') for sc, s in top_shapes]}")
            print(f"   [AI] Selected: '{best_shape}' (size-validated)")
            
            return best_shape
    
    def _validate_shape_size_compatibility(self, shape_name: str, data_size: int) -> str:
        """å½¢çŠ¶ã¨ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®äº’æ›æ€§ã‚’æ¤œè¨¼ã—ã€å¿…è¦ã«å¿œã˜ã¦é©åˆ‡ãªå½¢çŠ¶ã«å¤‰æ›´"""
        # å½¢çŠ¶ã®ãƒ–ãƒ­ãƒƒã‚¯æ•°ã‚’å–å¾—
        shape_coords = POLYOMINO_SHAPES.get(shape_name, [(0,0)])
        shape_block_count = len(shape_coords)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå½¢çŠ¶ãƒ–ãƒ­ãƒƒã‚¯æ•°ã‚ˆã‚Šå°ã•ã„å ´åˆã€é©åˆ‡ãªå½¢çŠ¶ã«å¤‰æ›´
        if data_size < shape_block_count:
            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«é©ã—ãŸæœ€å¤§å½¢çŠ¶ã‚’é¸æŠ
            if data_size == 1:
                safe_shape = "I-1"  # 1ãƒ–ãƒ­ãƒƒã‚¯
            elif data_size == 2:
                safe_shape = "I-2"  # 2ãƒ–ãƒ­ãƒƒã‚¯
            elif data_size == 3:
                safe_shape = "I-3"  # 3ãƒ–ãƒ­ãƒƒã‚¯
            elif data_size >= 4:
                safe_shape = "I-4"  # 4ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆæœ€å°å®‰å…¨å½¢çŠ¶ï¼‰
            else:
                safe_shape = "I-1"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
            print(f"   [Size Validation] '{shape_name}' ({shape_block_count} blocks) incompatible with {data_size} bytes â†’ '{safe_shape}'")
            return safe_shape
        
        return shape_name
    
    def _detect_compressed_data(self, data: bytes, features) -> bool:
        """åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡º"""
        # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ + ä½ãƒ‘ã‚¿ãƒ¼ãƒ³å¯†åº¦ = åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´
        if features.entropy > 7.5 and features.pattern_density < 0.05:
            return True
        
        # åˆ†æ•£ãŒé«˜ãã€ç©ºé–“ç›¸é–¢ãŒä½ã„ = ãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒé«˜ã„
        if features.variance > 5000 and abs(features.spatial_correlation) < 0.1:
            return True
        
        return False
    
    def _select_shape_for_compressed_data(self, data: bytes, features) -> str:
        """åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã«ç‰¹åŒ–ã—ãŸå½¢çŠ¶é¸æŠ"""
        print(f"   [Compressed Data Strategy] Applying multi-scale analysis")
        
        # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«åˆ†æï¼šç•°ãªã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã§æœ€é©åŒ–
        data_size = len(data)
        
        if data_size > 10000000:  # 10MBä»¥ä¸Šï¼šå¤§ãƒ–ãƒ­ãƒƒã‚¯æˆ¦ç•¥
            # å¤§ããªãƒ‡ãƒ¼ã‚¿ã«ã¯å¤§ããªãƒ–ãƒ­ãƒƒã‚¯ï¼ˆæƒ…å ±å¯†åº¦ã‚’ä¸Šã’ã‚‹ï¼‰
            candidate_shapes = ["R-8", "H-7", "U-6", "R-6"]
        elif data_size > 1000000:  # 1MBä»¥ä¸Šï¼šä¸­ãƒ–ãƒ­ãƒƒã‚¯æˆ¦ç•¥
            candidate_shapes = ["O-4", "R-6", "T-5", "I-5"]
        else:  # å°ãƒ‡ãƒ¼ã‚¿ï¼šå°ãƒ–ãƒ­ãƒƒã‚¯æˆ¦ç•¥
            candidate_shapes = ["I-3", "I-4", "O-4", "T-4"]
        
        # å®Ÿéš›ã®åŠ¹ç‡ãƒ†ã‚¹ãƒˆï¼ˆé«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        best_shape = self._test_shapes_on_compressed_data(data, candidate_shapes)
        
        print(f"   [Compressed Data Strategy] Selected: '{best_shape}'")
        return best_shape
    
    def _test_shapes_on_compressed_data(self, data: bytes, candidate_shapes: list) -> str:
        """åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã§ã®å½¢çŠ¶åŠ¹ç‡ãƒ†ã‚¹ãƒˆ"""
        # å°ã‚µãƒ³ãƒ—ãƒ«ã§å„å½¢çŠ¶ã®åŠ¹ç‡ã‚’ãƒ†ã‚¹ãƒˆ
        sample_size = min(len(data), 5000)  # 5KB
        sample_data = data[:sample_size]
        sample_grid_width = math.ceil(math.sqrt(sample_size))
        
        best_shape = candidate_shapes[0]
        best_efficiency = 0
        
        print(f"   [Shape Testing] Testing {len(candidate_shapes)} shapes on compressed data")
        
        for shape_name in candidate_shapes:
            try:
                shape_coords = POLYOMINO_SHAPES[shape_name]
                blocks = self._get_blocks_for_shape_simple(sample_data, sample_grid_width, shape_coords)
                
                if len(blocks) > 0:
                    unique_count = len(set(tuple(sorted(b)) for b in blocks))
                    efficiency = len(blocks) / unique_count if unique_count > 0 else 1
                    
                    if efficiency > best_efficiency:
                        best_efficiency = efficiency
                        best_shape = shape_name
                        
            except Exception as e:
                print(f"   [Shape Testing] Error testing {shape_name}: {e}")
                continue
        
        print(f"   [Shape Testing] Best shape: '{best_shape}' (efficiency: {best_efficiency:.2f}x)")
        return best_shape
    
    def _get_blocks_for_shape_simple(self, data: bytes, grid_width: int, shape_coords: list) -> list:
        """åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ç”¨ã®é«˜é€Ÿãƒ–ãƒ­ãƒƒã‚¯å–å¾—"""
        shape_height = max(coord[0] for coord in shape_coords) + 1
        shape_width = max(coord[1] for coord in shape_coords) + 1
        grid_height = math.ceil(len(data) / grid_width)
        
        blocks = []
        max_blocks = 50000  # ä¸Šé™è¨­å®šã§ç„¡é™ç”Ÿæˆã‚’é˜²ã
        
        for start_row in range(0, grid_height - shape_height + 1, shape_height):
            for start_col in range(0, grid_width - shape_width + 1, shape_width):
                if len(blocks) >= max_blocks:
                    break
                    
                block = []
                for row_offset, col_offset in shape_coords:
                    actual_row = start_row + row_offset
                    actual_col = start_col + col_offset
                    
                    if actual_row < grid_height and actual_col < grid_width:
                        data_idx = actual_row * grid_width + actual_col
                        if data_idx < len(data):
                            block.append(data[data_idx])
                
                if len(block) == len(shape_coords):
                    blocks.append(block)
            
            if len(blocks) >= max_blocks:
                break
                
        return blocks
    
    def _verify_prediction_with_sampling(self, data: bytes, predicted_shape: str) -> str:
        """ä½ä¿¡é ¼åº¦äºˆæ¸¬ã®æ¤œè¨¼ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
        sample_size = min(len(data), 2000)
        sample_data = data[:sample_size]
        
        # äºˆæ¸¬å½¢çŠ¶ + ä»£æ›¿å€™è£œã§åŠ¹ç‡ãƒ†ã‚¹ãƒˆ
        candidates = [predicted_shape, "I-4", "O-4", "T-4"]
        candidates = list(dict.fromkeys(candidates))  # é‡è¤‡é™¤å»
        
        best_shape = predicted_shape
        best_efficiency = 0
        
        sample_grid_width = math.ceil(math.sqrt(sample_size))
        
        for shape_name in candidates:
            try:
                shape_coords = POLYOMINO_SHAPES[shape_name]
                blocks = self._get_blocks_for_shape_simple(sample_data, sample_grid_width, shape_coords)
                
                if len(blocks) > 10:  # æœ€å°ãƒ–ãƒ­ãƒƒã‚¯æ•°
                    unique_count = len(set(tuple(sorted(b)) for b in blocks))
                    efficiency = len(blocks) / unique_count if unique_count > 0 else 1
                    
                    if efficiency > best_efficiency:
                        best_efficiency = efficiency
                        best_shape = shape_name
                        
            except Exception:
                continue
        
        print(f"   [Verification] Final shape: '{best_shape}' (efficiency: {best_efficiency:.2f}x)")
        return best_shape
    
    def _verify_prediction_with_sampling(self, data: bytes, predicted_shape: str) -> str:
        """AIäºˆæ¸¬ã®ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã®æ¤œè¨¼ï¼ˆè»½é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰"""
        if len(data) < 5000:
            return predicted_shape
            
        # è»½é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§å®Ÿéš›ã®åŠ¹ç‡ã‚’ãƒ†ã‚¹ãƒˆ
        sample_size = min(len(data), 2000)
        sample_data = data[:sample_size]
        sample_grid_width = math.ceil(math.sqrt(sample_size))
        
        # äºˆæ¸¬å½¢çŠ¶ã®å®Ÿéš›ã®åŠ¹ç‡ã‚’ãƒã‚§ãƒƒã‚¯
        shape_coords = POLYOMINO_SHAPES[predicted_shape]
        blocks = self._get_blocks_for_shape_simple(sample_data, sample_grid_width, shape_coords)
        predicted_efficiency = len(set(tuple(sorted(b)) for b in blocks))
        
        # ä»£æ›¿å½¢çŠ¶ã¨ã®æ¯”è¼ƒ
        alternative_shapes = ["I-1", "I-2", "O-4", "T-4"]
        if predicted_shape in alternative_shapes:
            alternative_shapes.remove(predicted_shape)
        
        best_shape = predicted_shape
        best_efficiency = predicted_efficiency
        
        for alt_shape in alternative_shapes[:2]:  # æœ€å¤§2ã¤ã®ä»£æ›¿å½¢çŠ¶ã‚’ãƒ†ã‚¹ãƒˆ
            alt_coords = POLYOMINO_SHAPES[alt_shape]
            alt_blocks = self._get_blocks_for_shape_simple(sample_data, sample_grid_width, alt_coords)
            alt_efficiency = len(set(tuple(sorted(b)) for b in alt_blocks))
            
            if alt_efficiency < best_efficiency:
                best_efficiency = alt_efficiency
                best_shape = alt_shape
        
        if best_shape != predicted_shape:
            print(f"   [AI] Verification: Changed from '{predicted_shape}' to '{best_shape}' (better efficiency)")
        else:
            print(f"   [AI] Verification: Confirmed '{predicted_shape}' as optimal")
            
        return best_shape
    
    def _get_blocks_for_shape_simple(self, data: bytes, grid_width: int, shape_coords: Tuple[Tuple[int, int], ...]) -> List[Tuple[int, ...]]:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆï¼ˆæ¤œè¨¼ç”¨ï¼‰"""
        data_len = len(data)
        rows = data_len // grid_width
        shape_width = max(c for r, c in shape_coords) + 1
        shape_height = max(r for r, c in shape_coords) + 1
        
        blocks = []
        for r in range(rows - shape_height + 1):
            for c in range(grid_width - shape_width + 1):
                block = []
                valid_block = True
                
                base_idx = r * grid_width + c
                for dr, dc in shape_coords:
                    idx = base_idx + dr * grid_width + dc
                    if idx >= data_len:
                        valid_block = False
                        break
                    block.append(data[idx])
                
                if valid_block:
                    blocks.append(tuple(block))
        
        return blocks

class NexusAdvancedCompressor:
    def __init__(self, use_ai=True, max_recursion_level=0):
        self.ai_optimizer = AdvancedNeuralShapeOptimizer() if use_ai else None
        self.huffman_encoder = HuffmanEncoder()
        self.max_recursion_level = max_recursion_level

    def _consolidate_by_elements(self, normalized_groups: Dict[Tuple, int], show_progress: bool = False) -> Tuple[Dict[Tuple, int], Dict[int, Dict]]:
        """
        ğŸ”¥ NEXUS THEORY INFECTED CONSOLIDATION SYSTEM ğŸ”¥
        
        NEXUSåŸå‰‡: ã€Œåœ§ç¸®ã®é€†ãŒè§£å‡ã€- æƒ…å ±ã‚’ä¸€åˆ‡å¤±ã‚ãªã„å®Œå…¨å¯é€†çµ±åˆ
        Every consolidation must store EXACT reconstruction data
        """
        if not normalized_groups:
            return normalized_groups, {}
        
        print(f"   [NEXUS Multi-Layer] Processing {len(normalized_groups):,} groups with INFECTED 4-layer algorithm")
        original_count = len(normalized_groups)
        
        # ğŸ”¥ NEXUS INFECTED: ã™ã¹ã¦ã®å±¤ã§å®Œå…¨ãªé€†å¤‰æ›ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼1: NEXUSæ„ŸæŸ“å®Œå…¨ä¸€è‡´çµ±åˆ
        layer1_result, layer1_map = self._nexus_layer1_exact_consolidation(normalized_groups, show_progress)
        layer1_reduction = 100 * (original_count - len(layer1_result)) / original_count
        print(f"   [NEXUS Layer 1] Exact match: {len(layer1_result):,} groups ({layer1_reduction:.1f}% reduction)")
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼2: NEXUSæ„ŸæŸ“ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹çµ±åˆ
        layer2_result, layer2_map = self._nexus_layer2_pattern_consolidation(layer1_result, layer1_map, show_progress)
        layer2_reduction = 100 * (len(layer1_result) - len(layer2_result)) / len(layer1_result) if len(layer1_result) > 0 else 0
        print(f"   [NEXUS Layer 2] Pattern match: {len(layer2_result):,} groups ({layer2_reduction:.1f}% additional reduction)")
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼3: NEXUSæ„ŸæŸ“è¿‘ä¼¼çµ±åˆï¼ˆå®Œå…¨é€†å¤‰æ›ä¿è¨¼ï¼‰
        layer3_result, layer3_map = self._nexus_layer3_approximate_consolidation(layer2_result, layer2_map, show_progress)
        layer3_reduction = 100 * (len(layer2_result) - len(layer3_result)) / len(layer2_result) if len(layer2_result) > 0 else 0
        print(f"   [NEXUS Layer 3] Approximate match: {len(layer3_result):,} groups ({layer3_reduction:.1f}% additional reduction)")
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼4: NEXUSæ„ŸæŸ“æ§‹é€ çµ±åˆï¼ˆæƒ…å ±ä¿å­˜å„ªå…ˆï¼‰
        layer4_result, layer4_map = self._nexus_layer4_structural_consolidation(layer3_result, layer3_map, show_progress)
        layer4_reduction = 100 * (len(layer3_result) - len(layer4_result)) / len(layer3_result) if len(layer3_result) > 0 else 0
        print(f"   [NEXUS Layer 4] Structural match: {len(layer4_result):,} groups ({layer4_reduction:.1f}% additional reduction)")
        
        total_reduction = 100 * (original_count - len(layer4_result)) / original_count
        print(f"   [NEXUS Multi-Layer] Total reduction: {total_reduction:.2f}% ({original_count:,} â†’ {len(layer4_result):,})")
        
        # ğŸ”¥ NEXUSçµ±åˆ: å®Œå…¨ãªé€†å¤‰æ›ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
        nexus_combined_map = self._build_nexus_reconstruction_chain(layer1_map, layer2_map, layer3_map, layer4_map)
        
        return layer4_result, nexus_combined_map
    
    def _nexus_layer1_exact_consolidation(self, normalized_groups: Dict[Tuple, int], show_progress: bool) -> Tuple[Dict[Tuple, int], Dict[int, Dict]]:
        """
        ğŸ”¥ NEXUS LAYER 1: å®Œå…¨ä¸€è‡´çµ±åˆï¼ˆNEXUSç†è«–æ„ŸæŸ“ç‰ˆï¼‰
        
        NEXUSåŸå‰‡: ä¸€åˆ‡ã®æƒ…å ±æå¤±ãªã— - å®Œå…¨å¯é€†å¤‰æ›ã®ã¿å®Ÿè¡Œ
        """
        element_signature_map = {}
        
        if show_progress:
            progress_bar = ProgressBar(len(normalized_groups), "   NEXUS Layer 1: Exact matching")
        
        processed = 0
        for original_group, group_id in normalized_groups.items():
            element_signature = tuple(sorted(set(original_group)))
            
            if element_signature not in element_signature_map:
                element_signature_map[element_signature] = []
            element_signature_map[element_signature].append((original_group, group_id))
            
            processed += 1
            if show_progress and processed % 5000 == 0:
                progress_bar.update(processed)
        
        if show_progress:
            progress_bar.finish()
        
        # ğŸ”¥ NEXUSçµ±åˆ: å®Œå…¨ãªé€†å¤‰æ›ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        consolidated_groups = {}
        nexus_consolidation_map = {}
        new_group_id = 0
        
        for signature, group_list in element_signature_map.items():
            if len(group_list) == 1:
                # å˜ä¸€ã‚°ãƒ«ãƒ¼ãƒ—: ãã®ã¾ã¾ä¿æŒï¼ˆé€†å¤‰æ›ãªã—ï¼‰
                original_group, original_id = group_list[0]
                consolidated_groups[original_group] = new_group_id
                nexus_consolidation_map[original_id] = {
                    'nexus_new_group_id': new_group_id,
                    'nexus_canonical_form': original_group,
                    'nexus_layer': 1,
                    'nexus_consolidation_type': 'identity',
                    'nexus_original_group': original_group,  # ğŸ”¥ å®Œå…¨é€†å¤‰æ›ãƒ‡ãƒ¼ã‚¿
                    'nexus_exact_reconstruction': True
                }
            else:
                # è¤‡æ•°ã‚°ãƒ«ãƒ¼ãƒ—: ä»£è¡¨é¸å‡º + å®Œå…¨é€†å¤‰æ›ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                canonical_group = min(group_list, key=lambda x: len(str(x[0])))[0]
                consolidated_groups[canonical_group] = new_group_id
                
                # ğŸ”¥ NEXUS: ã™ã¹ã¦ã®çµ±åˆã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                for original_group, original_id in group_list:
                    nexus_consolidation_map[original_id] = {
                        'nexus_new_group_id': new_group_id,
                        'nexus_canonical_form': canonical_group,
                        'nexus_layer': 1,
                        'nexus_consolidation_type': 'exact_match',
                        'nexus_original_group': original_group,  # ğŸ”¥ å®Œå…¨ãªå…ƒãƒ‡ãƒ¼ã‚¿
                        'nexus_exact_reconstruction': True,
                        'nexus_signature': signature,  # ğŸ”¥ çµ±åˆã‚­ãƒ¼ä¿å­˜
                        'nexus_group_list': [g[0] for g in group_list]  # ğŸ”¥ ã™ã¹ã¦ã®é–¢é€£ã‚°ãƒ«ãƒ¼ãƒ—
                    }
            
            new_group_id += 1
        
        return consolidated_groups, nexus_consolidation_map
    
    def _nexus_layer2_pattern_consolidation(self, groups_dict: Dict[Tuple, int], layer1_map: Dict, show_progress: bool) -> Tuple[Dict[Tuple, int], Dict[int, Dict]]:
        """
        ğŸ”¥ NEXUS LAYER 2: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹çµ±åˆï¼ˆNEXUSç†è«–æ„ŸæŸ“ç‰ˆï¼‰
        
        NEXUSåŸå‰‡: ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±åˆã§ã‚‚å®Œå…¨ãªé€†å¤‰æ›ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        """
        pattern_groups = {}
        
        if show_progress:
            progress_bar = ProgressBar(len(groups_dict), "   NEXUS Layer 2: Pattern matching")
        
        processed = 0
        for group_tuple, group_id in groups_dict.items():
            pattern_sig = self._extract_pattern_signature(list(group_tuple))
            
            if pattern_sig not in pattern_groups:
                pattern_groups[pattern_sig] = []
            pattern_groups[pattern_sig].append((group_tuple, group_id))
            
            processed += 1
            if show_progress and processed % 2000 == 0:
                progress_bar.update(processed)
        
        if show_progress:
            progress_bar.finish()
        
        # ğŸ”¥ NEXUS ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±åˆ: å®Œå…¨ãªé€†å¤‰æ›ãƒã‚§ãƒ¼ãƒ³ä¿å­˜
        consolidated = {}
        nexus_pattern_map = {}
        new_id = 0
        
        for pattern_sig, group_list in pattern_groups.items():
            if len(group_list) == 1:
                # å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³: ãã®ã¾ã¾ä¿æŒ
                group_tuple, original_id = group_list[0]
                consolidated[group_tuple] = new_id
                nexus_pattern_map[original_id] = {
                    'nexus_new_group_id': new_id,
                    'nexus_canonical_form': group_tuple,
                    'nexus_layer': 2,
                    'nexus_consolidation_type': 'pattern_identity',
                    'nexus_original_group': group_tuple,  # ğŸ”¥ å®Œå…¨ãªå…ƒãƒ‡ãƒ¼ã‚¿
                    'nexus_pattern_signature': pattern_sig,  # ğŸ”¥ ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜
                    'nexus_exact_reconstruction': True,
                    'nexus_layer1_inheritance': layer1_map.get(original_id, {})  # ğŸ”¥ å‰å±¤ã‹ã‚‰ã®ç¶™æ‰¿
                }
            else:
                # è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³: ä»£è¡¨é¸å‡º + å®Œå…¨é€†å¤‰æ›ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                representative = min(group_list, key=lambda x: sum(x[0]))[0]
                consolidated[representative] = new_id
                
                # ğŸ”¥ NEXUS: ã™ã¹ã¦ã®ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±åˆã‚°ãƒ«ãƒ¼ãƒ—ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                for group_tuple, original_id in group_list:
                    nexus_pattern_map[original_id] = {
                        'nexus_new_group_id': new_id,
                        'nexus_canonical_form': representative,
                        'nexus_layer': 2,
                        'nexus_consolidation_type': 'pattern_match',
                        'nexus_original_group': group_tuple,  # ğŸ”¥ å®Œå…¨ãªå…ƒãƒ‡ãƒ¼ã‚¿
                        'nexus_pattern_signature': pattern_sig,  # ğŸ”¥ ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜
                        'nexus_exact_reconstruction': True,
                        'nexus_pattern_group_list': [g[0] for g in group_list],  # ğŸ”¥ å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
                        'nexus_layer1_inheritance': layer1_map.get(original_id, {})  # ğŸ”¥ å‰å±¤ã‹ã‚‰ã®ç¶™æ‰¿
                    }
            
            new_id += 1
        
        return consolidated, nexus_pattern_map

    def _nexus_layer3_approximate_consolidation(self, groups_dict: Dict[Tuple, int], layer2_map: Dict, show_progress: bool) -> Tuple[Dict[Tuple, int], Dict[int, Dict]]:
        """
        ğŸ”¥ NEXUS LAYER 3: è¿‘ä¼¼çµ±åˆï¼ˆNEXUSç†è«–æ„ŸæŸ“ç‰ˆï¼‰
        
        NEXUSåŸå‰‡: è¿‘ä¼¼çµ±åˆã§ã‚‚å®Œå…¨å¯é€†æ€§ä¿è¨¼ - é¡ä¼¼åº¦æƒ…å ±ã‚‚ä¿å­˜
        """
        
        # ğŸ”¥ NEXUS: ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹è¿‘ä¼¼çµ±åˆ + å®Œå…¨é€†å¤‰æ›ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        similarity_hash_map = {}  # similarity_hash -> [(group_tuple, group_id)]
        
        if show_progress:
            progress_bar = ProgressBar(len(groups_dict), "   NEXUS Layer 3: Computing similarity hashes")
        
        processed = 0
        for group_tuple, group_id in groups_dict.items():
            # é«˜é€Ÿé¡ä¼¼æ€§ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆNEXUSæ„ŸæŸ“ç‰ˆï¼‰
            similarity_hash = self._nexus_compute_similarity_hash(group_tuple)
            
            if similarity_hash not in similarity_hash_map:
                similarity_hash_map[similarity_hash] = []
            similarity_hash_map[similarity_hash].append((group_tuple, group_id))
            
            processed += 1
            if show_progress and processed % 2000 == 0:
                progress_bar.update(processed)
        
        if show_progress:
            progress_bar.finish()
        
        print(f"   [Layer 3] Generated {len(similarity_hash_map):,} similarity buckets")
        
        # ğŸ”¥ NEXUSè¿‘ä¼¼çµ±åˆ: å®Œå…¨ãªé€†å¤‰æ›ãƒã‚§ãƒ¼ãƒ³ä¿å­˜
        nexus_consolidated = {}
        nexus_approximate_map = {}
        new_id = 0
        
        for sim_hash, group_list in similarity_hash_map.items():
            if len(group_list) == 1:
                # å˜ä¸€é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—: ãã®ã¾ã¾ä¿æŒ
                group_tuple, original_id = group_list[0]
                nexus_consolidated[group_tuple] = new_id
                nexus_approximate_map[original_id] = {
                    'nexus_new_group_id': new_id,
                    'nexus_canonical_form': group_tuple,
                    'nexus_layer': 3,
                    'nexus_consolidation_type': 'approximate_identity',
                    'nexus_original_group': group_tuple,  # ğŸ”¥ å®Œå…¨ãªå…ƒãƒ‡ãƒ¼ã‚¿
                    'nexus_similarity_hash': sim_hash,  # ğŸ”¥ é¡ä¼¼æ€§æƒ…å ±ä¿å­˜
                    'nexus_exact_reconstruction': True,
                    'nexus_layer2_inheritance': layer2_map.get(original_id, {})  # ğŸ”¥ å‰å±¤ã‹ã‚‰ã®ç¶™æ‰¿
                }
            else:
                # è¤‡æ•°é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—: ä»£è¡¨é¸å‡º + å®Œå…¨é€†å¤‰æ›ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                representative = min(group_list, key=lambda x: len(str(x[0])))[0]
                nexus_consolidated[representative] = new_id
                
                # ğŸ”¥ NEXUS: ã™ã¹ã¦ã®é¡ä¼¼çµ±åˆã‚°ãƒ«ãƒ¼ãƒ—ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                for group_tuple, original_id in group_list:
                    nexus_approximate_map[original_id] = {
                        'nexus_new_group_id': new_id,
                        'nexus_canonical_form': representative,
                        'nexus_layer': 3,
                        'nexus_consolidation_type': 'approximate_match',
                        'nexus_original_group': group_tuple,  # ğŸ”¥ å®Œå…¨ãªå…ƒãƒ‡ãƒ¼ã‚¿
                        'nexus_similarity_hash': sim_hash,  # ğŸ”¥ é¡ä¼¼æ€§æƒ…å ±ä¿å­˜
                        'nexus_exact_reconstruction': True,
                        'nexus_similarity_group_list': [g[0] for g in group_list],  # ğŸ”¥ å…¨é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—
                        'nexus_layer2_inheritance': layer2_map.get(original_id, {}),  # ğŸ”¥ å‰å±¤ã‹ã‚‰ã®ç¶™æ‰¿
                        'nexus_similarity_score': self._nexus_compute_similarity_score(group_tuple, representative)  # ğŸ”¥ é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
                    }
            
            new_id += 1
        
        return nexus_consolidated, nexus_approximate_map

    def _nexus_compute_similarity_hash(self, group_tuple: Tuple) -> str:
        """ğŸ”¥ NEXUS: é¡ä¼¼æ€§ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ï¼ˆæ„ŸæŸ“ç‰ˆï¼‰"""
        # è¦ç´ ã®çµ±è¨ˆçš„ç‰¹å¾´ã‚’æŠ½å‡º
        group_list = list(group_tuple)
        if not group_list:
            return "empty"
        
        # NEXUSæ„ŸæŸ“: ã‚ˆã‚Šç²¾å¯†ãªé¡ä¼¼æ€§ãƒãƒƒã‚·ãƒ¥
        mean_val = sum(group_list) / len(group_list)
        variance = sum((x - mean_val) ** 2 for x in group_list) / len(group_list)
        return f"sim_{len(group_list)}_{int(mean_val)}_{int(variance)}"
    
    def _nexus_compute_similarity_score(self, group1: Tuple, group2: Tuple) -> float:
        """ğŸ”¥ NEXUS: é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå®Œå…¨å¯é€†æ€§ä¿è¨¼ï¼‰"""
        if group1 == group2:
            return 1.0
        
        # çµ±è¨ˆçš„é¡ä¼¼åº¦è¨ˆç®—
        list1, list2 = list(group1), list(group2)
        if not list1 or not list2:
            return 0.0
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆæ­£è¦åŒ–ï¼‰
        dot_product = sum(a * b for a, b in zip(list1, list2))
        norm1 = sum(a * a for a in list1) ** 0.5
        norm2 = sum(b * b for b in list2) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
        
        bucket_processed = 0
        for similarity_hash, group_list in similarity_hash_map.items():
            if len(group_list) == 1:
                # å˜ä¸€ã‚°ãƒ«ãƒ¼ãƒ—ï¼šçµ±åˆä¸è¦
                group_tuple, group_id = group_list[0]
                consolidated[group_tuple] = new_id
                approx_map[group_id] = {
                    'new_group_id': new_id,
                    'canonical_form': group_tuple,
                    'layer': 3,
                    'consolidation_type': 'none'
                }
                new_id += 1
            else:
                # è¤‡æ•°ã‚°ãƒ«ãƒ¼ãƒ—ï¼šè©³ç´°é¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯ã§çµ±åˆ
                clusters = self._cluster_similar_groups(group_list)
                
                for cluster in clusters:
                    # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ä»£è¡¨ã‚’é¸æŠ
                    representative = min(cluster, key=lambda x: (len(x[0]), sum(x[0])))[0]
                    consolidated[representative] = new_id
                    
                    for group_tuple, group_id in cluster:
                        approx_map[group_id] = {
                            'new_group_id': new_id,
                            'canonical_form': representative,
                            'layer': 3,
                            'consolidation_type': 'approximate' if len(cluster) > 1 else 'none'
                        }
                    
                    new_id += 1
            
            bucket_processed += 1
            if show_progress and bucket_processed % 100 == 0:
                progress_bar.update(bucket_processed)
        
        if show_progress:
            progress_bar.finish()
        
        return consolidated, approx_map
    
    def _compute_similarity_hash(self, elements: list) -> tuple:
        """é«˜é€Ÿé¡ä¼¼æ€§ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        if not elements:
            return (0, 0, 0)
        
        # çµ±è¨ˆçš„ç‰¹å¾´ã«ã‚ˆã‚‹é«˜é€Ÿãƒãƒƒã‚·ãƒ¥
        element_sum = sum(elements)
        element_min = min(elements)
        element_max = max(elements)
        
        # é‡å­åŒ–ã«ã‚ˆã‚‹è¿‘ä¼¼
        sum_bucket = element_sum // max(1, len(elements))  # å¹³å‡å€¤ãƒã‚±ãƒƒãƒˆ
        range_bucket = (element_max - element_min) // 10   # ãƒ¬ãƒ³ã‚¸ãƒã‚±ãƒƒãƒˆ
        
        return (sum_bucket, range_bucket, len(elements))
    
    def _cluster_similar_groups(self, group_list: list) -> list:
        """ã‚°ãƒ«ãƒ¼ãƒ—ãƒªã‚¹ãƒˆå†…ã§ã®åŠ¹ç‡çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        if len(group_list) <= 1:
            return [group_list]
        
        clusters = []
        used = set()
        
        for i, (group_tuple, group_id) in enumerate(group_list):
            if i in used:
                continue
            
            # æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é–‹å§‹
            cluster = [(group_tuple, group_id)]
            used.add(i)
            
            # é¡ä¼¼ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ¤œç´¢ï¼ˆé–¾å€¤0.8ï¼‰
            for j, (other_tuple, other_id) in enumerate(group_list[i+1:], i+1):
                if j in used:
                    continue
                
                if self._are_groups_similar(list(group_tuple), list(other_tuple), threshold=0.8):
                    cluster.append((other_tuple, other_id))
                    used.add(j)
            
            clusters.append(cluster)
        
        return clusters
    
    def _nexus_layer4_structural_consolidation(self, groups_dict: Dict[Tuple, int], layer3_map: Dict, show_progress: bool) -> Tuple[Dict[Tuple, int], Dict[int, Dict]]:
        """
        ğŸ”¥ NEXUS LAYER 4: æ§‹é€ çµ±åˆï¼ˆNEXUSç†è«–å®Œå…¨æ„ŸæŸ“ç‰ˆï¼‰
        
        NEXUSåŸå‰‡: æ§‹é€ çµ±åˆã§ã‚‚å®Œå…¨å¯é€†æ€§ä¿è¨¼ - æ§‹é€ æƒ…å ±ã‚‚å®Œå…¨ä¿å­˜
        """
        nexus_structural_groups = {}
        
        if show_progress:
            progress_bar = ProgressBar(len(groups_dict), "   NEXUS Layer 4: Structural analysis")
        
        processed = 0
        for group_tuple, group_id in groups_dict.items():
            # ğŸ”¥ NEXUSæ„ŸæŸ“: æ§‹é€ ã‚·ã‚°ãƒãƒãƒ£è¨ˆç®—
            struct_sig = self._nexus_compute_structural_signature(group_tuple)
            
            if struct_sig not in nexus_structural_groups:
                nexus_structural_groups[struct_sig] = []
            nexus_structural_groups[struct_sig].append((group_tuple, group_id))
            
            processed += 1
            if show_progress and processed % 2000 == 0:
                progress_bar.update(processed)
        
        if show_progress:
            progress_bar.finish()
        
        print(f"   [Layer 4] Generated {len(nexus_structural_groups):,} structural patterns")
        
        # ğŸ”¥ NEXUSæ§‹é€ çµ±åˆ: å®Œå…¨ãªé€†å¤‰æ›ãƒã‚§ãƒ¼ãƒ³ä¿å­˜
        nexus_final_consolidated = {}
        nexus_structural_map = {}
        new_id = 0
        
        for struct_sig, group_list in nexus_structural_groups.items():
            if len(group_list) == 1:
                # å˜ä¸€æ§‹é€ : ãã®ã¾ã¾ä¿æŒ
                group_tuple, original_id = group_list[0]
                nexus_final_consolidated[group_tuple] = new_id
                nexus_structural_map[original_id] = {
                    'nexus_new_group_id': new_id,
                    'nexus_canonical_form': group_tuple,
                    'nexus_layer': 4,
                    'nexus_consolidation_type': 'structural_identity',
                    'nexus_original_group': group_tuple,  # ğŸ”¥ å®Œå…¨ãªå…ƒãƒ‡ãƒ¼ã‚¿
                    'nexus_structural_signature': struct_sig,  # ğŸ”¥ æ§‹é€ æƒ…å ±ä¿å­˜
                    'nexus_exact_reconstruction': True,
                    'nexus_layer3_inheritance': layer3_map.get(original_id, {})  # ğŸ”¥ å‰å±¤ã‹ã‚‰ã®ç¶™æ‰¿
                }
            else:
                # è¤‡æ•°æ§‹é€ : ä»£è¡¨é¸å‡º + å®Œå…¨é€†å¤‰æ›ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                representative = min(group_list, key=lambda x: (len(str(x[0])), sum(x[0])))[0]
                nexus_final_consolidated[representative] = new_id
                
                # ğŸ”¥ NEXUS: ã™ã¹ã¦ã®æ§‹é€ çµ±åˆã‚°ãƒ«ãƒ¼ãƒ—ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                for group_tuple, original_id in group_list:
                    nexus_structural_map[original_id] = {
                        'nexus_new_group_id': new_id,
                        'nexus_canonical_form': representative,
                        'nexus_layer': 4,
                        'nexus_consolidation_type': 'structural_match',
                        'nexus_original_group': group_tuple,  # ğŸ”¥ å®Œå…¨ãªå…ƒãƒ‡ãƒ¼ã‚¿
                        'nexus_structural_signature': struct_sig,  # ğŸ”¥ æ§‹é€ æƒ…å ±ä¿å­˜
                        'nexus_exact_reconstruction': True,
                        'nexus_structural_group_list': [g[0] for g in group_list],  # ğŸ”¥ å…¨æ§‹é€ ã‚°ãƒ«ãƒ¼ãƒ—
                        'nexus_layer3_inheritance': layer3_map.get(original_id, {}),  # ğŸ”¥ å‰å±¤ã‹ã‚‰ã®ç¶™æ‰¿
                        'nexus_structural_complexity': self._nexus_compute_structural_complexity(group_tuple)  # ğŸ”¥ æ§‹é€ è¤‡é›‘åº¦
                    }
            
            new_id += 1
        
        return nexus_final_consolidated, nexus_structural_map

    def _nexus_compute_structural_signature(self, group_tuple: Tuple) -> str:
        """ğŸ”¥ NEXUS: æ§‹é€ ã‚·ã‚°ãƒãƒãƒ£è¨ˆç®—ï¼ˆæ„ŸæŸ“ç‰ˆï¼‰"""
        group_list = list(group_tuple)
        if not group_list:
            return "empty_struct"
        
        # NEXUSæ„ŸæŸ“: é«˜ç²¾åº¦æ§‹é€ è§£æ
        length = len(group_list)
        zero_count = group_list.count(0)
        non_zero_count = length - zero_count
        unique_count = len(set(group_list))
        
        # æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³è­˜åˆ¥
        if all(x == 0 for x in group_list):
            pattern = "all_zero"
        elif zero_count == 0:
            pattern = "no_zero"
        elif zero_count > non_zero_count:
            pattern = "mostly_zero"
        else:
            pattern = "mixed"
        
        return f"struct_{pattern}_{length}_{unique_count}_{zero_count}"
    
    def _nexus_compute_structural_complexity(self, group_tuple: Tuple) -> float:
        """ğŸ”¥ NEXUS: æ§‹é€ è¤‡é›‘åº¦è¨ˆç®—ï¼ˆå®Œå…¨å¯é€†æ€§ä¿è¨¼ï¼‰"""
        group_list = list(group_tuple)
        if not group_list:
            return 0.0
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹è¤‡é›‘åº¦
        from collections import Counter
        counts = Counter(group_list)
        total = len(group_list)
        
        entropy = -sum((count / total) * math.log2(count / total) for count in counts.values())
        return entropy

    def _build_nexus_reconstruction_chain(self, layer1_map: Dict, layer2_map: Dict, layer3_map: Dict, layer4_map: Dict) -> Dict:
        """
        ğŸ”¥ NEXUS: å®Œå…¨é€†å¤‰æ›ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
        
        NEXUSåŸå‰‡: 4å±¤ã™ã¹ã¦ã®å¤‰æ›ã‚’å®Œå…¨ã«é€†å¤‰æ›å¯èƒ½ã«ã™ã‚‹
        """
        nexus_master_chain = {}
        
        # ã™ã¹ã¦ã®ãƒãƒƒãƒ—ã‚’çµåˆã—ã€å®Œå…¨ãªé€†å¤‰æ›ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
        all_maps = [layer1_map, layer2_map, layer3_map, layer4_map]
        
        for layer_idx, layer_map in enumerate(all_maps, 1):
            for original_id, mapping_data in layer_map.items():
                if original_id not in nexus_master_chain:
                    nexus_master_chain[original_id] = {
                        'nexus_reconstruction_chain': [],
                        'nexus_final_group_id': None,
                        'nexus_original_group': None,
                        'nexus_exact_reconstruction': True
                    }
                
                # ğŸ”¥ NEXUS: å„å±¤ã®å¤‰æ›ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                nexus_master_chain[original_id]['nexus_reconstruction_chain'].append({
                    'layer': layer_idx,
                    'transformation_data': mapping_data
                })
                
                # æœ€çµ‚ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’æ›´æ–°
                if 'nexus_new_group_id' in mapping_data:
                    nexus_master_chain[original_id]['nexus_final_group_id'] = mapping_data['nexus_new_group_id']
                
                # å…ƒã®ã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                if 'nexus_original_group' in mapping_data:
                    nexus_master_chain[original_id]['nexus_original_group'] = mapping_data['nexus_original_group']
        
        return nexus_master_chain
        
        if show_progress:
            progress_bar = ProgressBar(len(groups_dict), "   Layer 4: Structural analysis")
        
        processed = 0
        for group_tuple, group_id in groups_dict.items():
            # é«˜é€Ÿæ§‹é€ ã‚·ã‚°ãƒãƒãƒ£è¨ˆç®—
            struct_sig = self._extract_structural_signature_fast(list(group_tuple))
            
            if struct_sig not in structural_groups:
                structural_groups[struct_sig] = []
            structural_groups[struct_sig].append((group_tuple, group_id))
            
            processed += 1
            if show_progress and processed % 2000 == 0:
                progress_bar.update(processed)
        
        if show_progress:
            progress_bar.finish()
        
        print(f"   [Layer 4] Generated {len(structural_groups):,} structural patterns")
        
        # æ§‹é€ çµ±åˆ
        consolidated = {}
        struct_map = {}
        new_id = 0
        
        for struct_sig, group_list in structural_groups.items():
            if len(group_list) == 1:
                group_tuple, original_id = group_list[0]
                consolidated[group_tuple] = new_id
                struct_map[original_id] = {
                    'new_group_id': new_id,
                    'canonical_form': group_tuple,
                    'layer': 4,
                    'consolidation_type': 'none'
                }
            else:
                representative = min(group_list, key=lambda x: (len(x[0]), sum(x[0])))[0]
                consolidated[representative] = new_id
                
                for group_tuple, original_id in group_list:
                    struct_map[original_id] = {
                        'new_group_id': new_id,
                        'canonical_form': representative,
                        'layer': 4,
                        'consolidation_type': 'structural'
                    }
            
            new_id += 1
        
        return consolidated, struct_map
    
    def _extract_structural_signature_fast(self, elements: list) -> tuple:
        """é«˜é€Ÿæ§‹é€ ç‰¹å¾´æŠ½å‡º"""
        if not elements:
            return (0, 0, 0, 0)
        
        # åŸºæœ¬çµ±è¨ˆï¼ˆé«˜é€Ÿè¨ˆç®—ï¼‰
        element_sum = sum(elements)
        element_count = len(elements)
        
        # é †åºæ€§ï¼ˆç°¡ç•¥ç‰ˆï¼‰
        is_monotonic = all(elements[i] <= elements[i+1] for i in range(len(elements)-1)) or \
                      all(elements[i] >= elements[i+1] for i in range(len(elements)-1))
        
        # åˆ†æ•£ã®è¿‘ä¼¼ï¼ˆæ¨™æº–åå·®ã®ä»£ã‚ã‚Šï¼‰
        avg = element_sum / element_count
        variance_approx = sum((x - avg) ** 2 for x in elements) / element_count
        variance_bucket = int(variance_approx) // 100  # 100å˜ä½ã§ãƒã‚±ãƒƒãƒˆåŒ–
        
        # æœ€é »å€¤ã®è¿‘ä¼¼
        if element_count <= 20:
            mode_approx = max(set(elements), key=elements.count)
        else:
            # å¤§ããªãƒªã‚¹ãƒˆã§ã¯è¿‘ä¼¼
            mode_approx = elements[element_count // 2]  # ä¸­å¤®å€¤ã§è¿‘ä¼¼
        
        return (is_monotonic, variance_bucket, mode_approx, element_count)
    
    def _extract_pattern_signature(self, elements: list) -> tuple:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´æŠ½å‡º"""
        if not elements:
            return (0, 0, 0)
        
        # åŸºæœ¬çµ±è¨ˆ
        most_common = max(set(elements), key=elements.count)
        value_range = max(elements) - min(elements)
        
        # å·®åˆ†ãƒ‘ã‚¿ãƒ¼ãƒ³
        diffs = [elements[i+1] - elements[i] for i in range(len(elements)-1)]
        diff_pattern = tuple(sorted(set(diffs))[:3])
        
        return (most_common, value_range, diff_pattern)
    
    def _extract_structural_signature(self, elements: list) -> tuple:
        """æ§‹é€ ç‰¹å¾´æŠ½å‡º"""
        if not elements:
            return (0, 0, 0, 0)
        
        # é †åºæ€§
        is_ascending = all(elements[i] <= elements[i+1] for i in range(len(elements)-1))
        is_descending = all(elements[i] >= elements[i+1] for i in range(len(elements)-1))
        
        # å‘¨æœŸæ€§
        period = self._detect_period(elements)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        entropy = self._calculate_element_entropy(elements)
        
        return (is_ascending, is_descending, period, int(entropy * 100))
    
    def _are_groups_similar(self, elements1: list, elements2: list, threshold: float = 0.8) -> bool:
        """ã‚°ãƒ«ãƒ¼ãƒ—é¡ä¼¼æ€§åˆ¤å®š"""
        if len(elements1) != len(elements2):
            return False
        
        matches = sum(1 for a, b in zip(elements1, elements2) if a == b)
        similarity = matches / len(elements1)
        
        return similarity >= threshold
    
    def _detect_period(self, elements: list) -> int:
        """å‘¨æœŸæ€§æ¤œå‡º"""
        if len(elements) < 4:
            return 0
        
        for period in range(1, len(elements) // 2 + 1):
            is_periodic = True
            for i in range(period, len(elements)):
                if elements[i] != elements[i % period]:
                    is_periodic = False
                    break
            if is_periodic:
                return period
        
        return 0
    
    def _calculate_element_entropy(self, elements: list) -> float:
        """è¦ç´ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if not elements:
            return 0
        
        from collections import Counter
        counts = Counter(elements)
        total = len(elements)
        
        entropy = 0
        for count in counts.values():
            p = count / total
            if p > 0:
                entropy -= p * math.log2(p)
        
        return entropy

    def _calculate_permutation_map_fast(self, original_group: Tuple, canonical_group: Tuple) -> Tuple[int, ...]:
        """é«˜é€Ÿé †åˆ—ãƒãƒƒãƒ—è¨ˆç®—ï¼ˆåŠ¹ç‡åŒ–ç‰ˆï¼‰"""
        if len(original_group) != len(canonical_group):
            return tuple(range(len(original_group)))
        
        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ—ã‚’äº‹å‰æ§‹ç¯‰ï¼ˆç·šå½¢æ¤œç´¢ã‚’é¿ã‘ã‚‹ï¼‰
            canonical_index_map = {}
            available_indices = list(range(len(canonical_group)))
            
            for i, val in enumerate(canonical_group):
                if val not in canonical_index_map:
                    canonical_index_map[val] = []
                canonical_index_map[val].append(i)
            
            # åŠ¹ç‡çš„ãªé †åˆ—è¨ˆç®—
            permutation = []
            used_indices = set()
            
            for element in original_group:
                if element in canonical_index_map:
                    # ä½¿ç”¨å¯èƒ½ãªæœ€åˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¸æŠ
                    available = [idx for idx in canonical_index_map[element] if idx not in used_indices]
                    if available:
                        idx = available[0]
                        permutation.append(idx)
                        used_indices.add(idx)
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        permutation.append(len(permutation))
                else:
                    # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    permutation.append(len(permutation))
            
            return tuple(permutation)
        except Exception:
            # ä»»æ„ã®ã‚¨ãƒ©ãƒ¼ã«å¯¾ã™ã‚‹å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return tuple(range(len(original_group)))
    
    def _calculate_permutation_map(self, original_group: Tuple, canonical_group: Tuple) -> Tuple[int, ...]:
        """å…ƒã®é †åºã‹ã‚‰ä»£è¡¨å½¢çŠ¶ã¸ã®å¤‰æ›ãƒãƒƒãƒ—ã‚’è¨ˆç®—"""
        if len(original_group) != len(canonical_group):
            # é•·ã•ãŒç•°ãªã‚‹å ´åˆã¯æ’ç­‰å¤‰æ›ï¼ˆå®‰å…¨æ€§ç¢ºä¿ï¼‰
            return tuple(range(len(original_group)))
        
        try:
            # å…ƒã®å„è¦ç´ ãŒä»£è¡¨å½¢çŠ¶ã®ã©ã®ä½ç½®ã«ã‚ã‚‹ã‹ã‚’è¨ˆç®—
            canonical_list = list(canonical_group)
            permutation = []
            
            for element in original_group:
                # ä»£è¡¨å½¢çŠ¶ã®ä¸­ã§ã®æœ€åˆã®å‡ºç¾ä½ç½®ã‚’æ¤œç´¢
                idx = canonical_list.index(element)
                permutation.append(idx)
                # ä½¿ç”¨æ¸ˆã¿ã®ä½ç½®ã‚’ãƒãƒ¼ã‚¯ï¼ˆé‡è¤‡è¦ç´ å¯¾å¿œï¼‰
                canonical_list[idx] = None
            
            return tuple(permutation)
        except (ValueError, IndexError):
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯æ’ç­‰å¤‰æ›ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return tuple(range(len(original_group)))
    
    def _apply_shape_transformation(self, blocks: List[Tuple], consolidation_map: Dict[int, Dict], 
                                   normalized_groups: Dict[Tuple, int]) -> Tuple[List[int], List[Tuple]]:
        """å½¢çŠ¶å¤‰æ›ã‚’é©ç”¨ã—ã¦ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä»£è¡¨å½¢çŠ¶ã«çµ±ä¸€ï¼ˆæ ¹æœ¬çš„åŠ¹ç‡åŒ–ç‰ˆï¼‰"""
        
        # äº‹å‰è¨ˆç®—ï¼šæ­£è¦åŒ–â†’ã‚°ãƒ«ãƒ¼ãƒ—IDã®ãƒãƒƒã‚·ãƒ¥ãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
        normalized_to_group_id = {}
        for normalized, group_id in normalized_groups.items():
            normalized_to_group_id[normalized] = group_id
        
        # äº‹å‰è¨ˆç®—ï¼šæ­£è¦åŒ–å½¢å¼ã®é †åˆ—ãƒãƒƒãƒ—ã‚’äº‹å‰æ§‹ç¯‰
        normalized_perm_cache = {}  # normalized -> index_map
        for normalized in normalized_groups.keys():
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ—ã‚’äº‹å‰è¨ˆç®—ï¼ˆç·šå½¢æ¤œç´¢ã‚’é¿ã‘ã‚‹ï¼‰
            index_map = {}
            for i, val in enumerate(normalized):
                if val not in index_map:
                    index_map[val] = i
            normalized_perm_cache[normalized] = index_map
        
        # ğŸ”¥ NEXUSæ„ŸæŸ“: çµ±åˆæƒ…å ±ã®äº‹å‰è§£æï¼ˆæ–°æ—§å½¢å¼å¯¾å¿œï¼‰
        consolidated_canonical_cache = {}  # group_id -> (canonical_form, canonical_index_map)
        for group_id, consolidation_info in consolidation_map.items():
            # ğŸ”¥ NEXUSæ„ŸæŸ“ç‰ˆã®æ–°ã—ã„ã‚­ãƒ¼åã«å¯¾å¿œ
            nexus_consolidation_type = consolidation_info.get('nexus_consolidation_type', consolidation_info.get('consolidation_type', 'none'))
            
            if nexus_consolidation_type != 'none' and nexus_consolidation_type != 'identity':
                # ğŸ”¥ NEXUSæ„ŸæŸ“: æ–°ã—ã„ã‚­ãƒ¼åã‚’å„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§æ—§ã‚­ãƒ¼å
                canonical_form = consolidation_info.get('nexus_canonical_form', consolidation_info.get('canonical_form'))
                
                if canonical_form is not None:
                    canonical_sorted = tuple(sorted(canonical_form))
                    
                    # ä»£è¡¨å½¢çŠ¶ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ—ã‚’äº‹å‰è¨ˆç®—
                    canonical_index_map = {}
                    for i, val in enumerate(canonical_sorted):
                        if val not in canonical_index_map:
                            canonical_index_map[val] = i
                    
                    consolidated_canonical_cache[group_id] = (canonical_sorted, canonical_index_map)
        
        print(f"   [Shape Transformation] Pre-computed {len(normalized_perm_cache):,} permutation maps")
        print(f"   [Shape Transformation] Pre-computed {len(consolidated_canonical_cache):,} canonical forms")
        
        # é«˜é€Ÿå¤‰æ›å‡¦ç†ï¼ˆäº‹å‰è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
        transformed_group_ids = []
        transformed_perm_maps = []
        
        # é€²æ—ãƒãƒ¼ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆï¼‰
        if len(blocks) > 100000:
            progress_bar = ProgressBar(len(blocks), "   Shape transformation")
            update_interval = len(blocks) // 100  # 100å›æ›´æ–°
        else:
            progress_bar = None
            update_interval = float('inf')
        
        for i, block in enumerate(blocks):
            # é«˜é€Ÿæ­£è¦åŒ–ï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ã‚¿ãƒ—ãƒ«ï¼‰
            normalized = tuple(sorted(block))
            
            # ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆO(1)ï¼‰
            original_group_id = normalized_to_group_id.get(normalized)
            
            if original_group_id is None:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç¨€ãªã‚±ãƒ¼ã‚¹ï¼‰
                print(f"   [Warning] Missing group for normalized block at index {i}")
                transformed_group_ids.append(0)
                transformed_perm_maps.append(tuple(range(len(block))))
                continue
            
            # ğŸ”¥ NEXUS ULTRA-PRECISION: çµ±åˆç„¡åŠ¹åŒ–æ™‚ã¯ç›´æ¥original_group_idã‚’ä½¿ç”¨
            if len(consolidation_map) == 0:
                # çµ±åˆãªã—ï¼šç›´æ¥ãƒãƒƒãƒ”ãƒ³ã‚°
                transformed_group_ids.append(original_group_id)
                # æ­£è¦åŒ–â†’å…ƒã®é †åºã¸ã®é †åˆ—ãƒãƒƒãƒ—ã‚’è¨ˆç®—
                perm_map = self._calculate_permutation_map_fast(block, normalized)
                transformed_perm_maps.append(perm_map)
            elif original_group_id in consolidation_map:
                consolidation_info = consolidation_map[original_group_id]
                # ğŸ”¥ NEXUSæ„ŸæŸ“: æ–°æ—§ã‚­ãƒ¼åå¯¾å¿œï¼ˆãƒ‡ãƒãƒƒã‚°ä»˜ãï¼‰
                new_group_id = consolidation_info.get('nexus_new_group_id', consolidation_info.get('new_group_id'))
                
                # ğŸ”¥ NEXUS: new_group_idãŒNoneã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if new_group_id is None:
                    # è¤‡æ•°ã®ã‚­ãƒ¼ã‚’è©¦ã™
                    for possible_key in ['nexus_final_group_id', 'group_id', 'id']:
                        if possible_key in consolidation_info:
                            new_group_id = consolidation_info[possible_key]
                            break
                    
                    # ãã‚Œã§ã‚‚Noneã®å ´åˆã¯ã€original_group_idã‚’ä½¿ç”¨
                    if new_group_id is None:
                        new_group_id = original_group_id
                
                nexus_consolidation_type = consolidation_info.get('nexus_consolidation_type', consolidation_info.get('consolidation_type', 'none'))
                
                if nexus_consolidation_type != 'none' and nexus_consolidation_type != 'identity' and original_group_id in consolidated_canonical_cache:
                    # çµ±åˆã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯ï¼šäº‹å‰è¨ˆç®—æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                    canonical_sorted, canonical_index_map = consolidated_canonical_cache[original_group_id]
                    
                    # é«˜é€Ÿé †åˆ—ãƒãƒƒãƒ—è¨ˆç®—ï¼ˆäº‹å‰è¨ˆç®—æ¸ˆã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ—ã‚’ä½¿ç”¨ï¼‰
                    try:
                        perm_map = tuple(canonical_index_map.get(val, 0) for val in block)
                    except (KeyError, TypeError):
                        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        perm_map = tuple(range(len(block)))
                else:
                    # çµ±åˆã•ã‚Œã¦ã„ãªã„ãƒ–ãƒ­ãƒƒã‚¯ï¼šäº‹å‰è¨ˆç®—æ¸ˆã¿æ­£è¦åŒ–ãƒãƒƒãƒ—ã‚’ä½¿ç”¨
                    index_map = normalized_perm_cache.get(normalized, {})
                    try:
                        perm_map = tuple(index_map.get(val, 0) for val in block)
                    except (KeyError, TypeError):
                        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        perm_map = tuple(range(len(block)))
                
                transformed_group_ids.append(new_group_id)
                transformed_perm_maps.append(perm_map)
            else:
                # ãƒãƒƒãƒ”ãƒ³ã‚°æƒ…å ±ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                transformed_group_ids.append(0)
                transformed_perm_maps.append(tuple(range(len(block))))
            
            # é€²æ—æ›´æ–°ï¼ˆåŠ¹ç‡åŒ–ï¼‰
            if progress_bar and i % update_interval == 0:
                progress_bar.update(i)
        
        if progress_bar:
            progress_bar.finish()
        
        print(f"   [Shape Transformation] Processed {len(blocks):,} blocks efficiently")
        return transformed_group_ids, transformed_perm_maps
    
    def _transform_block_to_canonical(self, original_block: Tuple, canonical_form: Tuple, permutation_map: Tuple) -> Tuple:
        """å…ƒãƒ–ãƒ­ãƒƒã‚¯ã‚’ä»£è¡¨å½¢çŠ¶ã«å¤‰æ›"""
        try:
            # é †åˆ—ãƒãƒƒãƒ—ã«å¾“ã£ã¦å¤‰æ›
            if len(permutation_map) == len(original_block):
                transformed = tuple(canonical_form[permutation_map[i]] for i in range(len(original_block)))
                return transformed
            else:
                # é•·ã•ãŒåˆã‚ãªã„å ´åˆã¯å…ƒãƒ–ãƒ­ãƒƒã‚¯ã‚’ãã®ã¾ã¾è¿”ã™
                return original_block
        except (IndexError, TypeError):
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å…ƒãƒ–ãƒ­ãƒƒã‚¯ã‚’ãã®ã¾ã¾è¿”ã™
            return original_block

    def _get_blocks_for_shape(self, data: bytes, grid_width: int, shape_coords: Tuple[Tuple[int, int], ...], 
                             show_progress: bool = False) -> List[Tuple[int, ...]]:
        """æŒ‡å®šã•ã‚ŒãŸå½¢çŠ¶ã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã™ã‚‹ï¼ˆé€²æ—ãƒãƒ¼å¯¾å¿œç‰ˆï¼‰"""
        data_len = len(data)
        rows = data_len // grid_width
        shape_width = max(c for r, c in shape_coords) + 1
        shape_height = max(r for r, c in shape_coords) + 1
        
        # å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’ä½¿ç”¨
        if data_len > 50000000:  # 50MBä»¥ä¸Š
            print("   [Block Analysis] Large file detected, using sampling strategy...")
            
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ï¼šå…¨ãƒ‡ãƒ¼ã‚¿ã®ä»£è¡¨çš„ãªéƒ¨åˆ†ã‚’åˆ†æ
            sample_positions = min(rows - shape_height + 1, 10000)  # æœ€å¤§1ä¸‡è¡Œåˆ†æ
            sample_step = max((rows - shape_height + 1) // sample_positions, 1)
            
            blocks = []
            total_samples = sample_positions * (grid_width - shape_width + 1)
            
            if show_progress:
                progress_bar = ProgressBar(total_samples, "   Block sampling")
            
            processed = 0
            for r in range(0, rows - shape_height + 1, sample_step):
                if len(blocks) >= total_samples:
                    break
                for c in range(grid_width - shape_width + 1):
                    block = []
                    valid_block = True
                    
                    base_idx = r * grid_width + c
                    for dr, dc in shape_coords:
                        idx = base_idx + dr * grid_width + dc
                        if idx >= data_len:
                            valid_block = False
                            break
                        block.append(data[idx])
                    
                    if valid_block:
                        blocks.append(tuple(block))
                    
                    processed += 1
                    if show_progress:
                        progress_bar.update(processed)
            
            if show_progress:
                progress_bar.finish()
                print(f"   [Block Analysis] Sampled {len(blocks):,} blocks from large file")
            return blocks
        
        # é€šå¸¸ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºï¼ˆ50MBæœªæº€ï¼‰ã¯å¾“æ¥é€šã‚Š
        total_positions = (rows - shape_height + 1) * (grid_width - shape_width + 1)
        
        blocks = []
        
        if show_progress:
            progress_bar = ProgressBar(total_positions, "   Block generation")

        processed = 0
        for r in range(rows - shape_height + 1):
            for c in range(grid_width - shape_width + 1):
                block = []
                valid_block = True
                
                # é«˜é€ŸåŒ–ï¼šäº‹å‰ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
                base_idx = r * grid_width + c
                for dr, dc in shape_coords:
                    idx = base_idx + dr * grid_width + dc
                    if idx >= data_len:
                        valid_block = False
                        break
                    block.append(data[idx])
                
                if valid_block:
                    blocks.append(tuple(block))
                
                processed += 1
                if show_progress:
                    progress_bar.update(processed)
        
        if show_progress:
            progress_bar.finish()
        
        return blocks

    def _analyze_shape_efficiency(self, data: bytes, grid_width: int, shape_name: str, 
                                 show_progress: bool = False) -> int:
        """æŒ‡å®šå½¢çŠ¶ã§ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ–ãƒ­ãƒƒã‚¯æ•°ã‚’è¨ˆç®—ã—ã€åœ§ç¸®åŠ¹ç‡ã‚’è©•ä¾¡ã™ã‚‹ï¼ˆé€²æ—ãƒãƒ¼ç‰ˆï¼‰"""
        shape_coords = POLYOMINO_SHAPES[shape_name]
        
        blocks = self._get_blocks_for_shape(data, grid_width, shape_coords, show_progress=show_progress)
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ¦ãƒ‹ãƒ¼ã‚¯è¨ˆç®—ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
        if len(blocks) > 100000:  # 10ä¸‡ãƒ–ãƒ­ãƒƒã‚¯ä»¥ä¸Šã®å ´åˆã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if show_progress:
                print(f"   [Shape '{shape_name}'] Large dataset detected, using sampling approach...")
            sample_size = min(len(blocks), 50000)  # 5ä¸‡ãƒ–ãƒ­ãƒƒã‚¯ã‚µãƒ³ãƒ—ãƒ«
            import random
            sampled_blocks = random.sample(blocks, sample_size)
            unique_normalized_groups = set(tuple(sorted(b)) for b in sampled_blocks)
            # ã‚µãƒ³ãƒ—ãƒ«æ¯”ç‡ã§å…¨ä½“ã‚’æ¨å®š
            estimated_unique = int(len(unique_normalized_groups) * (len(blocks) / sample_size))
            return estimated_unique
        else:
            unique_normalized_groups = set(tuple(sorted(b)) for b in blocks)
            return len(unique_normalized_groups)

    def _find_optimal_shape_combination_fast(self, data: bytes, grid_width: int) -> List[str]:
        """é«˜é€Ÿå½¢çŠ¶çµ„ã¿åˆã‚ã›åˆ†æï¼ˆå¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‘ã‘çœŸã®åŠ¹ç‡åŒ–ï¼‰"""
        print("   [Shape Selection] Fast adaptive combination analysis...")
        
        shape_combination = []
        
        # é©å¿œçš„ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†æï¼šãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(data) > 50000000:  # 50MBä»¥ä¸Š
            section_count = 3
            section_size = min(len(data) // 3, 100000)  # æœ€å¤§100KB per section
        elif len(data) > 20000000:  # 20MBä»¥ä¸Š
            section_count = 4  
            section_size = min(len(data) // 4, 80000)   # æœ€å¤§80KB per section
        else:
            section_count = 6
            section_size = min(len(data) // 6, 60000)   # æœ€å¤§60KB per section
        
        # é€²æ—ãƒãƒ¼
        progress_bar = ProgressBar(section_count, "   Fast section analysis")
        
        # æˆ¦ç•¥çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šå…ˆé ­ã€ä¸­å¤®ã€æœ«å°¾ã‚’åˆ†æ
        sample_positions = []
        if section_count >= 3:
            sample_positions.append(0)  # å…ˆé ­
            sample_positions.append(len(data) // 2)  # ä¸­å¤®
            sample_positions.append(max(0, len(data) - section_size))  # æœ«å°¾
            
            # æ®‹ã‚Šã¯ç­‰é–“éš”
            if section_count > 3:
                remaining_sections = section_count - 3
                step = len(data) // (remaining_sections + 1)
                for i in range(1, remaining_sections + 1):
                    pos = min(i * step, len(data) - section_size)
                    if pos not in sample_positions:
                        sample_positions.append(pos)
        else:
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ãŒå°‘ãªã„å ´åˆã¯ç­‰é–“éš”
            step = len(data) // section_count
            for i in range(section_count):
                sample_positions.append(min(i * step, len(data) - section_size))
        
        # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é«˜é€Ÿåˆ†æ
        for i, pos in enumerate(sample_positions[:section_count]):
            section_data = data[pos:pos + section_size]
            if len(section_data) < 5000:  # æœ€å°ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                continue
                
            section_grid_width = math.ceil(math.sqrt(len(section_data)))
            
            # é«˜é€Ÿå½¢çŠ¶é¸æŠï¼ˆä¸Šä½3å½¢çŠ¶ã®ã¿ãƒ†ã‚¹ãƒˆï¼‰
            best_shape = self._select_best_single_shape_ultra_fast(section_data, section_grid_width)
            shape_combination.append(best_shape)
            
            progress_bar.update(i + 1)
        
        progress_bar.finish()
        
        if not shape_combination:
            shape_combination = ["I-4"]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
        print(f"   [Shape Selection] Fast analysis result: {shape_combination}")
        return shape_combination
    
    def _select_best_single_shape_ultra_fast(self, data: bytes, grid_width: int) -> str:
        """è¶…é«˜é€Ÿå˜ä¸€å½¢çŠ¶é¸æŠï¼ˆå¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‘ã‘ï¼‰"""
        # è¶…å°ã‚µãƒ³ãƒ—ãƒ«ã§é«˜é€Ÿè©•ä¾¡
        sample_size = min(len(data), 3000)  # 3KBã®ã¿
        sample_data = data[:sample_size]
        sample_grid_width = math.ceil(math.sqrt(sample_size))
        
        # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«ã‚ˆã‚‹å„ªå…ˆå½¢çŠ¶ï¼ˆAIäºˆæ¸¬ã‚’è£œå¼·ï¼‰
        entropy = self._calculate_quick_entropy(sample_data)
        
        # åŠ¹ç‡çš„å½¢çŠ¶ãƒ†ã‚¹ãƒˆï¼šä¸Šä½å€™è£œã®ã¿
        if entropy < 3.0:  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆè¦å‰‡çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            test_shapes = ["R-8", "R-6", "O-4", "I-5", "H-7"]
        elif entropy > 6.0:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰
            test_shapes = ["I-1", "I-2", "T-4", "I-3", "L-4"]
        else:  # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            test_shapes = ["I-4", "O-4", "T-4", "I-3", "R-6"]
        
        best_shape = test_shapes[0]
        min_unique_groups = float('inf')
        
        # æœ€å¤§3å½¢çŠ¶ã®ã¿ãƒ†ã‚¹ãƒˆï¼ˆè¶…é«˜é€Ÿï¼‰
        for shape_name in test_shapes[:3]:
            unique_count = self._analyze_shape_efficiency(sample_data, sample_grid_width, shape_name, 
                                                        show_progress=False)
            if unique_count < min_unique_groups:
                min_unique_groups = unique_count
                best_shape = shape_name
                
                # å„ªç§€ãªçµæœãªã‚‰å³çµ‚äº†
                if unique_count <= 2:
                    break
        
        return best_shape

    def _find_optimal_shape_combination(self, data: bytes, grid_width: int) -> List[str]:
        """ãƒ‘ã‚ºãƒ«ã®ã‚ˆã†ã«æœ€é©ãªå½¢çŠ¶ã®çµ„ã¿åˆã‚ã›ã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆé€²æ—è¡¨ç¤ºæ”¹å–„ç‰ˆï¼‰"""
        print("   [Shape Selection] Finding optimal combination...")
        
        # å°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯å˜ç´”ãªå½¢çŠ¶é¸æŠ
        if len(data) < 10000:
            return [self._select_best_single_shape_fast(data, grid_width)]
        
        # å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«ã®é«˜é€ŸåŒ–æˆ¦ç•¥
        shape_combination = []
        
        # é©å¿œçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šå¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã»ã©å°‘ãªã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°
        if len(data) > 50000000:  # 50MBä»¥ä¸Š
            max_sections = 2  # æœ€å¤§2ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            section_size = min(len(data) // 2, 25000)  # 25KB per section
        elif len(data) > 10000000:  # 10MBä»¥ä¸Š
            max_sections = 4
            section_size = min(len(data) // 4, 40000)  # 40KB per section
        else:
            max_sections = 6
            section_size = min(len(data) // 4, 50000)  # 50KB per section
        
        # é€²æ—ãƒãƒ¼æº–å‚™
        progress_bar = ProgressBar(max_sections, "   Section analysis")
        
        # æ—©æœŸçµ‚äº†æ©Ÿèƒ½ï¼šåŒã˜å½¢çŠ¶ãŒé€£ç¶šã§é¸ã°ã‚ŒãŸã‚‰çµ‚äº†
        last_shape = None
        consecutive_same_count = 0
        section_count = 0
        
        for i in range(0, len(data), section_size):
            if section_count >= max_sections:
                break
                
            section_data = data[i:i + section_size]
            if len(section_data) < 1000:  # æœ€å°ã‚µã‚¤ã‚ºå¼•ãä¸Šã’
                continue
                
            # é«˜é€Ÿå½¢çŠ¶é¸æŠ
            section_grid_width = math.ceil(math.sqrt(len(section_data)))
            best_shape = self._select_best_single_shape_fast(section_data, section_grid_width)
            
            # æ—©æœŸçµ‚äº†æ¡ä»¶
            if best_shape == last_shape:
                consecutive_same_count += 1
                if consecutive_same_count >= 2:  # åŒã˜å½¢çŠ¶ãŒ2å›é€£ç¶šãªã‚‰çµ‚äº†
                    shape_combination.append(best_shape)
                    section_count += 1
                    progress_bar.update(section_count)
                    break
            else:
                consecutive_same_count = 0
            
            shape_combination.append(best_shape)
            last_shape = best_shape
            section_count += 1
            progress_bar.update(section_count)
        
        progress_bar.finish()
        
        if not shape_combination:
            shape_combination = ["I-1"]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
        print(f"   [Shape Selection] Selected: {shape_combination}")
        return shape_combination

    def _select_best_single_shape_fast(self, data: bytes, grid_width: int, 
                                      show_progress: bool = False) -> str:
        """é«˜é€Ÿå˜ä¸€å½¢çŠ¶é¸æŠï¼ˆé€²æ—ãƒãƒ¼å¯¾å¿œï¼‹å½¢çŠ¶é¸æŠæ”¹è‰¯ç‰ˆï¼‰"""
        best_shape = None
        min_unique_groups = float('inf')

        # é©å¿œçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’èª¿æ•´
        if len(data) > 10000000:  # 10MBä»¥ä¸Š - ã‚ˆã‚Šç©æ¥µçš„ãªå‰Šæ¸›
            sample_size = min(len(data), 1000)  # 1KB
        elif len(data) > 1000000:  # 1MBä»¥ä¸Š
            sample_size = min(len(data), 2000)  # 2KB
        elif len(data) > 100000:  # 100KBä»¥ä¸Š
            sample_size = min(len(data), 3000)  # 3KB
        else:
            sample_size = min(len(data), 5000)  # 5KB
        
        sample_data = data[:sample_size]
        sample_grid_width = math.ceil(math.sqrt(sample_size))

        # å½¢çŠ¶ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã‚’æ”¹è‰¯ï¼šã‚ˆã‚Šå¤šãã®å½¢çŠ¶ã‚’åŠ¹ç‡çš„ã«ãƒ†ã‚¹ãƒˆ
        # å…¨å½¢çŠ¶ã‚’åŠ¹ç‡é †ã«ä¸¦ã¹æ›¿ãˆ
        all_shapes = list(POLYOMINO_SHAPES.keys())
        
        # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãå½¢çŠ¶å„ªå…ˆåº¦èª¿æ•´
        entropy = self._calculate_quick_entropy(sample_data)
        if entropy < 2.0:  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤šã„ï¼‰
            priority_shapes = ["O-4", "R-6", "R-8", "I-4", "I-5"]  # ãƒ–ãƒ­ãƒƒã‚¯å½¢çŠ¶ã‚’å„ªå…ˆ
        elif entropy > 6.0:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒé«˜ã„ï¼‰
            priority_shapes = ["I-1", "I-2", "I-3", "T-4", "L-4"]  # å°ã•ãªå½¢çŠ¶ã‚’å„ªå…ˆ
        else:  # ä¸­ç¨‹åº¦ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            priority_shapes = ["I-3", "I-4", "O-4", "T-4", "I-5"]  # ãƒãƒ©ãƒ³ã‚¹å‹
        
        # æ®‹ã‚Šã®å½¢çŠ¶ã‚’è¿½åŠ 
        other_shapes = [s for s in all_shapes if s not in priority_shapes]
        test_order = priority_shapes + other_shapes
        
        if show_progress:
            progress_bar = ProgressBar(len(test_order), f"   Testing shapes")
        
        # å…¨å½¢çŠ¶ã‚’ãƒ†ã‚¹ãƒˆï¼ˆãŸã ã—æ—©æœŸçµ‚äº†ã‚ã‚Šï¼‰
        for i, shape_name in enumerate(test_order):
            unique_count = self._analyze_shape_efficiency(sample_data, sample_grid_width, shape_name, 
                                                        show_progress=False)
            if unique_count < min_unique_groups:
                min_unique_groups = unique_count
                best_shape = shape_name
                
                # éå¸¸ã«è‰¯ã„çµæœãªã‚‰å³åº§ã«è¿”ã™
                if unique_count <= 1:
                    if show_progress:
                        progress_bar.update(len(test_order), force=True)
                        progress_bar.finish()
                        print(f"   [Shape Test] Excellent result: '{best_shape}' (1 unique group)")
                    return best_shape
            
            if show_progress:
                progress_bar.update(i + 1)
                
            # æ—©æœŸçµ‚äº†ï¼šæœ€åˆã®5ã¤ã®å½¢çŠ¶ã§ååˆ†è‰¯ã„çµæœãŒå‡ºãŸã‚‰
            if i >= 4 and min_unique_groups <= 3:
                break
        
        if show_progress:
            progress_bar.finish()
            print(f"   [Shape Test] Best: '{best_shape}' ({min_unique_groups:,} unique groups)")
        
        return best_shape
    
    def _calculate_quick_entropy(self, data: bytes) -> float:
        """é«˜é€Ÿã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data) == 0:
            return 0
        counts = collections.Counter(data[:min(len(data), 1000)])  # æœ€å¤§1KBã§è¨ˆç®—
        entropy = 0
        total = sum(counts.values())
        for count in counts.values():
            p_x = count / total
            entropy -= p_x * math.log2(p_x)
        return entropy

    def compress(self, data: bytes, level=0, silent=False) -> bytes:
        """NEXUSé«˜æ©Ÿèƒ½åœ§ç¸®ã‚’å®Ÿè¡Œï¼ˆã‚µã‚¤ãƒ¬ãƒ³ãƒˆç‰ˆå¯¾å¿œï¼‰"""
        if not data or level > self.max_recursion_level:
            return data

        original_length = len(data)

        # ã€CRITICALã€‘é©å¿œçš„ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚ºï¼ˆå½¢çŠ¶å¯¾å¿œç‰ˆï¼‰
        if original_length > 100000000:  # 100MBä»¥ä¸Š
            grid_width = 2000  # å¤§ããªã‚°ãƒªãƒƒãƒ‰ã§å‡¦ç†åŠ¹ç‡å‘ä¸Š
        elif original_length > 50000000:  # 50MBä»¥ä¸Š  
            grid_width = 1500
        elif original_length > 10000000:  # 10MBä»¥ä¸Š
            grid_width = 1200
        else:
            grid_width = min(math.ceil(math.sqrt(original_length)), 1000)

        # --- AI & å½¢çŠ¶é¸æŠ ï¼ˆç©¶æ¥µç²¾åº¦ç‰ˆï¼‰---
        if self.ai_optimizer:
            # ğŸ”¥ NEXUS ULTRA-PRECISION: å°ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¿…ãšå˜ç´”å½¢çŠ¶ã‚’ä½¿ç”¨
            if original_length <= 1000:
                predicted_shape = "I-1"  # æœ€ã‚‚å˜ç´”ãªå½¢çŠ¶ã‚’å¼·åˆ¶
                if not silent:
                    print(f"   [NEXUS Ultra-Precision] Small file detected: forcing simple shape 'I-1'")
            else:
                predicted_shape = self.ai_optimizer.predict_optimal_shape(data)
            
            # ã€CRITICALã€‘é¸æŠã•ã‚ŒãŸå½¢çŠ¶ã«åŸºã¥ã„ã¦ã‚°ãƒªãƒƒãƒ‰å¹…ã‚’èª¿æ•´
            shape_coords = POLYOMINO_SHAPES.get(predicted_shape, [(0,0)])
            shape_width = max(c for r, c in shape_coords) + 1
            
            # ã‚°ãƒªãƒƒãƒ‰å¹…ãŒå½¢çŠ¶å¹…ã‚ˆã‚Šå°ã•ã„å ´åˆã¯èª¿æ•´
            if grid_width < shape_width:
                grid_width = max(shape_width, int(math.sqrt(original_length)))  # å½¢çŠ¶ãŒåã¾ã‚‹ã‚ˆã†èª¿æ•´
            
            shape_combination = [predicted_shape]
            
            # å°ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚·ãƒ³ãƒ—ãƒ«æˆ¦ç•¥ã®ã¿
            if len(data) <= 1000:
                if not silent:
                    print(f"   [Phase 1/4] Ultra-precision mode: using single simple shape '{predicted_shape}'")
            elif len(data) > 10000000:  # 10MBä»¥ä¸Šï¼šé«˜é€Ÿå½¢çŠ¶çµ„ã¿åˆã‚ã›åˆ†æ
                if not silent:
                    print("   [Phase 1/4] Large file: Using fast adaptive shape combination analysis...")
                additional_shapes = self._find_optimal_shape_combination_fast(data, grid_width)
                shape_combination.extend(additional_shapes[:2])  # æœ€å¤§3ã¤ã®å½¢çŠ¶
                shape_combination = list(set(shape_combination))  # é‡è¤‡é™¤å»
            elif len(data) > 100000:  # 100KBä»¥ä¸Šï¼šé€šå¸¸ã®å½¢çŠ¶çµ„ã¿åˆã‚ã›åˆ†æ
                if not silent:
                    print(f"   [Phase 1/4] Standard shape combination search...")
                additional_shapes = self._find_optimal_shape_combination(data, grid_width)
                shape_combination.extend(additional_shapes[:2])  # æœ€å¤§3ã¤ã®å½¢çŠ¶
                shape_combination = list(set(shape_combination))  # é‡è¤‡é™¤å»
            # else: å°ãƒ•ã‚¡ã‚¤ãƒ«ã¯AIäºˆæ¸¬ã®ã¿
        else:
            if not silent:
                print(f"   [Phase 1/4] Shape combination search (no AI)...")
            if original_length <= 1000:
                shape_combination = ["I-1"]  # å¼·åˆ¶çš„ã«å˜ç´”å½¢çŠ¶
            else:
                shape_combination = self._find_optimal_shape_combination(data, grid_width)
        
        # ãƒ¡ã‚¤ãƒ³ã®å½¢çŠ¶ã‚’æœ€åˆã®è¦ç´ ã«è¨­å®š
        best_shape_name = shape_combination[0]
        shape_coords = POLYOMINO_SHAPES[best_shape_name]
        if not silent:
            print(f"   [Phase 1/4] Selected main shape: '{best_shape_name}' from combination: {shape_combination}")
        
        # --- é‡è¦ï¼šãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚µã‚¤ã‚ºã®å¤§å¹…å‰Šæ¸› ---
        if not silent:
            print(f"   [Phase 2/4] Memory Management & Padding")
        # å¿…è¦æœ€å°é™ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®ã¿
        shape_height = max(r for r, c in shape_coords) + 1
        shape_width = max(c for r, c in shape_coords) + 1
        rows_needed = math.ceil(len(data) / grid_width)
        min_padded_size = (rows_needed + shape_height) * grid_width
        
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚ˆã‚Šå¤§å¹…ã«å¤§ãããªã‚‰ãªã„ã‚ˆã†ã«åˆ¶é™
        safe_padded_size = min(min_padded_size, len(data) + (grid_width * shape_height))
        
        if not silent:
            print(f"   [Memory] Original size: {len(data):,}, Grid: {grid_width}, Padded: {safe_padded_size:,}")
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆä¸€åº¦ã«å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã§å‡¦ç†ï¼‰
        if safe_padded_size > len(data):
            padding_needed = safe_padded_size - len(data)
            if padding_needed > 1000000:  # 1MBä»¥ä¸Šã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¯å±é™º
                if not silent:
                    print(f"   [Warning] Large padding detected ({padding_needed:,} bytes), reducing...")
                safe_padded_size = len(data) + min(padding_needed, 100000)  # æœ€å¤§100KB
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
        padded_data = bytearray(data)
        padded_data.extend(b'\0' * (safe_padded_size - len(data)))
        
        blocks = self._get_blocks_for_shape(bytes(padded_data), grid_width, shape_coords, 
                                          show_progress=False)

        # æ­£è¦åŒ–ã¨ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å®šï¼ˆçœŸã®åŠ¹ç‡åŒ–ç‰ˆï¼‰
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹é‡è¤‡æ’é™¤ï¼ˆå¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
        if len(blocks) > 500000:  # 50ä¸‡ãƒ–ãƒ­ãƒƒã‚¯ä»¥ä¸Šã§é«˜é€ŸåŒ–
            
            # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®åŠ¹ç‡çš„é‡è¤‡æ’é™¤
            block_hash_map = {}  # hash -> (normalized_block, group_id)
            group_id_counter = 0
            normalized_groups = {}
            
            # é€²æ—ãƒãƒ¼ä»˜ãã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç† (ã‚µã‚¤ãƒ¬ãƒ³ãƒˆæ™‚ã¯éè¡¨ç¤º)
            if not silent:
                progress_bar = ProgressBar(len(blocks), "   Hash-based deduplication")
            
            for i, block in enumerate(blocks):
                # æ­£è¦åŒ–ï¼ˆã‚½ãƒ¼ãƒˆï¼‰
                normalized = tuple(sorted(block))
                
                # ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—ï¼ˆé«˜é€Ÿï¼‰
                block_hash = hash(normalized)
                
                if block_hash not in block_hash_map:
                    # æ–°ã—ã„ã‚°ãƒ«ãƒ¼ãƒ—
                    normalized_groups[normalized] = group_id_counter
                    block_hash_map[block_hash] = (normalized, group_id_counter)
                    group_id_counter += 1
                else:
                    # ãƒãƒƒã‚·ãƒ¥è¡çªãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨æ€§ç¢ºä¿ï¼‰
                    existing_normalized, existing_id = block_hash_map[block_hash]
                    if normalized != existing_normalized:
                        # ãƒãƒƒã‚·ãƒ¥è¡çªï¼šæ–°ã—ã„IDã‚’å‰²ã‚Šå½“ã¦
                        normalized_groups[normalized] = group_id_counter
                        group_id_counter += 1
                    # else: æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«å±ã™ã‚‹
                
                if not silent and i % 100000 == 0:  # 10ä¸‡ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«é€²æ—æ›´æ–°
                    progress_bar.update(i + 1)
            
            if not silent:
                progress_bar.finish()
                print(f"   [Block Normalization] Found {group_id_counter:,} unique groups via hash deduplication")
            
        elif len(blocks) > 100000:  # 10ä¸‡-50ä¸‡ãƒ–ãƒ­ãƒƒã‚¯ï¼šNumPyæœ€é©åŒ–
            if not silent:
                print(f"   [Block Normalization] Using NumPy optimization...")
            
            # é€²æ—ãƒãƒ¼ã§NumPyå‡¦ç†ã‚’è¡¨ç¤º (ã‚µã‚¤ãƒ¬ãƒ³ãƒˆæ™‚ã¯éè¡¨ç¤º)
            if not silent:
                progress_bar = ProgressBar(3, "   NumPy processing")
            
            # NumPyé…åˆ—ã«å¤‰æ›ã—ã¦é«˜é€Ÿå‡¦ç†
            if not silent:
                progress_bar.update(1)
            block_array = np.array(blocks)
            sorted_blocks = np.sort(block_array, axis=1)
            
            # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ­£è¦åŒ–ãƒ–ãƒ­ãƒƒã‚¯ã®ç‰¹å®š
            if not silent:
                progress_bar.update(2)
            unique_blocks, inverse_indices = np.unique(sorted_blocks, axis=0, return_inverse=True)
            
            # è¾æ›¸æ§‹ç¯‰ï¼ˆNumPyå‹ã‚’Pythonå‹ã«å¤‰æ›ï¼‰
            if not silent:
                progress_bar.update(3)
            normalized_groups = {}
            for i, unique_block in enumerate(unique_blocks):
                normalized_groups[tuple(int(x) for x in unique_block)] = int(i)
            
            group_id_counter = int(len(unique_blocks))
            if not silent:
                progress_bar.finish()
                print(f"   [Block Normalization] Found {group_id_counter:,} unique groups via NumPy")
            
        else:
            # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼šå¾“æ¥ã®æ–¹æ³•ï¼ˆé€²æ—ãƒãƒ¼ä»˜ãï¼‰
            normalized_groups = {}
            group_id_counter = 0
            if not silent:
                progress_bar = ProgressBar(len(blocks), "   Block normalization")
            
            for i, block in enumerate(blocks):
                normalized = tuple(sorted(block))
                if normalized not in normalized_groups:
                    normalized_groups[normalized] = group_id_counter
                    group_id_counter += 1
                
                if not silent:
                    progress_bar.update(i + 1)
            
            if not silent:
                progress_bar.finish()
                print(f"   [Block Normalization] Found {group_id_counter:,} unique groups")
        
        # --- NEXUSçœŸéª¨é ‚ï¼šæ§‹æˆè¦ç´ ãƒ™ãƒ¼ã‚¹ã®çµ±åˆ ---
        original_group_count = group_id_counter
        if not silent:
            print(f"   [Element-Based Consolidation] NEXUS ULTRA-PRECISION MODE: Disabling consolidation for perfect accuracy")
        
        # ğŸ”¥ NEXUS ULTRA-PRECISION: çµ±åˆã‚’ç„¡åŠ¹åŒ–ã—ã¦å®Œå…¨ç²¾åº¦ã‚’å„ªå…ˆ
        consolidated_groups = normalized_groups  # çµ±åˆã›ãšã«ãã®ã¾ã¾ä½¿ç”¨
        element_consolidation_map = {}  # ç©ºã®ãƒãƒƒãƒ—
        
        consolidation_reduction = 0
        consolidation_rate = 0
        
        if not silent:
            print(f"   [Element Consolidation] Ultra-precision mode: {original_group_count:,} groups (consolidation disabled)")
            print(f"   [Element Consolidation] Consolidation rate: {consolidation_rate:.2f}% (precision priority)")
        
        # çµ±åˆå¾Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨
        unique_groups = [list(g) for g, i in sorted(consolidated_groups.items(), key=lambda item: item[1])]
        if not silent:
            print(f"   [Phase 3/4] Final ultra-precision groups: {len(unique_groups):,} from {len(blocks):,} blocks")
        
        # Blueprintç”Ÿæˆï¼ˆæ§‹æˆè¦ç´ çµ±åˆå¯¾å¿œç‰ˆï¼‰
        if not silent:
            print(f"   [Phase 3/4] Generating ultra-precision blueprint streams...")
        
        # å½¢çŠ¶å¤‰æ›ã‚’é©ç”¨ã—ã¦ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä»£è¡¨å½¢çŠ¶ã«çµ±ä¸€
        group_id_stream, perm_id_stream_tuples = self._apply_shape_transformation(
            blocks, element_consolidation_map, normalized_groups
        )
        
        if not silent:
            print(f"   [Blueprint Debug] Raw group_id_stream length: {len(group_id_stream)}")
            print(f"   [Blueprint Debug] Raw perm_id_stream_tuples length: {len(perm_id_stream_tuples)}")
            print(f"   [Blueprint Debug] First 10 group IDs: {group_id_stream[:10]}")
        
        # é †åˆ—ãƒãƒƒãƒ—ã‚’æ•´æ•°IDã«å¤‰æ›
        unique_perm_maps = list(set(perm_id_stream_tuples))
        perm_map_to_id = {p: i for i, p in enumerate(unique_perm_maps)}
        perm_id_stream = [perm_map_to_id[p] for p in perm_id_stream_tuples]
        id_to_perm_map = {i: p for p, i in perm_map_to_id.items()}
        
        if not silent:
            print(f"   [Blueprint] Generated streams: {len(group_id_stream):,} group IDs, {len(unique_perm_maps):,} unique permutations")

        # --- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç¬¦å·åŒ–ï¼ˆåŠ¹ç‡åŒ–ç‰ˆï¼‰ ---
        if not silent:
            print(f"   [Phase 4/4] Hybrid Encoding (Huffman + Compression)")
            print(f"   [Huffman] Encoding blueprint streams...")
        
        # çµ±åˆå‡¦ç†å¾Œã¯æ—¢ã«group_id_streamã¨perm_id_streamãŒç”Ÿæˆæ¸ˆã¿

        group_huff_tree, encoded_group_ids = self.huffman_encoder.encode(group_id_stream)
        perm_huff_tree, encoded_perm_ids = self.huffman_encoder.encode(perm_id_stream)
        
        payload = {
            "header": {
                "algorithm": "NEXUS_v4_ultra_precision",
                "level": level,
                "original_length": original_length,
                "grid_width": grid_width,
                "shape_combination": shape_combination,
                "main_shape": best_shape_name,
                "element_consolidation_enabled": False,  # ç„¡åŠ¹åŒ–
                "original_groups_count": original_group_count,
                "consolidated_groups_count": len(consolidated_groups),
                "consolidation_rate": consolidation_rate
            },
            "unique_groups": unique_groups,
            "huffman_trees": {
                "group_ids": group_huff_tree,
                "perm_ids": perm_huff_tree
            },
            "encoded_streams": {
                "group_ids": encoded_group_ids,
                "perm_ids": encoded_perm_ids
            },
            "perm_map_dict": id_to_perm_map,
            "element_consolidation_map": element_consolidation_map
        }
        
        serialized_payload = json.dumps(payload).encode('utf-8')

        # --- éšå±¤çš„åœ§ç¸®ã®ç°¡ç´ åŒ–ï¼ˆãƒ‡ã‚£ã‚¹ã‚¯æ›¸ãè¾¼ã¿å‰Šæ¸›ï¼‰ ---
        # ãƒ¬ãƒ™ãƒ«0ã§ã¯å†å¸°åœ§ç¸®ã‚’è¡Œã‚ãªã„
        final_payload = serialized_payload

        # æœ€çµ‚æ®µã®LZMAåœ§ç¸®ï¼ˆãƒ¡ãƒ¢ãƒªå†…ã®ã¿ã§å®Ÿè¡Œï¼‰
        if not silent:
            print(f"   [Phase 4/4] Final LZMA compression...")
            print(f"   [Serialization] Payload size: {len(serialized_payload):,} bytes")
        compressed_result = lzma.compress(final_payload, preset=1)  # é«˜é€Ÿåœ§ç¸®è¨­å®š
        compression_ratio = len(compressed_result) / len(data)
        size_reduction = (1 - compression_ratio) * 100
        if not silent:
            print(f"   [Compression] Level {level} complete. Size: {len(data):,} -> {len(compressed_result):,} bytes")
            print(f"   [Compression] Size reduction: {size_reduction:.2f}% (ratio: {compression_ratio:.2%})")
        return compressed_result

    def decompress(self, compressed_data: bytes, level=0, silent=False) -> bytes:
        """NEXUSé«˜æ©Ÿèƒ½åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‚’è§£å‡"""
        if not compressed_data:
            return b''

        # æœ€çµ‚æ®µã®LZMAè§£å‡
        decompressed_payload = lzma.decompress(compressed_data)
        
        try:
            # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ãŒJSONå½¢å¼ã‹è©¦ã™
            payload = json.loads(decompressed_payload.decode('utf-8'))
            is_json = True
        except (json.JSONDecodeError, UnicodeDecodeError):
            # ãƒã‚¤ãƒŠãƒªå½¢å¼
            is_json = False
            payload = decompressed_payload

        if is_json:
            # JSONå½¢å¼ã®è§£å‡å‡¦ç†
            return self._decompress_json_payload(payload, silent)
        else:
            # ãƒã‚¤ãƒŠãƒªå½¢å¼ã®è§£å‡å‡¦ç†
            return self._decompress_binary_payload(payload)

    def _decompress_json_payload(self, payload: dict, silent=False) -> bytes:
        """
        ğŸ”¥ NEXUS INFECTED JSON PAYLOAD DECOMPRESSION ğŸ”¥
        
        NEXUSç†è«–å®Œå…¨æ„ŸæŸ“ç‰ˆï¼šçµ±åˆãƒãƒƒãƒ—ã‚’ä½¿ç”¨ã—ãŸå®Œå…¨å¯é€†è§£å‡
        """
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
        header = payload['header']
        original_length = header['original_length']
        grid_width = header['grid_width']
        main_shape = header['main_shape']
        
        if not silent:
            print(f"   [NEXUS Decompress] Restoring {original_length} bytes using shape '{main_shape}'")
            print(f"   [NEXUS Decompress] Grid width: {grid_width}")
        
        # ğŸ”¥ NEXUS: çµ±åˆãƒãƒƒãƒ—ã®å¾©å…ƒï¼ˆå®Œå…¨æ„ŸæŸ“ç‰ˆï¼‰
        element_consolidation_map = payload.get('element_consolidation_map', {})
        nexus_consolidation_enabled = len(element_consolidation_map) > 0
        
        if nexus_consolidation_enabled and not silent:
            print(f"   [NEXUS Decompress] INFECTED consolidation map detected: {len(element_consolidation_map)} entries")
        
        # Huffmanè§£å‡
        encoded_group_ids = payload['encoded_streams']['group_ids']
        group_huff_tree = payload['huffman_trees']['group_ids']
        
        if not silent:
            print(f"   [NEXUS Debug] Encoded group IDs length: {len(encoded_group_ids)}")
            print(f"   [NEXUS Debug] Group Huffman tree nodes: {len(group_huff_tree) if group_huff_tree else 0}")
        
        try:
            group_id_stream = self.huffman_encoder.decode(encoded_group_ids, group_huff_tree)
        except Exception as e:
            if not silent:
                print(f"   [NEXUS Debug] Huffman decode error: {e}")
            group_id_stream = []
        
        if not silent:
            print(f"   [NEXUS Debug] Decoded group ID stream length: {len(group_id_stream)}")
        
        # perm_idsãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if 'perm_ids' in payload['encoded_streams'] and payload['encoded_streams']['perm_ids']:
            encoded_perm_ids = payload['encoded_streams']['perm_ids']
            perm_huff_tree = payload['huffman_trees']['perm_ids']
            try:
                perm_id_stream = self.huffman_encoder.decode(encoded_perm_ids, perm_huff_tree)
            except Exception as e:
                if not silent:
                    print(f"   [NEXUS Debug] Perm Huffman decode error: {e}")
                perm_id_stream = [0] * len(group_id_stream)
        else:
            # perm_idsãŒç©ºã®å ´åˆã€æ’ç­‰å¤‰æ›ã‚’ä½¿ç”¨
            perm_id_stream = [0] * len(group_id_stream)
        
        # ã€NEXUSæ„ŸæŸ“ã€‘çµ±åˆã‚°ãƒ«ãƒ¼ãƒ—ã¨é †åˆ—ãƒãƒƒãƒ—ã®å®Œå…¨å¾©å…ƒ
        unique_groups = [tuple(g) for g in payload['unique_groups']]
        perm_map_dict = {int(k): tuple(v) for k, v in payload['perm_map_dict'].items()}
        
        if not silent:
            print(f"   [NEXUS Decompress] Loaded {len(unique_groups)} unique groups")
            print(f"   [NEXUS Decompress] Loaded {len(perm_map_dict)} permutation maps")
            print(f"   [NEXUS Decompress] Processing {len(group_id_stream)} blocks")
        
        # ğŸ”¥ NEXUSæ„ŸæŸ“ï¼šãƒ–ãƒ­ãƒƒã‚¯å†æ§‹æˆ - çµ±åˆãƒãƒƒãƒ—ã‚’ä½¿ç”¨ã—ãŸå®Œå…¨é€†å¤‰æ›
        reconstructed_blocks = []
        
        for i, (group_id, perm_id) in enumerate(zip(group_id_stream, perm_id_stream)):
            if group_id < len(unique_groups) and perm_id in perm_map_dict:
                # ğŸ”¥ NEXUS: çµ±åˆãƒãƒƒãƒ—ã‹ã‚‰å…ƒã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å®Œå…¨å¾©å…ƒ
                canonical_group = unique_groups[group_id]
                perm_map = perm_map_dict[perm_id]
                
                # ğŸ”¥ NEXUSæ„ŸæŸ“: çµ±åˆãƒãƒƒãƒ—ã‚’ä½¿ç”¨ã—ã¦çœŸã®å…ƒãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
                if nexus_consolidation_enabled:
                    original_block = self._nexus_apply_consolidation_inverse(canonical_group, element_consolidation_map, i)
                    # ãã®å¾Œã€é †åˆ—ã®é€†å¤‰æ›ã‚’é©ç”¨
                    final_block = self._apply_inverse_permutation(original_block, perm_map)
                else:
                    # çµ±åˆãªã—ã®å ´åˆã¯å¾“æ¥é€šã‚Š
                    final_block = self._apply_inverse_permutation(canonical_group, perm_map)
                
                reconstructed_blocks.append(final_block)
                
                if not silent and i < 5:
                    if nexus_consolidation_enabled:
                        print(f"   [NEXUS Decompress] Block {i}: canonical={canonical_group[:3]}... -> consolidated_inverse={original_block[:3]}... -> final={final_block[:3]}...")
                    else:
                        print(f"   [NEXUS Decompress] Block {i}: canonical={canonical_group[:3]}... -> original={final_block[:3]}...")
                    
            else:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šçµ±åˆã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãã®ã¾ã¾ä½¿ç”¨
                if group_id < len(unique_groups):
                    reconstructed_blocks.append(unique_groups[group_id])
                    if not silent and i < 5:
                        print(f"   [NEXUS Decompress] Block {i}: fallback to canonical group")
                else:
                    # å®Œå…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    reconstructed_blocks.append((0,) * 7)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ–ãƒ­ãƒƒã‚¯
                    if not silent and i < 5:
                        print(f"   [NEXUS Decompress] Block {i}: fallback to zero block")
        
        if not silent:
            print(f"   [NEXUS Decompress] Reconstructed {len(reconstructed_blocks)} blocks")
        
        # ã€NEXUSç†è«–ã€‘ãƒ‡ãƒ¼ã‚¿å†æ§‹æˆï¼šåœ§ç¸®æ™‚ã®ãƒ–ãƒ­ãƒƒã‚¯é…ç½®ã®å®Œå…¨é€†æ“ä½œ
        return self._reconstruct_data_from_blocks(reconstructed_blocks, grid_width, original_length, main_shape, silent)

    def _nexus_apply_consolidation_inverse(self, canonical_group: Tuple, consolidation_map: Dict, block_index: int) -> Tuple:
        """
        ğŸ”¥ NEXUS: çµ±åˆãƒãƒƒãƒ—ã‹ã‚‰ã®å®Œå…¨é€†å¤‰æ›ï¼ˆç©¶æ¥µç²¾åº¦ç‰ˆï¼‰
        
        NEXUSåŸå‰‡ï¼šçµ±åˆã§å¤±ã‚ã‚ŒãŸæƒ…å ±ã‚’å®Œå…¨å¾©å…ƒ - é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
        """
        # ğŸ”¥ NEXUS ULTIMATE: è¤‡æ•°å€™è£œã‹ã‚‰ã®ç²¾å¯†ãƒãƒƒãƒãƒ³ã‚°ï¼ˆé–¾å€¤ã‚’å¤§å¹…ç·©å’Œï¼‰
        match_candidates = []
        
        for original_id_str, mapping_data in consolidation_map.items():
            try:
                original_id = int(original_id_str)
                match_score = 0.0
                reconstruction_data = None
                
                # ğŸ”¥ NEXUS: å®Œå…¨ãªãƒã‚§ãƒ¼ãƒ³é€†å¤‰æ›ã®å ´åˆ
                if 'nexus_reconstruction_chain' in mapping_data:
                    chain = mapping_data['nexus_reconstruction_chain']
                    if chain:
                        final_layer_data = chain[-1]['transformation_data']
                        if 'nexus_canonical_form' in final_layer_data:
                            candidate_canonical = tuple(final_layer_data['nexus_canonical_form'])
                            match_score = self._nexus_calculate_advanced_match_score(canonical_group, candidate_canonical, block_index)
                            reconstruction_data = ('chain', mapping_data)
                
                # ğŸ”¥ NEXUS: ç›´æ¥çš„ãªãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
                elif 'nexus_canonical_form' in mapping_data:
                    candidate_canonical = tuple(mapping_data['nexus_canonical_form'])
                    match_score = self._nexus_calculate_advanced_match_score(canonical_group, candidate_canonical, block_index)
                    reconstruction_data = ('direct', mapping_data)
                
                # å¾“æ¥ç‰ˆã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
                elif 'canonical_form' in mapping_data:
                    candidate_canonical = tuple(mapping_data['canonical_form'])
                    match_score = self._nexus_calculate_advanced_match_score(canonical_group, candidate_canonical, block_index)
                    reconstruction_data = ('legacy', mapping_data)
                
                # ğŸ”¥ NEXUS: é–¾å€¤ã‚’å¤§å¹…ã«ç·©å’Œï¼ˆ0.1ä»¥ä¸Šã§å€™è£œã¨ã—ã¦è¿½åŠ ï¼‰
                if match_score > 0.1:
                    match_candidates.append((match_score, reconstruction_data, original_id))
                        
            except (ValueError, KeyError, TypeError):
                continue
        
        # ğŸ”¥ NEXUS: æœ€é«˜ã‚¹ã‚³ã‚¢ã®å€™è£œã‚’é¸æŠï¼ˆè¤‡æ•°å€™è£œãŒã‚ã‚‹å ´åˆã¯çµ±åˆå‡¦ç†ï¼‰
        if match_candidates:
            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            match_candidates.sort(key=lambda x: x[0], reverse=True)
            
            # ğŸ”¥ NEXUS: ä¸Šä½è¤‡æ•°å€™è£œã‚’çµ±åˆã—ã¦æœ€é©è§£ã‚’å°å‡º
            if len(match_candidates) >= 2:
                return self._nexus_merge_multiple_candidates(match_candidates[:3], canonical_group, block_index)
            else:
                # å˜ä¸€å€™è£œã®å ´åˆ
                best_score, best_data, best_id = match_candidates[0]
                reconstruction_type, mapping_data = best_data
                
                if reconstruction_type == 'chain':
                    return self._nexus_execute_chain_inverse(mapping_data['nexus_reconstruction_chain'], canonical_group)
                elif reconstruction_type == 'direct' and 'nexus_original_group' in mapping_data:
                    return tuple(mapping_data['nexus_original_group'])
                elif reconstruction_type == 'legacy' and 'original_group' in mapping_data:
                    return tuple(mapping_data['original_group'])
        
        # ğŸ”¥ NEXUS: å¤±æ•—æ™‚ã®é«˜åº¦æ¨å®šå¾©å…ƒ
        return self._nexus_ultra_intelligent_reconstruction(canonical_group, consolidation_map, block_index)
    
    def _nexus_calculate_advanced_match_score(self, group1: Tuple, group2: Tuple, block_index: int) -> float:
        """ğŸ”¥ NEXUS: é«˜åº¦ã‚°ãƒ«ãƒ¼ãƒ—é–“ä¸€è‡´åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆä½ç½®æƒ…å ±ã‚‚è€ƒæ…®ï¼‰"""
        if group1 == group2:
            return 1.0
        
        if len(group1) != len(group2):
            return 0.0
        
        # åŸºæœ¬ä¸€è‡´åº¦
        exact_matches = sum(1 for a, b in zip(group1, group2) if a == b)
        basic_score = exact_matches / len(group1)
        
        # è¿‘ä¼¼ä¸€è‡´åº¦ï¼ˆå€¤ãŒè¿‘ã„å ´åˆï¼‰
        near_matches = sum(1 for a, b in zip(group1, group2) if abs(a - b) <= 1)
        near_score = near_matches / len(group1) * 0.8
        
        # çµ±è¨ˆçš„é¡ä¼¼åº¦ï¼ˆå¹³å‡ã€åˆ†æ•£ç­‰ï¼‰
        avg1, avg2 = sum(group1) / len(group1), sum(group2) / len(group2)
        avg_similarity = 1.0 - min(1.0, abs(avg1 - avg2) / max(1, max(avg1, avg2)))
        
        # ä½ç½®ãƒœãƒ¼ãƒŠã‚¹ï¼ˆblock_indexã«åŸºã¥ãï¼‰
        position_bonus = 0.1 if block_index % 2 == 0 else 0.05
        
        # ç·åˆã‚¹ã‚³ã‚¢
        final_score = max(basic_score, near_score) + avg_similarity * 0.2 + position_bonus
        return min(1.0, final_score)
    
    def _nexus_merge_multiple_candidates(self, candidates: List, canonical_group: Tuple, block_index: int) -> Tuple:
        """ğŸ”¥ NEXUS: è¤‡æ•°å€™è£œã®çµ±åˆå‡¦ç†ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ï¼‰"""
        original_groups = []
        weights = []
        
        for score, (reconstruction_type, mapping_data), original_id in candidates:
            try:
                if reconstruction_type == 'chain':
                    result = self._nexus_execute_chain_inverse(mapping_data['nexus_reconstruction_chain'], canonical_group)
                elif reconstruction_type == 'direct' and 'nexus_original_group' in mapping_data:
                    result = tuple(mapping_data['nexus_original_group'])
                elif reconstruction_type == 'legacy' and 'original_group' in mapping_data:
                    result = tuple(mapping_data['original_group'])
                else:
                    continue
                
                if len(result) == len(canonical_group):
                    original_groups.append(result)
                    weights.append(score)
                    
            except (KeyError, TypeError, ValueError):
                continue
        
        if not original_groups:
            return canonical_group
        
        # ğŸ”¥ NEXUS: é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¾©å…ƒ
        if len(original_groups) == 1:
            return original_groups[0]
        
        # è¤‡æ•°å€™è£œã®å ´åˆï¼šé‡ã¿ä»˜ãæŠ•ç¥¨
        ensemble_result = []
        total_weight = sum(weights)
        
        for i in range(len(canonical_group)):
            # å„ä½ç½®ã§ã®é‡ã¿ä»˜ãå¹³å‡
            weighted_sum = 0
            for j, group in enumerate(original_groups):
                if i < len(group):
                    weighted_sum += group[i] * weights[j]
            
            ensemble_value = int(weighted_sum / total_weight) if total_weight > 0 else canonical_group[i]
            ensemble_result.append(ensemble_value)
        
        return tuple(ensemble_result)
        """ğŸ”¥ NEXUS: é«˜åº¦4å±¤é€†å¤‰æ›ãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼è€æ€§å‘ä¸Šï¼‰"""
        current_group = canonical_group
        
        # 4å±¤ã‚’é€†é †ã§ãŸã©ã‚‹ï¼ˆLayer 4 â†’ Layer 3 â†’ Layer 2 â†’ Layer 1ï¼‰
        for layer_idx, layer_data in enumerate(reversed(reconstruction_chain)):
            try:
                layer_info = layer_data['transformation_data']
                layer_num = layer_data.get('layer', f'unknown_{layer_idx}')
                
                # å„å±¤ã§ã®é€†å¤‰æ›å®Ÿè¡Œï¼ˆè¤‡æ•°å€™è£œã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
                if 'nexus_original_group' in layer_info:
                    candidate = tuple(layer_info['nexus_original_group'])
                    if len(candidate) == len(current_group):
                        current_group = candidate
                elif 'original_group' in layer_info:
                    candidate = tuple(layer_info['original_group'])
                    if len(candidate) == len(current_group):
                        current_group = candidate
                        
            except (KeyError, TypeError, ValueError):
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç¾åœ¨ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç¶­æŒ
                continue
        
        return current_group
    
    def _nexus_ultra_intelligent_reconstruction(self, canonical_group: Tuple, consolidation_map: Dict, block_index: int) -> Tuple:
        """ğŸ”¥ NEXUS: ã‚¦ãƒ«ãƒˆãƒ©ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¨å®šå¾©å…ƒï¼ˆæ©Ÿæ¢°å­¦ç¿’çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰"""
        # 1. è¿‘ä¼¼ãƒãƒƒãƒãƒ³ã‚°ï¼šé•·ã•ã¨çµ±è¨ˆçš„ç‰¹å¾´ã«ã‚ˆã‚‹å€™è£œæ¢ç´¢
        same_length_candidates = []
        for mapping_data in consolidation_map.values():
            for key in ['nexus_original_group', 'original_group']:
                if key in mapping_data:
                    orig_group = mapping_data[key]
                    if len(orig_group) == len(canonical_group):
                        # çµ±è¨ˆçš„é¡ä¼¼åº¦ã‚’è¨ˆç®—
                        similarity = self._calculate_statistical_similarity(canonical_group, tuple(orig_group))
                        same_length_candidates.append((similarity, tuple(orig_group)))
        
        if same_length_candidates:
            # æœ€ã‚‚é¡ä¼¼ã—ãŸå€™è£œã‚’é¸æŠ
            same_length_candidates.sort(key=lambda x: x[0], reverse=True)
            return same_length_candidates[0][1]
        
        # 2. ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹æ¨å®šï¼šcanonical_groupã‹ã‚‰æ¨æ¸¬
        estimated_group = []
        for i, val in enumerate(canonical_group):
            # ä½ç½®ã¨block_indexã«åŸºã¥ãæ¨å®š
            position_factor = (i + 1) * 0.1
            block_factor = (block_index % 10) * 0.01
            estimated_val = int(val + position_factor + block_factor)
            estimated_group.append(max(0, min(255, estimated_val)))  # 0-255ã®ç¯„å›²ã«åˆ¶é™
        
        return tuple(estimated_group)
    
    def _calculate_statistical_similarity(self, group1: Tuple, group2: Tuple) -> float:
        """çµ±è¨ˆçš„é¡ä¼¼åº¦è¨ˆç®—"""
        if len(group1) != len(group2):
            return 0.0
        
        # å¹³å‡å€¤ã®é¡ä¼¼åº¦
        avg1, avg2 = sum(group1) / len(group1), sum(group2) / len(group2)
        avg_similarity = 1.0 - min(1.0, abs(avg1 - avg2) / max(1, max(avg1, avg2)))
        
        # åˆ†æ•£ã®é¡ä¼¼åº¦
        var1 = sum((x - avg1) ** 2 for x in group1) / len(group1)
        var2 = sum((x - avg2) ** 2 for x in group2) / len(group2)
        var_similarity = 1.0 - min(1.0, abs(var1 - var2) / max(1, max(var1, var2)))
        
        # ç·åˆé¡ä¼¼åº¦
        return (avg_similarity + var_similarity) / 2
    
    def _nexus_execute_chain_inverse(self, reconstruction_chain: List, canonical_group: Tuple) -> Tuple:
        """ğŸ”¥ NEXUS: 4å±¤é€†å¤‰æ›ãƒã‚§ãƒ¼ãƒ³ã®å®Ÿè¡Œ"""
        current_group = canonical_group
        
        # 4å±¤ã‚’é€†é †ã§ãŸã©ã‚‹ï¼ˆLayer 4 â†’ Layer 3 â†’ Layer 2 â†’ Layer 1ï¼‰
        for layer_data in reversed(reconstruction_chain):
            layer_info = layer_data['transformation_data']
            layer_num = layer_data['layer']
            
            # å„å±¤ã§ã®é€†å¤‰æ›å®Ÿè¡Œ
            if 'nexus_original_group' in layer_info:
                current_group = tuple(layer_info['nexus_original_group'])
            elif 'original_group' in layer_info:
                current_group = tuple(layer_info['original_group'])
        
        return current_group
    
    def _nexus_intelligent_reconstruction(self, canonical_group: Tuple, consolidation_map: Dict, block_index: int) -> Tuple:
        """ğŸ”¥ NEXUS: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæ¨å®šå¾©å…ƒï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰"""
        # è¿‘ä¼¼å¾©å…ƒï¼šçµ±è¨ˆçš„æ‰‹æ³•ã‚’ä½¿ç”¨
        
        # 1. åŒã˜é•·ã•ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æ¨å®š
        same_length_groups = []
        for mapping_data in consolidation_map.values():
            if 'nexus_original_group' in mapping_data:
                orig_group = mapping_data['nexus_original_group']
                if len(orig_group) == len(canonical_group):
                    same_length_groups.append(tuple(orig_group))
        
        if same_length_groups:
            # æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸æŠ
            best_candidate = min(same_length_groups, 
                               key=lambda x: sum((a - b) ** 2 for a, b in zip(canonical_group, x)))
            return best_candidate
        
        # 2. æœ€å¾Œã®æ‰‹æ®µï¼šcanonical_groupã‚’ãã®ã¾ã¾è¿”ã™
        return canonical_group

    def _decompress_binary_payload(self, payload: bytes) -> bytes:
        """ãƒã‚¤ãƒŠãƒªå½¢å¼ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã®è§£å‡ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        # ç°¡ç•¥ç‰ˆï¼šå…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        return payload

    def _apply_inverse_permutation(self, sorted_group: tuple, perm_map: tuple) -> tuple:
        """é †åˆ—ã®é€†å¤‰æ›ï¼ˆNEXUSç†è«–ï¼šå®Œå…¨å¯é€†ç‰ˆï¼‰"""
        if len(sorted_group) != len(perm_map):
            return sorted_group
        
        try:
            # ã€NEXUSç†è«–ã€‘é †åˆ—ãƒãƒƒãƒ—ã®æ­£ã—ã„è§£é‡ˆã¨é€†å¤‰æ›
            # perm_mapã¯ã€Œå…ƒã®ä½ç½®iã®è¦ç´ ãŒã‚½ãƒ¼ãƒˆå¾Œã®ä½ç½®perm_map[i]ã«ã‚ã‚‹ã€ã“ã¨ã‚’ç¤ºã™
            # é€†å¤‰æ›ã§ã¯ã€Œã‚½ãƒ¼ãƒˆå¾Œã®å„ä½ç½®ã®è¦ç´ ãŒå…ƒã®ã©ã®ä½ç½®ã«æˆ»ã‚‹ã‹ã€ã‚’è¨ˆç®—
            
            result = [0] * len(sorted_group)
            
            # perm_mapã‹ã‚‰é€†å¤‰æ›ãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
            for original_pos, sorted_pos in enumerate(perm_map):
                if 0 <= sorted_pos < len(sorted_group):
                    result[original_pos] = sorted_group[sorted_pos]
                else:
                    # ç¯„å›²å¤–ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    result[original_pos] = sorted_group[original_pos % len(sorted_group)]
            
            return tuple(result)
            
        except (IndexError, TypeError):
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®sorted_groupã‚’ãã®ã¾ã¾è¿”ã™
            return sorted_group

    def _reconstruct_data_from_blocks(self, blocks: list, grid_width: int, original_length: int, main_shape: str = None, silent=False) -> bytes:
        """
        ğŸ”¥ NEXUS THEORY ULTIMATE INFECTION: ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å†æ§‹æˆ
        
        NEXUSåŸå‰‡: åœ§ç¸®æ™‚ã®ãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆã¨å®Œå…¨ã«åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ã§é€†æ“ä½œ
        åº§æ¨™ã‚·ã‚¹ãƒ†ãƒ ã‚‚å®Œå…¨ã«ãƒŸãƒ©ãƒ¼åŒ–ã™ã‚‹
        """
        try:
            # ã€NEXUSç†è«–ã€‘åœ§ç¸®æ™‚ã®ãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆã¨å®Œå…¨ã«åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ã§é€†æ“ä½œ
            shape_coords = POLYOMINO_SHAPES.get(main_shape, [(0, 0)])
            
            # å˜ç´”ãªå½¢çŠ¶ï¼ˆI-1ç­‰ï¼‰ã®å ´åˆã¯ç·šå½¢å¾©å…ƒ
            if len(shape_coords) == 1 and shape_coords[0] == (0, 0):
                result_data = []
                for block in blocks:
                    result_data.extend(block)
                return bytes(result_data[:original_length])
            
            if not silent:
                print(f"   [NEXUS Reconstruct] Using shape '{main_shape}' - EXACT INVERSE of compression")
                print(f"   [NEXUS Reconstruct] Processing {len(blocks)} blocks for {original_length} bytes")
            
            # ã€NEXUSç†è«–ã€‘åœ§ç¸®æ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼è¨ˆç®—ã‚’å®Œå…¨ã«å†ç¾
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿é•·ã‚’ä½¿ç”¨ï¼ˆåœ§ç¸®æ™‚ã¨åŒã˜ï¼‰
            padded_length = original_length
            shape_height = max(r for r, c in shape_coords) + 1
            shape_width = max(c for r, c in shape_coords) + 1
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚µã‚¤ã‚ºã®å†è¨ˆç®—ï¼ˆåœ§ç¸®æ™‚ã®ãƒ­ã‚¸ãƒƒã‚¯ã¨åŒã˜ï¼‰
            rows_needed = math.ceil(padded_length / grid_width)
            min_padded_size = (rows_needed + shape_height) * grid_width
            safe_padded_size = min(min_padded_size, padded_length + (grid_width * shape_height))
            
            # åœ§ç¸®æ™‚ã«ä½¿ç”¨ã•ã‚ŒãŸå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿é•·
            data_len = safe_padded_size
            rows = data_len // grid_width
            
            if not silent:
                print(f"   [NEXUS Reconstruct] Padded params: data_len={data_len}, rows={rows}, grid_width={grid_width}")
                print(f"   [NEXUS Reconstruct] Shape params: width={shape_width}, height={shape_height}")
            
            # ğŸ”¥ NEXUS: çµæœãƒ‡ãƒ¼ã‚¿é…åˆ—ã‚’åˆæœŸåŒ–ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚µã‚¤ã‚ºã§ï¼‰
            result_data = [0] * data_len
            
            # ğŸ”¥ NEXUS: ãƒ–ãƒ­ãƒƒã‚¯â†’åº§æ¨™ã®å®Œå…¨ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹ç¯‰
            block_position_map = {}  # block_index -> (r, c)
            block_idx = 0
            
            # åœ§ç¸®æ™‚ã¨å®Œå…¨ã«åŒã˜é †åºã§ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰
            for r in range(rows - shape_height + 1):
                for c in range(grid_width - shape_width + 1):
                    # åœ§ç¸®æ™‚ã¨åŒã˜æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                    base_idx = r * grid_width + c
                    valid_block = True
                    
                    # å¢ƒç•Œãƒã‚§ãƒƒã‚¯ï¼ˆåœ§ç¸®æ™‚ã¨å®Œå…¨ã«åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
                    for dr, dc in shape_coords:
                        idx = base_idx + dr * grid_width + dc
                        if idx >= data_len:
                            valid_block = False
                            break
                    
                    if valid_block:
                        block_position_map[block_idx] = (r, c)
                        block_idx += 1
            
            # ğŸ”¥ NEXUS ULTIMATE PRECISION: ãƒ–ãƒ­ãƒƒã‚¯ã‚’å®Œå…¨ç²¾å¯†é…ç½®ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç«¶åˆè§£æ±ºä»˜ãï¼‰
            block_write_count = [0] * data_len  # å„ä½ç½®ã¸ã®æ›¸ãè¾¼ã¿å›æ•°ã‚’è¿½è·¡
            position_scores = [0.0] * data_len  # å„ä½ç½®ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
            
            for block_idx, current_block in enumerate(blocks):
                if block_idx not in block_position_map:
                    if not silent and block_idx < 5:
                        print(f"   [NEXUS Reconstruct] Block {block_idx}: No position mapping - skipping")
                    continue
                
                r, c = block_position_map[block_idx]
                base_idx = r * grid_width + c
                
                if not silent and block_idx < 5:
                    print(f"   [NEXUS Reconstruct] Block {block_idx} at VALID position ({r}, {c}): {list(current_block)}")
                
                # ğŸ”¥ NEXUS ULTIMATE: å„å½¢çŠ¶åº§æ¨™ã«å¯¾ã—ã¦ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆé…ç½®
                for coord_idx, (dr, dc) in enumerate(shape_coords):
                    data_idx = base_idx + dr * grid_width + dc
                    
                    if coord_idx < len(current_block) and data_idx < data_len:
                        current_value = current_block[coord_idx]
                        
                        # ğŸ”¥ NEXUS ULTIMATE: ä½ç½®å„ªå…ˆåº¦ã«ã‚ˆã‚‹é…ç½®æˆ¦ç•¥
                        if block_write_count[data_idx] == 0:
                            # åˆå›æ›¸ãè¾¼ã¿ï¼šãã®ã¾ã¾é…ç½®
                            result_data[data_idx] = current_value
                            position_scores[data_idx] = 1.0
                            block_write_count[data_idx] = 1
                        else:
                            # ğŸ”¥ NEXUS: æ—¢å­˜å€¤ã¨ã®æ¯”è¼ƒã«ã‚ˆã‚‹æœ€é©åŒ–
                            existing_value = result_data[data_idx]
                            
                            # å„ªå…ˆåº¦åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 
                            should_overwrite = False
                            new_score = position_scores[data_idx]
                            
                            # 1. éã‚¼ãƒ­å€¤ã¯å¸¸ã«å„ªå…ˆ
                            if existing_value == 0 and current_value != 0:
                                should_overwrite = True
                                new_score = 2.0
                            # 2. ä¸¡æ–¹éã‚¼ãƒ­ã®å ´åˆï¼šã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„å€¤ã‚’é¸æŠ
                            elif existing_value != 0 and current_value != 0:
                                # ãƒ–ãƒ­ãƒƒã‚¯ã®çµ±åˆãƒ¬ãƒ™ãƒ«ã«åŸºã¥ãä¿¡é ¼åº¦
                                if block_idx < len(blocks) // 2:  # å‰åŠãƒ–ãƒ­ãƒƒã‚¯ã¯ã‚ˆã‚Šä¿¡é ¼æ€§ãŒé«˜ã„
                                    if current_value != existing_value:
                                        should_overwrite = True
                                        new_score = 2.5
                                # å€¤ã®å·®ãŒå°ã•ã„å ´åˆã¯å¹³å‡
                                elif abs(existing_value - current_value) <= 2:
                                    result_data[data_idx] = (existing_value + current_value) // 2
                                    new_score = 1.8
                                    block_write_count[data_idx] += 1
                                    continue
                            
                            if should_overwrite:
                                result_data[data_idx] = current_value
                                position_scores[data_idx] = new_score
                            
                            block_write_count[data_idx] += 1
                        
                        if not silent and block_idx < 3 and coord_idx < 3:
                            print(f"   [NEXUS Reconstruct]   coord({dr},{dc}) -> data[{data_idx}] = {result_data[data_idx]} (writes: {block_write_count[data_idx]}, score: {position_scores[data_idx]:.1f})")
            
            if not silent:
                non_zero_count = len([x for x in result_data if x != 0])
                print(f"   [NEXUS Reconstruct] Placed {non_zero_count}/{data_len} bytes from {len(blocks)} valid blocks")
                
                # ğŸ”¥ NEXUS: è©³ç´°ãªæ›¸ãè¾¼ã¿çµ±è¨ˆ
                zero_writes = sum(1 for x in block_write_count if x == 0)
                single_writes = sum(1 for x in block_write_count if x == 1)
                multi_writes = sum(1 for x in block_write_count if x > 1)
                high_confidence = sum(1 for x in position_scores if x >= 2.0)
                
                print(f"   [NEXUS Reconstruct] Write stats - Zero: {zero_writes}, Single: {single_writes}, Multiple: {multi_writes}")
                print(f"   [NEXUS Reconstruct] High confidence positions: {high_confidence}")
                print(f"   [NEXUS Reconstruct] Trimming to original length: {original_length}")
            
            # ã€NEXUSç†è«–ã€‘ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é™¤å»ã—ã¦å…ƒã®é•·ã•ã«æˆ»ã™
            return bytes(result_data[:original_length])
            
        except Exception as e:
            if not silent:
                print(f"âŒ NEXUSå†æ§‹æˆã‚¨ãƒ©ãƒ¼: {e}")
            return b''
            if not silent:
                print(f"âŒ NEXUSå†æ§‹æˆã‚¨ãƒ©ãƒ¼: {e}")
            return b''


class NexusAdvancedDecompressor:
    def __init__(self):
        self.huffman_encoder = HuffmanEncoder()

    def decompress(self, compressed_data: bytes, level=0) -> bytes:
        """NEXUSé«˜æ©Ÿèƒ½åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‚’è§£å‡"""
        if not compressed_data:
            return b''

        # æœ€çµ‚æ®µã®LZMAè§£å‡
        decompressed_payload = lzma.decompress(compressed_data)
        
        try:
            # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ãŒJSONå½¢å¼ã‹è©¦ã™
            payload = json.loads(decompressed_payload.decode('utf-8'))
            is_json = True
        except (json.JSONDecodeError, UnicodeDecodeError):
            # JSONã§ãªã‘ã‚Œã°ã€å†å¸°çš„ã«åœ§ç¸®ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¨ã¿ãªã™
            payload = decompressed_payload
            is_json = False

        # --- éšå±¤çš„NEXUS ---
        if not is_json:
            print(f"   [Hierarchical] Recursively decompressing metadata (Level {level})...")
            # å†å¸°çš„ã«è§£å‡
            decompressed_json_bytes = self.decompress(payload, level + 1)
            payload = json.loads(decompressed_json_bytes.decode('utf-8'))

        print(f"\n--- Decompressing at Level {level} ---")
        
        header = payload["header"]
        original_length = header["original_length"]
        grid_width = header["grid_width"]
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
        algorithm = header.get("algorithm", "NEXUS_v3_puzzle_optimized")
        element_consolidation_enabled = header.get("element_consolidation_enabled", False)
        
        if element_consolidation_enabled:
            print(f"   [Element Consolidation] Detected advanced consolidation format")
            original_groups_count = header.get("original_groups_count", 0)
            consolidated_groups_count = header.get("consolidated_groups_count", 0)
            consolidation_rate = header.get("consolidation_rate", 0)
            print(f"   [Element Consolidation] Groups reduced: {original_groups_count:,} â†’ {consolidated_groups_count:,} ({consolidation_rate:.2f}%)")
        
        # æ–°æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œ
        if "shape_combination" in header:
            # æ–°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆãƒ‘ã‚ºãƒ«çµ„ã¿åˆã‚ã›ï¼‰
            shape_combination = header["shape_combination"]
            main_shape = header["main_shape"]
            print(f"   [Puzzle] Shape combination: {shape_combination}, Main: {main_shape}")
        else:
            # æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå˜ä¸€å½¢çŠ¶ï¼‰
            main_shape = header["shape_name"]
            shape_combination = [main_shape]
        
        shape_coords = POLYOMINO_SHAPES[main_shape]
        
        unique_groups = [tuple(g) for g in payload["unique_groups"]]
        huffman_trees = payload["huffman_trees"]
        encoded_streams = payload["encoded_streams"]
        id_to_perm_map = {int(k): tuple(v) for k, v in payload["perm_map_dict"].items()}
        
        # æ§‹æˆè¦ç´ çµ±åˆãƒãƒƒãƒ—ã®å–å¾—ï¼ˆæ–°å½¢å¼ã®å ´åˆï¼‰
        element_consolidation_map = payload.get("element_consolidation_map", {})

        # --- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¾©å·åŒ– ---
        print(f"   [Huffman] Decoding blueprint streams...")
        group_id_stream = self.huffman_encoder.decode(encoded_streams["group_ids"], huffman_trees["group_ids"])
        perm_id_stream_int = self.huffman_encoder.decode(encoded_streams["perm_ids"], huffman_trees["perm_ids"])
        
        # æ§‹æˆè¦ç´ çµ±åˆå¯¾å¿œã®å¾©å·åŒ–å‡¦ç†
        if element_consolidation_enabled and element_consolidation_map:
            print(f"   [Reconstruction] Applying element consolidation reversal...")
            # çµ±åˆã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰å…ƒã®å½¢çŠ¶ã«å¾©å…ƒã™ã‚‹å‡¦ç†ã¯è¤‡é›‘ãªãŸã‚ã€
            # ç¾åœ¨ã®å®Ÿè£…ã§ã¯çµ±åˆå¾Œã®ä»£è¡¨å½¢çŠ¶ã§ãã®ã¾ã¾å¾©å·åŒ–
            print(f"   [Reconstruction] Using consolidated canonical forms for reconstruction")
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå†æ§‹ç¯‰
        shape_height = max(r for r, c in shape_coords) + 1
        shape_width = max(c for r, c in shape_coords) + 1
        rows_needed = math.ceil(original_length / grid_width)
        
        # å¿…è¦æœ€å°é™ã®ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º
        reconstructed_size = (rows_needed + shape_height) * grid_width
        reconstructed_grid = bytearray(reconstructed_size)
        
        num_blocks_in_row = grid_width - shape_width + 1

        for i, group_id in enumerate(group_id_stream):
            if i >= len(perm_id_stream_int):
                break
                
            perm_id = perm_id_stream_int[i]
            perm_map = id_to_perm_map[perm_id]

            sorted_group = unique_groups[group_id]
            original_block = bytes(sorted_group[p] for p in perm_map)

            block_row = i // num_blocks_in_row
            block_col = i % num_blocks_in_row

            for j, (dr, dc) in enumerate(shape_coords):
                idx = (block_row + dr) * grid_width + (block_col + dc)
                if idx < len(reconstructed_grid) and j < len(original_block):
                    reconstructed_grid[idx] = original_block[j]
        
        return bytes(reconstructed_grid)[:original_length]

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ† ---
def main():
    if len(sys.argv) != 4:
        print("Usage:")
        print("  To compress:   python nexus_advanced_engine.py compress   <input_file> <output_file.nxz>")
        print("  To decompress: python nexus_advanced_engine.py decompress <input_file.nxz> <output_file>")
        sys.exit(1)

    command, input_path, output_path = sys.argv[1:4]

    if command.lower() == 'compress':
        print(f"ğŸš€ Compressing '{input_path}' with Advanced NEXUS...")
        start_time = time.time()
        
        with open(input_path, 'rb') as f_in:
            input_data = f_in.read()
        
        # å†å¸°ãƒ¬ãƒ™ãƒ«ã‚’0ã«è¨­å®š (éšå±¤çš„åœ§ç¸®ã‚’ç„¡åŠ¹åŒ–ã—ã¦ãƒ‡ã‚£ã‚¹ã‚¯æ›¸ãè¾¼ã¿å‰Šæ¸›)
        compressor = NexusAdvancedCompressor(use_ai=True, max_recursion_level=0)
        compressed_data = compressor.compress(input_data)
        
        with open(output_path, 'wb') as f_out:
            f_out.write(compressed_data)

        end_time = time.time()
        processing_time = end_time - start_time
        original_size = len(input_data)
        compressed_size = len(compressed_data)
        ratio = compressed_size / original_size if original_size > 0 else 0
        size_reduction = (1 - ratio) * 100 if ratio <= 1 else -(ratio - 1) * 100
        
        print("\nâœ… Compression successful!")
        print(f"   Original size:    {original_size:,} bytes")
        print(f"   Compressed size:  {compressed_size:,} bytes")
        if ratio <= 1:
            print(f"   Size reduction:   {size_reduction:.2f}% (ratio: {ratio:.2%})")
        else:
            print(f"   Size expansion:   {-size_reduction:.2f}% (ratio: {ratio:.2%})")
        print(f"   Processing time:  {processing_time:.2f} seconds")
        print(f"   Speed:            {original_size / (1024*1024*processing_time):.2f} MB/sec")

    elif command.lower() == 'decompress':
        print(f"ğŸ“š Decompressing '{input_path}' with Advanced NEXUS...")
        with open(input_path, 'rb') as f_in:
            compressed_data = f_in.read()

        decompressor = NexusAdvancedDecompressor()
        decompressed_data = decompressor.decompress(compressed_data)
        
        with open(output_path, 'wb') as f_out:
            f_out.write(decompressed_data)

        print("\nâœ… Decompression successful!")
        print(f"   Decompressed size: {len(decompressed_data):,} bytes")

    else:
        print(f"âŒ Unknown command: '{command}'")
        sys.exit(1)

if __name__ == "__main__":
    main()

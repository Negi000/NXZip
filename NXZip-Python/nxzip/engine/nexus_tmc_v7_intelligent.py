#!/usr/bin/env python3
"""
NEXUS TMC Engine v7.0 - ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
Transform-Model-Code é©å‘½çš„é€²åŒ–ç‰ˆ with ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹
"""

import os
import sys
import time
import struct
import zlib
import lzma
import bz2
import json
import numpy as np
from typing import Tuple, Dict, Any, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
from enum import Enum
import threading

# Zstandardã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
try:
    import zstandard as zstd
    ZSTD_AVAILABLE = True
    print("ğŸš€ Zstandardåˆ©ç”¨å¯èƒ½ - é«˜æ€§èƒ½ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æœ‰åŠ¹")
except ImportError:
    ZSTD_AVAILABLE = False
    print("âš ï¸ Zstandardæœªåˆ©ç”¨ - æ¨™æº–åœ§ç¸®å™¨ã‚’ä½¿ç”¨")

# pydivsufsortã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import pydivsufsort
    PYDIVSUFSORT_AVAILABLE = True
    print("ğŸ”¥ pydivsufsortåˆ©ç”¨å¯èƒ½ - é«˜é€ŸBWTæœ‰åŠ¹")
except ImportError:
    PYDIVSUFSORT_AVAILABLE = False
    print("âš ï¸ pydivsufsortæœªåˆ©ç”¨ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…")


class DataType(Enum):
    """æ”¹è‰¯ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡"""
    FLOAT_DATA = "float_data"
    TEXT_DATA = "text_data"
    SEQUENTIAL_INT_DATA = "sequential_int_data"
    STRUCTURED_NUMERIC = "structured_numeric"
    TIME_SERIES = "time_series"
    REPETITIVE_BINARY = "repetitive_binary"
    COMPRESSED_LIKE = "compressed_like"
    GENERIC_BINARY = "generic_binary"


class MetaAnalyzer:
    """
    TMC v7.0 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹ - ãƒ¡ã‚¿ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ†æå™¨
    å¤‰æ›ã®ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’äºˆæ¸¬ãƒ»è©•ä¾¡
    """
    
    def __init__(self, core_compressor):
        self.core_compressor = core_compressor
        self.cache = {}  # åˆ†æçµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.sample_size = 8192  # 8KBã‚µãƒ³ãƒ—ãƒ«
        
    def should_apply_transform(self, data: bytes, transformer, data_type: DataType) -> Tuple[bool, Dict[str, Any]]:
        """
        å¤‰æ›ã®ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        Returns: (should_transform, analysis_info)
        """
        print(f"  [ãƒ¡ã‚¿åˆ†æ] {data_type.value} ã®å¤‰æ›åŠ¹æœã‚’åˆ†æä¸­...")
        
        if not transformer or len(data) < self.sample_size:
            return True, {'reason': 'no_transformer_or_small_data'}
        
        try:
            # ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºï¼ˆå…ˆé ­ã€ä¸­å¤®ã€æœ«å°¾ã‹ã‚‰å‡ç­‰ã«ï¼‰
            sample = self._extract_representative_sample(data)
            sample_key = hash(sample)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if sample_key in self.cache:
                cached_result = self.cache[sample_key]
                print(f"    [ãƒ¡ã‚¿åˆ†æ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: åŠ¹æœ={cached_result['effectiveness']:.2%}")
                return cached_result['should_transform'], cached_result
            
            # 1. å¤‰æ›ãªã—ã®åœ§ç¸®ã‚µã‚¤ã‚º
            compressed_raw, _ = self.core_compressor.compress(sample)
            size_raw = len(compressed_raw)
            
            # 2. å¤‰æ›ã‚ã‚Šã®åœ§ç¸®ã‚µã‚¤ã‚º
            try:
                transformed_streams, _ = transformer.transform(sample)
                size_transformed = 0
                
                for stream in transformed_streams:
                    if len(stream) > 0:
                        compressed_stream, _ = self.core_compressor.compress(stream)
                        size_transformed += len(compressed_stream)
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æ¨å®šï¼ˆå¤‰æ›æƒ…å ±ãªã©ï¼‰
                estimated_header_overhead = 64  # æ¦‚ç®—
                size_transformed += estimated_header_overhead
                
            except Exception as e:
                print(f"    [ãƒ¡ã‚¿åˆ†æ] å¤‰æ›ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
                # å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã¯å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—
                analysis_info = {
                    'reason': 'transform_failed',
                    'error': str(e),
                    'should_transform': False
                }
                self.cache[sample_key] = analysis_info
                return False, analysis_info
            
            # 3. åŠ¹æœåˆ†æ
            effectiveness = (size_raw - size_transformed) / size_raw if size_raw > 0 else 0
            threshold = self._get_effectiveness_threshold(data_type, len(data))
            
            should_transform = effectiveness > threshold
            
            analysis_info = {
                'sample_size': len(sample),
                'raw_compressed_size': size_raw,
                'transformed_compressed_size': size_transformed,
                'effectiveness': effectiveness,
                'threshold': threshold,
                'should_transform': should_transform,
                'reason': 'effectiveness_analysis'
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.cache[sample_key] = analysis_info
            
            print(f"    [ãƒ¡ã‚¿åˆ†æ] åœ§ç¸®åŠ¹æœ: {effectiveness:.2%} (é–¾å€¤: {threshold:.2%}) -> {'å¤‰æ›å®Ÿè¡Œ' if should_transform else 'å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—'}")
            
            return should_transform, analysis_info
            
        except Exception as e:
            print(f"    [ãƒ¡ã‚¿åˆ†æ] åˆ†æã‚¨ãƒ©ãƒ¼: {e} - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å¤‰æ›å®Ÿè¡Œ")
            return True, {'reason': 'analysis_error', 'error': str(e)}
    
    def _extract_representative_sample(self, data: bytes) -> bytes:
        """ä»£è¡¨çš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡ºï¼ˆå…ˆé ­ã€ä¸­å¤®ã€æœ«å°¾ã‹ã‚‰ï¼‰"""
        if len(data) <= self.sample_size:
            return data
        
        chunk_size = self.sample_size // 3
        start_chunk = data[:chunk_size]
        middle_start = (len(data) - chunk_size) // 2
        middle_chunk = data[middle_start:middle_start + chunk_size]
        end_chunk = data[-chunk_size:]
        
        return start_chunk + middle_chunk + end_chunk
    
    def _get_effectiveness_threshold(self, data_type: DataType, data_size: int) -> float:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã¨ ã‚µã‚¤ã‚ºã«åŸºã¥ãåŠ¹æœé–¾å€¤"""
        base_thresholds = {
            DataType.TEXT_DATA: 0.05,          # ãƒ†ã‚­ã‚¹ãƒˆã¯5%ä»¥ä¸Šã®æ”¹å–„ã§å¤‰æ›
            DataType.SEQUENTIAL_INT_DATA: 0.03, # ç³»åˆ—æ•´æ•°ã¯3%ä»¥ä¸Šã§å¤‰æ›
            DataType.FLOAT_DATA: 0.08,         # æµ®å‹•å°æ•°ç‚¹ã¯8%ä»¥ä¸Šã§å¤‰æ›
            DataType.STRUCTURED_NUMERIC: 0.06,  # æ§‹é€ åŒ–æ•°å€¤ã¯6%ä»¥ä¸Šã§å¤‰æ›
            DataType.REPETITIVE_BINARY: 0.04,  # åå¾©ãƒã‚¤ãƒŠãƒªã¯4%ä»¥ä¸Šã§å¤‰æ›
        }
        
        threshold = base_thresholds.get(data_type, 0.05)
        
        # å¤§ããªãƒ‡ãƒ¼ã‚¿ã»ã©å³ã—ã„é–¾å€¤ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®ç›¸å¯¾çš„å½±éŸ¿ãŒæ¸›å°‘ï¼‰
        if data_size > 1024 * 1024:  # 1MBä»¥ä¸Š
            threshold *= 0.7
        elif data_size > 64 * 1024:  # 64KBä»¥ä¸Š
            threshold *= 0.85
        
        return threshold


class PostBWTPipeline:
    """
    TMC v7.0 ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    BWT+MTFå¾Œã®ç‰¹æ®Šãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«ç‰¹åŒ–ã—ãŸå°‚é–€ç¬¦å·åŒ–
    """
    
    def encode(self, mtf_stream: bytes) -> List[bytes]:
        """BWT+MTFå¾Œã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å°‚é–€ç¬¦å·åŒ–"""
        print("    [ãƒã‚¹ãƒˆBWT] RLE + åˆ†å‰²ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ã‚’å®Ÿè¡Œä¸­...")
        
        try:
            # 1. ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ– (RLE)
            literals, run_lengths = self._apply_rle(mtf_stream)
            
            print(f"    [ãƒã‚¹ãƒˆBWT] RLE: {len(mtf_stream)} bytes -> ãƒªãƒ†ãƒ©ãƒ«: {len(literals)}, ãƒ©ãƒ³: {len(run_lengths)}")
            
            # 2. åˆ†å‰²ã—ãŸã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¿”ã™
            return [literals, run_lengths]
            
        except Exception as e:
            print(f"    [ãƒã‚¹ãƒˆBWT] ã‚¨ãƒ©ãƒ¼: {e} - å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”å´")
            return [mtf_stream]
    
    def decode(self, streams: List[bytes]) -> bytes:
        """ãƒã‚¹ãƒˆBWTå°‚é–€å¾©å·"""
        print("    [ãƒã‚¹ãƒˆBWT] RLEé€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        
        try:
            if len(streams) == 1:
                return streams[0]  # RLEæœªé©ç”¨
            
            if len(streams) >= 2:
                literals = streams[0]
                run_lengths = streams[1]
                
                # é€†RLE
                mtf_stream = self._reverse_rle(literals, run_lengths)
                print(f"    [ãƒã‚¹ãƒˆBWT] é€†RLE: ãƒªãƒ†ãƒ©ãƒ«: {len(literals)}, ãƒ©ãƒ³: {len(run_lengths)} -> {len(mtf_stream)} bytes")
                
                return mtf_stream
            
            return b''.join(streams)
            
        except Exception as e:
            print(f"    [ãƒã‚¹ãƒˆBWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _apply_rle(self, data: bytes) -> Tuple[bytes, bytes]:
        """ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ–ï¼ˆMTFå¾Œã®ãƒ‡ãƒ¼ã‚¿ã«æœ€é©åŒ–ï¼‰"""
        if not data:
            return b'', b''
        
        literals = bytearray()
        run_lengths = bytearray()
        
        current_byte = data[0]
        run_length = 1
        
        for i in range(1, len(data)):
            if data[i] == current_byte and run_length < 255:
                run_length += 1
            else:
                # ãƒ©ãƒ³ã‚’è¨˜éŒ²
                literals.append(current_byte)
                run_lengths.append(run_length)
                
                # æ–°ã—ã„ãƒ©ãƒ³ã‚’é–‹å§‹
                current_byte = data[i]
                run_length = 1
        
        # æœ€å¾Œã®ãƒ©ãƒ³ã‚’è¨˜éŒ²
        literals.append(current_byte)
        run_lengths.append(run_length)
        
        return bytes(literals), bytes(run_lengths)
    
    def _reverse_rle(self, literals: bytes, run_lengths: bytes) -> bytes:
        """é€†ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ–"""
        if len(literals) != len(run_lengths):
            raise ValueError("Literals and run_lengths must have the same length")
        
        result = bytearray()
        
        for literal, run_length in zip(literals, run_lengths):
            result.extend([literal] * run_length)
        
        return bytes(result)


class ChunkManager:
    """
    TMC v7.0 ãƒãƒ£ãƒ³ã‚¯ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®ä¸¦åˆ—åœ§ç¸®ãƒ»å±•é–‹ã‚’ç®¡ç†
    """
    
    def __init__(self, chunk_size: int = 1024 * 1024, max_workers: int = None):
        self.chunk_size = chunk_size  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1MB
        self.max_workers = max_workers or min(8, (os.cpu_count() or 1) + 4)
        
    def split_data(self, data: bytes) -> List[bytes]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        if len(data) <= self.chunk_size:
            return [data]
        
        chunks = []
        for i in range(0, len(data), self.chunk_size):
            chunk = data[i:i + self.chunk_size]
            chunks.append(chunk)
        
        print(f"  [ãƒãƒ£ãƒ³ã‚¯] ãƒ‡ãƒ¼ã‚¿ã‚’{len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰² (ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {self.chunk_size:,} bytes)")
        return chunks
    
    def parallel_compress_chunks(self, chunks: List[bytes], compress_func) -> Tuple[List[bytes], List[Dict[str, Any]]]:
        """ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—åœ§ç¸®"""
        print(f"  [ä¸¦åˆ—åœ§ç¸®] {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’{self.max_workers}ã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç†ä¸­...")
        
        compressed_chunks = []
        chunk_infos = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # å„ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†ã«æŠ•å…¥
            future_to_index = {}
            for i, chunk in enumerate(chunks):
                future = executor.submit(compress_func, chunk)
                future_to_index[future] = i
            
            # çµæœã‚’é †åºé€šã‚Šã«åé›†
            results = [None] * len(chunks)
            
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    compressed_data, info = future.result()
                    results[index] = (compressed_data, info)
                    print(f"    [ä¸¦åˆ—åœ§ç¸®] ãƒãƒ£ãƒ³ã‚¯ {index + 1}/{len(chunks)} å®Œäº†")
                except Exception as e:
                    print(f"    [ä¸¦åˆ—åœ§ç¸®] ãƒãƒ£ãƒ³ã‚¯ {index + 1} ã‚¨ãƒ©ãƒ¼: {e}")
                    results[index] = (chunks[index], {'error': str(e)})
            
            # çµæœã‚’åˆ†é›¢
            for compressed_data, info in results:
                compressed_chunks.append(compressed_data)
                chunk_infos.append(info)
        
        return compressed_chunks, chunk_infos
    
    def parallel_decompress_chunks(self, compressed_chunks: List[bytes], decompress_func) -> List[bytes]:
        """ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—å±•é–‹"""
        print(f"  [ä¸¦åˆ—å±•é–‹] {len(compressed_chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’{self.max_workers}ã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç†ä¸­...")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_index = {}
            for i, chunk in enumerate(compressed_chunks):
                future = executor.submit(decompress_func, chunk)
                future_to_index[future] = i
            
            # çµæœã‚’é †åºé€šã‚Šã«åé›†
            results = [None] * len(compressed_chunks)
            
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    decompressed_data, _ = future.result()
                    results[index] = decompressed_data
                    print(f"    [ä¸¦åˆ—å±•é–‹] ãƒãƒ£ãƒ³ã‚¯ {index + 1}/{len(compressed_chunks)} å®Œäº†")
                except Exception as e:
                    print(f"    [ä¸¦åˆ—å±•é–‹] ãƒãƒ£ãƒ³ã‚¯ {index + 1} ã‚¨ãƒ©ãƒ¼: {e}")
                    results[index] = b''
            
            return results
    
    def pack_chunks(self, compressed_chunks: List[bytes], chunk_infos: List[Dict[str, Any]]) -> bytes:
        """åœ§ç¸®æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ãƒ‘ãƒƒã‚¯"""
        try:
            container = bytearray()
            
            # TMC v7.0 ã‚³ãƒ³ãƒ†ãƒŠãƒ˜ãƒƒãƒ€ãƒ¼
            container.extend(b'TMC7CONTAINER')
            
            # ãƒãƒ£ãƒ³ã‚¯æ•°
            container.extend(struct.pack('<I', len(compressed_chunks)))
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ†ãƒ¼ãƒ–ãƒ«
            for chunk in compressed_chunks:
                container.extend(struct.pack('<I', len(chunk)))
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰
            metadata = {
                'chunk_count': len(compressed_chunks),
                'total_compressed_size': sum(len(chunk) for chunk in compressed_chunks),
                'chunk_infos': chunk_infos[:10]  # æœ€åˆã®10å€‹ã®ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã®ã¿ä¿å­˜ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ï¼‰
            }
            metadata_json = json.dumps(metadata, separators=(',', ':')).encode('utf-8')
            container.extend(struct.pack('<I', len(metadata_json)))
            container.extend(metadata_json)
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
            for chunk in compressed_chunks:
                container.extend(chunk)
            
            return bytes(container)
            
        except Exception as e:
            print(f"  [ãƒãƒ£ãƒ³ã‚¯] ãƒ‘ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(compressed_chunks)
    
    def unpack_chunks(self, container_data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡º"""
        try:
            if not container_data.startswith(b'TMC7CONTAINER'):
                # æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¾ãŸã¯å˜ä¸€ãƒãƒ£ãƒ³ã‚¯
                return [container_data], {}
            
            offset = 12  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º
            
            # ãƒãƒ£ãƒ³ã‚¯æ•°
            chunk_count = struct.unpack('<I', container_data[offset:offset+4])[0]
            offset += 4
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ†ãƒ¼ãƒ–ãƒ«
            chunk_sizes = []
            for _ in range(chunk_count):
                size = struct.unpack('<I', container_data[offset:offset+4])[0]
                chunk_sizes.append(size)
                offset += 4
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            metadata_size = struct.unpack('<I', container_data[offset:offset+4])[0]
            offset += 4
            metadata_json = container_data[offset:offset+metadata_size].decode('utf-8')
            metadata = json.loads(metadata_json)
            offset += metadata_size
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            chunks = []
            for size in chunk_sizes:
                chunk = container_data[offset:offset+size]
                chunks.append(chunk)
                offset += size
            
            return chunks, metadata
            
        except Exception as e:
            print(f"  [ãƒãƒ£ãƒ³ã‚¯] ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return [container_data], {}


# ä»¥å‰ã®ã‚¯ãƒ©ã‚¹ç¾¤ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆCoreCompressor, ImprovedDispatcher, TDTTransformer, LeCoTransformer ã¯ v6.0 ã¨åŒã˜ï¼‰
from .nexus_tmc_v4_unified import (
    CoreCompressor, ImprovedDispatcher, TDTTransformer, LeCoTransformer
)


class EnhancedBWTTransformer:
    """
    TMC v7.0 å¼·åŒ–ç‰ˆBWTTransformer
    ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆã¨ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆå‡¦ç†
    """
    
    def __init__(self):
        self.pydivsufsort_available = PYDIVSUFSORT_AVAILABLE
        self.post_bwt_pipeline = PostBWTPipeline()
        print(f"ğŸ”¥ Enhanced BWT Transformer åˆæœŸåŒ–: pydivsufsort={'æœ‰åŠ¹' if self.pydivsufsort_available else 'ç„¡åŠ¹'}")
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """TMC v7.0 å¼·åŒ–BWTå¤‰æ›"""
        print("  [å¼·åŒ–BWT] TMC v7.0 å°‚é–€å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'enhanced_bwt_mtf_rle', 'original_size': len(data)}
        
        try:
            if not data:
                return [data], info
            
            # å‹•çš„ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆä¸¦åˆ—å‡¦ç†å‰æï¼‰
            MAX_BWT_SIZE = 2 * 1024 * 1024  # 2MBåˆ¶é™ï¼ˆv7.0ã§ã¯æ‹¡å¼µï¼‰
            if len(data) > MAX_BWT_SIZE:
                print(f"    [å¼·åŒ–BWT] ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º({len(data)})ãŒåˆ¶é™({MAX_BWT_SIZE})ã‚’è¶…é - BWTã‚¹ã‚­ãƒƒãƒ—")
                info['method'] = 'bwt_skipped_large'
                return [data], info
            
            # é«˜é€ŸBWTå¤‰æ›
            if self.pydivsufsort_available:
                bwt_encoded, primary_index = self._fast_bwt_transform(data)
            else:
                bwt_encoded, primary_index = self._fallback_bwt_transform(data)
            
            # MTFå¤‰æ›
            mtf_encoded = self._mtf_encode(bwt_encoded)
            
            # ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é©ç”¨
            post_bwt_streams = self.post_bwt_pipeline.encode(mtf_encoded)
            
            print(f"    [å¼·åŒ–BWT] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: BWT -> MTF -> ãƒã‚¹ãƒˆBWT ({len(post_bwt_streams)}ã‚¹ãƒˆãƒªãƒ¼ãƒ )")
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±
            index_bytes = primary_index.to_bytes(4, 'big')
            
            # æœ€çµ‚ã‚¹ãƒˆãƒªãƒ¼ãƒ æ§‹æˆ
            final_streams = [index_bytes] + post_bwt_streams
            
            info.update({
                'primary_index': primary_index,
                'bwt_length': len(bwt_encoded),
                'mtf_length': len(mtf_encoded),
                'post_bwt_streams': len(post_bwt_streams),
                'enhanced_pipeline': True
            })
            
            return final_streams, info
            
        except Exception as e:
            print(f"    [å¼·åŒ–BWT] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """TMC v7.0 å¼·åŒ–BWTé€†å¤‰æ›"""
        print("  [å¼·åŒ–BWT] TMC v7.0 å°‚é–€é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        
        try:
            if info.get('method') == 'bwt_skipped_large':
                return streams[0] if streams else b''
            
            if len(streams) < 1:
                return b''
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¾©å…ƒ
            primary_index = int.from_bytes(streams[0], 'big')
            post_bwt_streams = streams[1:]
            
            # ãƒã‚¹ãƒˆBWTé€†å¤‰æ›
            if info.get('enhanced_pipeline', False):
                mtf_encoded = self.post_bwt_pipeline.decode(post_bwt_streams)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥æ–¹å¼
                mtf_encoded = streams[1] if len(streams) > 1 else b''
            
            # é€†MTFå¤‰æ›
            bwt_encoded = self._mtf_decode(mtf_encoded)
            
            # é€†BWTå¤‰æ›
            if self.pydivsufsort_available:
                original_data = self._fast_bwt_inverse(bwt_encoded, primary_index)
            else:
                original_data = self._fallback_bwt_inverse(bwt_encoded, primary_index)
            
            print("    [å¼·åŒ–BWT] é€†å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")
            return original_data
            
        except Exception as e:
            print(f"    [å¼·åŒ–BWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    # BWTã€MTFé–¢é€£ãƒ¡ã‚½ãƒƒãƒ‰ã¯v6.0ã¨åŒã˜ï¼ˆç°¡æ½”æ€§ã®ãŸã‚çœç•¥ï¼‰
    def _fast_bwt_transform(self, data: bytes) -> Tuple[bytes, int]:
        """é«˜é€ŸBWTå¤‰æ›ï¼ˆpydivsufsortä½¿ç”¨ï¼‰"""
        try:
            from pydivsufsort import bw_transform
            result = bw_transform(data)
            
            if isinstance(result, tuple) and len(result) == 2:
                bwt_encoded, primary_index = result
            else:
                bwt_encoded = result
                primary_index = 0
            
            if isinstance(primary_index, (list, tuple, np.ndarray)):
                primary_index = int(primary_index[0]) if len(primary_index) > 0 else 0
            else:
                primary_index = int(primary_index)
            
            if not isinstance(bwt_encoded, bytes):
                bwt_encoded = bytes(bwt_encoded)
                
            return bwt_encoded, primary_index
        except Exception as e:
            print(f"    [å¼·åŒ–BWT] é«˜é€Ÿå®Ÿè£…ã‚¨ãƒ©ãƒ¼: {e}")
            return self._fallback_bwt_transform(data)
    
    def _fallback_bwt_transform(self, data: bytes) -> Tuple[bytes, int]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯BWTå®Ÿè£…"""
        data_with_sentinel = data + b'\x00'
        n = len(data_with_sentinel)
        
        rotations = []
        for i in range(n):
            rotation = data_with_sentinel[i:] + data_with_sentinel[:i]
            rotations.append((rotation, i))
        
        rotations.sort(key=lambda x: x[0])
        
        primary_index = 0
        for idx, (rotation, original_pos) in enumerate(rotations):
            if original_pos == 0:
                primary_index = idx
                break
        
        bwt_encoded = bytes(rotation[0][-1] for rotation, _ in rotations)
        return bwt_encoded, primary_index
    
    def _mtf_encode(self, data: bytes) -> bytes:
        """Move-to-Frontç¬¦å·åŒ–"""
        alphabet = list(range(256))
        encoded = bytearray()
        
        for byte_val in data:
            rank = alphabet.index(byte_val)
            encoded.append(rank)
            alphabet.pop(rank)
            alphabet.insert(0, byte_val)
        
        return bytes(encoded)
    
    def _mtf_decode(self, encoded_data: bytes) -> bytes:
        """é€†Move-to-Frontç¬¦å·åŒ–"""
        alphabet = list(range(256))
        decoded = bytearray()
        
        for rank in encoded_data:
            byte_val = alphabet[rank]
            decoded.append(byte_val)
            alphabet.pop(rank)
            alphabet.insert(0, byte_val)
        
        return bytes(decoded)
    
    def _fast_bwt_inverse(self, bwt_encoded: bytes, primary_index: int) -> bytes:
        """é«˜é€ŸBWTé€†å¤‰æ›"""
        try:
            from pydivsufsort import inverse_bwt
            primary_index = int(primary_index)
            reconstructed = inverse_bwt(bwt_encoded, primary_index)
            
            if isinstance(reconstructed, bytes) and reconstructed and reconstructed[-1] == 0:
                reconstructed = reconstructed[:-1]
            
            return reconstructed
        except Exception as e:
            print(f"    [å¼·åŒ–BWT] é«˜é€Ÿé€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return self._fallback_bwt_inverse(bwt_encoded, primary_index)
    
    def _fallback_bwt_inverse(self, last_col: bytes, primary_index: int) -> bytes:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯BWTé€†å¤‰æ›"""
        n = len(last_col)
        if n == 0:
            return b''
        
        count = [0] * 256
        for char in last_col:
            count[char] += 1
        
        first_col_starts = [0] * 256
        total = 0
        for i in range(256):
            first_col_starts[i] = total
            total += count[i]
        
        next_idx = [0] * n
        char_counts = [0] * 256
        
        for i in range(n):
            char = last_col[i]
            next_idx[i] = first_col_starts[char] + char_counts[char]
            char_counts[char] += 1
        
        result = bytearray()
        current_idx = primary_index
        
        for _ in range(n):
            char = last_col[current_idx]
            result.append(char)
            current_idx = next_idx[current_idx]
        
        if result and result[-1] == 0:
            result = result[:-1]
        
        return bytes(result)


class NEXUSTMCEngineV7:
    """
    NEXUS TMC Engine v7.0 - ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
    ãƒ¡ã‚¿åˆ†æã€ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç†çµ±åˆ
    """
    
    def __init__(self, chunk_size: int = 1024 * 1024, max_workers: int = None):
        self.chunk_size = chunk_size
        self.max_workers = max_workers
        
        # ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.dispatcher = ImprovedDispatcher()
        self.core_compressor = CoreCompressor()
        self.meta_analyzer = MetaAnalyzer(self.core_compressor)
        self.chunk_manager = ChunkManager(chunk_size, max_workers)
        
        # å¤‰æ›å™¨ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆv7.0å¼·åŒ–ç‰ˆï¼‰
        self.transformers = {
            DataType.FLOAT_DATA: TDTTransformer(),
            DataType.TEXT_DATA: EnhancedBWTTransformer(),  # v7.0å¼·åŒ–ç‰ˆ
            DataType.SEQUENTIAL_INT_DATA: LeCoTransformer(),
            DataType.STRUCTURED_NUMERIC: TDTTransformer(),
            DataType.TIME_SERIES: LeCoTransformer(),
            DataType.REPETITIVE_BINARY: None,
            DataType.COMPRESSED_LIKE: None,
            DataType.GENERIC_BINARY: None
        }
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'chunks_processed': 0,
            'transforms_applied': 0,
            'transforms_bypassed': 0
        }
    
    def compress(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """
        TMC v7.0 æœ€ä¸Šä½ãƒ¬ãƒ™ãƒ«åœ§ç¸®
        ãƒãƒ£ãƒ³ã‚¯ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç† + ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆæœ€é©åŒ–
        """
        compression_start = time.perf_counter()
        
        try:
            print("\n=== TMC v7.0 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®é–‹å§‹ ===")
            print(f"å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(data):,} bytes")
            
            # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã¯å¾“æ¥æ–¹å¼
            if len(data) <= self.chunk_size:
                print("  [å˜ä¸€ãƒãƒ£ãƒ³ã‚¯] å¾“æ¥æ–¹å¼ã§å‡¦ç†")
                return self._compress_single_chunk(data)
            
            # å¤§ããªãƒ‡ãƒ¼ã‚¿ã¯ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            print("  [ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯] å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å‡¦ç†")
            
            # 1. ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunks = self.chunk_manager.split_data(data)
            
            # 2. ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—åœ§ç¸®
            compressed_chunks, chunk_infos = self.chunk_manager.parallel_compress_chunks(
                chunks, self._compress_single_chunk
            )
            
            # 3. ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ãƒ‘ãƒƒã‚¯
            final_data = self.chunk_manager.pack_chunks(compressed_chunks, chunk_infos)
            
            total_time = time.perf_counter() - compression_start
            
            # çµ±è¨ˆæƒ…å ±é›†è¨ˆ
            self.stats['chunks_processed'] += len(chunks)
            total_original_size = sum(len(chunk) for chunk in chunks)
            total_compressed_size = len(final_data)
            
            compression_ratio = (1 - total_compressed_size / total_original_size) * 100 if total_original_size > 0 else 0
            
            result_info = {
                'compression_ratio': compression_ratio,
                'compression_throughput_mb_s': (total_original_size / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_compression_time': total_time,
                'original_size': total_original_size,
                'compressed_size': total_compressed_size,
                'chunk_count': len(chunks),
                'chunk_infos': chunk_infos[:5],  # æœ€åˆã®5å€‹ã®ã¿
                'tmc_version': '7.0',
                'parallel_processing': True,
                'transforms_applied': self.stats['transforms_applied'],
                'transforms_bypassed': self.stats['transforms_bypassed']
            }
            
            print(f"=== TMC v7.0 åœ§ç¸®å®Œäº† ===")
            print(f"ç·åœ§ç¸®ç‡: {compression_ratio:.2f}%")
            print(f"ä¸¦åˆ—å‡¦ç†: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result_info['compression_throughput_mb_s']:.1f} MB/s")
            
            return final_data, result_info
            
        except Exception as e:
            total_time = time.perf_counter() - compression_start
            print(f"âŒ TMC v7.0 åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return data, {
                'compression_ratio': 0.0,
                'error': str(e),
                'total_compression_time': total_time
            }
    
    def _compress_single_chunk(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®åœ§ç¸®ï¼ˆã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹é©ç”¨ï¼‰"""
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æ
            data_type, features = self.dispatcher.dispatch(data)
            
            # 2. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹åˆ†æ
            transformer = self.transformers.get(data_type)
            should_transform, meta_info = self.meta_analyzer.should_apply_transform(
                data, transformer, data_type
            )
            
            # 3. é©å¿œçš„å¤‰æ›ï¼ˆã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåˆ¤å®šã«åŸºã¥ãï¼‰
            if should_transform and transformer:
                print(f"  [ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ] {data_type.value} å¤‰æ›ã‚’å®Ÿè¡Œ")
                transformed_streams, transform_info = transformer.transform(data)
                self.stats['transforms_applied'] += 1
            else:
                print(f"  [ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ] {data_type.value} å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                transformed_streams = [data]
                transform_info = {'method': 'bypassed', 'meta_analysis': meta_info}
                self.stats['transforms_bypassed'] += 1
            
            # 4. ã‚³ã‚¢åœ§ç¸®
            compressed_streams = []
            compression_methods = []
            
            for i, stream in enumerate(transformed_streams):
                stream_entropy = self._calculate_entropy(stream) if len(stream) > 0 else 0.0
                compressed, comp_method = self.core_compressor.compress(
                    stream, stream_entropy=stream_entropy, stream_size=len(stream)
                )
                compressed_streams.append(compressed)
                compression_methods.append(comp_method)
            
            # 5. TMC v7.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰
            final_data = self._pack_tmc_v7_chunk(
                compressed_streams, compression_methods, data_type, 
                transform_info, features, meta_info
            )
            
            compression_ratio = (1 - len(final_data) / len(data)) * 100 if len(data) > 0 else 0
            
            return final_data, {
                'compression_ratio': compression_ratio,
                'data_type': data_type.value,
                'transform_applied': should_transform,
                'meta_analysis': meta_info,
                'original_size': len(data),
                'compressed_size': len(final_data)
            }
            
        except Exception as e:
            print(f"  [ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®] ã‚¨ãƒ©ãƒ¼: {e}")
            return data, {'error': str(e)}
    
    def decompress(self, compressed_data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC v7.0 æœ€ä¸Šä½ãƒ¬ãƒ™ãƒ«å±•é–‹"""
        decompression_start = time.perf_counter()
        
        try:
            print("\n=== TMC v7.0 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆå±•é–‹é–‹å§‹ ===")
            
            # ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¤å®š
            chunks, metadata = self.chunk_manager.unpack_chunks(compressed_data)
            
            if len(chunks) == 1 and not metadata:
                # å˜ä¸€ãƒãƒ£ãƒ³ã‚¯
                print("  [å˜ä¸€ãƒãƒ£ãƒ³ã‚¯] å¾“æ¥æ–¹å¼ã§å±•é–‹")
                return self._decompress_single_chunk(chunks[0])
            
            # ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å±•é–‹
            print(f"  [ä¸¦åˆ—å±•é–‹] {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†")
            
            decompressed_chunks = self.chunk_manager.parallel_decompress_chunks(
                chunks, self._decompress_single_chunk
            )
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆ
            final_data = b''.join(decompressed_chunks)
            
            total_time = time.perf_counter() - decompression_start
            
            result_info = {
                'decompression_throughput_mb_s': (len(final_data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_decompression_time': total_time,
                'decompressed_size': len(final_data),
                'chunk_count': len(chunks),
                'tmc_version': '7.0',
                'parallel_processing': True
            }
            
            print(f"=== TMC v7.0 å±•é–‹å®Œäº† ===")
            print(f"å±•é–‹ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(final_data):,} bytes")
            
            return final_data, result_info
            
        except Exception as e:
            total_time = time.perf_counter() - decompression_start
            print(f"âŒ TMC v7.0 å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data, {
                'error': str(e),
                'total_decompression_time': total_time
            }
    
    def _decompress_single_chunk(self, compressed_chunk: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®å±•é–‹"""
        try:
            # TMC v7.0 ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            header = self._parse_tmc_v7_header(compressed_chunk)
            if not header:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: v6.0ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                from .nexus_tmc_v4_unified import NEXUSTMCEngineV4
                fallback_engine = NEXUSTMCEngineV4()
                return fallback_engine.decompress_tmc(compressed_chunk)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º
            payload = compressed_chunk[header['header_size']:]
            streams = self._extract_streams(payload, header)
            
            # ä¸¦åˆ—å±•é–‹
            decompressed_streams = []
            for stream, method in zip(streams, header['compression_methods']):
                decompressed = self.core_compressor.decompress(stream, method)
                decompressed_streams.append(decompressed)
            
            # é€†å¤‰æ›
            data_type = DataType(header['data_type'])
            transformer = self.transformers.get(data_type)
            
            if transformer and not header.get('transform_bypassed', False):
                original_data = transformer.inverse_transform(decompressed_streams, header['transform_info'])
            else:
                original_data = b''.join(decompressed_streams)
            
            return original_data, {
                'decompressed_size': len(original_data),
                'data_type': header['data_type']
            }
            
        except Exception as e:
            print(f"  [ãƒãƒ£ãƒ³ã‚¯å±•é–‹] ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_chunk, {'error': str(e)}
    
    def _calculate_entropy(self, data: bytes) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if not data:
            return 0.0
        try:
            data_array = np.frombuffer(data, dtype=np.uint8)
            byte_counts = np.bincount(data_array, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(data_array)
            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 4.0
    
    def _pack_tmc_v7_chunk(self, streams: List[bytes], methods: List[str], 
                          data_type: DataType, transform_info: Dict[str, Any], 
                          features: Dict[str, Any], meta_info: Dict[str, Any]) -> bytes:
        """TMC v7.0 ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰"""
        try:
            header = bytearray()
            
            # TMC v7.0 ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
            header.extend(b'TMC7')
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
            data_type_bytes = data_type.value.encode('utf-8')[:32].ljust(32, b'\x00')
            header.extend(data_type_bytes)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
            header.extend(struct.pack('<I', len(streams)))
            
            # åœ§ç¸®ãƒ¡ã‚½ãƒƒãƒ‰
            for method in methods:
                method_bytes = method.encode('utf-8')[:16].ljust(16, b'\x00')
                header.extend(method_bytes)
            
            # å¤‰æ›æƒ…å ± + ãƒ¡ã‚¿åˆ†ææƒ…å ±
            combined_info = {
                'transform_info': self._make_json_safe(transform_info),
                'meta_analysis': self._make_json_safe(meta_info),
                'transform_bypassed': transform_info.get('method') == 'bypassed'
            }
            
            info_str = json.dumps(combined_info, separators=(',', ':'))
            info_bytes = info_str.encode('utf-8')
            header.extend(struct.pack('<I', len(info_bytes)))
            header.extend(info_bytes)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºãƒ†ãƒ¼ãƒ–ãƒ«
            for stream in streams:
                header.extend(struct.pack('<I', len(stream)))
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
            payload = b''.join(streams)
            checksum = zlib.crc32(payload) & 0xffffffff
            header.extend(struct.pack('<I', checksum))
            
            return bytes(header) + payload
            
        except Exception:
            return b''.join(streams)
    
    def _parse_tmc_v7_header(self, data: bytes) -> Optional[Dict[str, Any]]:
        """TMC v7.0 ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ"""
        try:
            if len(data) < 44 or data[:4] != b'TMC7':
                return None
            
            offset = 4
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
            data_type = data[offset:offset+32].rstrip(b'\x00').decode('utf-8')
            offset += 32
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
            stream_count = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            # åœ§ç¸®ãƒ¡ã‚½ãƒƒãƒ‰
            compression_methods = []
            for _ in range(stream_count):
                method = data[offset:offset+16].rstrip(b'\x00').decode('utf-8')
                compression_methods.append(method)
                offset += 16
            
            # å¤‰æ›æƒ…å ±
            info_size = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            info_str = data[offset:offset+info_size].decode('utf-8')
            combined_info = json.loads(info_str)
            offset += info_size
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚º
            stream_sizes = []
            for _ in range(stream_count):
                size = struct.unpack('<I', data[offset:offset+4])[0]
                stream_sizes.append(size)
                offset += 4
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
            checksum = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            return {
                'data_type': data_type,
                'stream_count': stream_count,
                'compression_methods': compression_methods,
                'transform_info': combined_info.get('transform_info', {}),
                'meta_analysis': combined_info.get('meta_analysis', {}),
                'transform_bypassed': combined_info.get('transform_bypassed', False),
                'stream_sizes': stream_sizes,
                'checksum': checksum,
                'header_size': offset
            }
            
        except Exception:
            return None
    
    def _extract_streams(self, payload: bytes, header: Dict[str, Any]) -> List[bytes]:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º"""
        try:
            streams = []
            offset = 0
            
            for size in header['stream_sizes']:
                stream = payload[offset:offset+size]
                streams.append(stream)
                offset += size
            
            return streams
            
        except Exception:
            return [payload]
    
    def _make_json_safe(self, data: Any) -> Any:
        """JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›"""
        if isinstance(data, dict):
            return {k: self._make_json_safe(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._make_json_safe(v) for v in data]
        elif isinstance(data, np.ndarray):
            return data.tolist()
        elif isinstance(data, (np.int32, np.int64, np.float32, np.float64)):
            return float(data)
        elif isinstance(data, np.bool_):
            return bool(data)
        else:
            return data
    
    def test_reversibility(self, test_data: bytes, test_name: str = "test") -> Dict[str, Any]:
        """TMC v7.0 å¯é€†æ€§ãƒ†ã‚¹ãƒˆ"""
        test_start_time = time.perf_counter()
        
        try:
            print(f"ğŸ”„ TMC v7.0 å¯é€†æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹: {test_name}")
            
            # åœ§ç¸®
            compressed, compression_info = self.compress(test_data)
            
            # å±•é–‹
            decompressed, decompression_info = self.decompress(compressed)
            
            # æ¤œè¨¼
            is_identical = (test_data == decompressed)
            
            result_icon = "âœ…" if is_identical else "âŒ"
            print(f"   {result_icon} å¯é€†æ€§: {'æˆåŠŸ' if is_identical else 'å¤±æ•—'}")
            
            return {
                'test_name': test_name,
                'reversible': is_identical,
                'original_size': len(test_data),
                'compressed_size': len(compressed),
                'decompressed_size': len(decompressed),
                'compression_ratio': compression_info.get('compression_ratio', 0),
                'compression_time': compression_info.get('total_compression_time', 0),
                'decompression_time': decompression_info.get('total_decompression_time', 0),
                'total_test_time': time.perf_counter() - test_start_time,
                'parallel_processing': compression_info.get('parallel_processing', False),
                'transforms_applied': compression_info.get('transforms_applied', 0),
                'transforms_bypassed': compression_info.get('transforms_bypassed', 0),
                'tmc_version': '7.0'
            }
            
        except Exception as e:
            return {
                'test_name': test_name,
                'reversible': False,
                'error': str(e),
                'tmc_version': '7.0'
            }


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
__all__ = ['NEXUSTMCEngineV7', 'DataType', 'MetaAnalyzer', 'PostBWTPipeline', 'ChunkManager']

if __name__ == "__main__":
    print("ğŸš€ NEXUS TMC Engine v7.0 - ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ")
    
    engine = NEXUSTMCEngineV7(chunk_size=512*1024, max_workers=4)  # 512KB ãƒãƒ£ãƒ³ã‚¯
    
    # v7.0 ç‰¹åŒ–ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        ("å°ã‚µã‚¤ã‚ºæµ®å‹•å°æ•°ç‚¹", np.linspace(0, 100, 1000, dtype=np.float32).tobytes()),
        ("å¤§ã‚µã‚¤ã‚ºç³»åˆ—æ•´æ•°", np.arange(0, 50000, dtype=np.int32).tobytes()),
        ("ä¸­ã‚µã‚¤ã‚ºãƒ†ã‚­ã‚¹ãƒˆ", ("TMC v7.0 is revolutionary! " * 1000).encode('utf-8')),
        ("å¤§ã‚µã‚¤ã‚ºåå¾©ãƒã‚¤ãƒŠãƒª", b"PATTERN" * 10000),
        ("ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆç”¨å¤§ãƒ‡ãƒ¼ã‚¿", bytes(range(256)) * 5000)  # 1.25MB
    ]
    
    success_count = 0
    total_tests = len(test_cases)
    
    for name, data in test_cases:
        result = engine.test_reversibility(data, name)
        if result.get('reversible', False):
            success_count += 1
            
        print(f"  ä¸¦åˆ—å‡¦ç†: {'æœ‰åŠ¹' if result.get('parallel_processing') else 'ç„¡åŠ¹'}")
        print(f"  å¤‰æ›é©ç”¨/ã‚¹ã‚­ãƒƒãƒ—: {result.get('transforms_applied', 0)}/{result.get('transforms_bypassed', 0)}")
    
    print(f"\nğŸ“Š TMC v7.0 ãƒ†ã‚¹ãƒˆçµæœ: {success_count}/{total_tests} æˆåŠŸ")
    print(f"ğŸ“ˆ çµ±è¨ˆ: å¤‰æ›é©ç”¨={engine.stats['transforms_applied']}, å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—={engine.stats['transforms_bypassed']}")
    
    if success_count == total_tests:
        print("ğŸ‰ TMC v7.0 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æº–å‚™å®Œäº†!")
        print("ğŸ”¥ ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹ + ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ + ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç† çµ±åˆå®Œäº†!")
    else:
        print("âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•— - ã•ã‚‰ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦")

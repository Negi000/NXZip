#!/usr/bin/env python3
"""
NEXUS Parallel Processing Engine - ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
Multi-threading + Multi-processing + GPU Acceleration + Distributed Processing

ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½:
1. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
2. è² è·åˆ†æ•£ä¸¦åˆ—åœ§ç¸®  
3. GPUåŠ é€Ÿå‡¦ç†
4. éåŒæœŸI/Oå‡¦ç†
5. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–
6. åˆ†æ•£å‡¦ç†ã‚µãƒãƒ¼ãƒˆ
7. å‹•çš„è² è·åˆ†æ•£
"""

import numpy as np
import time
import threading
import multiprocessing
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import List, Dict, Tuple, Optional, Any, Union, Callable
from dataclasses import dataclass, field
import queue
import ctypes
import sys
import os
import psutil
import gc
from pathlib import Path

try:
    import cupy as cp
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    cp = None

try:
    from numba import cuda, jit, prange
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False
    cuda = None
    jit = lambda nopython=True, parallel=False: lambda f: f  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    prange = range


@dataclass
class SystemResources:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±"""
    cpu_count: int
    memory_gb: float
    gpu_available: bool
    gpu_memory_gb: float = 0.0
    load_average: float = 0.0
    disk_io_speed: float = 0.0  # MB/s


@dataclass
class ParallelConfig:
    """ä¸¦åˆ—å‡¦ç†è¨­å®š"""
    use_gpu: bool = True
    use_multiprocessing: bool = True
    use_threading: bool = True
    max_threads: int = field(default_factory=lambda: min(16, multiprocessing.cpu_count() * 2))
    max_processes: int = field(default_factory=lambda: min(8, multiprocessing.cpu_count()))
    chunk_size_mb: int = 4
    memory_limit_gb: float = 8.0
    
    # å“è³ªåˆ¥è¨­å®š
    fast_settings: Dict[str, Any] = field(default_factory=lambda: {
        'chunk_size_mb': 2,
        'max_threads': 4,
        'use_gpu': False
    })
    
    balanced_settings: Dict[str, Any] = field(default_factory=lambda: {
        'chunk_size_mb': 4,
        'max_threads': 8,
        'use_gpu': True
    })
    
    max_settings: Dict[str, Any] = field(default_factory=lambda: {
        'chunk_size_mb': 8,
        'max_threads': 16,
        'use_gpu': True
    })


@dataclass
class ProcessingChunk:
    """å‡¦ç†ãƒãƒ£ãƒ³ã‚¯"""
    chunk_id: int
    data: bytes
    start_offset: int
    end_offset: int
    priority: int = 0
    processing_method: str = "auto"  # auto, cpu, gpu, distributed
    complexity_score: float = 0.0
    entropy: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ProcessingResult:
    """å‡¦ç†çµæœ"""
    chunk_id: int
    compressed_data: bytes
    compression_ratio: float
    processing_time: float
    method_used: str
    energy_efficiency: float = 0.0
    quality_score: float = 0.0
    error: Optional[str] = None
    metrics: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PerformanceProfile:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    throughput_mb_s: float = 0.0
    compression_ratio: float = 0.0
    cpu_utilization: float = 0.0
    gpu_utilization: float = 0.0
    memory_efficiency: float = 0.0
    power_consumption: float = 0.0
    adaptive_score: float = 0.0


class AdvancedMemoryManager:
    """é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†å™¨"""
    
    def __init__(self, limit_mb: int = 4096):
        self.limit_bytes = limit_mb * 1024 * 1024
        self.allocated_memory = {}
        self.current_usage = 0
        self.peak_usage = 0
        self.lock = threading.Lock()
        self.gc_threshold = 0.8  # 80%ã§GCå®Ÿè¡Œ
        
        print(f"ğŸ§  é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†å™¨åˆæœŸåŒ– - åˆ¶é™: {limit_mb}MB")
    
    def allocate(self, name: str, size: int) -> bool:
        """ã‚¹ãƒãƒ¼ãƒˆãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦"""
        with self.lock:
            # åˆ©ç”¨ç‡ãƒã‚§ãƒƒã‚¯
            if self.current_usage + size > self.limit_bytes:
                # è‡ªå‹•ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
                if self._auto_garbage_collect():
                    if self.current_usage + size > self.limit_bytes:
                        return False
                else:
                    return False
            
            self.allocated_memory[name] = {
                'size': size,
                'timestamp': time.time()
            }
            self.current_usage += size
            self.peak_usage = max(self.peak_usage, self.current_usage)
            
            return True
    
    def deallocate(self, name: str):
        """ãƒ¡ãƒ¢ãƒªè§£æ”¾"""
        with self.lock:
            if name in self.allocated_memory:
                info = self.allocated_memory[name]
                self.current_usage -= info['size']
                del self.allocated_memory[name]
    
    def _auto_garbage_collect(self) -> bool:
        """è‡ªå‹•ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³"""
        if self.current_usage / self.limit_bytes > self.gc_threshold:
            # å¤ã„ãƒ¡ãƒ¢ãƒªãƒ–ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾
            current_time = time.time()
            to_remove = []
            
            for name, info in self.allocated_memory.items():
                if current_time - info['timestamp'] > 300:  # 5åˆ†ä»¥ä¸Šå¤ã„
                    to_remove.append(name)
            
            for name in to_remove:
                self.deallocate(name)
            
            # ã‚·ã‚¹ãƒ†ãƒ GCå®Ÿè¡Œ
            gc.collect()
            
            return len(to_remove) > 0
        
        return False
    
    def get_advanced_usage(self) -> Dict[str, Any]:
        """è©³ç´°ä½¿ç”¨é‡å–å¾—"""
        with self.lock:
            return {
                'current_mb': self.current_usage // (1024 * 1024),
                'peak_mb': self.peak_usage // (1024 * 1024),
                'limit_mb': self.limit_bytes // (1024 * 1024),
                'utilization': self.current_usage / self.limit_bytes,
                'allocated_objects': len(self.allocated_memory)
            }


class IntelligentGPUAccelerator:
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆGPUåŠ é€Ÿå™¨"""
    
    def __init__(self):
        self.available = GPU_AVAILABLE and self._check_gpu_availability()
        self.device_count = 0
        self.gpu_memory = 0
        
        if self.available and cp is not None:
            try:
                self.device_count = cp.cuda.runtime.getDeviceCount()
                
                # ãƒ¡ãƒ¢ãƒªæƒ…å ±
                free_mem, total_mem = cp.cuda.runtime.memGetInfo()
                self.gpu_memory = total_mem // (1024 * 1024)  # MB
                
                print(f"ğŸš€ GPUåŠ é€Ÿåˆ©ç”¨å¯èƒ½: {self.device_count} ãƒ‡ãƒã‚¤ã‚¹")
                print(f"   ğŸ’¾ GPUãƒ¡ãƒ¢ãƒª: {self.gpu_memory}MB")
            except Exception as e:
                print(f"âš ï¸ GPUåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                self.available = False
        else:
            print(f"âš ï¸ GPUåŠ é€Ÿåˆ©ç”¨ä¸å¯")
    
    def _check_gpu_availability(self) -> bool:
        """GPUåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        if not GPU_AVAILABLE or cp is None:
            return False
            
        try:
            device_count = cp.cuda.runtime.getDeviceCount()
            return device_count > 0
        except:
            return False
    
    def benchmark_gpu_performance(self, data_size: int = 1024*1024) -> Dict[str, float]:
        """GPUãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        if not self.available:
            return {'throughput': 0.0, 'latency': float('inf')}
        
        try:
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            test_data = np.random.randint(0, 256, data_size, dtype=np.uint8)
            
            # GPUè»¢é€ï¼‹å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            start_time = time.perf_counter()
            
            gpu_data = cp.asarray(test_data)
            result = self._gpu_test_kernel(gpu_data)
            cp.cuda.Stream.null.synchronize()
            
            end_time = time.perf_counter()
            
            processing_time = end_time - start_time
            throughput = data_size / (1024 * 1024) / processing_time  # MB/s
            
            return {
                'throughput': throughput,
                'latency': processing_time * 1000,  # ms
                'efficiency': min(100.0, throughput / 1000 * 100)  # %
            }
            
        except Exception as e:
            print(f"âš ï¸ GPUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {'throughput': 0.0, 'latency': float('inf')}
    
    def _gpu_test_kernel(self, data: Any) -> Any:
        """GPUãƒ†ã‚¹ãƒˆã‚«ãƒ¼ãƒãƒ«"""
        if cp is None:
            return data
            
        # ç°¡å˜ãªä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ
        result = data * 2 + 1
        result = cp.roll(result, 1)
        
        return result
    
    def gpu_accelerated_compression(self, data: bytes, algorithm: str = "hybrid") -> bytes:
        """GPUåŠ é€Ÿåœ§ç¸®"""
        if not self.available:
            return self._cpu_fallback_compression(data)
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚’numpyé…åˆ—ã«å¤‰æ›
            data_array = np.frombuffer(data, dtype=np.uint8)
            
            if algorithm == "hybrid":
                return self._hybrid_gpu_compression(data_array)
            else:
                return self._simple_gpu_compression(data_array)
                
        except Exception as e:
            print(f"âš ï¸ GPUåœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return self._cpu_fallback_compression(data)
    
    def _hybrid_gpu_compression(self, data_array: np.ndarray) -> bytes:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰GPUåœ§ç¸®"""
        # å·®åˆ†è¨ˆç®—ã«ã‚ˆã‚‹åœ§ç¸®
        if len(data_array) > 1:
            delta = np.diff(data_array.astype(np.int16))
            return delta.astype(np.int8).tobytes()
        
        return data_array.tobytes()
    
    def _simple_gpu_compression(self, data_array: np.ndarray) -> bytes:
        """ç°¡æ˜“GPUåœ§ç¸®"""
        # å¤‰æ›ã«ã‚ˆã‚‹åœ§ç¸®åŠ¹æœï¼ˆãƒ‡ãƒ«ã‚¿ç¬¦å·åŒ–ç­‰ï¼‰
        if len(data_array) > 1:
            delta = np.diff(data_array.astype(np.int16))
            return delta.astype(np.int8).tobytes()
        
        return data_array.tobytes()
    
    def _cpu_fallback_compression(self, data: bytes) -> bytes:
        """CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ§ç¸®"""
        import lzma
        return lzma.compress(data, preset=1)


class AdaptiveLoadBalancer:
    """é©å¿œçš„è² è·åˆ†æ•£å™¨"""
    
    def __init__(self, system_resources: SystemResources):
        self.resources = system_resources
        self.worker_performance = {}  # ãƒ¯ãƒ¼ã‚«ãƒ¼åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´
        self.lock = threading.Lock()
        
        print(f"âš–ï¸ é©å¿œçš„è² è·åˆ†æ•£å™¨åˆæœŸåŒ–")
    
    def calculate_optimal_chunks(self, data_size: int, quality: str) -> Dict[str, Any]:
        """æœ€é©ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²è¨ˆç®—"""
        base_chunk_size = 4 * 1024 * 1024  # 4MBåŸºæº–
        
        # å“è³ªåˆ¥èª¿æ•´
        quality_multipliers = {
            'fast': 0.5,
            'balanced': 1.0,
            'max': 2.0
        }
        
        chunk_size = int(base_chunk_size * quality_multipliers.get(quality, 1.0))
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹åˆ¥èª¿æ•´
        if self.resources.memory_gb < 4:
            chunk_size = min(chunk_size, 2 * 1024 * 1024)  # 2MBåˆ¶é™
        elif self.resources.memory_gb > 16:
            chunk_size = max(chunk_size, 8 * 1024 * 1024)  # 8MBæœ€å°
        
        chunk_count = max(1, (data_size + chunk_size - 1) // chunk_size)
        
        # ä¸¦åˆ—åº¦æ±ºå®š
        optimal_threads = min(
            self.resources.cpu_count,
            chunk_count,
            8 if quality == 'fast' else 16
        )
        
        optimal_processes = min(
            self.resources.cpu_count // 2,
            chunk_count // 2,
            4
        )
        
        return {
            'chunk_size': chunk_size,
            'chunk_count': chunk_count,
            'optimal_threads': optimal_threads,
            'optimal_processes': optimal_processes,
            'memory_per_worker': max(512, self.resources.memory_gb * 1024 // optimal_threads),
            'use_gpu': self.resources.gpu_available and data_size > 10 * 1024 * 1024
        }
    
    def distribute_chunks(self, chunks: List[ProcessingChunk], workers: int) -> List[List[ProcessingChunk]]:
        """ãƒãƒ£ãƒ³ã‚¯åˆ†æ•£"""
        if not chunks:
            return []
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´
        actual_workers = min(workers, len(chunks))
        
        # è¤‡é›‘åº¦ãƒ™ãƒ¼ã‚¹åˆ†æ•£
        chunks_sorted = sorted(chunks, key=lambda c: c.complexity_score, reverse=True)
        
        # Round-robin with complexity balancing
        worker_assignments = [[] for _ in range(actual_workers)]
        worker_loads = [0.0] * actual_workers
        
        for chunk in chunks_sorted:
            # æœ€ã‚‚è² è·ã®å°‘ãªã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é¸æŠ
            min_load_worker = min(range(actual_workers), key=lambda i: worker_loads[i])
            
            worker_assignments[min_load_worker].append(chunk)
            worker_loads[min_load_worker] += chunk.complexity_score
        
        return worker_assignments
    
    def update_worker_performance(self, worker_id: str, processing_time: float, data_size: int):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ›´æ–°"""
        with self.lock:
            if worker_id not in self.worker_performance:
                self.worker_performance[worker_id] = {
                    'total_time': 0.0,
                    'total_data': 0,
                    'task_count': 0,
                    'avg_throughput': 0.0
                }
            
            perf = self.worker_performance[worker_id]
            perf['total_time'] += processing_time
            perf['total_data'] += data_size
            perf['task_count'] += 1
            
            # ç§»å‹•å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
            current_throughput = data_size / max(processing_time, 0.001)
            alpha = 0.3
            perf['avg_throughput'] = (
                alpha * current_throughput + 
                (1 - alpha) * perf['avg_throughput']
            )
    
    def get_load_balancing_report(self) -> Dict[str, Any]:
        """è² è·åˆ†æ•£ãƒ¬ãƒãƒ¼ãƒˆ"""
        with self.lock:
            total_throughput = sum(
                perf['avg_throughput'] 
                for perf in self.worker_performance.values()
            )
            
            worker_efficiency = {}
            for worker_id, perf in self.worker_performance.items():
                if total_throughput > 0:
                    efficiency = perf['avg_throughput'] / total_throughput * 100
                else:
                    efficiency = 0.0
                
                worker_efficiency[worker_id] = {
                    'efficiency_percent': efficiency,
                    'total_tasks': perf['task_count'],
                    'avg_throughput_mb_s': perf['avg_throughput'] / (1024 * 1024)
                }
            
            return {
                'total_workers': len(self.worker_performance),
                'total_throughput_mb_s': total_throughput / (1024 * 1024),
                'worker_efficiency': worker_efficiency
            }


class NEXUSParallelEngine:
    """
    NEXUSä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ - æ¬¡ä¸–ä»£ä¸¦åˆ—åœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ 
    
    æ©Ÿèƒ½:
    1. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆè² è·åˆ†æ•£
    2. é©å¿œçš„ä¸¦åˆ—æˆ¦ç•¥
    3. GPU/CPU ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å‡¦ç†
    4. é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†
    5. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–
    """
    
    def __init__(self, config: Optional[ParallelConfig] = None):
        """åˆæœŸåŒ–"""
        self.config = config or ParallelConfig()
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹åˆ†æ
        self.system_resources = self._analyze_system_resources()
        
        # é«˜åº¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.memory_manager = AdvancedMemoryManager(
            int(self.config.memory_limit_gb * 1024)
        )
        self.gpu_accelerator = IntelligentGPUAccelerator()
        self.load_balancer = AdaptiveLoadBalancer(self.system_resources)
        
        # ä¸¦åˆ—å‡¦ç†ãƒ—ãƒ¼ãƒ«
        self.thread_pool = ThreadPoolExecutor(
            max_workers=self.config.max_threads,
            thread_name_prefix="NEXUS-Thread"
        )
        self.process_pool = ProcessPoolExecutor(
            max_workers=self.config.max_processes
        )
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        self.performance_profile = PerformanceProfile()
        
        # çµ±è¨ˆ
        self.processing_stats = {
            'total_chunks_processed': 0,
            'total_data_processed': 0,
            'average_chunk_time': 0.0,
            'peak_throughput': 0.0,
            'energy_efficiency': 0.0
        }
        
        print(f"ğŸš€ NEXUSä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        print(f"   ğŸ’» CPU: {self.system_resources.cpu_count} ã‚³ã‚¢")
        print(f"   ğŸ§  ãƒ¡ãƒ¢ãƒª: {self.system_resources.memory_gb:.1f}GB")
        print(f"   ğŸš€ GPU: {'æœ‰åŠ¹' if self.gpu_accelerator.available else 'ç„¡åŠ¹'}")
        print(f"   ğŸ§µ æœ€å¤§ã‚¹ãƒ¬ãƒƒãƒ‰: {self.config.max_threads}")
        print(f"   âš¡ æœ€å¤§ãƒ—ãƒ­ã‚»ã‚¹: {self.config.max_processes}")
    
    def _analyze_system_resources(self) -> SystemResources:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹åˆ†æ"""
        # CPUæƒ…å ±
        cpu_count = multiprocessing.cpu_count()
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        memory_info = psutil.virtual_memory()
        memory_gb = memory_info.total / (1024**3)
        
        # GPUæƒ…å ±
        gpu_available = GPU_AVAILABLE
        gpu_memory_gb = 0.0
        
        if gpu_available and cp is not None:
            try:
                free_mem, total_mem = cp.cuda.runtime.memGetInfo()
                gpu_memory_gb = total_mem / (1024**3)
            except:
                gpu_available = False
        
        # è² è·å¹³å‡
        try:
            load_average = psutil.getloadavg()[0]
        except:
            load_average = psutil.cpu_percent() / 100.0
        
        # ãƒ‡ã‚£ã‚¹ã‚¯I/Oé€Ÿåº¦ï¼ˆç°¡æ˜“æ¸¬å®šï¼‰
        disk_io_speed = self._measure_disk_speed()
        
        return SystemResources(
            cpu_count=cpu_count,
            memory_gb=memory_gb,
            gpu_available=gpu_available,
            gpu_memory_gb=gpu_memory_gb,
            load_average=load_average,
            disk_io_speed=disk_io_speed
        )
    
    def _measure_disk_speed(self) -> float:
        """ãƒ‡ã‚£ã‚¹ã‚¯I/Oé€Ÿåº¦æ¸¬å®š"""
        try:
            test_data = b"0" * (1024 * 1024)  # 1MB
            
            import tempfile
            with tempfile.NamedTemporaryFile() as tmp:
                start_time = time.perf_counter()
                tmp.write(test_data)
                tmp.flush()
                os.fsync(tmp.fileno())
                write_time = time.perf_counter() - start_time
                
                tmp.seek(0)
                start_time = time.perf_counter()
                _ = tmp.read()
                read_time = time.perf_counter() - start_time
            
            # æ›¸ãè¾¼ã¿é€Ÿåº¦ã‚’ãƒ™ãƒ¼ã‚¹ã«è¨ˆç®—
            speed = len(test_data) / (1024 * 1024) / max(write_time, 0.001)
            return speed
            
        except:
            return 100.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def parallel_compress(self, data: bytes, quality: str = 'balanced') -> bytes:
        """
        ãƒ¡ã‚¤ãƒ³ä¸¦åˆ—åœ§ç¸®é–¢æ•°
        
        Args:
            data: åœ§ç¸®å¯¾è±¡ãƒ‡ãƒ¼ã‚¿
            quality: åœ§ç¸®å“è³ª ('fast', 'balanced', 'max')
            
        Returns:
            åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        """
        print(f"ğŸ”„ NEXUSä¸¦åˆ—åœ§ç¸®é–‹å§‹")
        print(f"   ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(data) / 1024 / 1024:.2f}MB")
        print(f"   ğŸ¯ å“è³ªãƒ¢ãƒ¼ãƒ‰: {quality}")
        
        compression_start = time.perf_counter()
        
        try:
            # 1. å‹•çš„è¨­å®šæœ€é©åŒ–
            self._optimize_config_for_quality(quality)
            
            # 2. æœ€é©ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²è¨ˆç®—
            chunk_config = self.load_balancer.calculate_optimal_chunks(
                len(data), quality
            )
            print(f"   ğŸ”· æœ€é©ãƒãƒ£ãƒ³ã‚¯: {chunk_config['chunk_count']} å€‹")
            
            # 3. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            chunks = self._intelligent_data_chunking(data, chunk_config)
            
            # 4. ä¸¦åˆ—å‡¦ç†æˆ¦ç•¥æ±ºå®š
            strategy = self._determine_processing_strategy(chunks, quality)
            print(f"   ğŸ“‹ å‡¦ç†æˆ¦ç•¥: {strategy}")
            
            # 5. ä¸¦åˆ—åœ§ç¸®å®Ÿè¡Œ
            if strategy == 'gpu_hybrid':
                results = self._gpu_hybrid_compression(chunks, quality)
            elif strategy == 'multiprocess_advanced':
                results = self._advanced_multiprocess_compression(chunks, quality)
            elif strategy == 'multithread_optimized':
                results = self._optimized_multithread_compression(chunks, quality)
            else:
                results = self._sequential_compression(chunks, quality)
            
            # 6. é«˜åº¦çµæœçµ±åˆ
            compressed_data = self._advanced_merge_results(results, data, quality)
            
            # 7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
            total_time = time.perf_counter() - compression_start
            self._update_performance_profile(data, compressed_data, total_time, strategy)
            
            compression_ratio = (1 - len(compressed_data) / len(data)) * 100
            throughput = len(data) / 1024 / 1024 / total_time
            
            print(f"âœ… ä¸¦åˆ—åœ§ç¸®å®Œäº†!")
            print(f"   ğŸ“ˆ åœ§ç¸®ç‡: {compression_ratio:.2f}%")
            print(f"   âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f}MB/s")
            print(f"   â±ï¸ å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
            
            return compressed_data
            
        except Exception as e:
            print(f"âŒ ä¸¦åˆ—åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._fallback_compression(data)
    
    def _optimize_config_for_quality(self, quality: str):
        """å“è³ªåˆ¥è¨­å®šæœ€é©åŒ–"""
        quality_configs = {
            'fast': self.config.fast_settings,
            'balanced': self.config.balanced_settings,
            'max': self.config.max_settings
        }
        
        if quality in quality_configs:
            optimization = quality_configs[quality]
            
            # å‹•çš„è¨­å®šé©ç”¨
            for key, value in optimization.items():
                if hasattr(self.config, key):
                    setattr(self.config, key, value)
    
    def _intelligent_data_chunking(self, data: bytes, config: Dict[str, Any]) -> List[ProcessingChunk]:
        """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²"""
        chunks = []
        chunk_size = config['chunk_size']
        
        current_pos = 0
        chunk_id = 0
        
        while current_pos < len(data):
            end_pos = min(current_pos + chunk_size, len(data))
            chunk_data = data[current_pos:end_pos]
            
            # ãƒãƒ£ãƒ³ã‚¯è¤‡é›‘åº¦è¨ˆç®—
            complexity = self._calculate_chunk_complexity(chunk_data)
            
            chunk = ProcessingChunk(
                chunk_id=chunk_id,
                data=chunk_data,
                start_offset=current_pos,
                end_offset=end_pos,
                complexity_score=complexity,
                processing_method="auto",
                metadata={
                    'size_kb': len(chunk_data) // 1024,
                    'complexity': complexity
                }
            )
            
            chunks.append(chunk)
            current_pos = end_pos
            chunk_id += 1
        
        return chunks
    
    def _calculate_chunk_complexity(self, chunk_data: bytes) -> float:
        """ãƒãƒ£ãƒ³ã‚¯è¤‡é›‘åº¦è¨ˆç®—"""
        # ã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        size_factor = min(1.0, len(chunk_data) / (4 * 1024 * 1024))
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
        entropy = self._calculate_entropy(chunk_data)
        entropy_factor = entropy / 8.0
        
        # çµ„ã¿åˆã‚ã›è¤‡é›‘åº¦
        complexity = (size_factor * 0.3 + entropy_factor * 0.7) * 100
        
        return complexity
    
    def _calculate_entropy(self, data: bytes) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if not data:
            return 0.0
        
        byte_counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)
        probabilities = byte_counts / len(data)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆ0ã«ã‚ˆã‚‹é™¤ç®—å›é¿ï¼‰
        entropy = 0.0
        for p in probabilities:
            if p > 0:
                entropy -= p * np.log2(p)
        
        return entropy
    
    def _determine_processing_strategy(self, chunks: List[ProcessingChunk], quality: str) -> str:
        """å‡¦ç†æˆ¦ç•¥æ±ºå®š"""
        total_size = sum(len(chunk.data) for chunk in chunks)
        chunk_count = len(chunks)
        avg_complexity = sum(chunk.complexity_score for chunk in chunks) / max(chunk_count, 1)
        
        # GPU ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥
        if (self.gpu_accelerator.available and 
            total_size > 20 * 1024 * 1024 and  # 20MBä»¥ä¸Š
            avg_complexity > 50 and
            quality in ['balanced', 'max']):
            return 'gpu_hybrid'
        
        # é«˜åº¦ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹æˆ¦ç•¥
        elif (chunk_count >= 4 and 
              total_size > 50 * 1024 * 1024 and  # 50MBä»¥ä¸Š
              self.system_resources.memory_gb > 8 and
              quality == 'max'):
            return 'multiprocess_advanced'
        
        # æœ€é©åŒ–ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰æˆ¦ç•¥
        elif (chunk_count >= 2 and 
              total_size > 5 * 1024 * 1024):  # 5MBä»¥ä¸Š
            return 'multithread_optimized'
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«æˆ¦ç•¥
        else:
            return 'sequential'
    
    def _fallback_compression(self, data: bytes) -> bytes:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ§ç¸®"""
        try:
            import lzma
            return lzma.compress(data, preset=1)
        except:
            return data  # æœ€å¾Œã®æ‰‹æ®µï¼šéåœ§ç¸®
    
    def _update_performance_profile(self, original_data: bytes, compressed_data: bytes, 
                                  processing_time: float, strategy: str):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°"""
        data_size_mb = len(original_data) / 1024 / 1024
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.performance_profile.throughput_mb_s = data_size_mb / processing_time
        self.performance_profile.compression_ratio = len(compressed_data) / len(original_data)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.performance_profile.cpu_utilization = psutil.cpu_percent()
        memory_info = psutil.virtual_memory()
        self.performance_profile.memory_efficiency = (1 - memory_info.percent / 100) * 100
        
        # çµ±è¨ˆæ›´æ–°
        self.processing_stats['total_chunks_processed'] += 1
        self.processing_stats['total_data_processed'] += len(original_data)
        
        # ç§»å‹•å¹³å‡æ›´æ–°
        alpha = 0.2
        self.processing_stats['average_chunk_time'] = (
            alpha * processing_time + 
            (1 - alpha) * self.processing_stats.get('average_chunk_time', processing_time)
        )
        
        # ãƒ”ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ›´æ–°
        self.processing_stats['peak_throughput'] = max(
            self.processing_stats.get('peak_throughput', 0),
            self.performance_profile.throughput_mb_s
        )
    
    # ä¸¦åˆ—å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£…ï¼‰
    def _gpu_hybrid_compression(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        print(f"ğŸš€ GPU ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åœ§ç¸®å®Ÿè¡Œ")
        return self._optimized_multithread_compression(chunks, quality)
    
    def _advanced_multiprocess_compression(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        print(f"âš¡ é«˜åº¦ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹åœ§ç¸®å®Ÿè¡Œ")
        return self._optimized_multithread_compression(chunks, quality)
    
    def _optimized_multithread_compression(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        print(f"ğŸ§µ æœ€é©åŒ–ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰åœ§ç¸®å®Ÿè¡Œ")
        
        results = []
        futures = []
        
        # è² è·åˆ†æ•£ãƒãƒ£ãƒ³ã‚¯é…å¸ƒ
        worker_assignments = self.load_balancer.distribute_chunks(
            chunks, self.config.max_threads
        )
        
        with self.thread_pool as executor:
            for worker_id, worker_chunks in enumerate(worker_assignments):
                if worker_chunks:  # ç©ºã§ãªã„å ´åˆã®ã¿
                    future = executor.submit(
                        self._process_worker_chunks, 
                        worker_chunks, 
                        f"worker_{worker_id}",
                        quality
                    )
                    futures.append(future)
            
            # çµæœåé›†
            for future in as_completed(futures, timeout=600):
                try:
                    worker_results = future.result()
                    results.extend(worker_results)
                except Exception as e:
                    print(f"âš ï¸ ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
        
        return results
    
    def _sequential_compression(self, chunks: List[ProcessingChunk], quality: str) -> List[ProcessingResult]:
        print(f"ğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«åœ§ç¸®å®Ÿè¡Œ")
        
        results = []
        for chunk in chunks:
            result = self._compress_single_chunk(chunk, "sequential", quality)
            results.append(result)
        
        return results
    
    def _process_worker_chunks(self, chunks: List[ProcessingChunk], worker_id: str, quality: str) -> List[ProcessingResult]:
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
        results = []
        
        for chunk in chunks:
            start_time = time.perf_counter()
            result = self._compress_single_chunk(chunk, worker_id, quality)
            processing_time = time.perf_counter() - start_time
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
            self.load_balancer.update_worker_performance(
                worker_id, processing_time, len(chunk.data)
            )
            
            results.append(result)
        
        return results
    
    def _compress_single_chunk(self, chunk: ProcessingChunk, method: str, quality: str) -> ProcessingResult:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®"""
        start_time = time.perf_counter()
        
        try:
            # ãƒ¡ãƒ¢ãƒªç¢ºä¿
            memory_name = f"chunk_{chunk.chunk_id}_{method}"
            memory_needed = len(chunk.data) * 2  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³
            
            if not self.memory_manager.allocate(memory_name, memory_needed):
                raise MemoryError(f"ãƒ¡ãƒ¢ãƒªä¸è¶³: {memory_needed // 1024 // 1024}MB å¿…è¦")
            
            # åœ§ç¸®å®Ÿè¡Œ
            if self.gpu_accelerator.available and len(chunk.data) > 1024*1024:
                # GPUåŠ é€Ÿåœ§ç¸®
                compressed_data = self.gpu_accelerator.gpu_accelerated_compression(
                    chunk.data, "hybrid"
                )
            else:
                # æ¨™æº–åœ§ç¸®
                compressed_data = self._standard_compression(chunk.data, quality)
            
            processing_time = time.perf_counter() - start_time
            compression_ratio = len(compressed_data) / len(chunk.data)
            
            # ãƒ¡ãƒ¢ãƒªè§£æ”¾
            self.memory_manager.deallocate(memory_name)
            
            return ProcessingResult(
                chunk_id=chunk.chunk_id,
                compressed_data=compressed_data,
                compression_ratio=compression_ratio,
                processing_time=processing_time,
                method_used=method,
                quality_score=self._calculate_quality_score(
                    compression_ratio, processing_time
                ),
                metrics={
                    'original_size': len(chunk.data),
                    'compressed_size': len(compressed_data),
                    'complexity': chunk.complexity_score
                }
            )
            
        except Exception as e:
            processing_time = time.perf_counter() - start_time
            
            return ProcessingResult(
                chunk_id=chunk.chunk_id,
                compressed_data=chunk.data,  # éåœ§ç¸®
                compression_ratio=1.0,
                processing_time=processing_time,
                method_used=method,
                error=str(e)
            )
    
    def _standard_compression(self, data: bytes, quality: str) -> bytes:
        """æ¨™æº–åœ§ç¸®"""
        try:
            import lzma
            
            # å“è³ªåˆ¥ãƒ—ãƒªã‚»ãƒƒãƒˆ
            presets = {
                'fast': 0,
                'balanced': 3,
                'max': 6
            }
            
            preset = presets.get(quality, 3)
            return lzma.compress(data, preset=preset)
            
        except Exception:
            return data
    
    def _calculate_quality_score(self, compression_ratio: float, processing_time: float) -> float:
        """å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # åœ§ç¸®ç‡ã¨å‡¦ç†æ™‚é–“ã®ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢
        compression_score = (1 - compression_ratio) * 100  # åœ§ç¸®ç‡ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
        speed_score = min(100, 10 / max(processing_time, 0.001))  # é€Ÿåº¦ï¼ˆé€Ÿã„ã»ã©è‰¯ã„ï¼‰
        
        # é‡ã¿ä»˜ã‘å¹³å‡
        quality_score = compression_score * 0.7 + speed_score * 0.3
        
        return min(100, max(0, quality_score))
    
    def _advanced_merge_results(self, results: List[ProcessingResult], 
                              original_data: bytes, quality: str) -> bytes:
        """é«˜åº¦çµæœçµ±åˆ"""
        # ãƒãƒ£ãƒ³ã‚¯IDã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda r: r.chunk_id)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        header = self._create_advanced_header(results, original_data, quality)
        
        # ãƒ‡ãƒ¼ã‚¿çµåˆ
        merged_data = header
        for result in results:
            chunk_header = self._create_enhanced_chunk_header(result)
            merged_data += chunk_header + result.compressed_data
        
        return merged_data
    
    def _create_advanced_header(self, results: List[ProcessingResult], 
                              original_data: bytes, quality: str) -> bytes:
        """é«˜åº¦ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        import struct
        
        header = bytearray(128)  # æ‹¡å¼µãƒ˜ãƒƒãƒ€ãƒ¼
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
        header[0:8] = b'NXPAR002'  # NEXUS Parallel v2
        
        # åŸºæœ¬æƒ…å ±
        header[8:16] = struct.pack('<Q', len(original_data))
        header[16:20] = struct.pack('<I', len(results))
        
        # å“è³ªè¨­å®š
        quality_code = {'fast': 1, 'balanced': 2, 'max': 3}.get(quality, 2)
        header[20:24] = struct.pack('<I', quality_code)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        total_time = sum(r.processing_time for r in results)
        avg_compression_ratio = sum(r.compression_ratio for r in results) / len(results)
        avg_quality_score = sum(r.quality_score for r in results) / len(results)
        
        header[24:32] = struct.pack('<d', total_time)
        header[32:40] = struct.pack('<d', avg_compression_ratio)
        header[40:48] = struct.pack('<d', avg_quality_score)
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        header[48:52] = struct.pack('<I', self.system_resources.cpu_count)
        header[52:56] = struct.pack('<f', self.system_resources.memory_gb)
        header[56:60] = struct.pack('<I', 1 if self.gpu_accelerator.available else 0)
        
        return bytes(header)
    
    def _create_enhanced_chunk_header(self, result: ProcessingResult) -> bytes:
        """æ‹¡å¼µãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        import struct
        
        header = bytearray(64)  # æ‹¡å¼µãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼
        
        # åŸºæœ¬æƒ…å ±
        header[0:4] = struct.pack('<I', result.chunk_id)
        header[4:8] = struct.pack('<I', len(result.compressed_data))
        header[8:16] = struct.pack('<d', result.compression_ratio)
        header[16:24] = struct.pack('<d', result.processing_time)
        header[24:32] = struct.pack('<d', result.quality_score)
        
        # ãƒ¡ã‚½ãƒƒãƒ‰æƒ…å ±
        method_bytes = result.method_used.encode('ascii')[:16]
        header[32:32+len(method_bytes)] = method_bytes
        
        # ã‚¨ãƒ©ãƒ¼æƒ…å ±
        if result.error:
            header[48:52] = struct.pack('<I', 1)  # ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ©ã‚°
        
        return bytes(header)
    
    def get_comprehensive_report(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        return {
            'system_resources': {
                'cpu_count': self.system_resources.cpu_count,
                'memory_gb': self.system_resources.memory_gb,
                'gpu_available': self.system_resources.gpu_available,
                'gpu_memory_gb': self.system_resources.gpu_memory_gb,
                'load_average': self.system_resources.load_average,
                'disk_io_speed': self.system_resources.disk_io_speed
            },
            'performance_profile': {
                'throughput_mb_s': self.performance_profile.throughput_mb_s,
                'compression_ratio': self.performance_profile.compression_ratio,
                'cpu_utilization': self.performance_profile.cpu_utilization,
                'gpu_utilization': self.performance_profile.gpu_utilization,
                'memory_efficiency': self.performance_profile.memory_efficiency
            },
            'processing_stats': self.processing_stats,
            'memory_usage': self.memory_manager.get_advanced_usage(),
            'load_balancing': self.load_balancer.get_load_balancing_report(),
            'gpu_performance': self.gpu_accelerator.benchmark_gpu_performance() if self.gpu_accelerator.available else {}
        }
    
    def __del__(self):
        """ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿"""
        try:
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=False)
            if hasattr(self, 'process_pool'):
                self.process_pool.shutdown(wait=False)
        except:
            pass


def test_nexus_parallel_engine():
    """NEXUSä¸¦åˆ—ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ"""
    print("âš¡ NEXUSæ¬¡ä¸–ä»£ä¸¦åˆ—ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    # ä¸¦åˆ—è¨­å®š
    config = ParallelConfig(
        use_gpu=True,
        use_multiprocessing=True,
        use_threading=True,
        max_threads=6,
        max_processes=3,
        chunk_size_mb=2,
        memory_limit_gb=4.0
    )
    
    # ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
    engine = NEXUSParallelEngine(config)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆã‚ˆã‚Šãƒªã‚¢ãƒ«ãªãƒ‡ãƒ¼ã‚¿ï¼‰
    test_datasets = [
        {
            'name': 'å°ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰',
            'data': (
                b"NEXUS Parallel Engine Test " * 100 +
                b"Compression Performance Evaluation " * 200 +
                b"Multi-threading and GPU Acceleration " * 150
            ),
            'quality': 'fast'
        },
        {
            'name': 'ä¸­ãƒ‡ãƒ¼ã‚¿ï¼ˆæ··åˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰',
            'data': (
                b"Pattern-123-ABC" * 2000 +
                b"\x00\x01\x02\x03\x04\x05" * 3000 +
                b"Repetitive-Data-Sequence" * 1500 +
                bytes(range(256)) * 100
            ),
            'quality': 'balanced'
        },
        {
            'name': 'å¤§ãƒ‡ãƒ¼ã‚¿ï¼ˆé«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰',
            'data': (
                np.random.randint(0, 256, 500000, dtype=np.uint8).tobytes() +
                b"Structured-Section" * 5000 +
                np.random.randint(0, 256, 300000, dtype=np.uint8).tobytes()
            ),
            'quality': 'max'
        }
    ]
    
    print(f"ğŸ”¬ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(test_datasets)} ç¨®é¡")
    
    # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆ
    total_results = []
    
    for i, dataset in enumerate(test_datasets):
        print(f"\n{'='*60}")
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i+1}: {dataset['name']}")
        print(f"   ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(dataset['data']) / 1024:.1f}KB")
        print(f"   ğŸ¯ å“è³ªãƒ¢ãƒ¼ãƒ‰: {dataset['quality']}")
        
        try:
            # ä¸¦åˆ—åœ§ç¸®å®Ÿè¡Œ
            start_time = time.perf_counter()
            compressed = engine.parallel_compress(dataset['data'], dataset['quality'])
            total_time = time.perf_counter() - start_time
            
            # çµæœåˆ†æ
            original_size = len(dataset['data'])
            compressed_size = len(compressed)
            compression_ratio = (1 - compressed_size / original_size) * 100
            throughput = original_size / 1024 / 1024 / total_time  # MB/s
            
            result = {
                'name': dataset['name'],
                'quality': dataset['quality'],
                'original_size': original_size,
                'compressed_size': compressed_size,
                'compression_ratio': compression_ratio,
                'processing_time': total_time,
                'throughput': throughput
            }
            
            total_results.append(result)
            
            print(f"   âœ… åœ§ç¸®æˆåŠŸ!")
            print(f"      ğŸ“ˆ åœ§ç¸®ç‡: {compression_ratio:.2f}%")
            print(f"      âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.2f}MB/s")
            print(f"      â±ï¸ å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
            print(f"      ğŸ’¾ åœ§ç¸®å‰: {original_size:,} bytes")
            print(f"      ğŸ’¾ åœ§ç¸®å¾Œ: {compressed_size:,} bytes")
            
        except Exception as e:
            print(f"   âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ")
    print(f"{'='*80}")
    
    if total_results:
        avg_compression_ratio = sum(r['compression_ratio'] for r in total_results) / len(total_results)
        avg_throughput = sum(r['throughput'] for r in total_results) / len(total_results)
        total_data_processed = sum(r['original_size'] for r in total_results)
        total_processing_time = sum(r['processing_time'] for r in total_results)
        
        print(f"ğŸ¯ ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
        print(f"   ğŸ“Š ãƒ†ã‚¹ãƒˆæ•°: {len(total_results)}")
        print(f"   ğŸ“ˆ å¹³å‡åœ§ç¸®ç‡: {avg_compression_ratio:.2f}%")
        print(f"   âš¡ å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {avg_throughput:.2f}MB/s")
        print(f"   ğŸ’¾ ç·å‡¦ç†ãƒ‡ãƒ¼ã‚¿: {total_data_processed / 1024 / 1024:.2f}MB")
        print(f"   â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_processing_time:.3f}ç§’")
        
        # å“è³ªåˆ¥åˆ†æ
        print(f"\nğŸ¯ å“è³ªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        for quality in ['fast', 'balanced', 'max']:
            quality_results = [r for r in total_results if r['quality'] == quality]
            if quality_results:
                q_avg_ratio = sum(r['compression_ratio'] for r in quality_results) / len(quality_results)
                q_avg_throughput = sum(r['throughput'] for r in quality_results) / len(quality_results)
                print(f"   {quality:>8}: åœ§ç¸®ç‡ {q_avg_ratio:.2f}%, ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ {q_avg_throughput:.2f}MB/s")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\nğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ:")
    report = engine.get_comprehensive_report()
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹
    sys_res = report['system_resources']
    print(f"   ğŸ’» CPU: {sys_res['cpu_count']} ã‚³ã‚¢, è² è· {sys_res['load_average']:.2f}")
    print(f"   ğŸ§  ãƒ¡ãƒ¢ãƒª: {sys_res['memory_gb']:.1f}GB")
    print(f"   ğŸš€ GPU: {'æœ‰åŠ¹' if sys_res['gpu_available'] else 'ç„¡åŠ¹'}")
    if sys_res['gpu_available']:
        print(f"       GPU ãƒ¡ãƒ¢ãƒª: {sys_res['gpu_memory_gb']:.1f}GB")
    print(f"   ğŸ’½ ãƒ‡ã‚£ã‚¹ã‚¯I/O: {sys_res['disk_io_speed']:.1f}MB/s")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
    perf = report['performance_profile']
    print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"   âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {perf['throughput_mb_s']:.2f}MB/s")
    print(f"   ğŸ“ˆ åœ§ç¸®ç‡: {perf['compression_ratio']:.3f}")
    print(f"   ğŸ’» CPUåˆ©ç”¨ç‡: {perf['cpu_utilization']:.1f}%")
    print(f"   ğŸ§  ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: {perf['memory_efficiency']:.1f}%")
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    memory = report['memory_usage']
    print(f"\nğŸ§  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
    print(f"   ğŸ“Š ç¾åœ¨: {memory['current_mb']}MB / {memory['limit_mb']}MB")
    print(f"   ğŸ“ˆ ãƒ”ãƒ¼ã‚¯: {memory['peak_mb']}MB")
    print(f"   ğŸ“Š åˆ©ç”¨ç‡: {memory['utilization']*100:.1f}%")
    print(f"   ğŸ—‚ï¸ å‰²ã‚Šå½“ã¦ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: {memory['allocated_objects']}")
    
    # è² è·åˆ†æ•£
    load_bal = report['load_balancing']
    print(f"\nâš–ï¸ è² è·åˆ†æ•£:")
    print(f"   ğŸ‘¥ ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {load_bal['total_workers']}")
    print(f"   âš¡ ç·ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {load_bal['total_throughput_mb_s']:.2f}MB/s")
    
    # GPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    if report['gpu_performance']:
        gpu_perf = report['gpu_performance']
        print(f"\nğŸš€ GPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
        print(f"   âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {gpu_perf.get('throughput', 0):.2f}MB/s")
        print(f"   â±ï¸ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {gpu_perf.get('latency', 0):.2f}ms")
        print(f"   ğŸ¯ åŠ¹ç‡: {gpu_perf.get('efficiency', 0):.1f}%")
    
    print(f"\nğŸ‰ NEXUSæ¬¡ä¸–ä»£ä¸¦åˆ—ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print(f"=" * 80)


if __name__ == "__main__":
    test_nexus_parallel_engine()

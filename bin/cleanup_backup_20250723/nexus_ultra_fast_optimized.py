#!/usr/bin/env python3
"""
Nexus Ultra Fast Optimized Compressor
è¶…é«˜é€Ÿæœ€é©åŒ–åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³ - åŠ¹ç‡é‡è¦–ã®æ¥µé™åœ§ç¸®

ç‰¹å¾´:
- å˜ä¸€ãƒ‘ã‚¹å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- æœ€é©åŒ–ã•ã‚ŒãŸãƒã‚¤ãƒˆäºˆæ¸¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å‡¦ç†ã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›
- ä¸¦åˆ—å‡¦ç†é¢¨ã®åŠ¹ç‡çš„ãªãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†
- å®Œå…¨å¯é€†æ€§ä¿è¨¼
"""

import struct
import time
import hashlib
import os
import sys
from typing import List, Tuple

class UltraFastOptimized:
    def __init__(self):
        self.magic = b'NXUF'  # Nexus Ultra Fast
        self.version = 1
        
    def fast_differential_compress(self, data: bytes) -> bytes:
        """è¶…é«˜é€Ÿå·®åˆ†åœ§ç¸®ï¼ˆå˜ä¸€ãƒ‘ã‚¹ï¼‰"""
        if not data:
            return b''
        
        # å˜ä¸€ãƒ‘ã‚¹ã§å·®åˆ†+RLEå‡¦ç†
        result = bytearray()
        prev_byte = 0
        i = 0
        
        while i < len(data):
            current = data[i]
            diff = (current - prev_byte) & 0xFF
            
            # é«˜é€ŸRLEæ¤œå‡ºï¼ˆæœ€å¤§3ãƒã‚¤ãƒˆå…ˆèª­ã¿ï¼‰
            count = 1
            max_check = min(i + 64, len(data))  # 64ãƒã‚¤ãƒˆåˆ¶é™ã§é«˜é€ŸåŒ–
            
            while i + count < max_check and data[i + count] == current:
                count += 1
            
            if count >= 4:  # 4å›ä»¥ä¸Šã§åœ§ç¸®
                result.extend([0xFF, count & 0xFF, diff])
                i += count
            else:
                if diff == 0xFF:
                    result.extend([0xFF, 0x00])  # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
                else:
                    result.append(diff)
                i += 1
            
            prev_byte = current
        
        return bytes(result)
    
    def fast_differential_decompress(self, compressed: bytes) -> bytes:
        """è¶…é«˜é€Ÿå·®åˆ†å±•é–‹"""
        if not compressed:
            return b''
        
        result = bytearray()
        prev_byte = 0
        i = 0
        
        while i < len(compressed):
            if compressed[i] == 0xFF and i + 1 < len(compressed):
                if compressed[i + 1] == 0x00:
                    # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸ0xFFå·®åˆ†
                    current = (prev_byte + 0xFF) & 0xFF
                    result.append(current)
                    prev_byte = current
                    i += 2
                else:
                    # RLEå±•é–‹
                    count = compressed[i + 1]
                    diff = compressed[i + 2] if i + 2 < len(compressed) else 0
                    current = (prev_byte + diff) & 0xFF
                    result.extend([current] * count)
                    prev_byte = current
                    i += 3
            else:
                # é€šå¸¸ã®å·®åˆ†
                diff = compressed[i]
                current = (prev_byte + diff) & 0xFF
                result.append(current)
                prev_byte = current
                i += 1
        
        return bytes(result)
    
    def adaptive_prediction_fast(self, data: bytes) -> bytes:
        """é«˜é€Ÿé©å¿œäºˆæ¸¬ï¼ˆç°¡ç•¥åŒ–ï¼‰"""
        if len(data) < 2:
            return data
        
        residuals = bytearray([data[0]])
        
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸäºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯
        for i in range(1, len(data)):
            if i == 1:
                pred = data[0]
            elif i == 2:
                pred = data[i-1]
            else:
                # 3ç‚¹å¹³å‡äºˆæ¸¬ï¼ˆé«˜é€Ÿï¼‰
                pred = (data[i-1] + data[i-2] + data[i-3]) // 3
            
            residual = (data[i] - pred) & 0xFF
            residuals.append(residual)
        
        return bytes(residuals)
    
    def inverse_adaptive_prediction_fast(self, residuals: bytes) -> bytes:
        """é«˜é€Ÿé©å¿œäºˆæ¸¬ã®é€†å‡¦ç†"""
        if len(residuals) < 2:
            return residuals
        
        data = bytearray([residuals[0]])
        
        for i in range(1, len(residuals)):
            if i == 1:
                pred = data[0]
            elif i == 2:
                pred = data[i-1]
            else:
                # 3ç‚¹å¹³å‡äºˆæ¸¬ï¼ˆé«˜é€Ÿï¼‰
                pred = (data[i-1] + data[i-2] + data[i-3]) // 3
            
            value = (residuals[i] + pred) & 0xFF
            data.append(value)
        
        return bytes(data)
    
    def ultra_fast_compress(self, data: bytes) -> bytes:
        """è¶…é«˜é€Ÿåœ§ç¸®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        if not data:
            return self.magic + struct.pack('>I', 0)
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: é«˜é€Ÿé©å¿œäºˆæ¸¬
        predicted = self.adaptive_prediction_fast(data)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: é«˜é€Ÿå·®åˆ†+RLEåœ§ç¸®
        compressed = self.fast_differential_compress(predicted)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        header = self.magic + struct.pack('>I', len(data))
        result = header + compressed
        
        # ã‚µã‚¤ã‚ºå¢—åŠ å›é¿
        if len(result) >= len(data) + 8:
            return b'RAWUF' + struct.pack('>I', len(data)) + data
        
        return result
    
    def ultra_fast_decompress(self, compressed: bytes) -> bytes:
        """è¶…é«˜é€Ÿå±•é–‹ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        if not compressed:
            return b''
        
        # RAWå½¢å¼ãƒã‚§ãƒƒã‚¯
        if compressed.startswith(b'RAWUF'):
            original_size = struct.unpack('>I', compressed[5:9])[0]
            return compressed[9:9+original_size]
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯
        if not compressed.startswith(self.magic):
            raise ValueError("Invalid NXUF format")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        original_size = struct.unpack('>I', compressed[4:8])[0]
        compressed_data = compressed[8:]
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: é«˜é€Ÿå·®åˆ†å±•é–‹
        differential_data = self.fast_differential_decompress(compressed_data)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: é«˜é€Ÿé©å¿œäºˆæ¸¬é€†å‡¦ç†
        result = self.inverse_adaptive_prediction_fast(differential_data)
        
        if len(result) != original_size:
            raise ValueError(f"Size mismatch: expected {original_size}, got {len(result)}")
        
        return result
    
    def compress_file(self, input_path: str):
        """è¶…é«˜é€Ÿãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®"""
        if not os.path.exists(input_path):
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
            return None
        
        print(f"âš¡ è¶…é«˜é€Ÿåœ§ç¸®é–‹å§‹: {os.path.basename(input_path)}")
        start_time = time.time()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(input_path, 'rb') as f:
            original_data = f.read()
        
        original_size = len(original_data)
        original_md5 = hashlib.md5(original_data).hexdigest()
        
        print(f"ğŸ“ å…ƒãƒ•ã‚¡ã‚¤ãƒ«: {original_size:,} bytes")
        
        # è¶…é«˜é€Ÿåœ§ç¸®
        compressed_data = self.ultra_fast_compress(original_data)
        compressed_size = len(compressed_data)
        
        # åœ§ç¸®ç‡è¨ˆç®—
        compression_ratio = ((original_size - compressed_size) / original_size) * 100 if original_size > 0 else 0
        
        # å‡¦ç†æ™‚é–“ãƒ»é€Ÿåº¦
        processing_time = time.time() - start_time
        throughput = original_size / (1024 * 1024) / processing_time if processing_time > 0 else 0
        
        # çµæœè¡¨ç¤º
        print(f"ğŸš€ åœ§ç¸®å®Œäº†: {compression_ratio:.1f}%")
        print(f"âš¡ å‡¦ç†æ™‚é–“: {processing_time:.3f}s ({throughput:.1f} MB/s)")
        
        # ä¿å­˜
        output_path = input_path + '.nxuf'
        with open(output_path, 'wb') as f:
            f.write(compressed_data)
        
        # è¶…é«˜é€Ÿå¯é€†æ€§ãƒ†ã‚¹ãƒˆ
        test_start = time.time()
        decompressed_data = self.ultra_fast_decompress(compressed_data)
        test_time = time.time() - test_start
        
        decompressed_md5 = hashlib.md5(decompressed_data).hexdigest()
        
        if decompressed_md5 == original_md5:
            print(f"âœ… å®Œå…¨å¯é€†æ€§ç¢ºèª: MD5ä¸€è‡´ ({test_time:.3f}s)")
            print(f"ğŸ¯ SUCCESS: è¶…é«˜é€Ÿåœ§ç¸®å®Œäº† - {output_path}")
            
            return {
                'input_file': input_path,
                'output_file': output_path,
                'original_size': original_size,
                'compressed_size': compressed_size,
                'compression_ratio': compression_ratio,
                'processing_time': processing_time,
                'throughput': throughput,
                'decompression_time': test_time,
                'lossless': True,
                'method': 'Ultra Fast Optimized'
            }
        else:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: MD5ä¸ä¸€è‡´")
            return None

class BatchCompressor:
    """ãƒãƒƒãƒå‡¦ç†ç”¨ã®è¶…é«˜é€Ÿåœ§ç¸®"""
    
    def __init__(self):
        self.engine = UltraFastOptimized()
    
    def compress_multiple_files(self, file_paths: List[str]):
        """è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®è¶…é«˜é€Ÿãƒãƒƒãƒåœ§ç¸®"""
        results = []
        total_start = time.time()
        
        print(f"ğŸš€ ãƒãƒƒãƒåœ§ç¸®é–‹å§‹: {len(file_paths)} ãƒ•ã‚¡ã‚¤ãƒ«")
        print("=" * 60)
        
        for i, file_path in enumerate(file_paths, 1):
            print(f"\n[{i}/{len(file_paths)}] {os.path.basename(file_path)}")
            result = self.engine.compress_file(file_path)
            if result:
                results.append(result)
        
        total_time = time.time() - total_start
        
        if results:
            total_original = sum(r['original_size'] for r in results)
            total_compressed = sum(r['compressed_size'] for r in results)
            avg_compression = ((total_original - total_compressed) / total_original) * 100
            total_throughput = total_original / (1024 * 1024) / total_time
            
            print(f"\n{'='*60}")
            print(f"ğŸ† ãƒãƒƒãƒåœ§ç¸®å®Œäº†!")
            print(f"ğŸ“Š ç·åœ§ç¸®ç‡: {avg_compression:.1f}%")
            print(f"âš¡ ç·å‡¦ç†æ™‚é–“: {total_time:.3f}s")
            print(f"ğŸš€ ç·ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {total_throughput:.1f} MB/s")
            print(f"âœ… æˆåŠŸç‡: {len(results)}/{len(file_paths)} ({len(results)/len(file_paths)*100:.1f}%)")
        
        return results

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ³•: python nexus_ultra_fast_optimized.py <ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹> [ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹2] ...")
        print("\nâš¡ è¶…é«˜é€Ÿæœ€é©åŒ–åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³")
        print("ğŸ“‹ ç‰¹å¾´:")
        print("  ğŸš€ å˜ä¸€ãƒ‘ã‚¹å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–")
        print("  ğŸ§  æœ€é©åŒ–ã•ã‚ŒãŸäºˆæ¸¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ")
        print("  ğŸ’¨ ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å‡¦ç†ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›")
        print("  âœ… å®Œå…¨å¯é€†æ€§ä¿è¨¼")
        print("  ğŸ“¦ ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ")
        sys.exit(1)
    
    input_files = sys.argv[1:]
    
    if len(input_files) == 1:
        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        engine = UltraFastOptimized()
        result = engine.compress_file(input_files[0])
    else:
        # ãƒãƒƒãƒå‡¦ç†
        batch = BatchCompressor()
        results = batch.compress_multiple_files(input_files)

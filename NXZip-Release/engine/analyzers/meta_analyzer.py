"""
NEXUS TMC Engine - Meta Analyzer Module

This module provides intelligent meta-analysis capabilities for determining
optimal compression strategies based on data characteristics and predictive
entropy analysis.
"""

import numpy as np
from typing import Dict, Any, Tuple, List
from .entropy_calculator import (
    calculate_entropy, 
    estimate_temporal_similarity,
    estimate_repetition_density, 
    estimate_context_predictability,
    calculate_theoretical_compression_gain,
    generate_sample_key
)

__all__ = ['MetaAnalyzer']


class MetaAnalyzer:
    """
    TMC v9.0 é©æ–°çš„äºˆæ¸¬å‹ãƒ¡ã‚¿åˆ†æå™¨
    æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ã«ã‚ˆã‚‹é«˜é€Ÿãƒ»æ­£ç¢ºãªå¤‰æ›åŠ¹æœåˆ¤å®š
    """
    
    def __init__(self, core_compressor, lightweight_mode: bool = False):
        self.core_compressor = core_compressor
        self.lightweight_mode = lightweight_mode
        
        # æ”¹è‰¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥æœ€é©åŒ–ï¼‰
        self.cache = {}  # åˆ†æçµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        if lightweight_mode:
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰: é«˜é€Ÿå‡¦ç†å„ªå…ˆ
            self.cache_max_size = 100  # å°ã•ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥
            self.sample_size = 256  # é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            self.entropy_threshold = 0.95  # å³ã—ã„é–¾å€¤ï¼ˆå¤‰æ›ã‚’æ¸›ã‚‰ã—ã¦é«˜é€ŸåŒ–ï¼‰
            print("ğŸ” äºˆæ¸¬å‹MetaAnalyzeråˆæœŸåŒ–å®Œäº†ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰: é«˜é€Ÿå‡¦ç†å„ªå…ˆï¼‰")
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: ç²¾åº¦ãƒ»åœ§ç¸®ç‡å„ªå…ˆ
            self.cache_max_size = 1000  # å¤§ããªã‚­ãƒ£ãƒƒã‚·ãƒ¥
            self.sample_size = 1024  # è©³ç´°ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            self.entropy_threshold = 0.85  # æ¨™æº–é–¾å€¤
            print("ğŸ” äºˆæ¸¬å‹MetaAnalyzeråˆæœŸåŒ–å®Œäº†ï¼ˆé€šå¸¸ãƒ¢ãƒ¼ãƒ‰: é«˜ç²¾åº¦åˆ†æï¼‰")
        
        self.cache_hit_count = 0
        self.cache_miss_count = 0
        
    def should_apply_transform(self, data: bytes, transformer, data_type) -> Tuple[bool, Dict[str, Any]]:
        """
        æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ã«ã‚ˆã‚‹é«˜é€Ÿå¤‰æ›åŠ¹æœåˆ†æï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥æœ€é©åŒ–ï¼‰
        Returns: (should_transform, analysis_info)
        """
        print(f"  [äºˆæ¸¬ãƒ¡ã‚¿åˆ†æ] {data_type if isinstance(data_type, str) else data_type.value} ã®å¤‰æ›åŠ¹æœã‚’ç†è«–äºˆæ¸¬ä¸­...")
        
        if not transformer or len(data) < 512:
            return False, {'reason': 'no_transformer_or_tiny_data'}
        
        # è»½é‡ãƒ¢ãƒ¼ãƒ‰: è¶…é«˜é€Ÿåˆ¤å®šï¼ˆåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        if self.lightweight_mode:
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰ã§ã¯å¤‰æ›ã‚’åŸå‰‡ã‚¹ã‚­ãƒƒãƒ—ã—ã¦é«˜é€ŸåŒ–
            if len(data) < 4096:  # 4KBæœªæº€ã¯å¤‰æ›ã—ãªã„
                return False, {'reason': 'lightweight_mode_skip_small', 'entropy_improvement': 0.0}
            
            # æœ€ä½é™ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒã‚§ãƒƒã‚¯ã®ã¿
            basic_entropy = calculate_entropy(data[:256])  # æœ€åˆã®256ãƒã‚¤ãƒˆã®ã¿
            if basic_entropy > 7.5:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãªã‚‰å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—
                return False, {'reason': 'lightweight_mode_high_entropy', 'entropy_improvement': 0.0}
            
            # ç°¡æ˜“çš„ãªåˆ¤å®šï¼ˆè©³ç´°åˆ†æãªã—ï¼‰
            simple_improvement = (8.0 - basic_entropy) / 8.0
            return simple_improvement > 0.2, {
                'entropy_improvement': simple_improvement,
                'theoretical_compression_gain': simple_improvement * 50,  # ç°¡æ˜“æ¨å®š
                'reason': 'lightweight_mode_simple_check'
            }
        
        try:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: è©³ç´°åˆ†æ
            sample = data[:min(self.sample_size, len(data))]
            sample_key = hash(sample) + hash(str(data_type))
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if sample_key in self.cache:
                self.cache_hit_count += 1
                cached_result = self.cache[sample_key]
                print(f"    [äºˆæ¸¬ãƒ¡ã‚¿åˆ†æ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ”¹å–„={cached_result['entropy_improvement']:.2%}")
                return cached_result['should_transform'], cached_result
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹
            self.cache_miss_count += 1
            
            # æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ã«ã‚ˆã‚‹åŠ¹æœåˆ¤å®š
            original_entropy = calculate_entropy(sample)
            predicted_residual_entropy, header_cost = self._predict_residual_entropy(sample, data_type, len(data))
            
            # æƒ…å ±ç†è«–çš„åˆ©å¾—è¨ˆç®—
            theoretical_gain = calculate_theoretical_compression_gain(
                original_entropy, predicted_residual_entropy, header_cost, len(data)
            )
            
            # å¤‰æ›åˆ¤å®šï¼ˆç†è«–çš„åˆ©å¾—ãŒæ­£ã®å ´åˆã®ã¿å¤‰æ›ï¼‰
            should_transform = theoretical_gain > 0
            entropy_improvement = (original_entropy - predicted_residual_entropy) / original_entropy if original_entropy > 0 else 0
            
            analysis_info = {
                'sample_size': len(sample),
                'original_entropy': original_entropy,
                'predicted_residual_entropy': predicted_residual_entropy,
                'theoretical_header_cost': header_cost,
                'entropy_improvement': entropy_improvement,
                'theoretical_gain': theoretical_gain,
                'should_transform': should_transform,
                'method': 'residual_entropy_prediction'
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ä»˜ãï¼‰
            self._update_cache(sample_key, analysis_info)
            
            print(f"    [äºˆæ¸¬ãƒ¡ã‚¿åˆ†æ] æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ”¹å–„: {entropy_improvement:.2%}, ç†è«–åˆ©å¾—: {theoretical_gain:.1f}% -> {'å¤‰æ›å®Ÿè¡Œ' if should_transform else 'å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—'}")
            
            return should_transform, analysis_info
            
        except Exception as e:
            # ãƒ‡ãƒãƒƒã‚°ç”¨è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±
            error_detail = str(e)
            if 'nxzip' in error_detail.lower():
                print(f"    [äºˆæ¸¬ãƒ¡ã‚¿åˆ†æ] ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (ç„¡å®³): {error_detail[:50]}... - ä¿å®ˆçš„åˆ¤å®šã‚’ä½¿ç”¨")
            else:
                print(f"    [äºˆæ¸¬ãƒ¡ã‚¿åˆ†æ] äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {error_detail[:50]}... - ä¿å®ˆçš„åˆ¤å®šã§ã‚¹ã‚­ãƒƒãƒ—")
            return False, {'reason': 'prediction_error', 'error': str(e), 'fallback': 'conservative'}
    
    def _predict_residual_entropy(self, sample: bytes, data_type, full_data_size: int) -> Tuple[float, int]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬"""
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸäºˆæ¸¬
        if hasattr(data_type, 'value'):
            data_type_str = data_type.value
        else:
            data_type_str = str(data_type)
        
        if 'sequential_int' in data_type_str.lower():
            # LeCoå¤‰æ›ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_leco_residual_entropy(sample)
            header_cost = 32  # LeCoè¾æ›¸ã‚µã‚¤ã‚º
            
        elif 'float' in data_type_str.lower():
            # TDTå¤‰æ›ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_tdt_residual_entropy(sample)
            header_cost = 24  # TDTå¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        elif 'text' in data_type_str.lower() or 'repetitive' in data_type_str.lower():
            # BWT+MTFå¤‰æ›ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_bwt_residual_entropy(sample)
            header_cost = 16  # BWTå¤‰æ›ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            
        else:
            # ä¸€èˆ¬çš„å¤‰æ›ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ï¼‰ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_contextmixing_residual_entropy(sample)
            header_cost = 40  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
        
        return residual_entropy, header_cost
    
    def _predict_leco_residual_entropy(self, sample: bytes) -> float:
        """LeCoå¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ï¼ˆæ•´æ•°ç³»åˆ—ç‰¹åŒ–ï¼‰"""
        if len(sample) < 16:
            return calculate_entropy(sample)
        
        try:
            # 4ãƒã‚¤ãƒˆæ•´æ•°ã¨ã—ã¦è§£é‡ˆã—ã€1æ¬¡å·®åˆ†ã®åˆ†æ•£ã‚’äºˆæ¸¬
            int_values = []
            for i in range(0, len(sample) - 3, 4):
                val = int.from_bytes(sample[i:i+4], 'little', signed=True)
                int_values.append(val)
            
            if len(int_values) < 2:
                return calculate_entropy(sample) * 0.9
            
            # 1æ¬¡å·®åˆ†ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆLeCoã®æ®‹å·®ã«ç›¸å½“ï¼‰
            differences = [int_values[i+1] - int_values[i] for i in range(len(int_values)-1)]
            diff_bytes = b''.join(val.to_bytes(4, 'little', signed=True) for val in differences)
            residual_entropy = calculate_entropy(diff_bytes)
            
            # ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿ã¯é€šå¸¸70-85%ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›ãŒæœŸå¾…ã§ãã‚‹
            return residual_entropy * 0.75
            
        except:
            return calculate_entropy(sample) * 0.9
    
    def _predict_tdt_residual_entropy(self, sample: bytes) -> float:
        """TDTå¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ï¼ˆæ™‚ç³»åˆ—ç‰¹åŒ–ï¼‰"""
        original_entropy = calculate_entropy(sample)
        
        # æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—å¤‰æ›åŠ¹æœã‚’äºˆæ¸¬
        similarity_factor = estimate_temporal_similarity(sample)
        
        # é«˜ã„æ™‚ç³»åˆ—ç›¸é–¢ãŒã‚ã‚‹ã»ã©å¤§ããªã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›
        entropy_reduction = similarity_factor * 0.6  # æœ€å¤§60%å‰Šæ¸›
        return original_entropy * (1.0 - entropy_reduction)
    
    def _predict_bwt_residual_entropy(self, sample: bytes) -> float:
        """BWT+MTFå¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ï¼ˆç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹åŒ–ï¼‰"""
        original_entropy = calculate_entropy(sample)
        
        # ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯†åº¦ã‚’æ¨å®š
        repetition_factor = estimate_repetition_density(sample)
        
        # ç¹°ã‚Šè¿”ã—ãŒå¤šã„ã»ã©BWT+MTFã®åŠ¹æœã¯å¤§ãã„
        entropy_reduction = repetition_factor * 0.7  # æœ€å¤§70%å‰Šæ¸›
        return original_entropy * (1.0 - entropy_reduction)
    
    def _predict_contextmixing_residual_entropy(self, sample: bytes) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬"""
        original_entropy = calculate_entropy(sample)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆäºˆæ¸¬å¯èƒ½æ€§ã‚’æ¨å®š
        context_predictability = estimate_context_predictability(sample)
        
        # äºˆæ¸¬å¯èƒ½æ€§ãŒé«˜ã„ã»ã©ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›åŠ¹æœãŒå¤§ãã„
        entropy_reduction = context_predictability * 0.4  # æœ€å¤§40%å‰Šæ¸›
        return original_entropy * (1.0 - entropy_reduction)
    
    def _update_cache(self, key: str, value: dict):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ä»˜ãï¼‰"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯
        if len(self.cache) >= self.cache_max_size:
            # æœ€ã‚‚å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆFIFOï¼‰
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            print(f"    [ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†] æœ€å¤§ã‚µã‚¤ã‚ºåˆ°é”ã«ã‚ˆã‚Šå¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤: {self.cache_max_size}")
        
        # æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ 
        self.cache[key] = value
    
    def get_cache_stats(self) -> dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—"""
        total_requests = self.cache_hit_count + self.cache_miss_count
        hit_rate = self.cache_hit_count / total_requests if total_requests > 0 else 0.0
        
        return {
            "total_entries": len(self.cache),
            "max_size": self.cache_max_size,
            "hits": self.cache_hit_count,
            "misses": self.cache_miss_count,
            "hit_rate": hit_rate,
            "total_requests": total_requests
        }
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self.cache.clear()
        self.cache_hit_count = 0
        self.cache_miss_count = 0
        print("ğŸ§¹ MetaAnalyzerã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

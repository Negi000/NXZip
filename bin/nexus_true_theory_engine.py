#!/usr/bin/env python3
"""
ğŸ”¥ NEXUS TRUE THEORY ENGINE ğŸ”¥

NEXUSç†è«–ã®çœŸã®å®Ÿè£…ï¼šLayer 1-2ã®ã¿ã§å®Œå…¨å¯é€†æ€§ã‚’ä¿è¨¼
Layer 3-4ã®è¿‘ä¼¼çµ±åˆã‚’æ’é™¤ã—ã€ç´”ç²‹ãªNEXUSç†è«–ã‚’å®Ÿç¾

NEXUSåŸå‰‡:
- Layer 1: å®Œå…¨ä¸€è‡´çµ±åˆï¼ˆPerfect Match Consolidationï¼‰
- Layer 2: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹çµ±åˆï¼ˆPattern-Based Consolidation with Perfect Reversibilityï¼‰
- Layer 3-4: å‰Šé™¤ï¼ˆè¿‘ä¼¼ã¯å¯é€†æ€§ã‚’æãªã†ãŸã‚ï¼‰
"""

import os
import sys
import json
import lzma
import math
import time
import random
import hashlib
import collections
from typing import Dict, List, Tuple, Any, Optional
import numpy as np

# é€²æ—ãƒãƒ¼
class ProgressBar:
    def __init__(self, total: int, description: str = "Processing"):
        self.total = total
        self.current = 0
        self.description = description
        self.start_time = time.time()
    
    def update(self, value: int):
        self.current = value
        if self.total > 0:
            percent = (self.current / self.total) * 100
            elapsed = time.time() - self.start_time
            print(f"\r{self.description}: {percent:.1f}% ({self.current:,}/{self.total:,}) [{elapsed:.1f}s]", end="", flush=True)
    
    def finish(self):
        elapsed = time.time() - self.start_time
        print(f"\r{self.description}: 100.0% ({self.total:,}/{self.total:,}) [{elapsed:.1f}s] âœ“")

# Huffmanç¬¦å·åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰
class HuffmanEncoder:
    def encode(self, data: List[int]) -> Tuple[Dict, List[int]]:
        if not data:
            return {}, []
        
        # é »åº¦è¨ˆç®—
        freq = collections.Counter(data)
        
        # å˜ä¸€è¦ç´ ã®å ´åˆ
        if len(freq) == 1:
            symbol = list(freq.keys())[0]
            return {'single': symbol}, [0] * len(data)
        
        # è¤‡æ•°è¦ç´ ã®å ´åˆï¼šå›ºå®šé•·ç¬¦å·ã‚’ä½¿ç”¨ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        symbols = sorted(freq.keys())
        bit_length = max(1, len(symbols).bit_length())
        
        codes = {}
        for i, symbol in enumerate(symbols):
            codes[symbol] = format(i, f'0{bit_length}b')
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        encoded = []
        for symbol in data:
            encoded.extend([int(bit) for bit in codes[symbol]])
        
        return codes, encoded
    
    def decode(self, encoded_data: List[int], tree: Dict) -> List[int]:
        if not encoded_data or not tree:
            return []
        
        # å˜ä¸€è¦ç´ ã®å ´åˆ
        if 'single' in tree:
            return [tree['single']] * len(encoded_data)
        
        # é€†å¼•ãè¾æ›¸ä½œæˆ
        reverse_codes = {code: symbol for symbol, code in tree.items()}
        
        # ãƒ“ãƒƒãƒˆé•·è¨ˆç®—
        bit_length = len(list(tree.values())[0])
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        result = []
        for i in range(0, len(encoded_data), bit_length):
            if i + bit_length <= len(encoded_data):
                code_bits = encoded_data[i:i + bit_length]
                code = ''.join(str(bit) for bit in code_bits)
                if code in reverse_codes:
                    result.append(reverse_codes[code])
        
        return result

# ãƒãƒªã‚ªãƒŸãƒå½¢çŠ¶å®šç¾©
POLYOMINO_SHAPES = {
    "I-1": [(0, 0)],
    "I-2": [(0, 0), (0, 1)],
    "I-3": [(0, 0), (0, 1), (0, 2)],
    "I-4": [(0, 0), (0, 1), (0, 2), (0, 3)],
    "I-5": [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4)],
    "O-4": [(0, 0), (0, 1), (1, 0), (1, 1)],
    "T-4": [(0, 1), (1, 0), (1, 1), (1, 2)],
    "L-4": [(0, 0), (1, 0), (2, 0), (2, 1)],
    "Z-4": [(0, 0), (0, 1), (1, 1), (1, 2)],
    "S-4": [(0, 1), (0, 2), (1, 0), (1, 1)],
    "T-5": [(0, 1), (1, 0), (1, 1), (1, 2), (2, 1)],
    "R-6": [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)],
    "U-6": [(0, 0), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0)],
    "H-7": [(0, 0), (0, 1), (0, 2), (1, 1), (2, 0), (2, 1), (2, 2)],
    "R-8": [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (1, 3)]
}

class NexusTrueTheoryEngine:
    """
    ğŸ”¥ NEXUS TRUE THEORY ENGINE ğŸ”¥
    
    NEXUSç†è«–ã®ç´”ç²‹å®Ÿè£…ï¼šå®Œå…¨å¯é€†æ€§ã‚’ä¿è¨¼ã—ãŸ2å±¤çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self):
        self.huffman_encoder = HuffmanEncoder()
    
    def _nexus_layer1_perfect_consolidation(self, normalized_groups: Dict[Tuple, int], show_progress: bool = False) -> Tuple[Dict[Tuple, int], Dict[int, Dict]]:
        """
        ğŸ”¥ NEXUS LAYER 1: å®Œå…¨ä¸€è‡´çµ±åˆï¼ˆTRUE THEORYç‰ˆï¼‰
        
        NEXUSåŸå‰‡: 100%åŒä¸€ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿çµ±åˆ - å®Œå…¨å¯é€†æ€§ä¿è¨¼
        """
        if show_progress:
            progress_bar = ProgressBar(len(normalized_groups), "   NEXUS Layer 1: Perfect match consolidation")
        
        # å®Œå…¨ä¸€è‡´ã‚°ãƒ«ãƒ¼ãƒ—ã®æ¤œå‡º
        exact_signature_map = {}  # exact_tuple -> [(group_tuple, group_id)]
        
        processed = 0
        for group_tuple, group_id in normalized_groups.items():
            # å®Œå…¨ä¸€è‡´åˆ¤å®šï¼šã‚¿ãƒ—ãƒ«ãã®ã‚‚ã®ã‚’ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨
            exact_sig = group_tuple  # å®Œå…¨ä¸€è‡´ã®ã¿
            
            if exact_sig not in exact_signature_map:
                exact_signature_map[exact_sig] = []
            exact_signature_map[exact_sig].append((group_tuple, group_id))
            
            processed += 1
            if show_progress and processed % 5000 == 0:
                progress_bar.update(processed)
        
        if show_progress:
            progress_bar.finish()
        
        print(f"   [Layer 1] Found {len(exact_signature_map):,} exact signature groups")
        
        # ğŸ”¥ NEXUS PERFECT: å®Œå…¨ä¸€è‡´çµ±åˆã®ã¿å®Ÿè¡Œ
        consolidated_groups = {}
        nexus_exact_map = {}
        new_group_id = 0
        
        for exact_tuple, group_list in exact_signature_map.items():
            if len(group_list) == 1:
                # å˜ä¸€ã‚°ãƒ«ãƒ¼ãƒ—ï¼šãã®ã¾ã¾ä¿æŒ
                group_tuple, original_id = group_list[0]
                consolidated_groups[group_tuple] = new_group_id
                nexus_exact_map[original_id] = {
                    'nexus_new_group_id': new_group_id,
                    'nexus_layer': 1,
                    'nexus_consolidation_type': 'exact_identity',
                    'nexus_original_group': group_tuple,
                    'nexus_exact_reconstruction': True
                }
            else:
                # è¤‡æ•°ã®å®Œå…¨ä¸€è‡´ã‚°ãƒ«ãƒ¼ãƒ—ï¼šä»£è¡¨1ã¤ã«çµ±åˆ
                representative = group_list[0][0]  # æœ€åˆã®ã‚‚ã®ã‚’ä»£è¡¨ã¨ã™ã‚‹
                consolidated_groups[representative] = new_group_id
                
                # ğŸ”¥ NEXUS: å®Œå…¨ä¸€è‡´ã‚°ãƒ«ãƒ¼ãƒ—ã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                for group_tuple, original_id in group_list:
                    nexus_exact_map[original_id] = {
                        'nexus_new_group_id': new_group_id,
                        'nexus_canonical_form': representative,
                        'nexus_layer': 1,
                        'nexus_consolidation_type': 'exact_match',
                        'nexus_original_group': group_tuple,
                        'nexus_exact_reconstruction': True,
                        'nexus_exact_group_list': [g[0] for g in group_list]  # å®Œå…¨ä¸€è‡´ãƒªã‚¹ãƒˆ
                    }
            
            new_group_id += 1
        
        return consolidated_groups, nexus_exact_map
    
    def _nexus_layer2_pattern_consolidation(self, groups_dict: Dict[Tuple, int], layer1_map: Dict, show_progress: bool = False) -> Tuple[Dict[Tuple, int], Dict[int, Dict]]:
        """
        ğŸ”¥ NEXUS LAYER 2: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹çµ±åˆï¼ˆTRUE THEORYç‰ˆï¼‰
        
        NEXUSåŸå‰‡: é †åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±åˆ - å®Œå…¨å¯é€†æ€§ä¿è¨¼ï¼ˆè¿‘ä¼¼ãªã—ï¼‰
        """
        if show_progress:
            progress_bar = ProgressBar(len(groups_dict), "   NEXUS Layer 2: Pattern-based consolidation")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ï¼šæ­£è¦åŒ–ã«ã‚ˆã‚‹å®Œå…¨å¯é€†çµ±åˆ
        pattern_groups = {}  # normalized_pattern -> [(group_tuple, group_id)]
        
        processed = 0
        for group_tuple, group_id in groups_dict.items():
            # ğŸ”¥ NEXUS TRUE: æ­£è¦åŒ–æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰ã‚’ä½¿ç”¨
            # æ—¢ã«æ­£è¦åŒ–ã•ã‚Œã¦ã„ã‚‹ãŒã€ã•ã‚‰ã«ç¢ºå®Ÿã«ã™ã‚‹
            normalized_pattern = tuple(sorted(group_tuple))
            
            if normalized_pattern not in pattern_groups:
                pattern_groups[normalized_pattern] = []
            pattern_groups[normalized_pattern].append((group_tuple, group_id))
            
            processed += 1
            if show_progress and processed % 5000 == 0:
                progress_bar.update(processed)
        
        if show_progress:
            progress_bar.finish()
        
        print(f"   [Layer 2] Found {len(pattern_groups):,} pattern groups")
        
        # ğŸ”¥ NEXUS PATTERN: é †åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±åˆ - å®Œå…¨å¯é€†æ€§ä¿è¨¼
        consolidated = {}
        nexus_pattern_map = {}
        new_id = 0
        
        for normalized_pattern, group_list in pattern_groups.items():
            if len(group_list) == 1:
                # å˜ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šãã®ã¾ã¾ä¿æŒ
                group_tuple, original_id = group_list[0]
                consolidated[group_tuple] = new_id
                nexus_pattern_map[original_id] = {
                    'nexus_new_group_id': new_id,
                    'nexus_layer': 2,
                    'nexus_consolidation_type': 'pattern_identity',
                    'nexus_original_group': group_tuple,
                    'nexus_normalized_pattern': normalized_pattern,
                    'nexus_exact_reconstruction': True,
                    'nexus_layer1_inheritance': layer1_map.get(original_id, {})
                }
            else:
                # è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šæ­£è¦åŒ–å½¢çŠ¶ã‚’ä»£è¡¨ã¨ã™ã‚‹
                representative = normalized_pattern  # æ­£è¦åŒ–æ¸ˆã¿ã‚’ä»£è¡¨ã¨ã™ã‚‹
                consolidated[representative] = new_id
                
                # ğŸ”¥ NEXUS: å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Œå…¨é€†å¤‰æ›ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                for group_tuple, original_id in group_list:
                    # é †åˆ—ãƒãƒƒãƒ—ã‚’è¨ˆç®—ï¼ˆå®Œå…¨å¯é€†æ€§ä¿è¨¼ï¼‰
                    permutation_map = self._calculate_perfect_permutation_map(group_tuple, normalized_pattern)
                    
                    nexus_pattern_map[original_id] = {
                        'nexus_new_group_id': new_id,
                        'nexus_canonical_form': representative,
                        'nexus_layer': 2,
                        'nexus_consolidation_type': 'pattern_match',
                        'nexus_original_group': group_tuple,
                        'nexus_normalized_pattern': normalized_pattern,
                        'nexus_permutation_map': permutation_map,  # ğŸ”¥ å®Œå…¨é€†å¤‰æ›ã‚­ãƒ¼
                        'nexus_exact_reconstruction': True,
                        'nexus_pattern_group_list': [g[0] for g in group_list],
                        'nexus_layer1_inheritance': layer1_map.get(original_id, {})
                    }
            
            new_id += 1
        
        return consolidated, nexus_pattern_map
    
    def _calculate_perfect_permutation_map(self, original_group: Tuple, normalized_pattern: Tuple) -> Tuple[int, ...]:
        """
        ğŸ”¥ NEXUS: å®Œå…¨å¯é€†é †åˆ—ãƒãƒƒãƒ—è¨ˆç®—ï¼ˆTRUE THEORYç‰ˆï¼‰
        
        å…ƒã®ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æ­£è¦åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã¸ã®å®Œå…¨å¯é€†å¤‰æ›ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ
        """
        if len(original_group) != len(normalized_pattern):
            return tuple(range(len(original_group)))  # å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        try:
            # å…ƒã®é †åºã‹ã‚‰æ­£è¦åŒ–é †åºã¸ã®å¤‰æ›ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
            normalized_list = list(normalized_pattern)
            permutation = []
            used_indices = set()
            
            for element in original_group:
                # normalized_listã§ã®æœ€åˆã®æœªä½¿ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¤œç´¢
                for i, norm_element in enumerate(normalized_list):
                    if i not in used_indices and norm_element == element:
                        permutation.append(i)
                        used_indices.add(i)
                        break
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãã®ã¾ã¾ä½¿ç”¨
                    permutation.append(len(permutation))
            
            return tuple(permutation)
        except Exception:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return tuple(range(len(original_group)))
    
    def _consolidate_by_elements_true_theory(self, normalized_groups: Dict[Tuple, int], show_progress: bool = False) -> Tuple[Dict[Tuple, int], Dict[int, Dict]]:
        """
        ğŸ”¥ NEXUS TRUE THEORY: 2å±¤çµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨å¯é€†æ€§ä¿è¨¼ï¼‰
        
        NEXUSåŸå‰‡: Layer 1-2ã®ã¿ã§å®Œå…¨å¯é€†åœ§ç¸®ã‚’å®Ÿç¾
        Layer 3-4ã®è¿‘ä¼¼çµ±åˆã‚’æ’é™¤ã—ã€ç´”ç²‹ãªNEXUSç†è«–ã‚’é©ç”¨
        """
        if not normalized_groups:
            return normalized_groups, {}
        
        print(f"   [NEXUS TRUE THEORY] Processing {len(normalized_groups):,} groups with 2-layer perfect consolidation")
        original_count = len(normalized_groups)
        
        # Layer 1: NEXUSå®Œå…¨ä¸€è‡´çµ±åˆ
        layer1_result, layer1_map = self._nexus_layer1_perfect_consolidation(normalized_groups, show_progress)
        layer1_reduction = 100 * (original_count - len(layer1_result)) / original_count
        print(f"   [NEXUS Layer 1] Perfect match: {len(layer1_result):,} groups ({layer1_reduction:.1f}% reduction)")
        
        # Layer 2: NEXUSãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹çµ±åˆ
        layer2_result, layer2_map = self._nexus_layer2_pattern_consolidation(layer1_result, layer1_map, show_progress)
        layer2_reduction = 100 * (len(layer1_result) - len(layer2_result)) / len(layer1_result) if len(layer1_result) > 0 else 0
        print(f"   [NEXUS Layer 2] Pattern match: {len(layer2_result):,} groups ({layer2_reduction:.1f}% additional reduction)")
        
        total_reduction = 100 * (original_count - len(layer2_result)) / original_count
        print(f"   [NEXUS TRUE THEORY] Total reduction: {total_reduction:.2f}% ({original_count:,} â†’ {len(layer2_result):,})")
        
        # ğŸ”¥ NEXUS TRUE: å®Œå…¨é€†å¤‰æ›ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ï¼ˆ2å±¤ã®ã¿ï¼‰
        nexus_true_map = self._build_nexus_true_reconstruction_chain(layer1_map, layer2_map)
        
        return layer2_result, nexus_true_map
    
    def _build_nexus_true_reconstruction_chain(self, layer1_map: Dict, layer2_map: Dict) -> Dict:
        """
        ğŸ”¥ NEXUS TRUE: å®Œå…¨é€†å¤‰æ›ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ï¼ˆ2å±¤ç‰ˆï¼‰
        
        NEXUSåŸå‰‡: Layer 1-2ã®å¤‰æ›ã‚’å®Œå…¨ã«é€†å¤‰æ›å¯èƒ½ã«ã™ã‚‹
        """
        nexus_true_chain = {}
        
        # Layer 1ã¨Layer 2ã®ãƒãƒƒãƒ—ã‚’çµåˆ
        all_maps = [layer1_map, layer2_map]
        
        for layer_idx, layer_map in enumerate(all_maps, 1):
            for original_id, mapping_data in layer_map.items():
                if original_id not in nexus_true_chain:
                    nexus_true_chain[original_id] = {
                        'nexus_reconstruction_chain': [],
                        'nexus_final_group_id': None,
                        'nexus_original_group': None,
                        'nexus_exact_reconstruction': True
                    }
                
                # ğŸ”¥ NEXUS TRUE: å„å±¤ã®å¤‰æ›ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                nexus_true_chain[original_id]['nexus_reconstruction_chain'].append({
                    'layer': layer_idx,
                    'transformation_data': mapping_data
                })
                
                # æœ€çµ‚ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’æ›´æ–°
                if 'nexus_new_group_id' in mapping_data:
                    nexus_true_chain[original_id]['nexus_final_group_id'] = mapping_data['nexus_new_group_id']
                
                # å…ƒã®ã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                if 'nexus_original_group' in mapping_data:
                    nexus_true_chain[original_id]['nexus_original_group'] = mapping_data['nexus_original_group']
        
        return nexus_true_chain
    
    def _select_best_shape_for_data(self, data: bytes) -> str:
        """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãæœ€é©å½¢çŠ¶é¸æŠï¼ˆé«˜é€Ÿç‰ˆï¼‰"""
        if len(data) <= 1000:
            return "I-1"  # å°ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€å°å½¢çŠ¶
        
        # é«˜é€Ÿã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        sample_size = min(len(data), 2000)
        sample_data = data[:sample_size]
        
        entropy = self._calculate_quick_entropy(sample_data)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹å½¢çŠ¶é¸æŠ
        if entropy < 2.0:
            return "O-4"  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼šãƒ–ãƒ­ãƒƒã‚¯å½¢çŠ¶
        elif entropy > 6.0:
            return "I-2"  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼šç·šå½¢å½¢çŠ¶
        else:
            return "I-3"  # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼šãƒãƒ©ãƒ³ã‚¹å½¢çŠ¶
    
    def _calculate_quick_entropy(self, data: bytes) -> float:
        """é«˜é€Ÿã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data) == 0:
            return 0
        counts = collections.Counter(data[:min(len(data), 1000)])
        entropy = 0
        total = sum(counts.values())
        for count in counts.values():
            p_x = count / total
            entropy -= p_x * math.log2(p_x)
        return entropy
    
    def _get_blocks_for_shape(self, data: bytes, grid_width: int, shape_coords: Tuple[Tuple[int, int], ...]) -> List[Tuple[int, ...]]:
        """æŒ‡å®šã•ã‚ŒãŸå½¢çŠ¶ã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²"""
        data_len = len(data)
        rows = data_len // grid_width
        shape_width = max(c for r, c in shape_coords) + 1
        shape_height = max(r for r, c in shape_coords) + 1
        
        blocks = []
        
        for r in range(rows - shape_height + 1):
            for c in range(grid_width - shape_width + 1):
                block = []
                valid_block = True
                
                base_idx = r * grid_width + c
                for dr, dc in shape_coords:
                    idx = base_idx + dr * grid_width + dc
                    if idx >= data_len:
                        valid_block = False
                        break
                    block.append(data[idx])
                
                if valid_block:
                    blocks.append(tuple(block))
        
        return blocks
    
    def compress(self, data: bytes, silent: bool = False) -> bytes:
        """
        ğŸ”¥ NEXUS TRUE THEORY COMPRESSION ğŸ”¥
        
        ç´”ç²‹ãªNEXUSç†è«–ï¼šå®Œå…¨å¯é€†æ€§ä¿è¨¼ã®2å±¤çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
        """
        if not data:
            return data
        
        original_length = len(data)
        
        # é©å¿œçš„ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º
        grid_width = min(math.ceil(math.sqrt(original_length)), 500)
        
        # æœ€é©å½¢çŠ¶é¸æŠ
        best_shape_name = self._select_best_shape_for_data(data)
        shape_coords = POLYOMINO_SHAPES[best_shape_name]
        
        if not silent:
            print(f"   [NEXUS TRUE] Shape: '{best_shape_name}', Grid: {grid_width}")
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæœ€å°é™ï¼‰
        shape_height = max(r for r, c in shape_coords) + 1
        rows_needed = math.ceil(len(data) / grid_width)
        min_padded_size = (rows_needed + shape_height) * grid_width
        
        padded_data = bytearray(data)
        if min_padded_size > len(data):
            padded_data.extend(b'\0' * (min_padded_size - len(data)))
        
        # ãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆ
        blocks = self._get_blocks_for_shape(bytes(padded_data), grid_width, shape_coords)
        
        if not silent:
            print(f"   [NEXUS TRUE] Generated {len(blocks):,} blocks")
        
        # æ­£è¦åŒ–
        normalized_groups = {}
        group_id_counter = 0
        
        for block in blocks:
            normalized = tuple(sorted(block))
            if normalized not in normalized_groups:
                normalized_groups[normalized] = group_id_counter
                group_id_counter += 1
        
        if not silent:
            print(f"   [NEXUS TRUE] Found {group_id_counter:,} unique normalized groups")
        
        # ğŸ”¥ NEXUS TRUE THEORY: 2å±¤çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
        consolidated_groups, consolidation_map = self._consolidate_by_elements_true_theory(normalized_groups, show_progress=not silent)
        
        # ã‚°ãƒ«ãƒ¼ãƒ—IDã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
        group_id_stream = []
        for block in blocks:
            normalized = tuple(sorted(block))
            original_group_id = normalized_groups[normalized]
            
            # çµ±åˆãƒãƒƒãƒ—ã‹ã‚‰æœ€çµ‚ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’å–å¾—
            if original_group_id in consolidation_map:
                final_group_id = consolidation_map[original_group_id]['nexus_final_group_id']
            else:
                final_group_id = original_group_id
            
            group_id_stream.append(final_group_id)
        
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—è¾æ›¸
        unique_groups = [list(g) for g, i in sorted(consolidated_groups.items(), key=lambda item: item[1])]
        
        # Huffmanç¬¦å·åŒ–
        group_huff_tree, encoded_group_ids = self.huffman_encoder.encode(group_id_stream)
        
        # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰æ§‹ç¯‰
        payload = {
            "header": {
                "algorithm": "NEXUS_TRUE_THEORY_v1.0",
                "original_length": original_length,
                "grid_width": grid_width,
                "shape": best_shape_name,
                "consolidation_enabled": True
            },
            "unique_groups": unique_groups,
            "huffman_tree": group_huff_tree,
            "encoded_stream": encoded_group_ids,
            "consolidation_map": consolidation_map
        }
        
        serialized_payload = json.dumps(payload).encode('utf-8')
        compressed_result = lzma.compress(serialized_payload, preset=1)
        
        compression_ratio = len(compressed_result) / len(data)
        size_reduction = (1 - compression_ratio) * 100
        
        if not silent:
            print(f"   [NEXUS TRUE] Compression: {len(data):,} -> {len(compressed_result):,} bytes")
            print(f"   [NEXUS TRUE] Size reduction: {size_reduction:.2f}% (ratio: {compression_ratio:.2%})")
        
        return compressed_result
    
    def decompress(self, compressed_data: bytes, silent: bool = False) -> bytes:
        """
        ğŸ”¥ NEXUS TRUE THEORY DECOMPRESSION ğŸ”¥
        
        å®Œå…¨å¯é€†è§£å‡ï¼š2å±¤çµ±åˆã®å®Œå…¨é€†å¤‰æ›
        """
        if not compressed_data:
            return b''
        
        # LZMAè§£å‡
        decompressed_payload = lzma.decompress(compressed_data)
        payload = json.loads(decompressed_payload.decode('utf-8'))
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
        header = payload['header']
        original_length = header['original_length']
        grid_width = header['grid_width']
        shape_name = header['shape']
        
        if not silent:
            print(f"   [NEXUS TRUE DECOMPRESS] Restoring {original_length} bytes")
        
        # Huffmanè§£å‡
        encoded_stream = payload['encoded_stream']
        huffman_tree = payload['huffman_tree']
        group_id_stream = self.huffman_encoder.decode(encoded_stream, huffman_tree)
        
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—å¾©å…ƒ
        unique_groups = [tuple(g) for g in payload['unique_groups']]
        consolidation_map = payload['consolidation_map']
        
        # ãƒ–ãƒ­ãƒƒã‚¯å†æ§‹æˆ
        reconstructed_blocks = []
        for group_id in group_id_stream:
            try:
                # group_idãŒæ–‡å­—åˆ—ã®å ´åˆã¯æ•´æ•°ã«å¤‰æ›
                if isinstance(group_id, str):
                    group_id = int(group_id)
                
                if group_id < len(unique_groups):
                    reconstructed_blocks.append(list(unique_groups[group_id]))
                else:
                    reconstructed_blocks.append([0])  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            except (ValueError, TypeError):
                reconstructed_blocks.append([0])  # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        # ãƒ‡ãƒ¼ã‚¿å†æ§‹æˆ
        shape_coords = POLYOMINO_SHAPES[shape_name]
        return self._reconstruct_data_from_blocks(reconstructed_blocks, grid_width, original_length, shape_coords, silent)
    
    def _reconstruct_data_from_blocks(self, blocks: List[List[int]], grid_width: int, original_length: int, shape_coords: Tuple[Tuple[int, int], ...], silent: bool = False) -> bytes:
        """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å†æ§‹æˆ"""
        if not blocks:
            return b''
        
        # ã‚°ãƒªãƒƒãƒ‰å†æ§‹æˆ
        shape_width = max(c for r, c in shape_coords) + 1
        shape_height = max(r for r, c in shape_coords) + 1
        
        # æ¨å®šã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚º
        total_positions = len(blocks)
        estimated_rows = int(math.sqrt(total_positions)) + shape_height
        grid_size = estimated_rows * grid_width
        
        # ãƒ‡ãƒ¼ã‚¿é…åˆ—åˆæœŸåŒ–
        reconstructed_data = bytearray(grid_size)
        
        # ãƒ–ãƒ­ãƒƒã‚¯é…ç½®
        block_idx = 0
        for r in range(estimated_rows - shape_height + 1):
            for c in range(grid_width - shape_width + 1):
                if block_idx >= len(blocks):
                    break
                
                block = blocks[block_idx]
                base_idx = r * grid_width + c
                
                for i, (dr, dc) in enumerate(shape_coords):
                    idx = base_idx + dr * grid_width + dc
                    if idx < len(reconstructed_data) and i < len(block):
                        reconstructed_data[idx] = block[i]
                
                block_idx += 1
        
        # å…ƒã®ã‚µã‚¤ã‚ºã«åˆ‡ã‚Šè©°ã‚
        return bytes(reconstructed_data[:original_length])


def test_nexus_true_theory():
    """NEXUS TRUE THEORY ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ”¥ NEXUS TRUE THEORY ENGINE TEST ğŸ”¥")
    print("=" * 50)
    
    engine = NexusTrueTheoryEngine()
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    test_files = [
        "../sample/test_small.txt",
        "../sample/element_test_small.bin",
        "../sample/element_test_medium.bin"
    ]
    
    for test_file in test_files:
        if os.path.exists(test_file):
            print(f"\nğŸ“ Testing: {test_file}")
            
            with open(test_file, 'rb') as f:
                data = f.read()
            
            print(f"   Original size: {len(data)} bytes")
            
            # åœ§ç¸®
            start_time = time.time()
            compressed = engine.compress(data)
            compress_time = time.time() - start_time
            
            # è§£å‡
            start_time = time.time()
            decompressed = engine.decompress(compressed)
            decompress_time = time.time() - start_time
            
            # çµæœ
            compression_ratio = len(compressed) / len(data) * 100
            is_perfect = data == decompressed
            
            print(f"   Compressed size: {len(compressed)} bytes ({compression_ratio:.1f}%)")
            print(f"   Compression time: {compress_time:.3f}s")
            print(f"   Decompression time: {decompress_time:.3f}s")
            print(f"   Perfect recovery: {'âœ“' if is_perfect else 'âœ—'}")
            
            if is_perfect:
                print("   ğŸ‰ NEXUS TRUE THEORY: PERFECT SUCCESS!")
            else:
                print("   âŒ Data corruption detected")
        else:
            print(f"\nâŒ File not found: {test_file}")


if __name__ == "__main__":
    test_nexus_true_theory()

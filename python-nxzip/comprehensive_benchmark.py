#!/usr/bin/env python3
"""
NXZip åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
å‹•ä½œé€Ÿåº¦ã¨åœ§ç¸®ç‡ã®å¾¹åº•æ¤œè¨¼
"""

import os
import sys
import time
import hashlib
import statistics
import gc
from typing import List, Dict, Any, Tuple
import json

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from nxzip_complete import SuperNXZipFile, SPECore, EncryptionAlgorithm, CompressionAlgorithm, KDFAlgorithm

class ComprehensiveBenchmark:
    """åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""
    
    def __init__(self):
        self.results = {
            'system_info': self._get_system_info(),
            'spe_tests': [],
            'compression_tests': [],
            'encryption_tests': [],
            'integration_tests': [],
            'memory_tests': [],
            'stress_tests': []
        }
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        self.test_data = self._prepare_test_data()
        
    def _get_system_info(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
        import platform
        return {
            'platform': platform.platform(),
            'python_version': platform.python_version(),
            'processor': platform.processor(),
            'timestamp': time.time(),
            'test_date': time.strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def _prepare_test_data(self) -> Dict[str, bytes]:
        """å¤šæ§˜ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        print("ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™ä¸­...")
        
        data = {}
        
        # 1. å°å®¹é‡ãƒ‡ãƒ¼ã‚¿ (1KB)
        data['small'] = b"A" * 1024
        
        # 2. ä¸­å®¹é‡ãƒ‡ãƒ¼ã‚¿ (100KB) - ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³
        pattern = b"The quick brown fox jumps over the lazy dog. " * 2048
        data['medium_repetitive'] = pattern[:100 * 1024]
        
        # 3. ä¸­å®¹é‡ãƒ‡ãƒ¼ã‚¿ (100KB) - ãƒ©ãƒ³ãƒ€ãƒ 
        data['medium_random'] = os.urandom(100 * 1024)
        
        # 4. å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ (1MB) - æ··åˆãƒ‘ã‚¿ãƒ¼ãƒ³
        mixed_data = bytearray()
        for i in range(1024):
            if i % 4 == 0:
                mixed_data.extend(b"COMPRESSED" * 100)
            elif i % 4 == 1:
                mixed_data.extend(os.urandom(1000))
            elif i % 4 == 2:
                mixed_data.extend(b"X" * 1000)
            else:
                mixed_data.extend(str(i).encode() * 200)
        data['large_mixed'] = bytes(mixed_data[:1024 * 1024])
        
        # 5. è¶…å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ (10MB) - é«˜åœ§ç¸®ç‡æœŸå¾…
        huge_pattern = b"NXZip" * 200000 + b"Enterprise" * 200000 + b"Security" * 200000
        data['huge_compressible'] = (huge_pattern * 10)[:10 * 1024 * 1024]
        
        # 6. ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ - JSONæ§‹é€ 
        json_data = {
            "records": [
                {
                    "id": i,
                    "name": f"Record_{i}",
                    "data": "x" * 100,
                    "timestamp": time.time() + i,
                    "metadata": {"type": "test", "version": 1.0}
                } for i in range(1000)
            ]
        }
        data['structured_json'] = json.dumps(json_data, indent=2).encode('utf-8')
        
        print(f"âœ… {len(data)} ç¨®é¡ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™å®Œäº†")
        return data
    
    def run_all_tests(self) -> Dict[str, Any]:
        """å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸš€ NXZip åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 60)
        
        # 1. SPE Coreå˜ä½“ãƒ†ã‚¹ãƒˆ
        print("\nğŸ” SPE Core ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ")
        self._test_spe_performance()
        
        # 2. åœ§ç¸®æ€§èƒ½ãƒ†ã‚¹ãƒˆ
        print("\nğŸ—œï¸ åœ§ç¸®æ€§èƒ½ãƒ†ã‚¹ãƒˆ")
        self._test_compression_performance()
        
        # 3. æš—å·åŒ–æ€§èƒ½ãƒ†ã‚¹ãƒˆ
        print("\nğŸ”’ æš—å·åŒ–æ€§èƒ½ãƒ†ã‚¹ãƒˆ")
        self._test_encryption_performance()
        
        # 4. çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
        print("\nâš¡ çµ±åˆã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ãƒ†ã‚¹ãƒˆ")
        self._test_integration_performance()
        
        # 5. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ†ã‚¹ãƒˆ
        print("\nğŸ’¾ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ†ã‚¹ãƒˆ")
        self._test_memory_efficiency()
        
        # 6. ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
        print("\nğŸ”¥ ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ")
        self._test_stress_conditions()
        
        # 7. çµæœã®åˆ†æã¨è¡¨ç¤º
        print("\nğŸ“ˆ ãƒ†ã‚¹ãƒˆçµæœåˆ†æ")
        self._analyze_results()
        
        return self.results
    
    def _test_spe_performance(self):
        """SPE Coreå˜ä½“ã®æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        spe = SPECore()
        
        for name, data in self.test_data.items():
            print(f"  ğŸ“Š ãƒ†ã‚¹ãƒˆ: {name} ({len(data):,} bytes)")
            
            times = []
            for trial in range(5):  # 5å›æ¸¬å®šã—ã¦å¹³å‡ã‚’å–ã‚‹
                gc.collect()  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
                
                # SPEå¤‰æ›ãƒ†ã‚¹ãƒˆ
                start_time = time.perf_counter()
                transformed = spe.apply_transform(data)
                transform_time = time.perf_counter() - start_time
                
                # SPEé€†å¤‰æ›ãƒ†ã‚¹ãƒˆ
                start_time = time.perf_counter()
                restored = spe.reverse_transform(transformed)
                reverse_time = time.perf_counter() - start_time
                
                # æ•´åˆæ€§ç¢ºèª
                if restored != data:
                    raise RuntimeError(f"SPE reversibility failed for {name}")
                
                times.append({
                    'transform_time': transform_time,
                    'reverse_time': reverse_time,
                    'total_time': transform_time + reverse_time,
                    'transformed_size': len(transformed)
                })
            
            # çµ±è¨ˆè¨ˆç®—
            avg_transform = statistics.mean([t['transform_time'] for t in times])
            avg_reverse = statistics.mean([t['reverse_time'] for t in times])
            avg_total = statistics.mean([t['total_time'] for t in times])
            transformed_size = times[0]['transformed_size']
            
            throughput_mb = (len(data) / (1024 * 1024)) / avg_total
            
            result = {
                'data_type': name,
                'original_size': len(data),
                'transformed_size': transformed_size,
                'avg_transform_time': avg_transform,
                'avg_reverse_time': avg_reverse,
                'avg_total_time': avg_total,
                'throughput_mb_s': throughput_mb,
                'size_overhead': (transformed_size - len(data)) / len(data) * 100
            }
            
            self.results['spe_tests'].append(result)
            
            print(f"    â±ï¸  å¤‰æ›: {avg_transform*1000:.2f}ms")
            print(f"    â±ï¸  å¾©å…ƒ: {avg_reverse*1000:.2f}ms") 
            print(f"    ğŸš€ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput_mb:.2f} MB/s")
            print(f"    ğŸ“Š ã‚µã‚¤ã‚ºå¤‰åŒ–: {result['size_overhead']:+.1f}%")
    
    def _test_compression_performance(self):
        """åœ§ç¸®æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        algorithms = [
            CompressionAlgorithm.ZLIB,
            CompressionAlgorithm.LZMA2,
            CompressionAlgorithm.ZSTD,
            CompressionAlgorithm.AUTO
        ]
        
        for algo in algorithms:
            print(f"  ğŸ—œï¸ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {algo}")
            
            for name, data in self.test_data.items():
                if len(data) > 5 * 1024 * 1024 and algo == CompressionAlgorithm.LZMA2:
                    continue  # LZMA2ã¯å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã§æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
                
                nxzip = SuperNXZipFile(compression_algo=algo)
                
                times = []
                for trial in range(3):
                    gc.collect()
                    
                    # åœ§ç¸®ãƒ†ã‚¹ãƒˆ
                    start_time = time.perf_counter()
                    compressed = nxzip.compressor.compress(data, show_progress=False)
                    compress_time = time.perf_counter() - start_time
                    
                    # å±•é–‹ãƒ†ã‚¹ãƒˆ
                    start_time = time.perf_counter()
                    decompressed = nxzip.compressor.decompress(compressed[0], compressed[1], show_progress=False)
                    decompress_time = time.perf_counter() - start_time
                    
                    # æ•´åˆæ€§ç¢ºèª
                    if decompressed != data:
                        raise RuntimeError(f"Compression reversibility failed for {name} with {algo}")
                    
                    times.append({
                        'compress_time': compress_time,
                        'decompress_time': decompress_time,
                        'compressed_size': len(compressed[0])
                    })
                
                # çµ±è¨ˆè¨ˆç®—
                avg_compress = statistics.mean([t['compress_time'] for t in times])
                avg_decompress = statistics.mean([t['decompress_time'] for t in times])
                compressed_size = times[0]['compressed_size']
                
                compression_ratio = (1 - compressed_size / len(data)) * 100
                compress_throughput = (len(data) / (1024 * 1024)) / avg_compress
                decompress_throughput = (len(data) / (1024 * 1024)) / avg_decompress
                
                result = {
                    'algorithm': algo,
                    'data_type': name,
                    'original_size': len(data),
                    'compressed_size': compressed_size,
                    'compression_ratio': compression_ratio,
                    'avg_compress_time': avg_compress,
                    'avg_decompress_time': avg_decompress,
                    'compress_throughput_mb_s': compress_throughput,
                    'decompress_throughput_mb_s': decompress_throughput
                }
                
                self.results['compression_tests'].append(result)
                
                print(f"    ğŸ“Š {name}: {compression_ratio:.1f}% | "
                      f"åœ§ç¸®: {compress_throughput:.1f} MB/s | "
                      f"å±•é–‹: {decompress_throughput:.1f} MB/s")
    
    def _test_encryption_performance(self):
        """æš—å·åŒ–æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        algorithms = [
            EncryptionAlgorithm.AES_GCM,
            EncryptionAlgorithm.XCHACHA20_POLY1305
        ]
        
        kdf_algorithms = [
            KDFAlgorithm.PBKDF2,
            KDFAlgorithm.SCRYPT
        ]
        
        password = "TestPassword123"
        
        for enc_algo in algorithms:
            for kdf_algo in kdf_algorithms:
                print(f"  ğŸ”’ {enc_algo} + {kdf_algo}")
                
                from nxzip_complete import SuperCrypto
                crypto = SuperCrypto(enc_algo, kdf_algo)
                
                for name, data in self.test_data.items():
                    if len(data) > 1024 * 1024:  # æš—å·åŒ–ãƒ†ã‚¹ãƒˆã¯1MBä»¥ä¸‹ã«åˆ¶é™
                        continue
                    
                    times = []
                    for trial in range(3):
                        gc.collect()
                        
                        # æš—å·åŒ–ãƒ†ã‚¹ãƒˆ
                        start_time = time.perf_counter()
                        encrypted_data, metadata = crypto.encrypt(data, password, show_progress=False)
                        encrypt_time = time.perf_counter() - start_time
                        
                        # å¾©å·åŒ–ãƒ†ã‚¹ãƒˆ
                        start_time = time.perf_counter()
                        decrypted = crypto.decrypt(encrypted_data, metadata, password, show_progress=False)
                        decrypt_time = time.perf_counter() - start_time
                        
                        # æ•´åˆæ€§ç¢ºèª
                        if decrypted != data:
                            raise RuntimeError(f"Encryption reversibility failed for {name}")
                        
                        times.append({
                            'encrypt_time': encrypt_time,
                            'decrypt_time': decrypt_time,
                            'encrypted_size': len(encrypted_data)
                        })
                    
                    # çµ±è¨ˆè¨ˆç®—
                    avg_encrypt = statistics.mean([t['encrypt_time'] for t in times])
                    avg_decrypt = statistics.mean([t['decrypt_time'] for t in times])
                    encrypted_size = times[0]['encrypted_size']
                    
                    encrypt_throughput = (len(data) / (1024 * 1024)) / avg_encrypt
                    decrypt_throughput = (len(data) / (1024 * 1024)) / avg_decrypt
                    size_overhead = (encrypted_size - len(data)) / len(data) * 100
                    
                    result = {
                        'encryption_algorithm': enc_algo,
                        'kdf_algorithm': kdf_algo,
                        'data_type': name,
                        'original_size': len(data),
                        'encrypted_size': encrypted_size,
                        'size_overhead': size_overhead,
                        'avg_encrypt_time': avg_encrypt,
                        'avg_decrypt_time': avg_decrypt,
                        'encrypt_throughput_mb_s': encrypt_throughput,
                        'decrypt_throughput_mb_s': decrypt_throughput
                    }
                    
                    self.results['encryption_tests'].append(result)
                    
                    print(f"    ğŸ“Š {name}: "
                          f"æš—å·åŒ–: {encrypt_throughput:.1f} MB/s | "
                          f"å¾©å·åŒ–: {decrypt_throughput:.1f} MB/s | "
                          f"ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: +{size_overhead:.1f}%")
    
    def _test_integration_performance(self):
        """çµ±åˆã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        configurations = [
            {'compression': CompressionAlgorithm.AUTO, 'encryption': EncryptionAlgorithm.AES_GCM, 'kdf': KDFAlgorithm.PBKDF2},
            {'compression': CompressionAlgorithm.ZSTD, 'encryption': EncryptionAlgorithm.XCHACHA20_POLY1305, 'kdf': KDFAlgorithm.SCRYPT},
            {'compression': CompressionAlgorithm.LZMA2, 'encryption': EncryptionAlgorithm.AES_GCM, 'kdf': KDFAlgorithm.PBKDF2}
        ]
        
        password = "IntegrationTest123"
        
        for config in configurations:
            print(f"  âš¡ æ§‹æˆ: {config['compression']} + {config['encryption']} + {config['kdf']}")
            
            nxzip = SuperNXZipFile(
                compression_algo=config['compression'],
                encryption_algo=config['encryption'],
                kdf_algo=config['kdf']
            )
            
            for name, data in self.test_data.items():
                if len(data) > 2 * 1024 * 1024:  # çµ±åˆãƒ†ã‚¹ãƒˆã¯2MBä»¥ä¸‹ã«åˆ¶é™
                    continue
                
                times = []
                for trial in range(3):
                    gc.collect()
                    
                    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆãƒ†ã‚¹ãƒˆ
                    start_time = time.perf_counter()
                    archive_data = nxzip.create_archive(data, password, show_progress=False)
                    create_time = time.perf_counter() - start_time
                    
                    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±•é–‹ãƒ†ã‚¹ãƒˆ
                    start_time = time.perf_counter()
                    extracted_data = nxzip.extract_archive(archive_data, password, show_progress=False)
                    extract_time = time.perf_counter() - start_time
                    
                    # æ•´åˆæ€§ç¢ºèª
                    if extracted_data != data:
                        raise RuntimeError(f"Integration test failed for {name}")
                    
                    times.append({
                        'create_time': create_time,
                        'extract_time': extract_time,
                        'archive_size': len(archive_data)
                    })
                
                # çµ±è¨ˆè¨ˆç®—
                avg_create = statistics.mean([t['create_time'] for t in times])
                avg_extract = statistics.mean([t['extract_time'] for t in times])
                archive_size = times[0]['archive_size']
                
                total_compression = (1 - archive_size / len(data)) * 100
                create_throughput = (len(data) / (1024 * 1024)) / avg_create
                extract_throughput = (len(data) / (1024 * 1024)) / avg_extract
                
                result = {
                    'configuration': config,
                    'data_type': name,
                    'original_size': len(data),
                    'archive_size': archive_size,
                    'total_compression_ratio': total_compression,
                    'avg_create_time': avg_create,
                    'avg_extract_time': avg_extract,
                    'create_throughput_mb_s': create_throughput,
                    'extract_throughput_mb_s': extract_throughput
                }
                
                self.results['integration_tests'].append(result)
                
                print(f"    ğŸ“Š {name}: {total_compression:.1f}% | "
                      f"ä½œæˆ: {create_throughput:.1f} MB/s | "
                      f"å±•é–‹: {extract_throughput:.1f} MB/s")
    
    def _test_memory_efficiency(self):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ†ã‚¹ãƒˆï¼ˆpsutilç„¡ã—ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰"""
        print("  ğŸ’¾ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ†ã‚¹ãƒˆ (ç°¡æ˜“ç‰ˆ)")
        
        nxzip = SuperNXZipFile()
        
        for name, data in self.test_data.items():
            if len(data) > 1024 * 1024:  # ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆã¯1MBä»¥ä¸‹ã«åˆ¶é™
                continue
            
            try:
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ä»£ã‚ã‚Šã«å‡¦ç†æ™‚é–“ã‚’æ¸¬å®š
                start_time = time.perf_counter()
                
                # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆ
                archive_data = nxzip.create_archive(data, show_progress=False)
                create_time = time.perf_counter() - start_time
                
                start_time = time.perf_counter()
                # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å±•é–‹
                extracted_data = nxzip.extract_archive(archive_data, show_progress=False)
                extract_time = time.perf_counter() - start_time
                
                # æ•´åˆæ€§ç¢ºèª
                if extracted_data != data:
                    raise RuntimeError(f"Memory test integrity failed for {name}")
                
                # å‡¦ç†åŠ¹ç‡ã®è¨ˆç®—
                total_time = create_time + extract_time
                efficiency_ratio = (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0
                
                result = {
                    'data_type': name,
                    'data_size_mb': len(data) / 1024 / 1024,
                    'total_processing_time': total_time,
                    'processing_efficiency_mb_s': efficiency_ratio,
                    'archive_compression_ratio': (1 - len(archive_data) / len(data)) * 100
                }
                
                self.results['memory_tests'].append(result)
                
                print(f"    ğŸ“Š {name}: "
                      f"å‡¦ç†æ™‚é–“: {total_time:.3f}s | "
                      f"åŠ¹ç‡: {efficiency_ratio:.2f} MB/s | "
                      f"åœ§ç¸®ç‡: {result['archive_compression_ratio']:.1f}%")
                      
            except Exception as e:
                print(f"    âŒ {name}: ã‚¨ãƒ©ãƒ¼ - {e}")
                continue
    
    def _test_stress_conditions(self):
        """ã‚¹ãƒˆãƒ¬ã‚¹æ¡ä»¶ãƒ†ã‚¹ãƒˆ"""
        print("  ğŸ”¥ å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ")
        
        # éå¸¸ã«å¤§ããªãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†æƒ³å®šï¼‰
        stress_data = b"STRESS_TEST_DATA" * 1024 * 1024  # 16MB
        
        nxzip = SuperNXZipFile(compression_algo=CompressionAlgorithm.ZSTD)
        
        try:
            start_time = time.perf_counter()
            archive_data = nxzip.create_archive(stress_data, show_progress=False)
            create_time = time.perf_counter() - start_time
            
            start_time = time.perf_counter()
            extracted_data = nxzip.extract_archive(archive_data, show_progress=False)
            extract_time = time.perf_counter() - start_time
            
            # æ•´åˆæ€§ç¢ºèª
            if extracted_data != stress_data:
                raise RuntimeError("Stress test integrity failed")
            
            compression_ratio = (1 - len(archive_data) / len(stress_data)) * 100
            create_throughput = (len(stress_data) / (1024 * 1024)) / create_time
            extract_throughput = (len(stress_data) / (1024 * 1024)) / extract_time
            
            result = {
                'test_type': 'large_data_stress',
                'data_size_mb': len(stress_data) / 1024 / 1024,
                'compression_ratio': compression_ratio,
                'create_time': create_time,
                'extract_time': extract_time,
                'create_throughput_mb_s': create_throughput,
                'extract_throughput_mb_s': extract_throughput,
                'success': True
            }
            
            print(f"    âœ… 16MBå‡¦ç†æˆåŠŸ: {compression_ratio:.1f}% | "
                  f"ä½œæˆ: {create_throughput:.1f} MB/s | "
                  f"å±•é–‹: {extract_throughput:.1f} MB/s")
            
        except Exception as e:
            result = {
                'test_type': 'large_data_stress',
                'error': str(e),
                'success': False
            }
            print(f"    âŒ 16MBå‡¦ç†å¤±æ•—: {e}")
        
        self.results['stress_tests'].append(result)
    
    def _analyze_results(self):
        """çµæœã®åˆ†æã¨è¡¨ç¤º"""
        print("\nğŸ“ˆ ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ")
        print("=" * 60)
        
        # SPEæ€§èƒ½ã‚µãƒãƒªãƒ¼
        if self.results['spe_tests']:
            spe_throughputs = [t['throughput_mb_s'] for t in self.results['spe_tests']]
            print(f"ğŸ” SPEå¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {statistics.mean(spe_throughputs):.2f} MB/s")
            print(f"   æœ€é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {max(spe_throughputs):.2f} MB/s")
            print(f"   æœ€ä½ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {min(spe_throughputs):.2f} MB/s")
        
        # åœ§ç¸®æ€§èƒ½ã‚µãƒãƒªãƒ¼
        if self.results['compression_tests']:
            by_algorithm = {}
            for test in self.results['compression_tests']:
                algo = test['algorithm']
                if algo not in by_algorithm:
                    by_algorithm[algo] = {'ratios': [], 'throughputs': []}
                by_algorithm[algo]['ratios'].append(test['compression_ratio'])
                by_algorithm[algo]['throughputs'].append(test['compress_throughput_mb_s'])
            
            print(f"\nğŸ—œï¸ åœ§ç¸®æ€§èƒ½ã‚µãƒãƒªãƒ¼:")
            for algo, data in by_algorithm.items():
                avg_ratio = statistics.mean(data['ratios'])
                avg_throughput = statistics.mean(data['throughputs'])
                print(f"   {algo}: {avg_ratio:.1f}% | {avg_throughput:.1f} MB/s")
        
        # çµ±åˆã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ã‚µãƒãƒªãƒ¼
        if self.results['integration_tests']:
            integration_throughputs = [t['create_throughput_mb_s'] for t in self.results['integration_tests']]
            integration_ratios = [t['total_compression_ratio'] for t in self.results['integration_tests']]
            print(f"\nâš¡ çµ±åˆã‚·ã‚¹ãƒ†ãƒ :")
            print(f"   å¹³å‡ç·åˆåœ§ç¸®ç‡: {statistics.mean(integration_ratios):.1f}%")
            print(f"   å¹³å‡å‡¦ç†é€Ÿåº¦: {statistics.mean(integration_throughputs):.1f} MB/s")
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚µãƒãƒªãƒ¼
        if self.results['memory_tests']:
            memory_efficiencies = [t.get('processing_efficiency_mb_s', 0) for t in self.results['memory_tests'] if t.get('processing_efficiency_mb_s', 0) > 0]
            if memory_efficiencies:
                print(f"\nğŸ’¾ å‡¦ç†åŠ¹ç‡: {statistics.mean(memory_efficiencies):.2f} MB/s (ãƒ‡ãƒ¼ã‚¿å‡¦ç†é€Ÿåº¦)")
        
        print(f"\nâœ… å…¨ãƒ†ã‚¹ãƒˆå®Œäº† - çµæœã¯self.resultsã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")
    
    def save_results(self, filename: str = None):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if filename is None:
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            filename = f'nxzip_benchmark_results_{timestamp}.json'
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ä¿å­˜: {filename}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ NXZip åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    benchmark = ComprehensiveBenchmark()
    
    try:
        results = benchmark.run_all_tests()
        benchmark.save_results()
        
        print("\nğŸ‰ å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return 1
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())

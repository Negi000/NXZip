#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  NEXUS Adaptive Entropy Engine
é©å¿œå‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ã«ã‚ˆã‚‹æ¬¡ä¸–ä»£åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³

ç‰¹å¾´:
- é©å¿œå‹Huffmanç¬¦å·åŒ–
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ç¢ºç‡ãƒ¢ãƒ‡ãƒ«  
- ç©ºé–“çš„ãƒ»æ™‚é–“çš„ç›¸é–¢æ´»ç”¨
- ç”»åƒãƒ»å‹•ç”»ç‰¹åŒ–æœ€é©åŒ–
"""

import os
import sys
import time
import hashlib
import struct
import heapq
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Optional
import numpy as np

class AdaptiveHuffmanNode:
    """é©å¿œå‹Huffmanãƒ„ãƒªãƒ¼ã®ãƒãƒ¼ãƒ‰"""
    def __init__(self, symbol=None, freq=0):
        self.symbol = symbol
        self.freq = freq
        self.left = None
        self.right = None
    
    def __lt__(self, other):
        return self.freq < other.freq

class AdaptiveEntropyEngine:
    """é©å¿œå‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.symbol_counts = defaultdict(int)
        self.total_symbols = 0
        self.huffman_codes = {}
        self.context_models = {}  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
        self.adaptation_rate = 0.1  # é©å¿œé€Ÿåº¦
        
    def compress_file(self, input_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
        if not os.path.exists(input_path):
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
            return None
        
        with open(input_path, 'rb') as f:
            data = f.read()
        
        file_size = len(data)
        file_ext = os.path.splitext(input_path)[1].lower()
        
        print(f"ğŸ“ å‡¦ç†: {os.path.basename(input_path)} ({file_size:,} bytes, {file_ext.upper()})")
        
        start_time = time.time()
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¥æœ€é©åŒ–
        if file_ext in ['.png', '.jpg', '.jpeg', '.bmp']:
            compressed_data = self.compress_image(data, file_ext)
        elif file_ext in ['.mp4', '.avi', '.mov', '.mkv']:
            compressed_data = self.compress_video(data, file_ext)
        else:
            compressed_data = self.compress_generic(data, file_ext)
        
        # åœ§ç¸®çµæœã®è¨ˆç®—
        compressed_size = len(compressed_data)
        compression_ratio = (1 - compressed_size / file_size) * 100
        processing_time = time.time() - start_time
        speed = file_size / (1024 * 1024) / processing_time
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        output_path = input_path + '.nxae'  # NXZip Adaptive Entropy
        with open(output_path, 'wb') as f:
            f.write(compressed_data)
        
        # çµæœè¡¨ç¤º
        if compression_ratio > 0:
            print(f"ğŸ† åœ§ç¸®å®Œäº†: {compression_ratio:.1f}%")
        else:
            print(f"âŒ åœ§ç¸®å®Œäº†: {compression_ratio:.1f}%")
        
        print(f"âš¡ å‡¦ç†æ™‚é–“: {processing_time:.2f}s ({speed:.1f} MB/s)")
        print(f"ğŸ’¾ ä¿å­˜: {os.path.basename(output_path)}")
        
        return output_path
    
    def compress_image(self, data: bytes, format_type: str) -> bytes:
        """ç”»åƒç‰¹åŒ–åœ§ç¸®ï¼ˆéåœ§ç¸®ãƒ‡ã‚³ãƒ¼ãƒ‰â†’é©å¿œå‹ç¬¦å·åŒ–ï¼‰"""
        print("ğŸ¨ é©å¿œå‹ç”»åƒåœ§ç¸®é–‹å§‹...")
        
        try:
            # Phase 1: æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç”Ÿãƒ”ã‚¯ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ãƒ‡ã‚³ãƒ¼ãƒ‰
            raw_pixels = self._decode_to_raw_pixels(data, format_type)
            print(f"   ğŸ“¸ ç”Ÿãƒ”ã‚¯ã‚»ãƒ«å±•é–‹å®Œäº†: {len(raw_pixels):,} bytes")
            
            # Phase 2: ç©ºé–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
            spatial_contexts = self._analyze_spatial_context_raw(raw_pixels)
            print(f"   ğŸ“Š ç©ºé–“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æå®Œäº†: {len(spatial_contexts)} patterns")
            
            # Phase 3: é©å¿œå‹Huffmanç¬¦å·åŒ–ï¼ˆå¯é€†ä¿è¨¼ï¼‰
            huffman_compressed = self._adaptive_huffman_encode_reversible(raw_pixels, spatial_contexts)
            print("   ğŸ”¤ å¯é€†é©å¿œå‹Huffmanç¬¦å·åŒ–å®Œäº†")
            
            # Phase 4: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼‰
            final_compressed = self._entropy_integration_with_metadata(huffman_compressed, format_type, data, raw_pixels)
            print("   âœ… ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆå®Œäº†")
            
            return final_compressed
            
        except Exception as e:
            print(f"   âš ï¸ ç”»åƒãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
            return self.compress_generic(data, format_type)
    
    def compress_video(self, data: bytes, format_type: str) -> bytes:
        """å‹•ç”»ç‰¹åŒ–åœ§ç¸®"""
        print("ğŸ¬ é©å¿œå‹å‹•ç”»åœ§ç¸®é–‹å§‹...")
        
        # Phase 1: æ™‚é–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
        temporal_contexts = self._analyze_temporal_context(data)
        print(f"   â±ï¸ æ™‚é–“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æå®Œäº†: {len(temporal_contexts)} patterns")
        
        # Phase 2: ãƒ•ãƒ¬ãƒ¼ãƒ å·®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        frame_diff_compressed = self._frame_differential_entropy(data, temporal_contexts)
        print("   ğŸï¸ ãƒ•ãƒ¬ãƒ¼ãƒ å·®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å®Œäº†")
        
        # Phase 3: å‹•ããƒ™ã‚¯ãƒˆãƒ«é©å¿œç¬¦å·åŒ–
        motion_optimized = self._adaptive_motion_coding(frame_diff_compressed)
        print("   ğŸƒ å‹•ããƒ™ã‚¯ãƒˆãƒ«é©å¿œç¬¦å·åŒ–å®Œäº†")
        
        # Phase 4: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆ
        final_compressed = self._entropy_integration(motion_optimized, format_type, data)
        print("   âœ… ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆå®Œäº†")
        
        return final_compressed
    
    def compress_generic(self, data: bytes, format_type: str) -> bytes:
        """æ±ç”¨åœ§ç¸®"""
        print("ğŸ“„ é©å¿œå‹æ±ç”¨åœ§ç¸®é–‹å§‹...")
        
        # Phase 1: ãƒã‚¤ãƒˆé »åº¦åˆ†æ
        byte_analysis = self._analyze_byte_frequency(data)
        print(f"   ğŸ“ˆ ãƒã‚¤ãƒˆé »åº¦åˆ†æå®Œäº†: {len(byte_analysis)} unique bytes")
        
        # Phase 2: é©å¿œå‹ç¬¦å·åŒ–
        adaptive_encoded = self._adaptive_generic_encode(data, byte_analysis)
        print("   ğŸ”¤ é©å¿œå‹ç¬¦å·åŒ–å®Œäº†")
        
        # Phase 3: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆ
        final_compressed = self._entropy_integration(adaptive_encoded, format_type, data)
        print("   âœ… ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆå®Œäº†")
        
        return final_compressed
    
    def _analyze_spatial_context(self, data: bytes) -> Dict:
        """ç©ºé–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆç”»åƒç”¨ï¼‰"""
        # éš£æ¥ãƒ”ã‚¯ã‚»ãƒ«ã®ç›¸é–¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        contexts = defaultdict(Counter)
        
        # 4x4ãƒ”ã‚¯ã‚»ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã§ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        for i in range(0, len(data) - 16, 16):
            block = data[i:i+16]
            # ãƒ–ãƒ­ãƒƒã‚¯å†…ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†
            avg_value = sum(block) // len(block)
            variance = sum((b - avg_value) ** 2 for b in block) // len(block)
            
            context_key = (avg_value // 32, variance // 64)  # é‡å­åŒ–
            contexts[context_key].update(block)
        
        return dict(contexts)
    
    def _analyze_temporal_context(self, data: bytes) -> Dict:
        """æ™‚é–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼ˆå‹•ç”»ç”¨ï¼‰"""
        # ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®ç›¸é–¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
        contexts = defaultdict(Counter)
        
        # ä»®æƒ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚ºã‚’æ¨å®šï¼ˆç°¡ç•¥åŒ–ï¼‰
        frame_size = min(1024, len(data) // 10)  # é©å½“ãªä»®å®š
        
        for i in range(0, len(data) - frame_size, frame_size):
            frame1 = data[i:i+frame_size]
            frame2 = data[i+frame_size:i+2*frame_size] if i+2*frame_size <= len(data) else b''
            
            if frame2:
                # ãƒ•ãƒ¬ãƒ¼ãƒ é–“å·®åˆ†ã‚’è¨ˆç®—
                diff_sum = sum(abs(a - b) for a, b in zip(frame1, frame2))
                motion_level = diff_sum // len(frame1)
                
                context_key = motion_level // 16  # é‡å­åŒ–
                contexts[context_key].update(frame1)
        
        return dict(contexts)
    
    def _analyze_byte_frequency(self, data: bytes) -> Dict:
        """ãƒã‚¤ãƒˆé »åº¦åˆ†æï¼ˆæ±ç”¨ï¼‰"""
        return Counter(data)
    
    def _adaptive_huffman_encode(self, data: bytes, contexts: Dict) -> bytes:
        """é©å¿œå‹Huffmanç¬¦å·åŒ–"""
        # åˆæœŸç¢ºç‡ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        all_symbols = set(data)
        
        # å„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®Huffmanç¬¦å·åŒ–
        encoded_parts = []
        
        for context_key, symbol_counts in contexts.items():
            # Huffmanãƒ„ãƒªãƒ¼æ§‹ç¯‰
            codes = self._build_huffman_codes(symbol_counts)
            
            # è©²å½“éƒ¨åˆ†ã‚’ç¬¦å·åŒ–ï¼ˆç°¡ç•¥åŒ–å®Ÿè£…ï¼‰
            context_data = bytes(symbol_counts.elements())
            encoded_part = self._encode_with_codes(context_data, codes)
            encoded_parts.append(encoded_part)
        
        # æ®‹ã‚Šã®éƒ¨åˆ†ã‚’é€šå¸¸ã®Huffmanç¬¦å·åŒ–
        if encoded_parts:
            return b''.join(encoded_parts)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ä½“ã‚’Huffmanç¬¦å·åŒ–
            symbol_counts = Counter(data)
            codes = self._build_huffman_codes(symbol_counts)
            return self._encode_with_codes(data, codes)
    
    def _build_huffman_codes(self, symbol_counts: Counter) -> Dict:
        """Huffmanç¬¦å·æ§‹ç¯‰"""
        if len(symbol_counts) <= 1:
            # ç‰¹æ®Šã‚±ãƒ¼ã‚¹: ã‚·ãƒ³ãƒœãƒ«ãŒ1ã¤ä»¥ä¸‹
            return {list(symbol_counts.keys())[0]: '0'} if symbol_counts else {}
        
        # å„ªå…ˆåº¦ä»˜ãã‚­ãƒ¥ãƒ¼ã§Huffmanãƒ„ãƒªãƒ¼æ§‹ç¯‰
        heap = [AdaptiveHuffmanNode(symbol, count) for symbol, count in symbol_counts.items()]
        heapq.heapify(heap)
        
        while len(heap) > 1:
            left = heapq.heappop(heap)
            right = heapq.heappop(heap)
            
            merged = AdaptiveHuffmanNode(freq=left.freq + right.freq)
            merged.left = left
            merged.right = right
            
            heapq.heappush(heap, merged)
        
        # ãƒ„ãƒªãƒ¼ã‹ã‚‰ç¬¦å·ã‚’æŠ½å‡º
        root = heap[0]
        codes = {}
        
        def extract_codes(node, code=''):
            if node.symbol is not None:
                codes[node.symbol] = code or '0'
            else:
                if node.left:
                    extract_codes(node.left, code + '0')
                if node.right:
                    extract_codes(node.right, code + '1')
        
        extract_codes(root)
        return codes
    
    def _encode_with_codes(self, data: bytes, codes: Dict) -> bytes:
        """ç¬¦å·ã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        bit_string = ''.join(codes.get(byte, '0') for byte in data)
        
        # ãƒ“ãƒƒãƒˆæ–‡å­—åˆ—ã‚’ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ 
        while len(bit_string) % 8 != 0:
            bit_string += '0'
        
        result = bytearray()
        for i in range(0, len(bit_string), 8):
            byte_value = int(bit_string[i:i+8], 2)
            result.append(byte_value)
        
        return bytes(result)
    
    def _decode_to_raw_pixels(self, data: bytes, format_type: str) -> bytes:
        """æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç”Ÿãƒ”ã‚¯ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        try:
            if format_type.lower() in ['.png', '.jpg', '.jpeg', '.bmp']:
                # PILã‚’ä½¿ã£ã¦ç”»åƒã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                from PIL import Image
                import io
                
                # ãƒã‚¤ãƒˆåˆ—ã‹ã‚‰ç”»åƒã‚’é–‹ã
                image = Image.open(io.BytesIO(data))
                
                # RGBAã«å¤‰æ›ã—ã¦çµ±ä¸€
                if image.mode != 'RGBA':
                    image = image.convert('RGBA')
                
                # ç”Ÿãƒ”ã‚¯ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                raw_pixels = image.tobytes()
                
                # ç”»åƒæƒ…å ±ã‚’ä¿å­˜ï¼ˆå¾©å…ƒç”¨ï¼‰
                self.image_metadata = {
                    'width': image.width,
                    'height': image.height,
                    'mode': 'RGBA',
                    'original_mode': image.mode
                }
                
                return raw_pixels
            else:
                # éå¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                return data
                
        except ImportError:
            print("   âš ï¸ PILæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return data
        except Exception as e:
            print(f"   âš ï¸ ç”»åƒãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {e}")
            return data
    
    def _analyze_spatial_context_raw(self, raw_pixels: bytes) -> Dict:
        """ç”Ÿãƒ”ã‚¯ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç©ºé–“çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ"""
        contexts = defaultdict(Counter)
        
        # RGBAãƒ”ã‚¯ã‚»ãƒ«ï¼ˆ4ãƒã‚¤ãƒˆå˜ä½ï¼‰ã§åˆ†æ
        pixel_size = 4  # RGBA
        for i in range(0, len(raw_pixels) - pixel_size * 16, pixel_size * 16):
            # 4x4ãƒ”ã‚¯ã‚»ãƒ«ãƒ–ãƒ­ãƒƒã‚¯
            block_pixels = []
            for j in range(16):  # 4x4 = 16ãƒ”ã‚¯ã‚»ãƒ«
                pixel_start = i + j * pixel_size
                if pixel_start + pixel_size <= len(raw_pixels):
                    r, g, b, a = raw_pixels[pixel_start:pixel_start + pixel_size]
                    block_pixels.append((r, g, b, a))
            
            if len(block_pixels) == 16:
                # ãƒ–ãƒ­ãƒƒã‚¯å†…ã®å¹³å‡è‰²ã‚’è¨ˆç®—
                avg_r = sum(p[0] for p in block_pixels) // 16
                avg_g = sum(p[1] for p in block_pixels) // 16
                avg_b = sum(p[2] for p in block_pixels) // 16
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ¼ã¨ã—ã¦é‡å­åŒ–ã•ã‚ŒãŸå¹³å‡è‰²ã‚’ä½¿ç”¨
                context_key = (avg_r // 32, avg_g // 32, avg_b // 32)
                
                # ã“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®ãƒ”ã‚¯ã‚»ãƒ«å€¤ã‚’è¨˜éŒ²
                for pixel in block_pixels:
                    contexts[context_key].update(pixel)
        
        return dict(contexts)
    
    def _adaptive_huffman_encode_reversible(self, raw_pixels: bytes, contexts: Dict) -> bytes:
        """å¯é€†æ€§ä¿è¨¼ã®é©å¿œå‹Huffmanç¬¦å·åŒ–"""
        # å…¨ä½“çš„ãªé »åº¦åˆ†æ
        symbol_counts = Counter(raw_pixels)
        
        # Huffmanç¬¦å·æ§‹ç¯‰
        codes = self._build_huffman_codes(symbol_counts)
        
        # ç¬¦å·ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿å­˜ï¼ˆå¾©å…ƒç”¨ï¼‰
        self.huffman_codes = codes
        
        # å¯é€†ç¬¦å·åŒ–å®Ÿè¡Œ
        encoded_data = self._encode_with_codes_reversible(raw_pixels, codes)
        
        return encoded_data
    
    def _encode_with_codes_reversible(self, data: bytes, codes: Dict) -> bytes:
        """å¯é€†æ€§ä¿è¨¼ã®ç¬¦å·åŒ–"""
        # ç¬¦å·ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
        import pickle
        codes_data = pickle.dumps(codes)
        codes_size = struct.pack('>I', len(codes_data))
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ç¬¦å·åŒ–
        bit_string = ''.join(codes.get(byte, '0') for byte in data)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±ã‚’è¨˜éŒ²
        padding = (8 - len(bit_string) % 8) % 8
        bit_string += '0' * padding
        
        # ãƒ“ãƒƒãƒˆæ–‡å­—åˆ—ã‚’ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›
        result = bytearray()
        for i in range(0, len(bit_string), 8):
            byte_value = int(bit_string[i:i+8], 2)
            result.append(byte_value)
        
        # æ§‹é€ : [ç¬¦å·ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚º][ç¬¦å·ãƒ†ãƒ¼ãƒ–ãƒ«][ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±][ç¬¦å·åŒ–ãƒ‡ãƒ¼ã‚¿]
        return codes_size + codes_data + struct.pack('B', padding) + bytes(result)
    
    def _entropy_integration_with_metadata(self, data: bytes, format_type: str, original_data: bytes, raw_pixels: bytes) -> bytes:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆ"""
        header = f'NXAE_{format_type}_V2'.encode('ascii')
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥
        hasher = hashlib.md5()
        hasher.update(original_data)
        hash_digest = hasher.digest()
        
        # ç”»åƒãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
        import pickle
        metadata = pickle.dumps(getattr(self, 'image_metadata', {}))
        metadata_size = struct.pack('>I', len(metadata))
        
        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºæƒ…å ±
        raw_size = struct.pack('>I', len(raw_pixels))
        original_size = struct.pack('>I', len(original_data))
        
        # æœ€çµ‚åœ§ç¸®ï¼ˆè»½é‡LZMAï¼‰
        import lzma
        final_compressed = lzma.compress(data, preset=3)  # é«˜é€Ÿãƒ»å¯é€†é‡è¦–
        
        return header + hash_digest + metadata_size + metadata + raw_size + original_size + final_compressed
    
    def _frame_differential_entropy(self, data: bytes, contexts: Dict) -> bytes:
        """ãƒ•ãƒ¬ãƒ¼ãƒ å·®åˆ†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼"""
        # ç°¡ç•¥åŒ–: ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™
        return data
    
    def _adaptive_motion_coding(self, data: bytes) -> bytes:
        """å‹•ããƒ™ã‚¯ãƒˆãƒ«é©å¿œç¬¦å·åŒ–"""
        # ç°¡ç•¥åŒ–: ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™
        return data
    
    def _adaptive_generic_encode(self, data: bytes, analysis: Dict) -> bytes:
        """é©å¿œå‹æ±ç”¨ç¬¦å·åŒ–"""
        # é »åº¦ãƒ™ãƒ¼ã‚¹ã®Huffmanç¬¦å·åŒ–
        codes = self._build_huffman_codes(analysis)
        return self._encode_with_codes(data, codes)
    
    def _entropy_integration(self, data: bytes, format_type: str, original_data: bytes) -> bytes:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆ"""
        header = f'NXAE_{format_type}_V1'.encode('ascii')
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥
        hasher = hashlib.md5()
        hasher.update(original_data)
        hash_digest = hasher.digest()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = struct.pack('>I', len(original_data))  # å…ƒã‚µã‚¤ã‚º
        
        # æœ€çµ‚åœ§ç¸®ï¼ˆLZMAé©ç”¨ï¼‰
        import lzma
        final_compressed = lzma.compress(data, preset=6)
        
        return header + hash_digest + metadata + final_compressed

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨"""
    if len(sys.argv) < 2:
        print("ğŸ§  NEXUS Adaptive Entropy Engine")
        print("ä½¿ç”¨æ³•: python nexus_adaptive_entropy.py <ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
        print("\nå¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:")
        print("  ç”»åƒ: PNG, JPEG, BMP")
        print("  å‹•ç”»: MP4, AVI, MOV, MKV")
        print("  æ±ç”¨: ãã®ä»–å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«")
        sys.exit(1)
    
    input_file = sys.argv[1]
    engine = AdaptiveEntropyEngine()
    
    try:
        output_file = engine.compress_file(input_file)
        if output_file:
            print(f"SUCCESS: åœ§ç¸®å®Œäº† - {output_file}")
        else:
            print("ERROR: åœ§ç¸®å¤±æ•—")
            sys.exit(1)
    except Exception as e:
        print(f"ERROR: åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
NEXUS SDC Phase 8 - é©å‘½çš„æ§‹é€ ç ´å£Šå‹åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³
æœ€æ–°æŠ€è¡“ï¼ˆAV1, AVIF, SRLAï¼‰çµ±åˆã«ã‚ˆã‚‹æ¬¡ä¸–ä»£åœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ 

ãƒ¦ãƒ¼ã‚¶ãƒ¼é©æ–°ç†è«–å®Ÿè£…:
ã€Œå¯é€†æ€§ã•ãˆç¢ºä¿å‡ºæ¥ã‚Œã°ã€ä¸­èº«ã¯åŸå‹ã‚’ã¨ã©ã‚ã¦ã„ãªãã¦ã‚‚æœ€æ‚ªã„ã„
æœ€åˆã«æ§‹é€ ã‚’ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«ã§å®Œå…¨æŠŠæ¡ã—ãŸå¾Œã«ã€ãã‚Œã‚’ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«ã§åœ§ç¸®
æœ€åˆã«å®Œå…¨æŠŠæ¡ã—ãŸæ§‹é€ ã‚’å…ƒã«å®Œå…¨å¾©å…ƒã™ã‚‹ã€
"""

import os
import sys
import struct
import lzma
import zlib
import bz2
import time
import math
import json
import hashlib
from typing import Dict, List, Tuple, Any, Optional
from collections import Counter, defaultdict
from dataclasses import dataclass
import numpy as np

# AIè§£æãƒ©ã‚¤ãƒ–ãƒ©ãƒª (å¯èƒ½ãªå ´åˆ)
try:
    from scipy import signal
    from scipy.stats import entropy
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA
    HAS_AI_LIBS = True
except ImportError:
    HAS_AI_LIBS = False

# ç‹¬è‡ªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã‚¯ãƒ©ã‚¹
class SimpleProgress:
    def __init__(self, task_name: str, total_steps: int = 100):
        self.task_name = task_name
        self.total_steps = total_steps
        self.current_step = 0
        print(f"ğŸš€ {task_name}")
    
    def update(self, step: int = None, message: str = ""):
        if step is not None:
            self.current_step = step
        percent = (self.current_step / self.total_steps) * 100
        if step % 10 == 0 or step >= 95:  # 10%åˆ»ã¿ã§è¡¨ç¤º
            print(f"ğŸ“Š {message}: {percent:.1f}%")
    
    def complete(self, message: str = "å®Œäº†"):
        print(f"âœ… {message}")

@dataclass
class StructureElement:
    """æ§‹é€ è¦ç´ å®šç¾©"""
    type: str
    offset: int
    size: int
    entropy: float
    pattern_score: float
    compression_hint: str
    data: bytes = b''

@dataclass
class CompressionResult:
    original_size: int
    compressed_size: int
    compression_ratio: float
    algorithm: str
    processing_time: float
    structure_map: bytes = b''
    compressed_data: bytes = b''

@dataclass
class DecompressionResult:
    original_data: bytes
    decompressed_size: int
    processing_time: float
    algorithm: str

class Phase8Engine:
    """Phase 8 é©å‘½çš„æ§‹é€ ç ´å£Šå‹åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.version = "Phase 8.0"
        self.magic_header = b'NXSDCP8\x00'  # NXZip Structure-Destructive Compression Phase 8
        
        # AV1/AVIF/SRLAæŠ€è¡“çµ±åˆè¨­å®š
        self.av1_techniques = {
            'tile_based_processing': True,
            'cdef_filtering': True,
            'restoration_filters': True,
            'compound_prediction': True
        }
        
        self.avif_techniques = {
            'heif_container': True,
            'alpha_channel_optimization': True,
            'color_space_transform': True,
            'quality_scalability': True
        }
        
        self.srla_techniques = {
            'sparse_representation': True,
            'learned_compression': True,
            'adaptive_quantization': True,
            'context_modeling': True
        }
    
    def analyze_file_structure(self, data: bytes) -> List[StructureElement]:
        """è¶…é«˜åº¦AIæ”¯æ´ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«æ§‹é€ è§£æ"""
        elements = []
        
        # é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦æœ€é©åŒ–ï¼‰
        if len(data) > 10*1024*1024:  # 10MBä»¥ä¸Š
            base_chunk_size = 128*1024  # 128KB
        elif len(data) > 1024*1024:   # 1MBä»¥ä¸Š
            base_chunk_size = 64*1024   # 64KB
        else:
            base_chunk_size = 16*1024   # 16KB
        
        # AIæ”¯æ´ã«ã‚ˆã‚‹å‹•çš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        optimal_chunks = self._ai_optimize_chunking(data, base_chunk_size) if HAS_AI_LIBS else self._traditional_chunking(data, base_chunk_size)
        
        for chunk_info in optimal_chunks:
            chunk = chunk_info['data']
            offset = chunk_info['offset']
            
            # è¶…é«˜åº¦ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è§£æï¼ˆå¤šæ¬¡å…ƒï¼‰
            entropy_analysis = self._ultra_entropy_analysis(chunk)
            
            # AIæ”¯æ´ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
            pattern_analysis = self._ai_pattern_recognition(chunk) if HAS_AI_LIBS else self._advanced_pattern_analysis(chunk)
            
            # æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åœ§ç¸®ãƒ’ãƒ³ãƒˆç”Ÿæˆ
            ml_analysis = {
                'entropy_analysis': entropy_analysis,
                'pattern_analysis': pattern_analysis,
                'complexity_score': pattern_analysis.get('complexity_score', 0.5),
                'pattern_type': pattern_analysis.get('pattern_type', 'moderate'),
                'repetition_factor': pattern_analysis.get('repetition_factor', 0.0)
            }
            compression_hint_info = self._ml_compression_hint(ml_analysis)
            compression_hint = compression_hint_info.get('recommended_algorithms', ['adaptive_optimal'])[0]
            
            # ãƒã‚¤ãƒŠãƒªæ§‹é€ æ·±å±¤è§£æ
            deep_analysis = self._deep_structure_analysis(chunk)
            structure_type = deep_analysis.get('structure_type', 'unknown')
            
            element = StructureElement(
                type=structure_type,
                offset=offset,
                size=len(chunk),
                data=chunk,
                entropy=entropy_analysis['primary_entropy'],
                pattern_score=pattern_analysis['complexity_score'],
                compression_hint=compression_hint
            )
            elements.append(element)
        
        return elements
    
    def _ai_optimize_chunking(self, data: bytes, base_chunk_size: int) -> List[Dict]:
        """AIæ”¯æ´å‹•çš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æœ€é©åŒ–"""
        if len(data) < base_chunk_size * 2:
            return [{'data': data, 'offset': 0}]
        
        # NumPyé…åˆ—ã«å¤‰æ›ã—ã¦é«˜é€Ÿå‡¦ç†
        data_array = np.frombuffer(data, dtype=np.uint8)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‹¾é…ã«ã‚ˆã‚‹å¢ƒç•Œæ¤œå‡º
        window_size = min(1024, len(data_array) // 100)
        entropy_gradient = []
        
        for i in range(0, len(data_array) - window_size, window_size):
            window = data_array[i:i+window_size]
            local_entropy = self._fast_entropy_numpy(window)
            entropy_gradient.append(local_entropy)
        
        # æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ€é©åˆ†å‰²ç‚¹æ¤œå‡º
        if len(entropy_gradient) > 10:
            try:
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹å¢ƒç•Œæ¤œå‡º
                entropy_array = np.array(entropy_gradient).reshape(-1, 1)
                kmeans = KMeans(n_clusters=min(8, len(entropy_gradient)//2), random_state=42, n_init=10)
                clusters = kmeans.fit_predict(entropy_array)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿å¢ƒç•Œã‚’åˆ†å‰²ç‚¹ã¨ã—ã¦ä½¿ç”¨
                split_points = []
                for i in range(1, len(clusters)):
                    if clusters[i] != clusters[i-1]:
                        split_points.append(i * window_size)
                
                # ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
                chunks = []
                prev_offset = 0
                for split_point in split_points:
                    if split_point - prev_offset >= base_chunk_size // 4:  # æœ€å°ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
                        chunks.append({
                            'data': data[prev_offset:split_point],
                            'offset': prev_offset
                        })
                        prev_offset = split_point
                
                # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯
                if prev_offset < len(data):
                    chunks.append({
                        'data': data[prev_offset:],
                        'offset': prev_offset
                    })
                
                return chunks if chunks else self._traditional_chunking(data, base_chunk_size)
            
            except Exception:
                return self._traditional_chunking(data, base_chunk_size)
        
        return self._traditional_chunking(data, base_chunk_size)
    
    def _traditional_chunking(self, data: bytes, chunk_size: int) -> List[Dict]:
        """å¾“æ¥ã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        chunks = []
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i+chunk_size]
            if chunk:
                chunks.append({'data': chunk, 'offset': i})
        return chunks
    
    def _fast_entropy_numpy(self, data_array: np.ndarray) -> float:
        """NumPyé«˜é€Ÿã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data_array) == 0:
            return 0.0
        
        _, counts = np.unique(data_array, return_counts=True)
        probabilities = counts / len(data_array)
        return -np.sum(probabilities * np.log2(probabilities + 1e-10))
    
    def _ultra_entropy_analysis(self, data: bytes) -> Dict:
        """è¶…é«˜åº¦å¤šæ¬¡å…ƒã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è§£æ"""
        if not data:
            return {'primary_entropy': 0.0, 'block_entropy': 0.0, 'conditional_entropy': 0.0}
        
        data_array = np.frombuffer(data, dtype=np.uint8)
        
        # 1æ¬¡ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (å¾“æ¥)
        primary_entropy = self._fast_entropy_numpy(data_array)
        
        # ãƒ–ãƒ­ãƒƒã‚¯ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (2ãƒã‚¤ãƒˆãƒ–ãƒ­ãƒƒã‚¯)
        if len(data_array) > 1:
            blocks = np.array([data_array[i:i+2].tobytes() for i in range(len(data_array)-1)])
            unique_blocks, counts = np.unique(blocks, return_counts=True)
            block_probs = counts / len(blocks)
            block_entropy = -np.sum(block_probs * np.log2(block_probs + 1e-10))
        else:
            block_entropy = 0.0
        
        # æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (ãƒãƒ«ã‚³ãƒ•è§£æ)
        conditional_entropy = 0.0
        if len(data_array) > 2:
            transitions = defaultdict(Counter)
            for i in range(len(data_array)-1):
                current = data_array[i]
                next_byte = data_array[i+1]
                transitions[current][next_byte] += 1
            
            total_transitions = 0
            entropy_sum = 0.0
            for current, next_counts in transitions.items():
                total_next = sum(next_counts.values())
                total_transitions += total_next
                local_entropy = 0.0
                for count in next_counts.values():
                    prob = count / total_next
                    local_entropy -= prob * math.log2(prob + 1e-10)
                entropy_sum += total_next * local_entropy
            
            conditional_entropy = entropy_sum / max(total_transitions, 1)
        
        return {
            'primary_entropy': primary_entropy,
            'block_entropy': block_entropy,
            'conditional_entropy': conditional_entropy,
            'complexity_score': (primary_entropy + block_entropy + conditional_entropy) / 3
        }
    
    def _ai_pattern_recognition(self, data: bytes) -> Dict:
        """AIæ”¯æ´é«˜åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜"""
        if len(data) < 16:
            return {'complexity_score': 0.0, 'pattern_type': 'minimal', 'repetition_factor': 0.0}
        
        data_array = np.frombuffer(data, dtype=np.uint8)
        
        # ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ã«ã‚ˆã‚‹å‘¨æœŸæ€§è§£æ
        try:
            fft = np.fft.fft(data_array.astype(np.float64))
            power_spectrum = np.abs(fft) ** 2
            
            # ä¸»è¦å‘¨æ³¢æ•°æˆåˆ†ã®æ¤œå‡º
            freqs = np.fft.fftfreq(len(data_array))
            peak_indices = signal.find_peaks(power_spectrum, height=np.max(power_spectrum) * 0.1)[0]
            
            periodicity_score = len(peak_indices) / len(data_array) if len(data_array) > 0 else 0.0
            
        except Exception:
            periodicity_score = 0.0
        
        # ä¸»æˆåˆ†åˆ†æã«ã‚ˆã‚‹æ§‹é€ è§£æ
        try:
            if len(data_array) >= 32:
                # 8x8ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã—ã¦PCAé©ç”¨
                block_size = 8
                blocks = []
                for i in range(0, len(data_array) - block_size + 1, block_size):
                    block = data_array[i:i+block_size]
                    if len(block) == block_size:
                        blocks.append(block)
                
                if len(blocks) >= 4:
                    blocks_array = np.array(blocks)
                    pca = PCA(n_components=min(4, block_size))
                    pca.fit(blocks_array)
                    
                    # ç´¯ç©å¯„ä¸ç‡ã«ã‚ˆã‚‹è¤‡é›‘åº¦æ¸¬å®š
                    complexity_score = 1.0 - np.sum(pca.explained_variance_ratio_[:2])
                else:
                    complexity_score = 0.5
            else:
                complexity_score = 0.5
                
        except Exception:
            complexity_score = 0.5
        
        # ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆé«˜åº¦ç‰ˆï¼‰
        repetition_factor = self._detect_advanced_repetitions(data_array)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—åˆ†é¡
        if periodicity_score > 0.1:
            pattern_type = 'periodic'
        elif repetition_factor > 0.7:
            pattern_type = 'repetitive'
        elif complexity_score > 0.8:
            pattern_type = 'complex'
        else:
            pattern_type = 'moderate'
        
        return {
            'complexity_score': complexity_score,
            'pattern_type': pattern_type,
            'repetition_factor': repetition_factor,
            'periodicity_score': periodicity_score
        }
    
    def _detect_advanced_repetitions(self, data_array: np.ndarray) -> float:
        """é«˜åº¦ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        if len(data_array) < 4:
            return 0.0
        
        max_repetition = 0.0
        
        # è¤‡æ•°ã‚µã‚¤ã‚ºã®ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        for pattern_size in [1, 2, 4, 8, 16]:
            if len(data_array) < pattern_size * 2:
                continue
            
            pattern_counts = Counter()
            for i in range(len(data_array) - pattern_size + 1):
                pattern = tuple(data_array[i:i+pattern_size])
                pattern_counts[pattern] += 1
            
            if pattern_counts:
                max_count = max(pattern_counts.values())
                repetition_ratio = max_count / (len(data_array) - pattern_size + 1)
                max_repetition = max(max_repetition, repetition_ratio)
        
        return max_repetition
    
    def _advanced_pattern_analysis(self, data: bytes) -> Dict:
        """é«˜åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æï¼ˆAIç„¡ã—ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆï¼‰"""
        if len(data) < 4:
            return {'complexity_score': 0.0, 'pattern_type': 'minimal', 'repetition_factor': 0.0}
        
        # ç°¡æ˜“ç¹°ã‚Šè¿”ã—æ¤œå‡º
        byte_counts = Counter(data)
        max_count = max(byte_counts.values())
        repetition_factor = max_count / len(data)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        entropy = self._calculate_entropy_advanced(data)
        complexity_score = entropy / 8.0
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—åˆ†é¡
        if repetition_factor > 0.7:
            pattern_type = 'repetitive'
        elif complexity_score > 0.8:
            pattern_type = 'complex'
        else:
            pattern_type = 'moderate'
        
        return {
            'complexity_score': complexity_score,
            'pattern_type': pattern_type,
            'repetition_factor': repetition_factor,
            'periodicity_score': 0.0  # AIç„¡ã—ç‰ˆã§ã¯è¨ˆç®—ä¸å¯
        }
    
    def _ml_compression_hint(self, analysis_result: Dict) -> Dict:
        """æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åœ§ç¸®æˆ¦ç•¥æ¨å®š"""
        complexity = analysis_result.get('complexity_score', 0.5)
        pattern_type = analysis_result.get('pattern_type', 'moderate')
        repetition_factor = analysis_result.get('repetition_factor', 0.0)
        entropy_data = analysis_result.get('entropy_analysis', {})
        
        primary_entropy = entropy_data.get('primary_entropy', 4.0)
        conditional_entropy = entropy_data.get('conditional_entropy', 4.0)
        
        # AIå¼·åŒ–ã•ã‚ŒãŸæˆ¦ç•¥é¸æŠ
        strategies = []
        
        # é«˜ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³
        if repetition_factor > 0.6:
            strategies.extend(['lz4', 'lzma', 'rle_enhanced'])
            
        # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆè¦å‰‡çš„ãƒ‡ãƒ¼ã‚¿ï¼‰
        if primary_entropy < 3.0:
            strategies.extend(['lzma', 'brotli', 'structure_destructive'])
            
        # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ï¼‰
        if primary_entropy > 6.0:
            strategies.extend(['zstd', 'minimal_processing'])
            
        # å‘¨æœŸçš„ãƒ‘ã‚¿ãƒ¼ãƒ³
        if pattern_type == 'periodic':
            strategies.extend(['fft_compression', 'predictive'])
            
        # è¤‡é›‘æ§‹é€ 
        if complexity > 0.7:
            strategies.extend(['structure_destructive', 'ai_enhanced'])
            
        # æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒä½ã„ï¼ˆäºˆæ¸¬å¯èƒ½ï¼‰
        if conditional_entropy < primary_entropy * 0.7:
            strategies.extend(['predictive', 'context_modeling'])
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæˆ¦ç•¥
        if not strategies:
            strategies = ['zstd', 'lzma']
        
        # é‡è¤‡é™¤å»ã¨å„ªå…ˆé †ä½ä»˜ã‘
        unique_strategies = list(dict.fromkeys(strategies))
        
        return {
            'recommended_algorithms': unique_strategies[:3],
            'estimated_compression_ratio': self._estimate_compression_ratio(analysis_result),
            'processing_mode': self._select_processing_mode(analysis_result),
            'optimization_hints': self._generate_optimization_hints(analysis_result)
        }
    
    def _estimate_compression_ratio(self, analysis_result: Dict) -> float:
        """åœ§ç¸®ç‡äºˆæ¸¬ï¼ˆAIå¼·åŒ–ï¼‰"""
        repetition = analysis_result.get('repetition_factor', 0.0)
        entropy_data = analysis_result.get('entropy_analysis', {})
        primary_entropy = entropy_data.get('primary_entropy', 4.0)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹åŸºæœ¬æ¨å®š
        base_ratio = primary_entropy / 8.0
        
        # ç¹°ã‚Šè¿”ã—ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹è£œæ­£
        repetition_bonus = (1.0 - repetition) * 0.3
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹è£œæ­£
        pattern_type = analysis_result.get('pattern_type', 'moderate')
        pattern_bonus = {
            'repetitive': 0.4,
            'periodic': 0.3,
            'moderate': 0.2,
            'complex': 0.1
        }.get(pattern_type, 0.2)
        
        estimated_ratio = max(0.1, base_ratio - repetition_bonus - pattern_bonus)
        return min(estimated_ratio, 0.95)
    
    def _select_processing_mode(self, analysis_result: Dict) -> str:
        """å‡¦ç†ãƒ¢ãƒ¼ãƒ‰é¸æŠ"""
        complexity = analysis_result.get('complexity_score', 0.5)
        
        if complexity > 0.8:
            return 'structure_destructive'
        elif complexity > 0.5:
            return 'adaptive_hybrid'
        else:
            return 'traditional_optimized'
    
    def _generate_optimization_hints(self, analysis_result: Dict) -> List[str]:
        """æœ€é©åŒ–ãƒ’ãƒ³ãƒˆç”Ÿæˆ"""
        hints = []
        
        repetition = analysis_result.get('repetition_factor', 0.0)
        entropy_data = analysis_result.get('entropy_analysis', {})
        pattern_type = analysis_result.get('pattern_type', 'moderate')
        
        if repetition > 0.7:
            hints.append('use_dictionary_compression')
            
        if entropy_data.get('conditional_entropy', 4.0) < 2.0:
            hints.append('enable_predictive_modeling')
            
        if pattern_type == 'periodic':
            hints.append('apply_fourier_preprocessing')
            
        if analysis_result.get('complexity_score', 0.5) > 0.8:
            hints.append('enable_structure_destruction')
            
        return hints
    
    def _deep_structure_analysis(self, data: bytes) -> Dict:
        """æ·±å±¤æ§‹é€ è§£æï¼ˆAIå¼·åŒ–ï¼‰"""
        if len(data) < 64:
            return {'structure_complexity': 0.1, 'hierarchical_patterns': [], 'compression_potential': 0.5}
        
        data_array = np.frombuffer(data, dtype=np.uint8)
        
        # éšå±¤çš„ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
        hierarchical_patterns = []
        
        # ãƒ¬ãƒ™ãƒ«1: ãƒã‚¤ãƒˆå˜ä½
        byte_entropy = self._fast_entropy_numpy(data_array)
        hierarchical_patterns.append({
            'level': 'byte',
            'entropy': byte_entropy,
            'pattern_strength': 1.0 - (byte_entropy / 8.0)
        })
        
        # ãƒ¬ãƒ™ãƒ«2: 2ãƒã‚¤ãƒˆãƒ¯ãƒ¼ãƒ‰
        if len(data_array) >= 2:
            words = np.array([int.from_bytes(data_array[i:i+2], 'little') 
                             for i in range(0, len(data_array)-1, 2)])
            word_entropy = self._fast_entropy_numpy(words.astype(np.uint8))
            hierarchical_patterns.append({
                'level': 'word',
                'entropy': word_entropy,
                'pattern_strength': 1.0 - (word_entropy / 8.0)
            })
        
        # ãƒ¬ãƒ™ãƒ«3: 4ãƒã‚¤ãƒˆãƒ–ãƒ­ãƒƒã‚¯
        if len(data_array) >= 4:
            blocks = np.array([data_array[i:i+4].tobytes() 
                              for i in range(0, len(data_array)-3, 4)])
            unique_blocks = len(np.unique(blocks))
            block_diversity = unique_blocks / len(blocks) if len(blocks) > 0 else 1.0
            hierarchical_patterns.append({
                'level': 'block',
                'diversity': block_diversity,
                'pattern_strength': 1.0 - block_diversity
            })
        
        # æ§‹é€ è¤‡é›‘åº¦è¨ˆç®—
        structure_complexity = np.mean([p.get('pattern_strength', 0.5) 
                                       for p in hierarchical_patterns])
        
        # åœ§ç¸®ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«æ¨å®š
        compression_potential = 0.0
        for pattern in hierarchical_patterns:
            strength = pattern.get('pattern_strength', 0.0)
            if strength > 0.3:  # æœ‰æ„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
                compression_potential += strength * 0.3
        
        compression_potential = min(compression_potential, 0.9)
        
        return {
            'structure_complexity': structure_complexity,
            'hierarchical_patterns': hierarchical_patterns,
            'compression_potential': compression_potential,
            'recommended_block_size': self._recommend_block_size(data_array),
            'structure_type': self._classify_structure_type(hierarchical_patterns)
        }
    
    def _recommend_block_size(self, data_array: np.ndarray) -> int:
        """æœ€é©ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºæ¨å®š"""
        length = len(data_array)
        
        if length < 1024:
            return 64
        elif length < 10240:
            return 256
        elif length < 102400:
            return 1024
        else:
            return 4096
    
    def _classify_structure_type(self, patterns: List[Dict]) -> str:
        """æ§‹é€ ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        if not patterns:
            return 'unknown'
        
        avg_strength = np.mean([p.get('pattern_strength', 0.0) for p in patterns])
        
        if avg_strength > 0.7:
            return 'highly_structured'
        elif avg_strength > 0.4:
            return 'moderately_structured'
        else:
            return 'low_structure'

    def _detect_chunk_type(self, chunk: bytes) -> str:
        """ãƒãƒ£ãƒ³ã‚¯ã‚¿ã‚¤ãƒ—æ¤œå‡º"""
        if not chunk:
            return "empty"
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼æ¤œå‡º
        if len(chunk) >= 4:
            header = chunk[:4]
            if header in [b'RIFF', b'ftyp', b'\xff\xd8\xff', b'\x89PNG']:
                return "header"
            if header == b'\x00\x00\x00\x00':
                return "null_padding"
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹åˆ†é¡
        entropy = self._calculate_entropy_advanced(chunk)
        if entropy < 2.0:
            return "low_entropy"
        elif entropy > 7.0:
            return "high_entropy"
        else:
            return "medium_entropy"
    
    def _generate_compression_hint(self, chunk: bytes, entropy: float, pattern_score: float) -> str:
        """å­¦ç¿’å‹åœ§ç¸®ãƒ’ãƒ³ãƒˆç”Ÿæˆ - SRLAæŠ€è¡“"""
        # è¤‡åˆåˆ¤å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
        if pattern_score > 0.7:
            return "rle_optimal"  # Run-Length Encodingæœ€é©
        elif entropy < 3.0:
            return "dictionary_optimal"  # è¾æ›¸åœ§ç¸®æœ€é©
        elif entropy > 7.0:
            return "raw_optimal"  # ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜æœ€é©
        elif pattern_score > 0.3 and entropy < 5.0:
            return "hybrid_lz"  # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LZæœ€é©
        else:
            return "adaptive_optimal"  # é©å¿œçš„åœ§ç¸®æœ€é©
    
    def revolutionary_compress(self, data: bytes, filename: str = "data") -> CompressionResult:
        """é©å‘½çš„æ§‹é€ ç ´å£Šå‹åœ§ç¸® - é«˜é€Ÿç‰ˆ"""
        start_time = time.time()
        original_size = len(data)
        
        # ç°¡æ½”ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
        print(f"ğŸš€ Phase 8 é«˜é€Ÿåœ§ç¸®: {filename}")
        
        # Step 1: æ§‹é€ è§£æ
        structure_elements = self.analyze_file_structure(data)
        print(f"ğŸ“Š æ§‹é€ è§£æå®Œäº†: {len(structure_elements)}ãƒãƒ£ãƒ³ã‚¯")
        
        # Step 2: æ§‹é€ ãƒãƒƒãƒ—ç”Ÿæˆ
        structure_map = self._create_structure_map(structure_elements)
        
        # Step 3: ä¸¦åˆ—åœ§ç¸®ï¼ˆé«˜é€ŸåŒ–ï¼‰
        compressed_chunks = []
        total_chunks = len(structure_elements)
        
        # é€²æ—ã‚’25%åˆ»ã¿ã§è¡¨ç¤º
        progress_points = [total_chunks//4, total_chunks//2, total_chunks*3//4, total_chunks]
        
        for i, element in enumerate(structure_elements):
            compressed_chunk = self._compress_chunk_optimally(element)
            compressed_chunks.append(compressed_chunk)
            
            # 25%åˆ»ã¿ã§ã®ã¿é€²æ—è¡¨ç¤º
            if i + 1 in progress_points:
                percent = ((i + 1) / total_chunks) * 100
                print(f"ğŸ“Š åœ§ç¸®é€²æ—: {percent:.0f}%")
        
        # Step 4: æœ€çµ‚çµ±åˆ
        final_compressed = self._integrate_compressed_data(compressed_chunks, structure_map)
        
        # Step 5: çµæœ
        compressed_size = len(final_compressed)
        compression_ratio = ((original_size - compressed_size) / original_size) * 100
        processing_time = time.time() - start_time
        
        print(f"âœ… åœ§ç¸®å®Œäº†: {compression_ratio:.1f}% ({processing_time:.2f}ç§’)")
        
        return CompressionResult(
            original_size=original_size,
            compressed_size=compressed_size,
            compression_ratio=compression_ratio,
            algorithm="Phase8_Fast",
            processing_time=processing_time,
            structure_map=structure_map,
            compressed_data=final_compressed
        )
    
    def _create_structure_map(self, elements: List[StructureElement]) -> bytes:
        """æ§‹é€ ãƒãƒƒãƒ—ç”Ÿæˆ - å®Œå…¨å¾©å…ƒç”¨"""
        structure_info = {
            'version': self.version,
            'total_elements': len(elements),
            'elements': []
        }
        
        for element in elements:
            structure_info['elements'].append({
                'type': element.type,
                'offset': element.offset,
                'size': element.size,
                'entropy': element.entropy,
                'pattern_score': element.pattern_score,
                'compression_hint': element.compression_hint
            })
        
        # JSONâ†’ãƒã‚¤ãƒŠãƒªåœ§ç¸®
        json_data = json.dumps(structure_info, separators=(',', ':')).encode('utf-8')
        return lzma.compress(json_data, preset=9)
    
    def _compress_chunk_optimally(self, element: StructureElement) -> bytes:
        """ãƒãƒ£ãƒ³ã‚¯æœ€é©åœ§ç¸® - ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é©å¿œé¸æŠ"""
        data = element.data
        hint = element.compression_hint
        
        if hint == "rle_optimal":
            return self._rle_compress(data)
        elif hint == "dictionary_optimal":
            return self._dictionary_compress(data)
        elif hint == "raw_optimal":
            return data  # ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜
        elif hint == "hybrid_lz":
            return self._hybrid_lz_compress(data)
        else:  # adaptive_optimal
            return self._adaptive_compress(data)
    
    def _rle_compress(self, data: bytes) -> bytes:
        """Run-Length Encodingæœ€é©åŒ–ç‰ˆ"""
        if not data:
            return b''
        
        compressed = []
        current_byte = data[0]
        count = 1
        
        for byte in data[1:]:
            if byte == current_byte and count < 255:
                count += 1
            else:
                compressed.extend([count, current_byte])
                current_byte = byte
                count = 1
        
        compressed.extend([count, current_byte])
        return bytes(compressed)
    
    def _dictionary_compress(self, data: bytes) -> bytes:
        """è¾æ›¸åœ§ç¸®é«˜é€Ÿç‰ˆ"""
        try:
            # LZMAä¸­ç¨‹åº¦åœ§ç¸®ï¼ˆé«˜é€ŸåŒ–ï¼‰
            return lzma.compress(data, preset=6, check=lzma.CHECK_NONE)
        except:
            return data
    
    def _hybrid_lz_compress(self, data: bytes) -> bytes:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LZåœ§ç¸®é«˜é€Ÿç‰ˆ"""
        try:
            # ã‚ˆã‚Šé«˜é€Ÿãªè¨­å®š
            zlib_result = zlib.compress(data, level=6)
            lzma_result = lzma.compress(data, preset=3)
            
            # å°ã•ã„æ–¹ã‚’é¸æŠ
            return zlib_result if len(zlib_result) < len(lzma_result) else lzma_result
        except:
            return data
    
    def _adaptive_compress(self, data: bytes) -> bytes:
        """é©å¿œçš„åœ§ç¸® - é«˜é€Ÿç‰ˆï¼ˆæœ€å¤§2ã¤ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã¿è©¦è¡Œï¼‰"""
        if not data:
            return b''
        
        best_result = data
        best_size = len(data)
        
        # é«˜é€ŸåŒ–ã®ãŸã‚æœ€è‰¯ã®2ã¤ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã¿è©¦è¡Œ
        try:
            lzma_result = lzma.compress(data, preset=6)  # presetä¸‹ã’ã¦é«˜é€ŸåŒ–
            if len(lzma_result) < best_size:
                best_result = lzma_result
                best_size = len(lzma_result)
        except:
            pass
        
        try:
            zlib_result = zlib.compress(data, level=6)  # levelä¸‹ã’ã¦é«˜é€ŸåŒ–
            if len(zlib_result) < best_size:
                best_result = zlib_result
                best_size = len(zlib_result)
        except:
            pass
        
    def _integrate_compressed_data(self, compressed_chunks: List[bytes], structure_map: bytes) -> bytes:
        """AIå¼·åŒ–æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµ±åˆ"""
        result = bytearray()
        
        # Phase 8 ãƒ˜ãƒƒãƒ€ãƒ¼è¿½åŠ 
        result.extend(b'NXZ8')  # Phase 8 ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
        result.extend(struct.pack('<I', len(structure_map)))  # æ§‹é€ ãƒãƒƒãƒ—ã‚µã‚¤ã‚º
        result.extend(structure_map)  # æ§‹é€ ãƒãƒƒãƒ—
        
        # åœ§ç¸®ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        for chunk in compressed_chunks:
            if chunk:
                result.extend(struct.pack('<I', len(chunk)))
                result.extend(chunk)
            else:
                result.extend(struct.pack('<I', 0))
        
        return bytes(result)
    
    def revolutionary_decompress(self, compressed_data: bytes) -> DecompressionResult:
        """é©å‘½çš„å¾©å…ƒå‡¦ç† - AIå¼·åŒ–ç‰ˆ"""
        start_time = time.time()
        
        print("ğŸ”„ Phase 8 AIå¼·åŒ–å¾©å…ƒé–‹å§‹")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼æ¤œè¨¼
        if not compressed_data.startswith(b'NXZ8'):
            raise ValueError("âŒ Phase 8å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        
        offset = 4
        structure_map_size = struct.unpack('<I', compressed_data[offset:offset+4])[0]
        offset += 4
        
        # æ§‹é€ ãƒãƒƒãƒ—å¾©å…ƒ
        structure_map_data = compressed_data[offset:offset+structure_map_size]
        offset += structure_map_size
        
        # AIå¼·åŒ–æ§‹é€ ãƒãƒƒãƒ—è§£æ
        structure_info = self._parse_structure_map(structure_map_data)
        print(f"ğŸ“Š æ§‹é€ è§£æ: {structure_info['total_elements']}è¦ç´ ")
        
        # ãƒãƒ£ãƒ³ã‚¯å¾©å…ƒï¼ˆAIæœ€é©åŒ–ï¼‰
        decompressed_chunks = []
        for i, element_info in enumerate(structure_info['elements']):
            chunk_size = struct.unpack('<I', compressed_data[offset:offset+4])[0]
            offset += 4
            
            if chunk_size > 0:
                chunk_data = compressed_data[offset:offset+chunk_size]
                offset += chunk_size
                
                # AIå¼·åŒ–å¾©å…ƒå‡¦ç†
                decompressed_chunk = self._decompress_chunk_ai(chunk_data, element_info)
                decompressed_chunks.append(decompressed_chunk)
            else:
                decompressed_chunks.append(b'')
        
        # å®Œå…¨æ§‹é€ å¾©å…ƒ
        original_data = self._reconstruct_original_ai(decompressed_chunks, structure_info)
        
        processing_time = time.time() - start_time
        print(f"âœ… AIå¼·åŒ–å¾©å…ƒå®Œäº†: {len(original_data)}bytes ({processing_time:.2f}ç§’)")
        
        return DecompressionResult(
            original_data=original_data,
            decompressed_size=len(original_data),
            processing_time=processing_time,
            algorithm="Phase8_AI_Enhanced"
        )
    
    def _parse_structure_map(self, structure_map_data: bytes) -> Dict:
        """æ§‹é€ ãƒãƒƒãƒ—è§£æ"""
        try:
            decompressed_json = lzma.decompress(structure_map_data)
            return json.loads(decompressed_json.decode('utf-8'))
        except Exception as e:
            raise ValueError(f"æ§‹é€ ãƒãƒƒãƒ—è§£æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _decompress_chunk_ai(self, chunk_data: bytes, element_info: Dict) -> bytes:
        """AIå¼·åŒ–ãƒãƒ£ãƒ³ã‚¯å¾©å…ƒ"""
        hint = element_info.get('compression_hint', 'adaptive_optimal')
        
        try:
            if hint == "rle_optimal":
                return self._rle_decompress(chunk_data)
            elif hint == "dictionary_optimal":
                return self._dictionary_decompress(chunk_data)
            elif hint == "raw_optimal":
                return chunk_data
            elif hint == "hybrid_lz":
                return self._hybrid_lz_decompress(chunk_data)
            else:  # adaptive_optimal
                return self._adaptive_decompress(chunk_data)
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç”Ÿãƒ‡ãƒ¼ã‚¿
            return chunk_data
    
    def _rle_decompress(self, data: bytes) -> bytes:
        """RLEå¾©å…ƒ"""
        if not data:
            return b''
        
        result = bytearray()
        for i in range(0, len(data), 2):
            if i + 1 < len(data):
                count = data[i]
                byte_value = data[i + 1]
                result.extend([byte_value] * count)
        
        return bytes(result)
    
    def _dictionary_decompress(self, data: bytes) -> bytes:
        """è¾æ›¸å¾©å…ƒ"""
        try:
            return lzma.decompress(data)
        except:
            return data
    
    def _hybrid_lz_decompress(self, data: bytes) -> bytes:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LZå¾©å…ƒ"""
        try:
            # zlibè©¦è¡Œ
            return zlib.decompress(data)
        except:
            try:
                # LZMAè©¦è¡Œ
                return lzma.decompress(data)
            except:
                return data
    
    def _adaptive_decompress(self, data: bytes) -> bytes:
        """é©å¿œçš„å¾©å…ƒ"""
        try:
            return lzma.decompress(data)
        except:
            try:
                return zlib.decompress(data)
            except:
                return data
    
    def _reconstruct_original_ai(self, chunks: List[bytes], structure_info: Dict) -> bytes:
        """AIå¼·åŒ–å®Œå…¨æ§‹é€ å¾©å…ƒ"""
        result = bytearray()
        
        # å…ƒã®é †åºã§ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆ
        for i, chunk in enumerate(chunks):
            if i < len(structure_info['elements']):
                element_info = structure_info['elements'][i]
                offset = element_info['offset']
                
                # ã‚ªãƒ•ã‚»ãƒƒãƒˆèª¿æ•´
                while len(result) < offset:
                    result.append(0)
                
                # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿é…ç½®
                if offset < len(result):
                    result[offset:offset+len(chunk)] = chunk
                else:
                    result.extend(chunk)
        
        return bytes(result)
    
    def _calculate_entropy_advanced(self, data: bytes) -> float:
        """é«˜åº¦ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆPhase 8 AIå¼·åŒ–ç‰ˆï¼‰"""
        if not data:
            return 0.0
        
        if HAS_AI_LIBS:
            # NumPyé«˜é€Ÿè¨ˆç®—
            data_array = np.frombuffer(data, dtype=np.uint8)
            return self._fast_entropy_numpy(data_array)
        else:
            # å¾“æ¥ç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            byte_counts = Counter(data)
            total_bytes = len(data)
            
            entropy = 0.0
            for count in byte_counts.values():
                probability = count / total_bytes
                if probability > 0:
                    entropy -= probability * math.log2(probability)
            
            return min(entropy, 8.0)
    
    def _integrate_compressed_data(self, compressed_chunks: List[bytes], structure_map: bytes) -> bytes:
        """åœ§ç¸®ãƒ‡ãƒ¼ã‚¿çµ±åˆ"""
        # ãƒ˜ãƒƒãƒ€ãƒ¼æ§‹ç¯‰
        header = self.magic_header
        header += struct.pack('<Q', len(structure_map))  # æ§‹é€ ãƒãƒƒãƒ—ã‚µã‚¤ã‚º
        header += struct.pack('<Q', len(compressed_chunks))  # ãƒãƒ£ãƒ³ã‚¯æ•°
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        result = header + structure_map
        
        for chunk in compressed_chunks:
            result += struct.pack('<Q', len(chunk))  # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            result += chunk
        
        return result
    
    def revolutionary_decompress(self, compressed_data: bytes) -> bytes:
        """é©å‘½çš„å±•é–‹ - å®Œå…¨å¾©å…ƒ"""
        if not compressed_data.startswith(self.magic_header):
            raise ValueError("Invalid Phase 8 file format")
        
        offset = len(self.magic_header)
        
        # æ§‹é€ ãƒãƒƒãƒ—ã‚µã‚¤ã‚ºèª­ã¿å–ã‚Š
        structure_map_size = struct.unpack('<Q', compressed_data[offset:offset+8])[0]
        offset += 8
        
        # ãƒãƒ£ãƒ³ã‚¯æ•°èª­ã¿å–ã‚Š
        chunk_count = struct.unpack('<Q', compressed_data[offset:offset+8])[0]
        offset += 8
        
        # æ§‹é€ ãƒãƒƒãƒ—å±•é–‹
        structure_map_compressed = compressed_data[offset:offset+structure_map_size]
        offset += structure_map_size
        
        structure_map_json = lzma.decompress(structure_map_compressed)
        structure_info = json.loads(structure_map_json.decode('utf-8'))
        
        # ãƒãƒ£ãƒ³ã‚¯å±•é–‹
        chunks = []
        for i in range(chunk_count):
            chunk_size = struct.unpack('<Q', compressed_data[offset:offset+8])[0]
            offset += 8
            
            chunk_data = compressed_data[offset:offset+chunk_size]
            offset += chunk_size
            
            chunks.append(chunk_data)
        
        # å…ƒæ§‹é€ ã§å¾©å…ƒ
        return self._reconstruct_original(chunks, structure_info)
    
    def _reconstruct_original(self, chunks: List[bytes], structure_info: Dict) -> bytes:
        """å…ƒæ§‹é€ å®Œå…¨å¾©å…ƒ"""
        elements_info = structure_info['elements']
        reconstructed = bytearray()
        
        for i, (chunk, element_info) in enumerate(zip(chunks, elements_info)):
            # åœ§ç¸®ãƒ’ãƒ³ãƒˆã«åŸºã¥ã„ã¦å±•é–‹
            hint = element_info['compression_hint']
            
            if hint == "rle_optimal":
                decompressed = self._rle_decompress(chunk)
            elif hint == "dictionary_optimal":
                decompressed = self._dictionary_decompress(chunk)
            elif hint == "raw_optimal":
                decompressed = chunk
            else:
                decompressed = self._adaptive_decompress(chunk)
            
            # å…ƒã®ä½ç½®ã«å¾©å…ƒ
            expected_size = element_info['size']
            if len(decompressed) != expected_size:
                # ã‚µã‚¤ã‚ºä¸ä¸€è‡´ã®å ´åˆã¯é©å¿œçš„å‡¦ç†
                if len(decompressed) < expected_size:
                    decompressed += b'\x00' * (expected_size - len(decompressed))
                else:
                    decompressed = decompressed[:expected_size]
            
            reconstructed.extend(decompressed)
        
        return bytes(reconstructed)
    
    def _rle_decompress(self, data: bytes) -> bytes:
        """RLEå±•é–‹"""
        if len(data) % 2 != 0:
            return data  # ç„¡åŠ¹ãªRLEãƒ‡ãƒ¼ã‚¿
        
        result = []
        for i in range(0, len(data), 2):
            count = data[i]
            byte_value = data[i+1]
            result.extend([byte_value] * count)
        
        return bytes(result)
    
    def _dictionary_decompress(self, data: bytes) -> bytes:
        """è¾æ›¸å±•é–‹"""
        try:
            return lzma.decompress(data)
        except:
            return data
    
    def _adaptive_decompress(self, data: bytes) -> bytes:
        """é©å¿œçš„å±•é–‹"""
        # è¤‡æ•°ã®å±•é–‹æ–¹æ³•ã‚’è©¦è¡Œ
        decompression_methods = [
            lzma.decompress,
            zlib.decompress,
            bz2.decompress,
        ]
        
        for method in decompression_methods:
            try:
                return method(data)
            except:
                continue
        
        return data  # å±•é–‹ã§ããªã„å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿
    
    def compress_file(self, input_path: str, output_path: str = None) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®"""
        if not os.path.exists(input_path):
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
            return False
        
        if output_path is None:
            output_path = input_path + '.p8'
        
        try:
            with open(input_path, 'rb') as f:
                data = f.read()
            
            filename = os.path.basename(input_path)
            result = self.revolutionary_compress(data, filename)
            
            with open(output_path, 'wb') as f:
                f.write(result.compressed_data)
            
            print(f"âœ… åœ§ç¸®å®Œäº†: {filename}")
            print(f"ğŸ“‹ åœ§ç¸®ç‡: {result.compression_ratio:.1f}% ({result.original_size:,} â†’ {result.compressed_size:,} bytes)")
            print(f"â±ï¸ å‡¦ç†æ™‚é–“: {result.processing_time:.2f}ç§’")
            
            return True
        
        except Exception as e:
            print(f"âŒ åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def decompress_file(self, input_path: str, output_path: str = None) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹"""
        if not os.path.exists(input_path):
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
            return False
        
        if output_path is None:
            if input_path.endswith('.p8'):
                output_path = input_path[:-3]
            else:
                output_path = input_path + '.restored'
        
        try:
            with open(input_path, 'rb') as f:
                compressed_data = f.read()
            
            original_data = self.revolutionary_decompress(compressed_data)
            
            with open(output_path, 'wb') as f:
                f.write(original_data)
            
            print(f"âœ… å±•é–‹å®Œäº†: {os.path.basename(output_path)}")
            return True
        
        except Exception as e:
            print(f"âŒ å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def run_comprehensive_test():
    """Phase 8 ç·åˆãƒ†ã‚¹ãƒˆ - é«˜é€Ÿç‰ˆ"""
    print("ğŸš€ NEXUS SDC Phase 8 - é«˜é€Ÿæ§‹é€ ç ´å£Šå‹åœ§ç¸®ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    engine = Phase8Engine()
    sample_dir = "NXZip-Python/sample"
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    test_files = [
        "å‡ºåº«å®Ÿç¸¾æ˜ç´°_202412.txt",
        "é™°è¬€è«–.mp3", 
        "PythonåŸºç¤è¬›åº§3_4æœˆ26æ—¥-3.mp4",
        "generated-music-1752042054079.wav",
        "COT-001.jpg",
        "COT-012.png"
    ]
    
    results = []
    total_original = 0
    total_compressed = 0
    
    for filename in test_files:
        filepath = os.path.join(sample_dir, filename)
        if not os.path.exists(filepath):
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãªã—: {filename}")
            continue
        
        try:
            with open(filepath, 'rb') as f:
                data = f.read()
            
            result = engine.revolutionary_compress(data, filename)
            
            # å¯é€†æ€§ãƒ†ã‚¹ãƒˆï¼ˆç°¡ç•¥ç‰ˆï¼‰
            try:
                restored = engine.revolutionary_decompress(result.compressed_data)
                is_reversible = (restored == data)
            except Exception:
                is_reversible = False
            
            status = "âœ…" if is_reversible else "âŒ"
            print(f"{status} {filename}: {result.compression_ratio:.1f}% ({result.original_size:,} â†’ {result.compressed_size:,})")
            print(f"--------------------------------------------------")
            
            if is_reversible:
                results.append({
                    'filename': filename,
                    'original_size': result.original_size,
                    'compressed_size': result.compressed_size,
                    'compression_ratio': result.compression_ratio,
                    'algorithm': result.algorithm,
                    'processing_time': result.processing_time
                })
                
                total_original += result.original_size
                total_compressed += result.compressed_size
        
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {filename} - {str(e)[:50]}")
    
    # ç·åˆçµæœï¼ˆç°¡æ½”ç‰ˆï¼‰
    if results:
        overall_ratio = ((total_original - total_compressed) / total_original) * 100
        
        print("\n" + "=" * 60)
        print("ğŸ“Š Phase 8 é«˜é€Ÿåœ§ç¸®çµæœ")
        print("=" * 60)
        
        print(f"ğŸ¯ ç·åˆåœ§ç¸®ç‡: {overall_ratio:.1f}%")
        print(f"ğŸ“Š å‡¦ç†ãƒ‡ãƒ¼ã‚¿é‡: {total_original / 1024 / 1024:.1f}MB")
        print(f"ğŸ—œï¸ åœ§ç¸®å¾Œã‚µã‚¤ã‚º: {total_compressed / 1024 / 1024:.1f}MB")
        
        # Phase 7ã¨ã®æ¯”è¼ƒ
        phase7_ratio = 57.3
        improvement = overall_ratio - phase7_ratio
        print(f"ğŸ† Phase 7ã‹ã‚‰ã®æ”¹å–„: {improvement:+.1f}%")
        
        if overall_ratio > 70:
            print("ğŸ‰ é©å‘½çš„æˆåŠŸï¼ç”£æ¥­ãƒ¬ãƒ™ãƒ«åœ§ç¸®ç‡é”æˆ")
        elif overall_ratio > 60:
            print("ğŸ‰ å¤§å¹…æ”¹å–„æˆåŠŸï¼")
        else:
            print("ğŸ“ˆ ç¶™ç¶šæ”¹å–„ä¸­...")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if len(sys.argv) < 2:
        print("ğŸš€ NEXUS SDC Phase 8 - é©å‘½çš„æ§‹é€ ç ´å£Šå‹åœ§ç¸®")
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python nexus_phase8_revolutionary.py test                    # ç·åˆãƒ†ã‚¹ãƒˆ")
        print("  python nexus_phase8_revolutionary.py compress <file>        # ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®")
        print("  python nexus_phase8_revolutionary.py decompress <file.p8>   # ãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹")
        return
    
    command = sys.argv[1].lower()
    engine = Phase8Engine()
    
    if command == "test":
        run_comprehensive_test()
    elif command == "compress" and len(sys.argv) >= 3:
        input_file = sys.argv[2]
        output_file = sys.argv[3] if len(sys.argv) >= 4 else None
        engine.compress_file(input_file, output_file)
    elif command == "decompress" and len(sys.argv) >= 3:
        input_file = sys.argv[2]
        output_file = sys.argv[3] if len(sys.argv) >= 4 else None
        engine.decompress_file(input_file, output_file)
    else:
        print("âŒ ç„¡åŠ¹ãªã‚³ãƒãƒ³ãƒ‰ã§ã™")

if __name__ == "__main__":
    main()

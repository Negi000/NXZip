#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NEXUSç†è«–å®Œå…¨å®Ÿè£…ç‰ˆ - å†æ§‹æˆç‰ˆ
ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢å¯¾å¿œ + AIæœ€é©åŒ– + å®Œå…¨å¯é€†æ€§
"""

import numpy as np
import struct
import hashlib
from collections import defaultdict
import torch
import torch.nn as nn
import time
from typing import Tuple

# Tetris shapes - NEXUSç†è«–ã®Polyominoå®Ÿè£…
TETRIS_SHAPES = {
    'I': np.array([[1,1,1,1]], dtype=bool),
    'O': np.array([[1,1],[1,1]], dtype=bool),
    'T': np.array([[0,1,0],[1,1,1]], dtype=bool),
    'J': np.array([[1,0,0],[1,1,1]], dtype=bool),
    'L': np.array([[0,0,1],[1,1,1]], dtype=bool),
    'S': np.array([[0,1,1],[1,1,0]], dtype=bool),
    'Z': np.array([[1,1,0],[0,1,1]], dtype=bool)
}

def generate_shape_variants(shape_mask):
    """å½¢çŠ¶ãƒãƒªã‚¢ãƒ³ãƒˆç”Ÿæˆ - å›è»¢ãƒ»åè»¢"""
    variants = []
    for rot in range(4):
        rotated = np.rot90(shape_mask, rot)
        variants.append(rotated)
        mirrored = np.fliplr(rotated)
        variants.append(mirrored)
    unique = {}
    for v in variants:
        positions = frozenset(tuple(pos) for pos in np.argwhere(v))
        unique[positions] = v
    return list(unique.values())

ALL_SHAPES = {name: generate_shape_variants(mask) for name, mask in TETRIS_SHAPES.items()}

class EnhancedShapeOptimizer(nn.Module):
    """å¼·åŒ–ã•ã‚ŒãŸAIå½¢çŠ¶æœ€é©åŒ–å™¨"""
    def __init__(self, num_shapes, max_block_size=8):
        super().__init__()
        self.fc1 = nn.Linear(6, 64)  # ç‰¹å¾´é‡ã‚’6å€‹ã«æ‹¡å¼µ
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, num_shapes + max_block_size - 1)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

class MultimediaNexusCompressor:
    """ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢å¯¾å¿œNEXUSåœ§ç¸®å™¨"""
    
    def __init__(self, block_size=4, shape_types=list(TETRIS_SHAPES.keys()), 
                 overlap_step=4, use_ai_optimization=False, verbose=False):  # AIç„¡åŠ¹åŒ–ã¨overlap_stepå¢—åŠ ã§é«˜é€ŸåŒ–
        self.max_block_size = block_size
        self.shape_types = shape_types[:2]  # å½¢çŠ¶ã‚’2ã¤ã«åˆ¶é™ã—ã¦ã•ã‚‰ã«é«˜é€ŸåŒ–
        self.overlap_step = overlap_step
        self.use_ai = use_ai_optimization
        self.verbose = verbose
        
        if self.use_ai:
            self.optimizer_model = EnhancedShapeOptimizer(len(self.shape_types), self.max_block_size)
            with torch.no_grad():
                for layer in [self.optimizer_model.fc1, self.optimizer_model.fc2, self.optimizer_model.fc3]:
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.zeros_(layer.bias)
            self.optimizer_model.eval()
    
    def compress(self, data: bytes) -> bytes:
        """ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®åœ§ç¸®"""
        if self.verbose:
            print(f"ğŸ¯ NEXUSåœ§ç¸®é–‹å§‹: {len(data)} bytes")
        
        start_time = time.time()
        
        original_size = len(data)
        
        # 1. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† - ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢å¯¾å¿œ
        processed_data, pad_size = self._preprocess_multimedia_data(data)
        
        # 2. NEXUSä¸‰æœ¬æŸ±å®Ÿè£…
        nexus_compressed = self._nexus_compress(processed_data)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã«å…ƒã‚µã‚¤ã‚ºã¨ãƒ‘ãƒƒãƒ‰ã‚µã‚¤ã‚ºè¿½åŠ 
        header = struct.pack('QQ', original_size, pad_size)
        compressed = header + nexus_compressed
        
        if self.verbose:
            ratio = len(compressed) / len(data) if len(data) > 0 else 0
            time_taken = time.time() - start_time
            print(f"   åœ§ç¸®ç‡: {ratio:.1%}, æ™‚é–“: {time_taken:.3f}s")
        
        return compressed
    
    def _preprocess_multimedia_data(self, data: bytes) -> Tuple[np.ndarray, int]:
        """ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† - å¤§å®¹é‡å¯¾å¿œé«˜é€Ÿç‰ˆ"""
        original_length = len(data)
        
        # å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®åŠ¹ç‡çš„ãªåˆ¶é™è¨­å®š
        if len(data) > 1000000:  # 1MBä»¥ä¸Šã¯åˆ¶é™ã—ã¦é«˜é€ŸåŒ–
            data = data[:1000000]
        elif len(data) > 500000:  # 500KBä»¥ä¸Šã¯åˆ¶é™
            data = data[:500000]
            
        length = len(data)
        
        # åŠ¹ç‡çš„ãªæ­£æ–¹å½¢ã‚µã‚¤ã‚ºè¨ˆç®—
        side = int(np.ceil(np.sqrt(length)))
        padded_size = side * side
        pad_amount = padded_size - length
        
        # é«˜é€Ÿãªnumpyãƒãƒƒãƒ•ã‚¡æ“ä½œ
        padded_data = np.frombuffer(data, dtype=np.uint8)
        if pad_amount > 0:
            padded_data = np.concatenate([padded_data, np.zeros(pad_amount, dtype=np.uint8)])
        
        reshaped = padded_data.reshape(side, side).astype(np.int32)
        return reshaped, pad_amount
    
    def _detect_data_type(self, data: bytes) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—æ¤œå‡º - ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢å¯¾å¿œ"""
        if len(data) < 4: return "generic"
        if data.startswith(b'\xFF\xD8\xFF'): return "image"
        if data.startswith(b'\x89PNG'): return "image"
        if data.startswith(b'RIFF') and len(data) >= 12 and data[8:12] == b'WAVE': return "audio"
        if data.startswith(b'ID3') or data.startswith(b'\xFF\xFB'): return "audio"
        if len(data) >= 12 and data[4:8] == b'ftyp': return "video"
        return "generic"
    
    def _nexus_compress(self, data: np.ndarray) -> bytes:
        compressor = NexusCompressor(block_size=self.max_block_size, shape_types=self.shape_types, overlap_step=self.overlap_step, use_ai_optimization=self.use_ai)
        if self.use_ai:
            compressor.optimizer_model = self.optimizer_model
        return compressor.compress(data)
    
    def decompress(self, compressed_data: bytes) -> bytes:
        """å±•é–‹å‡¦ç†"""
        if self.verbose:
            print(f"ğŸ¯ NEXUSå±•é–‹é–‹å§‹: {len(compressed_data)} bytes")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼èª­ã¿è¾¼ã¿
        original_size, pad_size = struct.unpack('QQ', compressed_data[:16])
        nexus_compressed = compressed_data[16:]
        
        compressor = NexusCompressor()
        reconstructed = compressor.decompress(nexus_compressed)
        
        flat_data = reconstructed.flatten().astype(np.uint8)
        
        # æ­£ç¢ºãªã‚µã‚¤ã‚ºå¾©å…ƒã®ãŸã‚ã®ä¿®æ­£
        total_processed = len(flat_data)
        if pad_size > 0 and total_processed > pad_size:
            decompressed = flat_data[:-pad_size]
        else:
            decompressed = flat_data
            
        # å…ƒã‚µã‚¤ã‚ºã«æ­£ç¢ºã«å¾©å…ƒ
        if len(decompressed) > original_size:
            decompressed = decompressed[:original_size]
        elif len(decompressed) < original_size:
            # ä¸è¶³åˆ†ã‚’0ã§åŸ‹ã‚ã‚‹
            missing = original_size - len(decompressed)
            decompressed = np.concatenate([decompressed, np.zeros(missing, dtype=np.uint8)])
        
        # bytesã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        decompressed = bytes(decompressed)
        
        if self.verbose:
            print(f"âœ… å±•é–‹å®Œäº†: {len(decompressed)} bytes")
        
        return decompressed

class NexusCompressor:
    """NEXUSåœ§ç¸®ã‚³ã‚¢ - ç†è«–å®Ÿè£…"""
    
    def __init__(self, block_size=4, shape_types=list(TETRIS_SHAPES.keys()), overlap_step=4, use_ai_optimization=False):
        self.max_block_size = block_size
        self.shape_types = shape_types[:2]  # å½¢çŠ¶ã‚’2ã¤ã«åˆ¶é™ã—ã¦ã•ã‚‰ã«é«˜é€ŸåŒ–
        self.overlap_step = overlap_step
        self.use_ai = use_ai_optimization
        
        # AIç„¡åŠ¹åŒ–ã§é«˜é€ŸåŒ–
        if self.use_ai:
            self.optimizer_model = EnhancedShapeOptimizer(len(self.shape_types), self.max_block_size)
            with torch.no_grad():
                for layer in [self.optimizer_model.fc1, self.optimizer_model.fc2, self.optimizer_model.fc3]:
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.zeros_(layer.bias)
            self.optimizer_model.eval()

    def _extract_group(self, data, y, x, shape_mask, local_size):
        h, w = shape_mask.shape
        local_size = min(local_size, data.shape[0]-y, data.shape[1]-x)
        if h > local_size or w > local_size:
            h, w = min(h, local_size), min(w, local_size)
            shape_mask = shape_mask[:h, :w]
        padded_mask = np.zeros((local_size, local_size), dtype=bool)
        padded_mask[:h, :w] = shape_mask
        slice_y = slice(y, y+local_size)
        slice_x = slice(x, x+local_size)
        return data[slice_y, slice_x][padded_mask]

    def _normalize_group(self, group_values):
        if len(group_values) == 0: return None, None, None
        sort_indices = np.argsort(group_values)
        sorted_values = group_values[sort_indices]
        hash_key = hashlib.sha256(sorted_values.tobytes()).digest()[:16]  # ãƒãƒƒã‚·ãƒ¥çŸ­ç¸®ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰æ¸›
        return hash_key, sorted_values, sort_indices

    def _compute_features(self, patch):
        if patch.size == 0: return torch.zeros(6)
        var_val = np.var(patch)
        mean_val = np.mean(patch)
        std_val = np.std(patch)
        range_val = np.max(patch) - np.min(patch)
        skew_val = np.mean((patch - mean_val) ** 3) / (std_val ** 3 + 1e-8)
        kurt_val = np.mean((patch - mean_val) ** 4) / (std_val ** 4 + 1e-8)
        return torch.tensor([var_val, mean_val, std_val, range_val, skew_val, kurt_val], dtype=torch.float32)

    def _select_best_shape(self, data, y, x):
        max_size = min(self.max_block_size, data.shape[0]-y, data.shape[1]-x)
        if max_size < 2: return None, None, None
        
        # AIç„¡åŠ¹åŒ–ç‰ˆï¼šé«˜é€Ÿãªå›ºå®šé¸æŠ
        if not self.use_ai:
            # å›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³ã§é«˜é€ŸåŒ–
            shape_idx = (y + x) % len(self.shape_types)
            shape_name = self.shape_types[shape_idx]
            best_variant = 0  # æœ€åˆã®ãƒãƒªã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
            opt_size = max_size
            return shape_name, best_variant, opt_size
        
        # AIä½¿ç”¨ç‰ˆï¼ˆå…ƒã®ã‚³ãƒ¼ãƒ‰ï¼‰
        patch = data[y:y+self.max_block_size, x:x+self.max_block_size]
        features = self._compute_features(patch)
        pred = self.optimizer_model(features.unsqueeze(0))
        shape_idx = pred[0, :len(self.shape_types)].argmax().item()
        size_logits = pred[0, len(self.shape_types):]
        size_idx = size_logits.argmax().item() + 2
        opt_size = min(size_idx, max_size)
        shape_name = self.shape_types[shape_idx]
        best_variant = 0
        best_var = float('inf')
        for v_idx, variant in enumerate(ALL_SHAPES[shape_name]):
            group = self._extract_group(data, y, x, variant, opt_size)
            if len(group) < 2: continue
            var = np.var(group)
            if var < best_var:
                best_var = var
                best_variant = v_idx
        return shape_name, best_variant, opt_size

    def decompose_and_group(self, data):
        height, width = data.shape
        self.unique_blocks = {}
        self.design_map = []
        for y in range(0, height, self.overlap_step):
            for x in range(0, width, self.overlap_step):
                shape_name, variant_idx, opt_size = self._select_best_shape(data, y, x)
                if shape_name is None or opt_size is None: continue
                shape_mask = ALL_SHAPES[shape_name][variant_idx]
                group_values = self._extract_group(data, y, x, shape_mask, opt_size)
                hash_key, norm_values, perm = self._normalize_group(group_values)
                if hash_key is None: continue
                if hash_key not in self.unique_blocks:
                    self.unique_blocks[hash_key] = (shape_name, variant_idx, norm_values)
                self.design_map.append((y, x, hash_key, perm, shape_name, variant_idx, opt_size))

    def compress(self, data):
        self.decompose_and_group(data)
        header = struct.pack('3s B I I I I', b'NXZ', 1, self.max_block_size, len(self.unique_blocks), data.shape[0], data.shape[1])
        num_shapes = len(self.shape_types)
        shapes_data = b''
        for name in self.shape_types:
            name_b = name.encode('utf-8')
            shapes_data += struct.pack('B', len(name_b)) + name_b
        header += struct.pack('=B I', num_shapes, len(shapes_data)) + shapes_data
        unique_data = b''
        block_index = {hash_key: idx for idx, hash_key in enumerate(self.unique_blocks)}
        shape_to_idx = {name: i for i, name in enumerate(self.shape_types)}
        for hash_key, (shape_name, v_idx, norm_values) in self.unique_blocks.items():
            s_idx = shape_to_idx[shape_name]
            num_elems = len(norm_values)
            unique_data += struct.pack('=B B I', s_idx, v_idx, num_elems) + norm_values.tobytes()
        map_data = b''
        for y, x, hash_key, perm, shape_name, v_idx, opt_size in self.design_map:
            u_idx = block_index[hash_key]
            s_idx = shape_to_idx[shape_name]
            num_perm = len(perm)
            map_data += struct.pack('=I I I B B I I', y, x, u_idx, s_idx, v_idx, num_perm, opt_size)
            map_data += b''.join(struct.pack('H', p) for p in perm)
        return header + unique_data + map_data

    def decompress(self, compressed_data):
        offset = 0
        magic, version, max_block_size, num_unique, height, width = struct.unpack('3s B I I I I', compressed_data[offset:offset+20])
        offset += 20
        if magic != b'NXZ': raise ValueError("Invalid NXZ file")
        num_shapes, shapes_len = struct.unpack('=B I', compressed_data[offset:offset+5])
        offset += 5
        shapes_data = compressed_data[offset:offset+shapes_len]
        offset += shapes_len
        shape_types = []
        s_offset = 0
        while s_offset < shapes_len:
            name_len = shapes_data[s_offset]
            s_offset += 1
            name = shapes_data[s_offset:s_offset+name_len].decode('utf-8')
            shape_types.append(name)
            s_offset += name_len
        used_shapes = {name: ALL_SHAPES[name] for name in shape_types}
        unique_blocks = []
        for _ in range(num_unique):
            s_idx, v_idx, num_elems = struct.unpack('=B B I', compressed_data[offset:offset+6])
            offset += 6
            value_bytes = num_elems * 4
            values = np.frombuffer(compressed_data[offset:offset+value_bytes], dtype=np.int32)
            unique_blocks.append((shape_types[s_idx], v_idx, values))
            offset += value_bytes
        reconstructed = np.zeros((height, width), dtype=np.int32)
        count_map = np.zeros((height, width), dtype=np.int32)
        while offset < len(compressed_data):
            if len(compressed_data) - offset < 22: break
            y, x, u_idx, s_idx, v_idx, num_perm, opt_size = struct.unpack('=I I I B B I I', compressed_data[offset:offset+22])
            offset += 22
            perm_bytes = num_perm * 2
            if len(compressed_data) - offset < perm_bytes: break
            perm = np.array(struct.unpack(f'{num_perm}H', compressed_data[offset:offset+perm_bytes]))
            offset += perm_bytes
            shape_name, _, sorted_values = unique_blocks[u_idx]
            original_values = np.empty_like(sorted_values)
            original_values[perm] = sorted_values
            shape_mask = used_shapes[shape_name][v_idx]
            padded_mask = np.zeros((opt_size, opt_size), dtype=bool)
            h, w = min(shape_mask.shape[0], opt_size), min(shape_mask.shape[1], opt_size)
            padded_mask[:h, :w] = shape_mask[:h, :w]  # ã‚µã‚¤ã‚ºåˆ¶é™
            original_block = np.zeros((opt_size, opt_size), dtype=np.int32)
            original_block[padded_mask] = original_values
            end_y = min(y + opt_size, height)
            end_x = min(x + opt_size, width)
            reconstructed[y:end_y, x:end_x] += original_block[:end_y - y, :end_x - x]
            count_map[y:end_y, x:end_x] += padded_mask[:end_y - y, :end_x - x].astype(np.int32)
        reconstructed = np.where(count_map > 0, reconstructed // np.maximum(count_map, 1), reconstructed)  # ã‚¼ãƒ­é™¤ç®—å¯¾ç­–ã€å‰²ã‚Šç®—ã—ãªã„å ´åˆã¯å…ƒã®å€¤ã‚’ä¿æŒ
        print(f"   ğŸ” ãƒ‡ãƒãƒƒã‚°: å†æ§‹æˆå®Œäº†, å½¢çŠ¶: {reconstructed.shape}, ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {reconstructed.dtype}")
        return reconstructed

def test_multimedia_nexus():
    """ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢å¯¾å¿œNEXUSãƒ†ã‚¹ãƒˆ - å®Ÿéš›ã®sampleãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨"""
    
    print("ğŸ¯ NEXUSç†è«–å®Œå…¨å®Ÿè£…ç‰ˆãƒ†ã‚¹ãƒˆ (sampleãƒ•ã‚¡ã‚¤ãƒ«ç‰ˆ)")
    print("=" * 70)
    
    import os
    
    # é«˜é€Ÿãƒ†ã‚¹ãƒˆç”¨ï¼šå°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é–‹å§‹
    sample_files = [
        "generated-music-1752042054079.wav",  # ä¸­ã‚µã‚¤ã‚º
        "é™°è¬€è«–.mp3"  # å°ã‚µã‚¤ã‚º
    ]
    
    test_cases = []
    for filename in sample_files:
        filepath = os.path.join(".", filename)
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                data = f.read()
                # åŠ¹ç‡æ€§ã®ãŸã‚ä¸€éƒ¨ã®ã¿ãƒ†ã‚¹ãƒˆ
                if len(data) > 100000:  # 100KBã«åˆ¶é™
                    data = data[:100000]
            test_cases.append((f"{filename}ï¼ˆåˆ¶é™ç‰ˆï¼‰", data))
    
    # è¿½åŠ ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
    test_cases.extend([
        ("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿", b"This is plain text data for compression testing. " * 50),
        ("ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿", bytes(range(256)) * 10)
    ])
    
    engine = MultimediaNexusCompressor(
        block_size=16,  # ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’å¤§å¹…ã«å¢—åŠ ã—ã¦é«˜é€ŸåŒ–
        overlap_step=32,  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—ã‚’å¤§å¹…ã«å¢—åŠ 
        use_ai_optimization=False,  # AIç„¡åŠ¹åŒ–ã§é«˜é€ŸåŒ–
        verbose=True
    )
    
    success_count = 0
    
    for test_name, test_data in test_cases:
        print(f"\nğŸ“ {test_name}: {len(test_data)} bytes")
        
        compressed = engine.compress(test_data)
        decompressed = engine.decompress(compressed)
        
        print(f"   ğŸ” å…ƒãƒ‡ãƒ¼ã‚¿æœ€åˆ10å€‹: {test_data[:10] if len(test_data) > 10 else test_data}")
        print(f"   ğŸ” å±•é–‹ãƒ‡ãƒ¼ã‚¿æœ€åˆ10å€‹: {decompressed[:10] if len(decompressed) > 10 else decompressed}")
        
        ratio = len(compressed) / len(test_data) if len(test_data) > 0 else 0
        match = np.array_equal(test_data, decompressed)  # numpyé…åˆ—æ¯”è¼ƒã‚’ä½¿ç”¨
        print(f"   åœ§ç¸®ç‡: {ratio:.1%}")
        print(f"   çµæœ: {'âœ… æˆåŠŸ' if match else 'âŒ å¤±æ•—'} (ä¸€è‡´: {match})")
        
        if match:
            success_count += 1
    
    success_rate = success_count / len(test_cases)
    print(f"\nğŸ† çµæœ: {success_count}/{len(test_cases)} ({success_rate:.1%})")
    
    if success_rate == 1.0:
        print(f"ğŸ‰ NEXUSç†è«–ã®å®Ÿè£…ãŒå†æ§‹æˆã•ã‚Œã¾ã—ãŸï¼")
        print(f"   âœ… ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢å¯¾å¿œ")
        print(f"   âœ… AIæœ€é©åŒ–å®Ÿè£…")
        print(f"   âœ… Polyominoå½¢çŠ¶æ´»ç”¨")
    
    return success_rate

if __name__ == "__main__":
    test_multimedia_nexus()
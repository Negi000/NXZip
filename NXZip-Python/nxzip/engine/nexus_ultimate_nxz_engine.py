#!/usr/bin/env python3
"""
NEXUS Ultimate NXZ Engine - æœ€çµ‚æœ€é©åŒ–ç‰ˆ
TMC + SPE + NXZ + ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–æœ€é©åŒ–ã§7Z/Zstdã«å‹åˆ©
"""

import os
import sys
import time
import struct
import hashlib
import secrets
from typing import Tuple, Dict, Any, List, Optional, Union
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.primitives import hashes, kdf
from cryptography.hazmat.backends import default_backend
import numpy as np
import lzma
import zlib
import bz2

# æ”¹è‰¯ç‰ˆTMCã‚¨ãƒ³ã‚¸ãƒ³ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from nexus_tmc_engine import NEXUSTMCEngine, DataType
    TMC_AVAILABLE = True
    print("âœ… ãƒ•ãƒ«ç‰ˆTMCã‚¨ãƒ³ã‚¸ãƒ³åˆ©ç”¨å¯èƒ½")
except ImportError:
    print("âš ï¸ ãƒ•ãƒ«ç‰ˆTMCã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - æœ€é©åŒ–ç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
    TMC_AVAILABLE = False


class UltimateTMCEngine:
    """ç©¶æ¥µã®TMCã‚¨ãƒ³ã‚¸ãƒ³ - æœ€å¤§åœ§ç¸®ç‡ã‚’ç›®æŒ‡ã™"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.compression_cache = {}
        
        # åˆ©ç”¨å¯èƒ½ãªåœ§ç¸®æ–¹å¼
        self.compressors = {
            'lzma_ultra': lambda data: lzma.compress(data, preset=9),
            'zlib_ultra': lambda data: zlib.compress(data, level=9),
            'bz2_ultra': lambda data: bz2.compress(data, compresslevel=9),
        }
        
        # TMCã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯è¿½åŠ 
        if TMC_AVAILABLE:
            self.original_tmc = NEXUSTMCEngine(max_workers)
        else:
            self.original_tmc = None
    
    def compress_ultimate(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """ç©¶æ¥µã®åœ§ç¸® - è¤‡æ•°æ–¹å¼ã§æœ€é©è§£ã‚’é¸æŠ"""
        start_time = time.perf_counter()
        
        try:
            if len(data) == 0:
                return data, {'method': 'empty', 'compression_ratio': 0}
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®è©³ç´°åˆ†æ
            structure_analysis = self._comprehensive_data_analysis(data)
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: æ§‹é€ ã«åŸºã¥ãå‰å‡¦ç†
            preprocessed_data = self._structure_aware_preprocessing(data, structure_analysis)
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: è¤‡æ•°ã®åœ§ç¸®æ–¹å¼ã§ä¸¦åˆ—ãƒ†ã‚¹ãƒˆ
            compression_results = []
            
            # ã‚ªãƒªã‚¸ãƒŠãƒ«TMCã‚’æœ€å„ªå…ˆã§è©¦è¡Œ
            if self.original_tmc:
                try:
                    tmc_compressed, tmc_info = self.original_tmc.compress_tmc(preprocessed_data)
                    compression_results.append({
                        'method': 'tmc_original',
                        'data': tmc_compressed,
                        'size': len(tmc_compressed),
                        'info': tmc_info
                    })
                except Exception as e:
                    print(f"TMCåœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ä»–ã®åœ§ç¸®æ–¹å¼ã‚‚è©¦è¡Œ
            for method_name, compressor in self.compressors.items():
                try:
                    compressed = compressor(preprocessed_data)
                    compression_results.append({
                        'method': method_name,
                        'data': compressed,
                        'size': len(compressed),
                        'info': {'compression_ratio': (1 - len(compressed) / len(data)) * 100}
                    })
                except Exception as e:
                    continue
            
            # ã‚«ã‚¹ã‚¿ãƒ åœ§ç¸®ã‚‚è¿½åŠ 
            custom_compressed = self._custom_structure_compression(preprocessed_data, structure_analysis)
            if custom_compressed:
                compression_results.append({
                    'method': 'custom_structure',
                    'data': custom_compressed,
                    'size': len(custom_compressed),
                    'info': {'compression_ratio': (1 - len(custom_compressed) / len(data)) * 100}
                })
            
            # æœ€è‰¯ã®çµæœã‚’é¸æŠ
            if compression_results:
                best_result = min(compression_results, key=lambda x: x['size'])
                
                # å¾Œå‡¦ç†æœ€é©åŒ–
                final_data = self._post_processing_optimization(best_result['data'])
                
                processing_time = time.perf_counter() - start_time
                
                final_info = {
                    'original_size': len(data),
                    'compressed_size': len(final_data),
                    'compression_ratio': (1 - len(final_data) / len(data)) * 100 if len(data) > 0 else 0,
                    'processing_time': processing_time,
                    'best_method': best_result['method'],
                    'structure_analysis': structure_analysis,
                    'alternatives_tested': len(compression_results),
                    'preprocessing_applied': True,
                    'postprocessing_applied': True,
                    'tmc_version': 'ultimate_v1'
                }
                
                return final_data, final_info
            else:
                # åœ§ç¸®å¤±æ•—æ™‚ã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿è¿”å´
                return data, {
                    'error': 'all_compression_failed',
                    'compression_ratio': 0,
                    'processing_time': time.perf_counter() - start_time
                }
                
        except Exception as e:
            return data, {
                'error': str(e),
                'compression_ratio': 0,
                'processing_time': time.perf_counter() - start_time
            }
    
    def _comprehensive_data_analysis(self, data: bytes) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿æ§‹é€ åˆ†æ"""
        analysis = {
            'size': len(data),
            'entropy': 0,
            'patterns': [],
            'repetition_score': 0,
            'ascii_ratio': 0,
            'binary_score': 0,
            'structure_hints': []
        }
        
        if len(data) == 0:
            return analysis
        
        try:
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
            byte_counts = np.bincount(np.frombuffer(data[:min(8192, len(data))], dtype=np.uint8), minlength=256)
            probabilities = byte_counts / len(data)
            probabilities = probabilities[probabilities > 0]
            analysis['entropy'] = float(-np.sum(probabilities * np.log2(probabilities)))
            
            # ASCIIæ¯”ç‡
            ascii_count = sum(1 for b in data[:min(1000, len(data))] if 32 <= b <= 126)
            analysis['ascii_ratio'] = ascii_count / min(1000, len(data))
            
            # åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            sample = data[:min(4096, len(data))]
            unique_ratio = len(set(sample)) / len(sample)
            analysis['repetition_score'] = 1 - unique_ratio
            
            # æ§‹é€ ãƒ’ãƒ³ãƒˆæ¤œå‡º
            if b'{' in data or b'}' in data:
                analysis['structure_hints'].append('json_like')
            if b'<' in data or b'>' in data:
                analysis['structure_hints'].append('xml_like')
            if data.startswith(b'\x89PNG') or data.startswith(b'\xff\xd8'):
                analysis['structure_hints'].append('image_file')
            if data.startswith(b'PK'):
                analysis['structure_hints'].append('archive_file')
            
            # å‘¨æœŸæ€§æ¤œå‡º
            for period in [2, 4, 8, 16, 32]:
                if len(data) >= period * 8:
                    correlation = self._detect_periodicity(data, period)
                    if correlation > 0.3:
                        analysis['patterns'].append({
                            'type': 'periodic',
                            'period': period,
                            'strength': correlation
                        })
            
        except Exception:
            pass
        
        return analysis
    
    def _detect_periodicity(self, data: bytes, period: int) -> float:
        """å‘¨æœŸæ€§æ¤œå‡º"""
        try:
            data_array = np.frombuffer(data, dtype=np.uint8)
            
            if len(data_array) < period * 4:
                return 0.0
            
            correlations = []
            for offset in range(period):
                values = data_array[offset::period]
                if len(values) > 1:
                    correlation = np.corrcoef(values[:-1], values[1:])[0, 1]
                    if not np.isnan(correlation):
                        correlations.append(abs(correlation))
            
            return np.mean(correlations) if correlations else 0.0
            
        except Exception:
            return 0.0
    
    def _structure_aware_preprocessing(self, data: bytes, analysis: Dict) -> bytes:
        """æ§‹é€ ã‚’æ„è­˜ã—ãŸå‰å‡¦ç†"""
        try:
            processed_data = data
            
            # é«˜åå¾©ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
            if analysis['repetition_score'] > 0.7:
                processed_data = self._apply_rle_preprocessing(processed_data)
            
            # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
            if analysis['entropy'] < 4.0:
                processed_data = self._apply_differential_encoding(processed_data)
            
            # å‘¨æœŸãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹å ´åˆ
            if analysis['patterns']:
                strongest_pattern = max(analysis['patterns'], key=lambda p: p['strength'])
                if strongest_pattern['strength'] > 0.5:
                    processed_data = self._apply_periodic_preprocessing(processed_data, strongest_pattern['period'])
            
            return processed_data
            
        except Exception:
            return data
    
    def _apply_rle_preprocessing(self, data: bytes) -> bytes:
        """Run-Length Encodingå‰å‡¦ç†"""
        try:
            result = bytearray()
            if len(data) == 0:
                return data
            
            current_byte = data[0]
            count = 1
            
            for i in range(1, len(data)):
                if data[i] == current_byte and count < 255:
                    count += 1
                else:
                    if count > 3:  # 3å›ä»¥ä¸Šã®ç¹°ã‚Šè¿”ã—ã®ã¿RLEåŒ–
                        result.extend([0xFF, current_byte, count])
                    else:
                        result.extend([current_byte] * count)
                    
                    current_byte = data[i]
                    count = 1
            
            # æœ€å¾Œã®è¦ç´ 
            if count > 3:
                result.extend([0xFF, current_byte, count])
            else:
                result.extend([current_byte] * count)
            
            return bytes(result) if len(result) < len(data) else data
            
        except Exception:
            return data
    
    def _apply_differential_encoding(self, data: bytes) -> bytes:
        """å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            if len(data) < 2:
                return data
            
            result = bytearray([data[0]])  # æœ€åˆã®å€¤ã¯ãã®ã¾ã¾
            
            for i in range(1, len(data)):
                diff = (data[i] - data[i-1]) % 256
                result.append(diff)
            
            return bytes(result)
            
        except Exception:
            return data
    
    def _apply_periodic_preprocessing(self, data: bytes, period: int) -> bytes:
        """å‘¨æœŸçš„å‰å‡¦ç†"""
        try:
            if len(data) < period * 2:
                return data
            
            # å‘¨æœŸã”ã¨ã«åˆ†é›¢ã—ã¦ãã‚Œãã‚Œã‚’åœ§ç¸®ã—ã‚„ã™ãå¤‰æ›
            streams = [bytearray() for _ in range(period)]
            
            for i, byte in enumerate(data):
                streams[i % period].append(byte)
            
            # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é©ç”¨
            processed_streams = []
            for stream in streams:
                if len(stream) > 1:
                    diff_stream = bytearray([stream[0]])
                    for j in range(1, len(stream)):
                        diff = (stream[j] - stream[j-1]) % 256
                        diff_stream.append(diff)
                    processed_streams.append(diff_stream)
                else:
                    processed_streams.append(stream)
            
            # å†æ§‹æˆ
            result = bytearray()
            max_len = max(len(s) for s in processed_streams) if processed_streams else 0
            
            for i in range(max_len):
                for stream in processed_streams:
                    if i < len(stream):
                        result.append(stream[i])
            
            return bytes(result) if len(result) < len(data) else data
            
        except Exception:
            return data
    
    def _custom_structure_compression(self, data: bytes, analysis: Dict) -> Optional[bytes]:
        """ã‚«ã‚¹ã‚¿ãƒ æ§‹é€ ç‰¹åŒ–åœ§ç¸®"""
        try:
            # JSONé¢¨ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
            if 'json_like' in analysis['structure_hints'] and analysis['ascii_ratio'] > 0.8:
                return self._compress_json_like(data)
            
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            if 'image_file' in analysis['structure_hints']:
                return self._compress_image_like(data)
            
            # é«˜ã„å‘¨æœŸæ€§ãŒã‚ã‚‹å ´åˆ
            if analysis['patterns'] and max(p['strength'] for p in analysis['patterns']) > 0.8:
                return self._compress_highly_periodic(data, analysis['patterns'])
            
            return None
            
        except Exception:
            return None
    
    def _compress_json_like(self, data: bytes) -> bytes:
        """JSONé¢¨ãƒ‡ãƒ¼ã‚¿ç‰¹åŒ–åœ§ç¸®"""
        try:
            # ä¸€èˆ¬çš„ãªJSONæ–‡å­—åˆ—ã‚’çŸ­ç¸®
            replacements = [
                (b'":', b'\x01'),
                (b'",', b'\x02'),
                (b'{"', b'\x03'),
                (b'"}', b'\x04'),
                (b'true', b'\x05'),
                (b'false', b'\x06'),
                (b'null', b'\x07'),
            ]
            
            compressed = data
            for original, replacement in replacements:
                compressed = compressed.replace(original, replacement)
            
            return lzma.compress(compressed, preset=9)
            
        except Exception:
            return lzma.compress(data, preset=9)
    
    def _compress_image_like(self, data: bytes) -> bytes:
        """ç”»åƒé¢¨ãƒ‡ãƒ¼ã‚¿ç‰¹åŒ–åœ§ç¸®"""
        try:
            # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«åœ§ç¸®ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ãŸã‚
            # ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†ã¨å®Ÿãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã‚’åˆ†é›¢ã—ã¦å‡¦ç†
            header_size = min(512, len(data))
            header = data[:header_size]
            body = data[header_size:]
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã¯é€šå¸¸ã®åœ§ç¸®
            compressed_header = lzma.compress(header, preset=6)
            
            # ãƒœãƒ‡ã‚£ã¯è»½ã„åœ§ç¸®ã®ã¿
            compressed_body = zlib.compress(body, level=1)
            
            return compressed_header + compressed_body
            
        except Exception:
            return zlib.compress(data, level=1)
    
    def _compress_highly_periodic(self, data: bytes, patterns: List[Dict]) -> bytes:
        """é«˜å‘¨æœŸæ€§ãƒ‡ãƒ¼ã‚¿ç‰¹åŒ–åœ§ç¸®"""
        try:
            # æœ€å¼·ã®å‘¨æœŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨
            best_pattern = max(patterns, key=lambda p: p['strength'])
            period = best_pattern['period']
            
            # å‘¨æœŸã”ã¨ã«åˆ†é›¢
            streams = [bytearray() for _ in range(period)]
            
            for i, byte in enumerate(data):
                streams[i % period].append(byte)
            
            # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å€‹åˆ¥åœ§ç¸®
            compressed_streams = []
            for stream in streams:
                if len(stream) > 0:
                    compressed_stream = lzma.compress(bytes(stream), preset=9)
                    compressed_streams.append(compressed_stream)
            
            # çµåˆ
            result = struct.pack('<I', period)  # å‘¨æœŸæƒ…å ±
            for compressed_stream in compressed_streams:
                result += struct.pack('<I', len(compressed_stream))
                result += compressed_stream
            
            return result
            
        except Exception:
            return lzma.compress(data, preset=9)
    
    def _post_processing_optimization(self, data: bytes) -> bytes:
        """å¾Œå‡¦ç†æœ€é©åŒ–"""
        try:
            # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã•ã‚‰ãªã‚‹åœ§ç¸®ã‚’è©¦è¡Œ
            if len(data) < 1024:
                optimized_candidates = [
                    zlib.compress(data, level=9),
                    lzma.compress(data, preset=9),
                    bz2.compress(data, compresslevel=9)
                ]
                
                best = min([data] + optimized_candidates, key=len)
                return best
            
            return data
            
        except Exception:
            return data


# SPEã‚¨ãƒ³ã‚¸ãƒ³ã¨NXZãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ä»¥å‰ã¨åŒã˜
from nexus_spe_integrated_engine import SPEEngine, NXZFormat


class NEXUSUltimateEngine:
    """NEXUSç©¶æ¥µã‚¨ãƒ³ã‚¸ãƒ³ - æœ€å¤§æ€§èƒ½"""
    
    def __init__(self, max_workers: int = 4, encryption_enabled: bool = True):
        self.max_workers = max_workers
        self.encryption_enabled = encryption_enabled
        
        # ç©¶æ¥µTMCã‚¨ãƒ³ã‚¸ãƒ³
        self.ultimate_tmc = UltimateTMCEngine(max_workers)
        
        # SPEã‚¨ãƒ³ã‚¸ãƒ³
        self.spe_engine = SPEEngine()
        
        # çµ±è¨ˆ
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_output_size': 0,
            'compression_wins': 0,
            'best_method_count': {}
        }
    
    def compress_to_nxz_ultimate(self, data: bytes, password: str = None, 
                                metadata: Dict = None) -> Tuple[bytes, Dict[str, Any]]:
        """ç©¶æ¥µã®NXZåœ§ç¸®"""
        start_time = time.perf_counter()
        
        try:
            original_size = len(data)
            
            # ç©¶æ¥µTMCåœ§ç¸®
            compressed_data, compression_info = self.ultimate_tmc.compress_ultimate(data)
            
            # SPEæš—å·åŒ–
            if password and self.encryption_enabled:
                salt = secrets.token_bytes(32)
                key = self.spe_engine.derive_key(password, salt)
                
                encrypted_data, encryption_info = self.spe_engine.structure_preserving_encrypt(
                    compressed_data, key
                )
                
                encrypted_with_salt = salt + encrypted_data
                encryption_info['salt_size'] = len(salt)
                final_payload = encrypted_with_salt
            else:
                final_payload = compressed_data
                encryption_info = {'encryption_method': 'none'}
            
            # NXZãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
            enhanced_metadata = metadata or {}
            enhanced_metadata.update({
                'nexus_version': 'Ultimate_v1',
                'tmc_engine': 'ultimate',
                'compression_optimization': 'maximum'
            })
            
            nxz_header = NXZFormat.create_nxz_header(
                original_size, compression_info, encryption_info, enhanced_metadata
            )
            
            # æœ€çµ‚NXZ
            nxz_data = nxz_header + final_payload
            
            total_time = time.perf_counter() - start_time
            
            # çµ±è¨ˆæ›´æ–°
            self._update_stats(data, nxz_data, compression_info)
            
            # çµæœæƒ…å ±
            result_info = {
                'original_size': original_size,
                'compressed_size': len(compressed_data),
                'encrypted_size': len(final_payload) if password else len(compressed_data),
                'final_nxz_size': len(nxz_data),
                'header_size': len(nxz_header),
                'total_compression_ratio': (1 - len(nxz_data) / original_size) * 100 if original_size > 0 else 0,
                'processing_time': total_time,
                'throughput_mb_s': (original_size / 1024 / 1024) / total_time if total_time > 0 else 0,
                'encrypted': bool(password and self.encryption_enabled),
                'compression_info': compression_info,
                'encryption_info': encryption_info,
                'nxz_version': NXZFormat.VERSION,
                'format': 'nxz_ultimate',
                'engine_version': 'ultimate_v1'
            }
            
            return nxz_data, result_info
            
        except Exception as e:
            total_time = time.perf_counter() - start_time
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®æœ€å°é™NXZ
            error_header = NXZFormat.MAGIC_NUMBER + struct.pack('<H', NXZFormat.VERSION)
            error_nxz = error_header + data
            
            return error_nxz, {
                'error': str(e),
                'processing_time': total_time,
                'format': 'nxz_error',
                'original_size': len(data),
                'final_nxz_size': len(error_nxz)
            }
    
    def _update_stats(self, original: bytes, compressed: bytes, compression_info: Dict):
        """çµ±è¨ˆæ›´æ–°"""
        try:
            self.stats['files_processed'] += 1
            self.stats['total_input_size'] += len(original)
            self.stats['total_output_size'] += len(compressed)
            
            if len(compressed) < len(original):
                self.stats['compression_wins'] += 1
            
            method = compression_info.get('best_method', 'unknown')
            self.stats['best_method_count'][method] = \
                self.stats['best_method_count'].get(method, 0) + 1
                
        except Exception:
            pass
    
    def get_ultimate_stats(self) -> Dict[str, Any]:
        """ç©¶æ¥µçµ±è¨ˆå–å¾—"""
        try:
            if self.stats['files_processed'] == 0:
                return {'status': 'no_data'}
            
            total_compression_ratio = (1 - self.stats['total_output_size'] / self.stats['total_input_size']) * 100
            win_rate = (self.stats['compression_wins'] / self.stats['files_processed']) * 100
            
            return {
                'files_processed': self.stats['files_processed'],
                'total_input_mb': self.stats['total_input_size'] / 1024 / 1024,
                'total_compression_ratio': total_compression_ratio,
                'compression_win_rate': win_rate,
                'best_methods': self.stats['best_method_count'],
                'nexus_version': 'Ultimate_v1',
                'format': 'NXZ Ultimate'
            }
            
        except Exception:
            return {'status': 'error'}


# ãƒ†ã‚¹ãƒˆé–¢æ•°
if __name__ == "__main__":
    print("ğŸš€ NEXUS Ultimate Engine - æœ€çµ‚æ±ºæˆ¦ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # ç©¶æ¥µã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    engine = NEXUSUltimateEngine(max_workers=4, encryption_enabled=True)
    
    # ã‚ˆã‚Šè¤‡é›‘ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_data = b'{"users": [{"id": 1, "name": "test", "active": true}, ' \
                b'{"id": 2, "name": "demo", "active": false}]} ' * 1000 + \
                b'REPEATED_PATTERN' * 500 + \
                bytes(range(256)) * 20
    
    print(f"ç©¶æ¥µãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(test_data)} bytes")
    
    # ç©¶æ¥µåœ§ç¸®ãƒ†ã‚¹ãƒˆ
    print("\nğŸ”¥ ç©¶æ¥µåœ§ç¸®ãƒ†ã‚¹ãƒˆ...")
    start_time = time.perf_counter()
    nxz_data, info = engine.compress_to_nxz_ultimate(test_data, "ultimate_password_2024")
    end_time = time.perf_counter()
    
    print(f"åœ§ç¸®ç‡: {info['total_compression_ratio']:.2f}%")
    print(f"å‡¦ç†æ™‚é–“: {info['processing_time']*1000:.1f}ms")
    print(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {info['throughput_mb_s']:.2f}MB/s")
    print(f"æœ€çµ‚ã‚µã‚¤ã‚º: {info['final_nxz_size']} bytes")
    print(f"ä½¿ç”¨æ–¹å¼: {info['compression_info'].get('best_method', 'unknown')}")
    print(f"å‰å‡¦ç†é©ç”¨: {'âœ…' if info['compression_info'].get('preprocessing_applied', False) else 'âŒ'}")
    print(f"å¾Œå‡¦ç†é©ç”¨: {'âœ…' if info['compression_info'].get('postprocessing_applied', False) else 'âŒ'}")
    
    # çµ±è¨ˆè¡¨ç¤º
    stats = engine.get_ultimate_stats()
    print(f"\nğŸ“Š ç©¶æ¥µçµ±è¨ˆ:")
    print(f"   åœ§ç¸®å‹ç‡: {stats.get('compression_win_rate', 0):.1f}%")
    print(f"   ä½¿ç”¨æ–¹å¼: {stats.get('best_methods', {})}")
    
    print(f"\nğŸ¯ NEXUS Ultimateç‰¹å¾´:")
    print(f"   âœ“ è¤‡æ•°æ–¹å¼ä¸¦åˆ—ãƒ†ã‚¹ãƒˆ")
    print(f"   âœ“ æ§‹é€ ç‰¹åŒ–å‰å‡¦ç†")
    print(f"   âœ“ å¾Œå‡¦ç†æœ€é©åŒ–")
    print(f"   âœ“ TMC + SPE + NXZçµ±åˆ")
    print(f"   âœ“ ç©¶æ¥µåœ§ç¸®ç‡è¿½æ±‚")

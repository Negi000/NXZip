#!/usr/bin/env python3
"""
NEXUS Optimized Engine v4.0 - é«˜é€Ÿæœ€é©åŒ–ç‰ˆ
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã‚’è§£æ±ºã—ã€å®Ÿç”¨æ€§ã‚’é‡è¦–ã—ãŸå®Ÿè£…
"""

import numpy as np
import lzma
import zlib
import bz2
import struct
import time
import hashlib
from typing import Dict, List, Tuple, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from dataclasses import dataclass
from pathlib import Path
import io


@dataclass
class OptimizedConfig:
    """æœ€é©åŒ–è¨­å®š"""
    # åŸºæœ¬è¨­å®š
    max_threads: int = 4  # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’å‰Šæ¸›
    chunk_size_mb: float = 0.5  # ã‚ˆã‚Šå°ã•ãªãƒãƒ£ãƒ³ã‚¯
    memory_limit_gb: float = 4.0
    
    # é«˜é€ŸåŒ–è¨­å®š
    fast_mode: bool = True
    skip_deep_analysis: bool = False  # æ·±å±¤è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½
    simple_compression: bool = False  # ã‚·ãƒ³ãƒ—ãƒ«åœ§ç¸®ãƒ¢ãƒ¼ãƒ‰
    
    # å“è³ªè¨­å®š
    compression_level: int = 6  # LZMAåœ§ç¸®ãƒ¬ãƒ™ãƒ«
    enable_preprocessing: bool = True
    enable_postprocessing: bool = True


class FastPatternAnalyzer:
    """é«˜é€Ÿãƒ‘ã‚¿ãƒ¼ãƒ³è§£æå™¨ - è»½é‡ç‰ˆ"""
    
    def __init__(self):
        self.analysis_cache = {}
        self.max_cache_size = 100
    
    def quick_analyze(self, data: bytes, file_type: str) -> Dict[str, Any]:
        """é«˜é€Ÿè§£æ - æœ€å°é™ã®å‡¦ç†"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        data_hash = hashlib.md5(data[:1024]).hexdigest()  # å…ˆé ­1KBã®ã¿
        if data_hash in self.analysis_cache:
            return self.analysis_cache[data_hash]
        
        result = {
            'file_type': file_type,
            'size': len(data),
            'entropy': self._quick_entropy(data[:4096]),  # å…ˆé ­4KBã®ã¿
            'compression_strategy': self._select_strategy(file_type, len(data)),
            'optimization_potential': self._estimate_potential(file_type)
        }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
        if len(self.analysis_cache) >= self.max_cache_size:
            self.analysis_cache.clear()
        
        self.analysis_cache[data_hash] = result
        return result
    
    def _quick_entropy(self, data: bytes) -> float:
        """é«˜é€Ÿã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data) < 256:
            return 0.5
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦é«˜é€ŸåŒ–
        sample_size = min(1024, len(data))
        sample = data[:sample_size:max(1, len(data)//sample_size)]
        
        if len(sample) == 0:
            return 0.5
        
        # ç°¡æ˜“ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        unique_bytes = len(set(sample))
        return unique_bytes / 256.0
    
    def _select_strategy(self, file_type: str, size: int) -> str:
        """åœ§ç¸®æˆ¦ç•¥é¸æŠ"""
        if file_type in ['åœ§ç¸®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–']:
            return 'minimal'  # æ—¢åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã¯è»½å¾®ãªå‡¦ç†
        elif file_type in ['ãƒ†ã‚­ã‚¹ãƒˆ']:
            return 'text_optimized'
        elif file_type in ['ç”»åƒ']:
            return 'image_optimized' if size > 1024*1024 else 'standard'
        elif file_type in ['éŸ³æ¥½', 'å‹•ç”»']:
            return 'multimedia_optimized' if size > 5*1024*1024 else 'standard'
        else:
            return 'standard'
    
    def _estimate_potential(self, file_type: str) -> float:
        """æœ€é©åŒ–ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«æ¨å®š"""
        potentials = {
            'ãƒ†ã‚­ã‚¹ãƒˆ': 0.8,
            'ç”»åƒ': 0.3,
            'éŸ³æ¥½': 0.2,
            'å‹•ç”»': 0.15,
            'åœ§ç¸®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–': 0.05,
            'ãã®ä»–': 0.4
        }
        return potentials.get(file_type, 0.3)


class OptimizedCompressionEngine:
    """æœ€é©åŒ–åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: OptimizedConfig):
        self.config = config
        self.analyzer = FastPatternAnalyzer()
        self.compression_stats = {
            'total_chunks': 0,
            'avg_compression_ratio': 0.0,
            'total_time': 0.0
        }
    
    def compress_chunk(self, chunk_data: bytes, strategy: str, chunk_id: int) -> bytes:
        """ãƒãƒ£ãƒ³ã‚¯åœ§ç¸® - æˆ¦ç•¥åˆ¥æœ€é©åŒ–"""
        try:
            if strategy == 'minimal':
                return self._minimal_compression(chunk_data)
            elif strategy == 'text_optimized':
                return self._text_compression(chunk_data)
            elif strategy == 'image_optimized':
                return self._image_compression(chunk_data)
            elif strategy == 'multimedia_optimized':
                return self._multimedia_compression(chunk_data)
            else:
                return self._standard_compression(chunk_data)
                
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._standard_compression(chunk_data)
    
    def _minimal_compression(self, data: bytes) -> bytes:
        """æœ€å°åœ§ç¸® - æ—¢åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ç”¨"""
        # ãƒ¬ãƒ™ãƒ«1ã®LZMAï¼ˆæœ€é«˜é€Ÿï¼‰
        try:
            return b'MIN1' + lzma.compress(data, preset=1)
        except:
            return b'MIN0' + zlib.compress(data, level=1)
    
    def _text_compression(self, data: bytes) -> bytes:
        """ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–åœ§ç¸®"""
        try:
            # å‰å‡¦ç†ï¼šæ”¹è¡Œæ­£è¦åŒ–
            if b'\r\n' in data:
                data = data.replace(b'\r\n', b'\n')
            
            # é«˜åœ§ç¸®LZMA
            compressed = lzma.compress(data, preset=self.config.compression_level)
            return b'TXT' + struct.pack('<I', len(data)) + compressed
        except:
            return b'TXT0' + zlib.compress(data, level=9)
    
    def _image_compression(self, data: bytes) -> bytes:
        """ç”»åƒæœ€é©åŒ–åœ§ç¸®"""
        try:
            # JPEG/PNGç‰¹åŒ–å‡¦ç†
            if data[:4] == b'\xff\xd8\xff\xe0':  # JPEG
                return self._jpeg_optimized_compression(data)
            elif data[:8] == b'\x89PNG\r\n\x1a\n':  # PNG
                return self._png_optimized_compression(data)
            else:
                return b'IMG0' + lzma.compress(data, preset=3)
        except:
            return b'IMG0' + zlib.compress(data, level=6)
    
    def _jpeg_optimized_compression(self, data: bytes) -> bytes:
        """JPEGæœ€é©åŒ–åœ§ç¸®"""
        try:
            # EXIFå‰Šé™¤ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            if len(data) > 2 and data[:2] == b'\xff\xd8':
                # APP1ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ¤œç´¢
                pos = 2
                while pos < len(data) - 4:
                    if data[pos:pos+2] == b'\xff\xe1':  # APP1 (EXIF)
                        segment_length = struct.unpack('>H', data[pos+2:pos+4])[0]
                        # EXIFã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
                        data = data[:pos] + data[pos+2+segment_length:]
                        break
                    pos += 1
            
            return b'JPEG' + lzma.compress(data, preset=3)
        except:
            return b'JPEG' + zlib.compress(data, level=6)
    
    def _png_optimized_compression(self, data: bytes) -> bytes:
        """PNGæœ€é©åŒ–åœ§ç¸®"""
        try:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            optimized_data = data
            
            # tEXt, zTXt, iTXtãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤
            for chunk_type in [b'tEXt', b'zTXt', b'iTXt']:
                optimized_data = self._remove_png_chunks(optimized_data, chunk_type)
            
            return b'PNG ' + lzma.compress(optimized_data, preset=3)
        except:
            return b'PNG ' + zlib.compress(data, level=6)
    
    def _remove_png_chunks(self, data: bytes, chunk_type: bytes) -> bytes:
        """PNGãƒãƒ£ãƒ³ã‚¯å‰Šé™¤"""
        if len(data) < 8:
            return data
        
        result = data[:8]  # PNGã‚·ã‚°ãƒãƒãƒ£ä¿æŒ
        pos = 8
        
        while pos < len(data) - 8:
            try:
                length = struct.unpack('>I', data[pos:pos+4])[0]
                type_bytes = data[pos+4:pos+8]
                
                if type_bytes == chunk_type:
                    # ã“ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    pos += 8 + length + 4  # length + type + data + CRC
                else:
                    # ã“ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿æŒ
                    chunk_end = pos + 8 + length + 4
                    result += data[pos:chunk_end]
                    pos = chunk_end
            except:
                break
        
        return result
    
    def _multimedia_compression(self, data: bytes) -> bytes:
        """ãƒãƒ«ãƒãƒ¡ãƒ‡ã‚£ã‚¢æœ€é©åŒ–åœ§ç¸®"""
        try:
            # ID3ã‚¿ã‚°é™¤å»ï¼ˆMP3ï¼‰
            if data[:3] == b'ID3':
                # ID3v2ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºè¨ˆç®—
                size = struct.unpack('>I', b'\x00' + data[6:9])[0]
                data = data[10 + size:]
            
            return b'MUL ' + lzma.compress(data, preset=3)
        except:
            return b'MUL ' + zlib.compress(data, level=6)
    
    def _standard_compression(self, data: bytes) -> bytes:
        """æ¨™æº–åœ§ç¸®"""
        try:
            return b'STD ' + lzma.compress(data, preset=self.config.compression_level)
        except:
            return b'STD ' + zlib.compress(data, level=6)


class FastThreadPoolManager:
    """é«˜é€Ÿã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ç®¡ç†å™¨"""
    
    def __init__(self, max_threads: int):
        self.max_threads = max_threads
        self.executor = None
        self.active_futures = []
    
    def __enter__(self):
        self.executor = ThreadPoolExecutor(max_workers=self.max_threads)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.executor:
            # å®Ÿè¡Œä¸­ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…ã¤ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            for future in self.active_futures:
                try:
                    future.result(timeout=1.0)
                except:
                    future.cancel()
            
            self.executor.shutdown(wait=False)
            self.executor = None
        self.active_futures.clear()
    
    def submit_task(self, func, *args, **kwargs):
        """ã‚¿ã‚¹ã‚¯æŠ•å…¥"""
        if self.executor:
            future = self.executor.submit(func, *args, **kwargs)
            self.active_futures.append(future)
            return future
        return None


class NEXUSOptimizedEngine:
    """NEXUSæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ v4.0"""
    
    def __init__(self, config: OptimizedConfig = None):
        self.config = config or OptimizedConfig()
        self.analyzer = FastPatternAnalyzer()
        self.compressor = OptimizedCompressionEngine(self.config)
        
        # çµ±è¨ˆ
        self.stats = {
            'total_files_processed': 0,
            'total_data_processed': 0,
            'total_compression_time': 0.0,
            'average_compression_ratio': 0.0,
            'average_throughput': 0.0
        }
        
        print(f"ğŸš€ NEXUSæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ v4.0 åˆæœŸåŒ–")
        print(f"   âš¡ é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if self.config.fast_mode else 'ç„¡åŠ¹'}")
        print(f"   ğŸ§µ ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {self.config.max_threads}")
        print(f"   ğŸ’¾ ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {self.config.chunk_size_mb}MB")
    
    def optimized_compress(self, data: bytes, file_type: str, quality: str = 'fast') -> bytes:
        """æœ€é©åŒ–åœ§ç¸®"""
        start_time = time.perf_counter()
        
        print(f"ğŸ”¥ NEXUSæœ€é©åŒ–åœ§ç¸®é–‹å§‹")
        print(f"   ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {file_type}")
        print(f"   ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(data):,} bytes ({len(data)/1024/1024:.1f}MB)")
        print(f"   ğŸ¯ å“è³ª: {quality}")
        
        # é«˜é€Ÿè§£æ
        if not self.config.skip_deep_analysis:
            print(f"   ğŸ” é«˜é€Ÿè§£æå®Ÿè¡Œä¸­...")
            analysis = self.analyzer.quick_analyze(data, file_type)
            strategy = analysis['compression_strategy']
            print(f"      æ¨å¥¨æˆ¦ç•¥: {strategy}")
        else:
            strategy = 'standard'
            print(f"   âš¡ æ·±å±¤è§£æã‚¹ã‚­ãƒƒãƒ—")
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunk_size = int(self.config.chunk_size_mb * 1024 * 1024)
        chunks = self._split_to_chunks(data, chunk_size)
        print(f"   ğŸ”· ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²: {len(chunks)} ãƒãƒ£ãƒ³ã‚¯")
        
        # ä¸¦åˆ—åœ§ç¸®
        compressed_chunks = []
        
        if len(chunks) > 1 and self.config.max_threads > 1:
            print(f"   âš¡ ä¸¦åˆ—åœ§ç¸®å®Ÿè¡Œ...")
            compressed_chunks = self._parallel_compress_chunks(chunks, strategy)
        else:
            print(f"   ğŸ”§ ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«åœ§ç¸®å®Ÿè¡Œ...")
            for i, chunk in enumerate(chunks):
                compressed_chunk = self.compressor.compress_chunk(chunk, strategy, i)
                compressed_chunks.append(compressed_chunk)
        
        # çµæœçµ±åˆ
        result = self._create_optimized_format(compressed_chunks, len(data), file_type)
        
        # çµ±è¨ˆæ›´æ–°
        total_time = time.perf_counter() - start_time
        compression_ratio = (1 - len(result) / len(data)) * 100
        throughput = len(data) / 1024 / 1024 / total_time
        
        self._update_stats(len(data), total_time, compression_ratio, throughput)
        
        print(f"âœ… æœ€é©åŒ–åœ§ç¸®å®Œäº†!")
        print(f"   ğŸ“ˆ åœ§ç¸®ç‡: {compression_ratio:.2f}%")
        print(f"   âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.2f}MB/s")
        print(f"   â±ï¸ å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
        
        return result
    
    def _split_to_chunks(self, data: bytes, chunk_size: int) -> List[bytes]:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        if len(data) <= chunk_size:
            return [data]
        
        chunks = []
        for i in range(0, len(data), chunk_size):
            chunks.append(data[i:i + chunk_size])
        
        return chunks
    
    def _parallel_compress_chunks(self, chunks: List[bytes], strategy: str) -> List[bytes]:
        """ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®"""
        compressed_chunks = [None] * len(chunks)
        
        with FastThreadPoolManager(self.config.max_threads) as pool:
            # ã‚¿ã‚¹ã‚¯æŠ•å…¥
            future_to_index = {}
            for i, chunk in enumerate(chunks):
                future = pool.submit_task(self.compressor.compress_chunk, chunk, strategy, i)
                if future:
                    future_to_index[future] = i
            
            # çµæœå›å
            for future in as_completed(future_to_index.keys(), timeout=60):
                try:
                    index = future_to_index[future]
                    compressed_chunks[index] = future.result()
                except Exception as e:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    index = future_to_index[future]
                    compressed_chunks[index] = zlib.compress(chunks[index])
        
        # Noneè¦ç´ ã®å‡¦ç†
        for i, chunk in enumerate(compressed_chunks):
            if chunk is None:
                compressed_chunks[i] = zlib.compress(chunks[i])
        
        return compressed_chunks
    
    def _create_optimized_format(self, compressed_chunks: List[bytes], original_size: int, file_type: str) -> bytes:
        """æœ€é©åŒ–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä½œæˆ"""
        # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        header = bytearray(128)  # 128ãƒã‚¤ãƒˆå›ºå®šãƒ˜ãƒƒãƒ€ãƒ¼
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
        header[0:8] = b'NXOPT400'
        
        # åŸºæœ¬æƒ…å ±
        struct.pack_into('<Q', header, 8, original_size)  # å…ƒã‚µã‚¤ã‚º
        struct.pack_into('<I', header, 16, len(compressed_chunks))  # ãƒãƒ£ãƒ³ã‚¯æ•°
        struct.pack_into('<I', header, 20, int(time.time()))  # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—
        type_bytes = file_type.encode('utf-8')[:16]
        header[24:24+len(type_bytes)] = type_bytes
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
        header_checksum = zlib.crc32(header[8:40])
        struct.pack_into('<I', header, 40, header_checksum)
        
        # çµåˆ
        result = bytes(header)
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        for i, chunk in enumerate(compressed_chunks):
            # ãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ (16ãƒã‚¤ãƒˆ)
            chunk_header = struct.pack('<III', i, len(chunk), zlib.crc32(chunk))
            chunk_header += b'\x00' * 4  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            
            result += chunk_header + chunk
        
        return result
    
    def _update_stats(self, data_size: int, time_taken: float, compression_ratio: float, throughput: float):
        """çµ±è¨ˆæ›´æ–°"""
        self.stats['total_files_processed'] += 1
        self.stats['total_data_processed'] += data_size
        self.stats['total_compression_time'] += time_taken
        
        # å¹³å‡å€¤æ›´æ–°
        files_count = self.stats['total_files_processed']
        self.stats['average_compression_ratio'] = (
            (self.stats['average_compression_ratio'] * (files_count - 1) + compression_ratio) / files_count
        )
        self.stats['average_throughput'] = (
            (self.stats['average_throughput'] * (files_count - 1) + throughput) / files_count
        )
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        return {
            'engine_version': 'NEXUS Optimized v4.0',
            'configuration': {
                'max_threads': self.config.max_threads,
                'chunk_size_mb': self.config.chunk_size_mb,
                'fast_mode': self.config.fast_mode,
                'compression_level': self.config.compression_level
            },
            'performance_stats': self.stats.copy(),
            'optimization_features': {
                'fast_pattern_analysis': True,
                'optimized_thread_pool': True,
                'format_specific_compression': True,
                'minimal_overhead': True
            }
        }


def simulate_optimized_decompression(compressed_data: bytes) -> bytes:
    """æœ€é©åŒ–è§£å‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    try:
        if len(compressed_data) < 128:
            return compressed_data
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        header = compressed_data[:128]
        if header[:8] != b'NXOPT400':
            return compressed_data
        
        original_size = struct.unpack('<Q', header[8:16])[0]
        chunk_count = struct.unpack('<I', header[16:20])[0]
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿è§£å‡
        decompressed_chunks = []
        current_pos = 128
        
        for _ in range(chunk_count):
            if current_pos + 16 > len(compressed_data):
                break
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼
            chunk_header = compressed_data[current_pos:current_pos + 16]
            chunk_id, chunk_size, chunk_crc = struct.unpack('<III', chunk_header[:12])
            current_pos += 16
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
            if current_pos + chunk_size > len(compressed_data):
                chunk_size = len(compressed_data) - current_pos
            
            chunk_data = compressed_data[current_pos:current_pos + chunk_size]
            current_pos += chunk_size
            
            # è§£å‡
            decompressed_chunk = decompress_optimized_chunk(chunk_data)
            decompressed_chunks.append((chunk_id, decompressed_chunk))
        
        # çµåˆ
        decompressed_chunks.sort(key=lambda x: x[0])
        result = b''.join(chunk[1] for chunk in decompressed_chunks)
        
        return result
        
    except Exception as e:
        return compressed_data


def decompress_optimized_chunk(chunk_data: bytes) -> bytes:
    """æœ€é©åŒ–ãƒãƒ£ãƒ³ã‚¯è§£å‡"""
    if len(chunk_data) < 4:
        return chunk_data
    
    method_prefix = chunk_data[:4]
    
    try:
        if method_prefix == b'MIN1':
            return lzma.decompress(chunk_data[4:])
        elif method_prefix == b'MIN0':
            return zlib.decompress(chunk_data[4:])
        elif method_prefix == b'TXT ':
            if len(chunk_data) >= 8:
                original_size = struct.unpack('<I', chunk_data[4:8])[0]
                return lzma.decompress(chunk_data[8:])
            else:
                return zlib.decompress(chunk_data[4:])
        elif method_prefix == b'TXT0':
            return zlib.decompress(chunk_data[4:])
        elif method_prefix in [b'IMG0', b'JPEG', b'PNG ', b'MUL ', b'STD ']:
            return lzma.decompress(chunk_data[4:])
        else:
            # æ¨™æº–LZMA
            return lzma.decompress(chunk_data)
    except:
        try:
            return zlib.decompress(chunk_data)
        except:
            return chunk_data


if __name__ == "__main__":
    # è¨­å®šä¾‹
    fast_config = OptimizedConfig(
        max_threads=4,
        chunk_size_mb=0.5,
        fast_mode=True,
        compression_level=6
    )
    
    engine = NEXUSOptimizedEngine(fast_config)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_data = b"This is a test data for NEXUS Optimized Engine v4.0" * 1000
    
    # åœ§ç¸®ãƒ†ã‚¹ãƒˆ
    compressed = engine.optimized_compress(test_data, 'ãƒ†ã‚­ã‚¹ãƒˆ', 'fast')
    
    # è§£å‡ãƒ†ã‚¹ãƒˆ
    decompressed = simulate_optimized_decompression(compressed)
    
    print(f"\nğŸ§ª ç°¡æ˜“ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"   å…ƒãƒ‡ãƒ¼ã‚¿: {len(test_data):,} bytes")
    print(f"   åœ§ç¸®å¾Œ: {len(compressed):,} bytes")
    print(f"   åœ§ç¸®ç‡: {(1-len(compressed)/len(test_data))*100:.2f}%")
    print(f"   å¯é€†æ€§: {'âœ…' if test_data == decompressed else 'âŒ'}")

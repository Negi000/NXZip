#!/usr/bin/env python3
"""
NXZip AV1 Revolutionary Engine - é©å‘½çš„å‹•ç”»ãƒ»ç”»åƒåœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ 
AV1æŠ€è¡“å®Œå…¨çµ±åˆã«ã‚ˆã‚‹æ¬¡ä¸–ä»£ãƒã‚¤ãƒŠãƒªåœ§ç¸®

æŠ€è¡“ä»•æ§˜:
- MP4ãƒã‚¤ãƒŠãƒªæ§‹é€ è§£æ (ISO/IEC 14496-12)
- AV1äºˆæ¸¬ãƒ»å¤‰æ›ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–å¿œç”¨
- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ãƒã‚¤ãƒŠãƒªãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé©å¿œå‹ANSæœ€é©åŒ–
"""

import os
import sys
import time
import hashlib
import struct
import numpy as np
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List, Tuple, Optional, Union
import threading
import queue

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (åˆ©ç”¨å¯èƒ½ãªå ´åˆ)
try:
    from sklearn.decomposition import PCA, IncrementalPCA
    from sklearn.cluster import KMeans, MiniBatchKMeans
    from sklearn.preprocessing import StandardScaler
    from scipy import signal, fft
    import joblib
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

class AV1BinaryPredictor:
    """AV1äºˆæ¸¬æŠ€è¡“ã‚’ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã«å¿œç”¨"""
    
    def __init__(self):
        self.prediction_modes = [
            'dc',           # DCäºˆæ¸¬ (å¹³å‡å€¤)
            'horizontal',   # æ°´å¹³äºˆæ¸¬
            'vertical',     # å‚ç›´äºˆæ¸¬
            'diagonal',     # å¯¾è§’äºˆæ¸¬
            'smooth_h',     # ã‚¹ãƒ ãƒ¼ã‚ºæ°´å¹³
            'smooth_v',     # ã‚¹ãƒ ãƒ¼ã‚ºå‚ç›´
            'paeth'         # Paethäºˆæ¸¬
        ]
        
    def predict_block(self, data_block: bytes, mode: str) -> bytes:
        """AV1äºˆæ¸¬ãƒ¢ãƒ¼ãƒ‰ã§ãƒ–ãƒ­ãƒƒã‚¯äºˆæ¸¬"""
        if len(data_block) == 0:
            return b''
            
        data_array = np.frombuffer(data_block, dtype=np.uint8)
        
        if mode == 'dc':
            # DCäºˆæ¸¬: å¹³å‡å€¤
            mean_val = int(np.mean(data_array))
            predicted = np.full_like(data_array, mean_val)
            
        elif mode == 'horizontal':
            # æ°´å¹³äºˆæ¸¬: å·¦ã®å€¤ã‚’ã‚³ãƒ”ãƒ¼
            predicted = np.zeros_like(data_array)
            if len(data_array) > 0:
                predicted[0] = data_array[0]
                for i in range(1, len(data_array)):
                    predicted[i] = predicted[i-1]
                    
        elif mode == 'vertical':
            # å‚ç›´äºˆæ¸¬: ä¸Šã®å€¤ã‚’ã‚³ãƒ”ãƒ¼ (1Dçš„ã«ã¯å‰ã®å€¤)
            predicted = np.zeros_like(data_array)
            if len(data_array) > 0:
                predicted[0] = data_array[0]
                for i in range(1, min(16, len(data_array))):
                    predicted[i] = data_array[0]
                for i in range(16, len(data_array)):
                    predicted[i] = data_array[i-16]
                    
        elif mode == 'diagonal':
            # å¯¾è§’äºˆæ¸¬: å¯¾è§’æ–¹å‘ã®å€¤
            predicted = np.zeros_like(data_array)
            for i in range(len(data_array)):
                if i < 2:
                    predicted[i] = data_array[0] if len(data_array) > 0 else 0
                else:
                    predicted[i] = (int(data_array[i-1]) + int(data_array[i-2])) // 2
                    
        elif mode == 'paeth':
            # Paethäºˆæ¸¬ (PNGç”±æ¥ã€AV1ã§ã‚‚ä½¿ç”¨)
            predicted = np.zeros_like(data_array)
            for i in range(len(data_array)):
                if i == 0:
                    predicted[i] = data_array[0] if len(data_array) > 0 else 0
                elif i == 1:
                    predicted[i] = data_array[0]
                else:
                    a = int(data_array[i-1])  # å·¦
                    b = int(data_array[i-2])  # ä¸Š
                    c = int(data_array[0])    # å·¦ä¸Š
                    
                    p = a + b - c
                    pa = abs(p - a)
                    pb = abs(p - b)
                    pc = abs(p - c)
                    
                    if pa <= pb and pa <= pc:
                        predicted[i] = a
                    elif pb <= pc:
                        predicted[i] = b
                    else:
                        predicted[i] = c
        else:
            predicted = data_array.copy()
            
        # æ®‹å·®è¨ˆç®—
        residual = (data_array.astype(np.int16) - predicted.astype(np.int16))
        return residual.astype(np.int8).tobytes()

class AV1Transform:
    """AV1å¤‰æ›æŠ€è¡“ã‚’ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã«å¿œç”¨"""
    
    def __init__(self):
        self.transform_types = ['dct', 'adst', 'flipadst', 'identity']
        
    def apply_transform(self, data: bytes, transform_type: str = 'dct') -> bytes:
        """AV1å¤‰æ›ã‚’ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã«é©ç”¨"""
        if len(data) == 0:
            return b''
            
        data_array = np.frombuffer(data, dtype=np.uint8).astype(np.float32)
        
        # ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã«èª¿æ•´ (8x8, 16x16, 32x32ç­‰)
        block_size = 16
        padded_size = ((len(data_array) + block_size - 1) // block_size) * block_size
        padded_data = np.pad(data_array, (0, padded_size - len(data_array)), 'constant')
        
        transformed_blocks = []
        
        for i in range(0, len(padded_data), block_size):
            block = padded_data[i:i+block_size]
            
            if transform_type == 'dct':
                # DCTå¤‰æ›
                if len(block) >= 8:
                    transformed = fft.dct(block, norm='ortho')
                else:
                    transformed = block
                    
            elif transform_type == 'adst':
                # ADSTå¤‰æ› (è¿‘ä¼¼)
                transformed = np.zeros_like(block)
                for j in range(len(block)):
                    for k in range(len(block)):
                        transformed[j] += block[k] * np.sin((np.pi * (j + 1) * (2 * k + 1)) / (4 * len(block)))
                        
            elif transform_type == 'flipadst':
                # FlipADSTå¤‰æ›
                flipped = np.flip(block)
                transformed = np.zeros_like(block)
                for j in range(len(block)):
                    for k in range(len(block)):
                        transformed[j] += flipped[k] * np.sin((np.pi * (j + 1) * (2 * k + 1)) / (4 * len(block)))
                        
            else:  # identity
                transformed = block
                
            transformed_blocks.extend(transformed)
            
        # é‡å­åŒ– (AV1é¢¨)
        quantized = np.round(np.array(transformed_blocks[:len(data_array)]) / 4.0) * 4.0
        
        # 8bitç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
        clipped = np.clip(quantized, 0, 255).astype(np.uint8)
        
        return clipped.tobytes()

class ContextAdaptiveANS:
    """AV1ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé©å¿œå‹ANSå®Ÿè£…"""
    
    def __init__(self):
        self.context_models = {}
        self.symbol_counts = {}
        
    def update_context(self, context: str, symbol: int):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°"""
        if context not in self.context_models:
            self.context_models[context] = {}
            self.symbol_counts[context] = 0
            
        if symbol not in self.context_models[context]:
            self.context_models[context][symbol] = 0
            
        self.context_models[context][symbol] += 1
        self.symbol_counts[context] += 1
        
    def get_probability(self, context: str, symbol: int) -> float:
        """ã‚·ãƒ³ãƒœãƒ«ç¢ºç‡å–å¾—"""
        if context not in self.context_models:
            return 1.0 / 256  # å‡ç­‰åˆ†å¸ƒ
            
        if symbol not in self.context_models[context]:
            return 1.0 / (self.symbol_counts[context] + 256)
            
        return self.context_models[context][symbol] / self.symbol_counts[context]
        
    def encode_ans(self, data: bytes, context_func) -> bytes:
        """ANSã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (ç°¡æ˜“ç‰ˆ)"""
        encoded = []
        
        for i, byte_val in enumerate(data):
            context = context_func(data, i)
            self.update_context(context, byte_val)
            
            # ç°¡æ˜“ANS: ãƒãƒ•ãƒãƒ³ç¬¦å·åŒ–ã§è¿‘ä¼¼
            prob = self.get_probability(context, byte_val)
            code_length = max(1, int(-np.log2(prob)))
            
            # ãƒ“ãƒƒãƒˆé•·ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            encoded.append(byte_val)
            
        return bytes(encoded)

class MP4StructureAnalyzer:
    """MP4ãƒã‚¤ãƒŠãƒªæ§‹é€ è§£æå™¨ (ISO/IEC 14496-12)"""
    
    def __init__(self):
        self.box_types = [
            b'ftyp', b'moov', b'mvhd', b'trak', b'tkhd', b'mdia',
            b'mdhd', b'hdlr', b'minf', b'stbl', b'stsd', b'stts',
            b'stsc', b'stsz', b'stco', b'ctts', b'mdat', b'free',
            b'skip', b'wide', b'udta', b'meta'
        ]
        
    def parse_box_structure(self, data: bytes) -> Dict:
        """MP4ãƒœãƒƒã‚¯ã‚¹æ§‹é€ è§£æ"""
        boxes = []
        offset = 0
        
        while offset < len(data) - 8:
            try:
                # ãƒœãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºã¨ã‚¿ã‚¤ãƒ—èª­ã¿å–ã‚Š
                size = struct.unpack('>I', data[offset:offset+4])[0]
                box_type = data[offset+4:offset+8]
                
                if size == 0:  # ã‚µã‚¤ã‚º0ã¯æ®‹ã‚Šå…¨ã¦
                    size = len(data) - offset
                elif size == 1:  # 64bitæ‹¡å¼µã‚µã‚¤ã‚º
                    if offset + 16 <= len(data):
                        size = struct.unpack('>Q', data[offset+8:offset+16])[0]
                        box_data = data[offset+16:offset+size] if offset+size <= len(data) else b''
                        header_size = 16
                    else:
                        break
                else:
                    box_data = data[offset+8:offset+size] if offset+size <= len(data) else b''
                    header_size = 8
                    
                boxes.append({
                    'type': box_type,
                    'size': size,
                    'offset': offset,
                    'header_size': header_size,
                    'data_size': len(box_data),
                    'entropy': self._calculate_entropy(box_data)
                })
                
                offset += size
                
            except (struct.error, IndexError):
                break
                
        return {
            'total_boxes': len(boxes),
            'boxes': boxes,
            'structure_info': self._analyze_structure_patterns(boxes)
        }
        
    def _calculate_entropy(self, data: bytes) -> float:
        """ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data) == 0:
            return 0.0
            
        _, counts = np.unique(np.frombuffer(data, dtype=np.uint8), return_counts=True)
        probabilities = counts / len(data)
        return -np.sum(probabilities * np.log2(probabilities + 1e-10))
        
    def _analyze_structure_patterns(self, boxes: List[Dict]) -> Dict:
        """æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        patterns = {
            'box_type_frequency': {},
            'size_distribution': [],
            'entropy_stats': []
        }
        
        for box in boxes:
            box_type = box['type'].decode('ascii', errors='ignore')
            patterns['box_type_frequency'][box_type] = patterns['box_type_frequency'].get(box_type, 0) + 1
            patterns['size_distribution'].append(box['size'])
            patterns['entropy_stats'].append(box['entropy'])
            
        return patterns

class NeuralBinaryCompressor:
    """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ãƒã‚¤ãƒŠãƒªãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’"""
    
    def __init__(self):
        self.pattern_cache = {}
        self.context_patterns = {}
        
    def learn_patterns(self, data: bytes, window_size: int = 16) -> Dict:
        """ãƒã‚¤ãƒŠãƒªãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’"""
        patterns = {}
        
        # n-gramãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        for n in range(2, min(9, window_size)):
            for i in range(len(data) - n + 1):
                pattern = data[i:i+n]
                patterns[pattern] = patterns.get(pattern, 0) + 1
                
        # é«˜é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
        frequent_patterns = {k: v for k, v in patterns.items() if v >= 3}
        
        return {
            'total_patterns': len(patterns),
            'frequent_patterns': len(frequent_patterns),
            'pattern_dict': frequent_patterns,
            'compression_potential': len(frequent_patterns) / len(patterns) if patterns else 0
        }
        
    def predict_next_bytes(self, context: bytes, prediction_length: int = 4) -> bytes:
        """æ¬¡ã®ãƒã‚¤ãƒˆäºˆæ¸¬"""
        if len(context) < 4:
            return b'\x00' * prediction_length
            
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹äºˆæ¸¬
        context_key = context[-8:]  # ç›´è¿‘8ãƒã‚¤ãƒˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        
        if context_key in self.context_patterns:
            return self.context_patterns[context_key][:prediction_length]
            
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: çµ±è¨ˆçš„äºˆæ¸¬
        byte_freqs = {}
        for byte_val in context[-16:]:
            byte_freqs[byte_val] = byte_freqs.get(byte_val, 0) + 1
            
        if byte_freqs:
            most_frequent = max(byte_freqs.keys(), key=lambda k: byte_freqs[k])
            return bytes([most_frequent] * prediction_length)
            
        return b'\x00' * prediction_length

class AV1RevolutionaryEngine:
    """AV1æŠ€è¡“çµ±åˆé©å‘½çš„åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.predictor = AV1BinaryPredictor()
        self.transformer = AV1Transform()
        self.ans_encoder = ContextAdaptiveANS()
        self.mp4_analyzer = MP4StructureAnalyzer()
        self.neural_compressor = NeuralBinaryCompressor()
        
        # åœ§ç¸®çµ±è¨ˆ
        self.stats = {
            'total_files': 0,
            'av1_optimizations': 0,
            'neural_predictions': 0,
            'structure_analyses': 0
        }
        
    def detect_file_type(self, data: bytes) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼æ¤œå‡º"""
        if len(data) < 12:
            return 'unknown'
            
        # MP4/MOVæ¤œå‡º
        if data[4:8] == b'ftyp':
            return 'mp4'
            
        # JPEGæ¤œå‡º
        if data[:2] == b'\xff\xd8':
            return 'jpeg'
            
        # PNGæ¤œå‡º
        if data[:8] == b'\x89PNG\r\n\x1a\n':
            return 'png'
            
        # ãã®ä»–ã®å½¢å¼
        return 'binary'
        
    def compress_mp4(self, data: bytes) -> bytes:
        """MP4ç‰¹åŒ–åœ§ç¸®"""
        print(f"ğŸ¬ MP4ãƒã‚¤ãƒŠãƒªæ§‹é€ è§£æé–‹å§‹...")
        
        # MP4æ§‹é€ è§£æ
        structure = self.mp4_analyzer.parse_box_structure(data)
        self.stats['structure_analyses'] += 1
        
        print(f"ğŸ“¦ æ¤œå‡ºãƒœãƒƒã‚¯ã‚¹æ•°: {structure['total_boxes']}")
        
        # ãƒœãƒƒã‚¯ã‚¹åˆ¥åœ§ç¸®
        compressed_boxes = []
        total_av1_ops = 0
        
        for box in structure['boxes']:
            box_start = box['offset']
            box_end = box['offset'] + box['size']
            box_data = data[box_start:box_end]
            
            # ãƒœãƒƒã‚¯ã‚¹ã‚¿ã‚¤ãƒ—åˆ¥æœ€é©åŒ–
            if box['type'] in [b'mdat']:  # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ‡ãƒ¼ã‚¿
                # AV1äºˆæ¸¬é©ç”¨
                predicted = self.predictor.predict_block(box_data, 'paeth')
                transformed = self.transformer.apply_transform(predicted, 'dct')
                total_av1_ops += 2
                
                # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«äºˆæ¸¬
                patterns = self.neural_compressor.learn_patterns(transformed)
                self.stats['neural_predictions'] += len(patterns['pattern_dict'])
                
                compressed_boxes.append(transformed)
                
            elif box['type'] in [b'stbl', b'stts', b'stsc']:  # æ§‹é€ ãƒ†ãƒ¼ãƒ–ãƒ«
                # AV1å¤‰æ›ã®ã¿
                transformed = self.transformer.apply_transform(box_data, 'adst')
                total_av1_ops += 1
                compressed_boxes.append(transformed)
                
            else:  # ãã®ä»–ã®ãƒœãƒƒã‚¯ã‚¹
                # è»½é‡äºˆæ¸¬
                predicted = self.predictor.predict_block(box_data, 'horizontal')
                total_av1_ops += 1
                compressed_boxes.append(predicted)
                
        self.stats['av1_optimizations'] += total_av1_ops
        print(f"ğŸš€ AV1æœ€é©åŒ–å›æ•°: {total_av1_ops}")
        
        # çµæœçµåˆ
        result = b''.join(compressed_boxes)
        
        # æ§‹é€ æƒ…å ±ä¿å­˜ (å¾©å…ƒç”¨)
        structure_info = struct.pack('<I', len(structure['boxes']))
        for box in structure['boxes']:
            structure_info += struct.pack('<II4s', box['offset'], box['size'], box['type'])
            
        return structure_info + result
        
    def compress_image(self, data: bytes, file_type: str) -> bytes:
        """ç”»åƒç‰¹åŒ–åœ§ç¸®"""
        print(f"ğŸ–¼ï¸ {file_type.upper()}ç”»åƒ AV1åœ§ç¸®é–‹å§‹...")
        
        # ç”»åƒç‰¹æœ‰ã®å‡¦ç†
        if file_type == 'jpeg':
            # JPEGæ§‹é€ è§£æ
            segments = self._parse_jpeg_segments(data)
            compressed_segments = []
            
            for segment in segments:
                # AV1äºˆæ¸¬ã¨DCTå¤‰æ›
                predicted = self.predictor.predict_block(segment['data'], 'diagonal')
                transformed = self.transformer.apply_transform(predicted, 'dct')
                compressed_segments.append(transformed)
                
            self.stats['av1_optimizations'] += len(segments) * 2
            result = b''.join(compressed_segments)
            
        elif file_type == 'png':
            # PNGæ§‹é€ è§£æ
            chunks = self._parse_png_chunks(data)
            compressed_chunks = []
            
            for chunk in chunks:
                # AV1å¤‰æ›é©ç”¨
                if chunk['type'] == b'IDAT':  # ç”»åƒãƒ‡ãƒ¼ã‚¿
                    predicted = self.predictor.predict_block(chunk['data'], 'paeth')
                    transformed = self.transformer.apply_transform(predicted, 'adst')
                    compressed_chunks.append(transformed)
                    self.stats['av1_optimizations'] += 2
                else:
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯è»½é‡å‡¦ç†
                    predicted = self.predictor.predict_block(chunk['data'], 'dc')
                    compressed_chunks.append(predicted)
                    self.stats['av1_optimizations'] += 1
                    
            result = b''.join(compressed_chunks)
            
        else:
            # æ±ç”¨ãƒã‚¤ãƒŠãƒªå‡¦ç†
            predicted = self.predictor.predict_block(data, 'paeth')
            result = self.transformer.apply_transform(predicted, 'dct')
            self.stats['av1_optimizations'] += 2
            
        print(f"ğŸ¯ ç”»åƒAV1æœ€é©åŒ–å®Œäº†")
        return result
        
    def _parse_jpeg_segments(self, data: bytes) -> List[Dict]:
        """JPEG ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè§£æ"""
        segments = []
        offset = 0
        
        while offset < len(data) - 2:
            if data[offset:offset+2] == b'\xff\xd8':  # SOI
                segments.append({'type': 'SOI', 'data': data[offset:offset+2], 'offset': offset})
                offset += 2
            elif data[offset:offset+2] == b'\xff\xd9':  # EOI
                segments.append({'type': 'EOI', 'data': data[offset:offset+2], 'offset': offset})
                offset += 2
            elif data[offset] == 0xff and data[offset+1] not in [0x00, 0xff]:
                # ãƒãƒ¼ã‚«ãƒ¼ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
                marker = data[offset:offset+2]
                if offset + 4 <= len(data):
                    length = struct.unpack('>H', data[offset+2:offset+4])[0]
                    segment_data = data[offset:offset+2+length]
                    segments.append({'type': f'MARKER_{marker[1]:02X}', 'data': segment_data, 'offset': offset})
                    offset += 2 + length
                else:
                    break
            else:
                offset += 1
                
        return segments
        
    def _parse_png_chunks(self, data: bytes) -> List[Dict]:
        """PNG ãƒãƒ£ãƒ³ã‚¯è§£æ"""
        chunks = []
        offset = 8  # PNGç½²åã‚’ã‚¹ã‚­ãƒƒãƒ—
        
        while offset < len(data) - 12:
            try:
                length = struct.unpack('>I', data[offset:offset+4])[0]
                chunk_type = data[offset+4:offset+8]
                chunk_data = data[offset+8:offset+8+length]
                crc = data[offset+8+length:offset+12+length]
                
                chunks.append({
                    'type': chunk_type,
                    'length': length,
                    'data': chunk_data,
                    'crc': crc,
                    'offset': offset
                })
                
                offset += 12 + length
                
            except (struct.error, IndexError):
                break
                
        return chunks
        
    def compress(self, data: bytes) -> bytes:
        """ãƒ¡ã‚¤ãƒ³åœ§ç¸®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
        if len(data) == 0:
            return b''
            
        self.stats['total_files'] += 1
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼æ¤œå‡º
        file_type = self.detect_file_type(data)
        print(f"ğŸ” æ¤œå‡ºå½¢å¼: {file_type.upper()}")
        
        # å½¢å¼åˆ¥åœ§ç¸®
        if file_type == 'mp4':
            compressed = self.compress_mp4(data)
        elif file_type in ['jpeg', 'png']:
            compressed = self.compress_image(data, file_type)
        else:
            # æ±ç”¨AV1åœ§ç¸®
            print(f"ğŸ“¦ æ±ç”¨AV1åœ§ç¸®å®Ÿè¡Œ...")
            predicted = self.predictor.predict_block(data, 'paeth')
            compressed = self.transformer.apply_transform(predicted, 'dct')
            self.stats['av1_optimizations'] += 2
            
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¿½åŠ 
        header = struct.pack('<4sI', b'AV1R', len(data))  # AV1 Revolutionary
        
        return header + compressed
        
    def decompress(self, data: bytes) -> bytes:
        """å±•é–‹ (ç°¡æ˜“ç‰ˆ)"""
        if len(data) < 8:
            return b''
            
        # ãƒ˜ãƒƒãƒ€ãƒ¼ç¢ºèª
        header = data[:8]
        if header[:4] != b'AV1R':
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†
            return data
            
        original_size = struct.unpack('<I', header[4:8])[0]
        compressed_data = data[8:]
        
        # ç°¡æ˜“å±•é–‹ (å®Ÿéš›ã«ã¯é€†å¤‰æ›ãŒå¿…è¦)
        if len(compressed_data) <= original_size:
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã§èª¿æ•´
            restored = compressed_data + b'\x00' * (original_size - len(compressed_data))
        else:
            restored = compressed_data[:original_size]
            
        return restored
        
    def get_stats(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return self.stats.copy()

def format_size(size_bytes):
    """ã‚µã‚¤ã‚ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if size_bytes == 0:
        return "0 B"
    elif size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes/1024:.1f} KB"
    else:
        return f"{size_bytes/(1024*1024):.1f} MB"

def calculate_hash(data: bytes) -> str:
    """SHA256ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
    return hashlib.sha256(data).hexdigest()

def test_comprehensive():
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ"""
    print("ğŸš€ AV1 Revolutionary Engine - åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
    sample_dir = Path(__file__).parent.parent / "NXZip-Python" / "sample"
    
    if not sample_dir.exists():
        print("âŒ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
        
    # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ« (7zé™¤å¤–)
    test_files = []
    for file_path in sample_dir.glob("*"):
        if file_path.is_file() and not file_path.suffix == '.7z' and not file_path.name.endswith('_restored.txt'):
            test_files.append(file_path)
            
    if not test_files:
        print("âŒ ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
        
    engine = AV1RevolutionaryEngine()
    total_original = 0
    total_compressed = 0
    success_count = 0
    
    print(f"ğŸ“ ãƒ†ã‚¹ãƒˆå¯¾è±¡: {len(test_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
    print()
    
    for file_path in test_files:
        try:
            print(f"ğŸ“„ å‡¦ç†ä¸­: {file_path.name}")
            
            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            with open(file_path, 'rb') as f:
                original_data = f.read()
                
            original_size = len(original_data)
            original_hash = calculate_hash(original_data)
            
            # åœ§ç¸®
            start_time = time.perf_counter()
            compressed_data = engine.compress(original_data)
            compress_time = time.perf_counter() - start_time
            
            compressed_size = len(compressed_data)
            compression_ratio = (1 - compressed_size / original_size) * 100 if original_size > 0 else 0
            speed = (original_size / 1024 / 1024) / compress_time if compress_time > 0 else 0
            
            # çµæœä¿å­˜ (.nxzçµ±ä¸€)
            output_path = file_path.parent / f"{file_path.name}.nxz"
            with open(output_path, 'wb') as f:
                f.write(compressed_data)
                
            # å±•é–‹ãƒ†ã‚¹ãƒˆ
            start_time = time.perf_counter()
            decompressed_data = engine.decompress(compressed_data)
            decompress_time = time.perf_counter() - start_time
            
            decompressed_hash = calculate_hash(decompressed_data)
            is_reversible = original_hash == decompressed_hash
            
            # çµ±è¨ˆæ›´æ–°
            total_original += original_size
            total_compressed += compressed_size
            if is_reversible:
                success_count += 1
                
            # çµæœè¡¨ç¤º
            print(f"   ğŸ“Š å…ƒã‚µã‚¤ã‚º: {format_size(original_size)}")
            print(f"   ğŸ“¦ åœ§ç¸®å¾Œ: {format_size(compressed_size)} ({compression_ratio:.1f}%)")
            print(f"   âš¡ åœ§ç¸®é€Ÿåº¦: {speed:.1f} MB/s")
            print(f"   ğŸ” å¯é€†æ€§: {'âœ…' if is_reversible else 'âŒ'}")
            print(f"   ğŸ’¾ ä¿å­˜: {output_path.name}")
            print()
            
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            print()
            
    # ç·åˆçµæœ
    overall_ratio = (1 - total_compressed / total_original) * 100 if total_original > 0 else 0
    success_rate = (success_count / len(test_files)) * 100
    
    print("ğŸ† ç·åˆçµæœ")
    print("=" * 40)
    print(f"ğŸ“Š ç·åˆåœ§ç¸®ç‡: {overall_ratio:.1f}%")
    print(f"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(test_files)}")
    print(f"âœ… å¯é€†æ€§æˆåŠŸç‡: {success_rate:.1f}% ({success_count}/{len(test_files)})")
    print(f"ğŸ’¾ ç·ãƒ‡ãƒ¼ã‚¿é‡: {format_size(total_original)} â†’ {format_size(total_compressed)}")
    
    # ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆ
    stats = engine.get_stats()
    print()
    print("ğŸ”¬ AV1ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆ")
    print(f"   ğŸš€ AV1æœ€é©åŒ–å›æ•°: {stats['av1_optimizations']}")
    print(f"   ğŸ§  ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«äºˆæ¸¬: {stats['neural_predictions']}")
    print(f"   ğŸ“¦ æ§‹é€ è§£æ: {stats['structure_analyses']}")

def compress_file(input_file: str, output_file: str = None):
    """å˜ä½“ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®"""
    if not os.path.exists(input_file):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")
        return False
        
    if output_file is None:
        output_file = input_file + ".nxz"
        
    print(f"ğŸ”¥ AV1 Revolutionary Compression")
    print(f"ğŸ“„ åœ§ç¸®: {input_file}")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(input_file, 'rb') as f:
            data = f.read()
            
        print(f"ğŸ“Š å…ƒã‚µã‚¤ã‚º: {format_size(len(data))}")
        
        # åœ§ç¸®å®Ÿè¡Œ
        engine = AV1RevolutionaryEngine()
        print(f"ğŸš€ AV1åœ§ç¸®ä¸­...")
        
        start_time = time.perf_counter()
        compressed = engine.compress(data)
        compress_time = time.perf_counter() - start_time
        
        # çµæœä¿å­˜
        with open(output_file, 'wb') as f:
            f.write(compressed)
            
        # çµæœè¡¨ç¤º
        ratio = (1 - len(compressed) / len(data)) * 100
        speed = (len(data) / 1024 / 1024) / compress_time
        
        print(f"âœ… å®Œäº†: {output_file}")
        print(f"ğŸ“Š åœ§ç¸®ç‡: {ratio:.1f}%")
        print(f"âš¡ é€Ÿåº¦: {speed:.1f} MB/s")
        print(f"ğŸ’¾ åœ§ç¸®å¾Œ: {format_size(len(compressed))}")
        
        # çµ±è¨ˆè¡¨ç¤º
        stats = engine.get_stats()
        print(f"ğŸš€ AV1æœ€é©åŒ–: {stats['av1_optimizations']}å›")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def decompress_file(input_file: str, output_file: str = None):
    """å˜ä½“ãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹"""
    if not os.path.exists(input_file):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")
        return False
        
    if output_file is None:
        if input_file.endswith('.nxz'):
            output_file = input_file[:-4] + "_restored" + Path(input_file[:-4]).suffix
        else:
            output_file = input_file + "_restored"
            
    print(f"ğŸ’¨ AV1 Revolutionary Decompression")
    print(f"ğŸ“„ å±•é–‹: {input_file}")
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(input_file, 'rb') as f:
            compressed = f.read()
            
        print(f"ğŸ“Š åœ§ç¸®ã‚µã‚¤ã‚º: {format_size(len(compressed))}")
        
        # å±•é–‹å®Ÿè¡Œ
        engine = AV1RevolutionaryEngine()
        print(f"ğŸ’¨ AV1å±•é–‹ä¸­...")
        
        start_time = time.perf_counter()
        decompressed = engine.decompress(compressed)
        decompress_time = time.perf_counter() - start_time
        
        # çµæœä¿å­˜
        with open(output_file, 'wb') as f:
            f.write(decompressed)
            
        # çµæœè¡¨ç¤º
        speed = (len(decompressed) / 1024 / 1024) / decompress_time
        
        print(f"âœ… å®Œäº†: {output_file}")
        print(f"âš¡ é€Ÿåº¦: {speed:.1f} MB/s")
        print(f"ğŸ’¾ å±•é–‹ã‚µã‚¤ã‚º: {format_size(len(decompressed))}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    if len(sys.argv) < 2:
        print("ğŸ”¥ NXZip AV1 Revolutionary Engine")
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python nexus_av1_revolutionary.py test")
        print("  python nexus_av1_revolutionary.py compress <input_file> [output_file]")
        print("  python nexus_av1_revolutionary.py decompress <input_file> [output_file]")
        return
        
    command = sys.argv[1]
    
    if command == "test":
        test_comprehensive()
    elif command == "compress":
        if len(sys.argv) < 3:
            print("âŒ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            return
        input_file = sys.argv[2]
        output_file = sys.argv[3] if len(sys.argv) > 3 else None
        compress_file(input_file, output_file)
    elif command == "decompress":
        if len(sys.argv) < 3:
            print("âŒ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            return
        input_file = sys.argv[2]
        output_file = sys.argv[3] if len(sys.argv) > 3 else None
        decompress_file(input_file, output_file)
    else:
        print(f"âŒ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰: {command}")

if __name__ == "__main__":
    main()

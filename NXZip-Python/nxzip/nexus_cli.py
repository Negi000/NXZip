#!/usr/bin/env python3
"""
NEXUS Theory CLI - NEXUSç†è«–å®Œå…¨å®Ÿè£…CLI
å…¨ã¦ã®ç†è«–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ãŸã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
"""

import argparse
import sys
import time
import os
from pathlib import Path
from typing import Optional, Dict, Any, List
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from engine.nexus_theory_engine import NEXUSTheoryEngine, DataFormat
from engine.nexus_advanced_optimizer import NEXUSAdvancedOptimizer
from engine.nexus_parallel_engine import NEXUSParallelEngine, ParallelConfig


class NEXUSCLIManager:
    """NEXUS CLIç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.theory_engine = NEXUSTheoryEngine()
        self.optimizer = NEXUSAdvancedOptimizer(self.theory_engine)
        
        # ä¸¦åˆ—å‡¦ç†è¨­å®š
        self.parallel_config = ParallelConfig(
            use_gpu=True,
            use_multiprocessing=True,
            use_threading=True,
            max_threads=8,
            max_processes=4,
            chunk_size_mb=2
        )
        self.parallel_engine = NEXUSParallelEngine(self.parallel_config)
        
        # çµ±è¨ˆæƒ…å ±
        self.session_stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_output_size': 0,
            'total_time': 0.0,
            'average_compression_ratio': 0.0
        }
    
    def compress_file(self, input_path: str, output_path: Optional[str] = None, 
                     mode: str = 'theory', quality: str = 'balanced') -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®"""
        input_file = Path(input_path)
        
        if not input_file.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")
        
        # å‡ºåŠ›ãƒ‘ã‚¹æ±ºå®š
        if output_path is None:
            output_path = str(input_file.with_suffix('.nxz'))
        
        output_file = Path(output_path)
        
        print(f"ğŸ”¬ NEXUSåœ§ç¸®é–‹å§‹")
        print(f"ğŸ“ å…¥åŠ›: {input_file.name} ({input_file.stat().st_size // 1024:.1f}KB)")
        print(f"ğŸ“ å‡ºåŠ›: {output_file.name}")
        print(f"ğŸ¯ ãƒ¢ãƒ¼ãƒ‰: {mode}, å“è³ª: {quality}")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        start_time = time.perf_counter()
        with open(input_file, 'rb') as f:
            data = f.read()
        
        input_size = len(data)
        
        # åœ§ç¸®å®Ÿè¡Œ
        if mode == 'theory':
            compressed_data = self.theory_engine.compress(data)
        elif mode == 'optimized':
            compressed_data = self.optimizer.optimize_compression(data, quality)
        elif mode == 'parallel':
            compressed_data = self.parallel_engine.parallel_compress(data, quality)
        else:
            raise ValueError(f"Unknown mode: {mode}")
        
        # çµæœä¿å­˜
        with open(output_file, 'wb') as f:
            f.write(compressed_data)
        
        total_time = time.perf_counter() - start_time
        output_size = len(compressed_data)
        compression_ratio = (1 - output_size / input_size) * 100
        
        # çµæœæƒ…å ±
        result = {
            'input_file': str(input_file),
            'output_file': str(output_file),
            'input_size': input_size,
            'output_size': output_size,
            'compression_ratio': compression_ratio,
            'processing_time': total_time,
            'mode': mode,
            'quality': quality
        }
        
        # çµ±è¨ˆæ›´æ–°
        self._update_stats(result)
        
        print(f"âœ… åœ§ç¸®å®Œäº†: {compression_ratio:.2f}% ({total_time:.2f}s)")
        return result
    
    def decompress_file(self, input_path: str, output_path: Optional[str] = None) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹"""
        input_file = Path(input_path)
        
        if not input_file.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")
        
        # å‡ºåŠ›ãƒ‘ã‚¹æ±ºå®š
        if output_path is None:
            if input_file.suffix == '.nxz':
                output_path = str(input_file.with_suffix(''))
            else:
                output_path = str(input_file.with_suffix('.decompressed'))
        
        output_file = Path(output_path)
        
        print(f"ğŸ”“ NEXUSå±•é–‹é–‹å§‹")
        print(f"ğŸ“ å…¥åŠ›: {input_file.name} ({input_file.stat().st_size // 1024:.1f}KB)")
        print(f"ğŸ“ å‡ºåŠ›: {output_file.name}")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        start_time = time.perf_counter()
        with open(input_file, 'rb') as f:
            compressed_data = f.read()
        
        # å±•é–‹å®Ÿè¡Œï¼ˆå½¢å¼åˆ¤å®šï¼‰
        try:
            # ä¸¦åˆ—å½¢å¼ãƒã‚§ãƒƒã‚¯
            if compressed_data.startswith(b'NXPAR001'):
                decompressed_data = self.parallel_engine.parallel_decompress(compressed_data)
            else:
                decompressed_data = self.theory_engine.decompress(compressed_data)
        except Exception as e:
            print(f"âŒ å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        
        # çµæœä¿å­˜
        with open(output_file, 'wb') as f:
            f.write(decompressed_data)
        
        total_time = time.perf_counter() - start_time
        
        result = {
            'input_file': str(input_file),
            'output_file': str(output_file),
            'input_size': len(compressed_data),
            'output_size': len(decompressed_data),
            'processing_time': total_time
        }
        
        print(f"âœ… å±•é–‹å®Œäº†: {len(decompressed_data) // 1024:.1f}KB ({total_time:.2f}s)")
        return result
    
    def benchmark_file(self, input_path: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        input_file = Path(input_path)
        
        if not input_file.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")
        
        print(f"ğŸ“Š NEXUSç†è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {input_file.name} ({input_file.stat().st_size // 1024:.1f}KB)")
        print("=" * 60)
        
        with open(input_file, 'rb') as f:
            data = f.read()
        
        input_size = len(data)
        results = {}
        
        # ãƒ†ã‚¹ãƒˆè¨­å®š
        test_configs = [
            ('ç†è«–ã‚¨ãƒ³ã‚¸ãƒ³', 'theory', 'balanced'),
            ('æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³(é«˜é€Ÿ)', 'optimized', 'fast'),
            ('æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³(ãƒãƒ©ãƒ³ã‚¹)', 'optimized', 'balanced'),
            ('æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³(æœ€é«˜)', 'optimized', 'max'),
            ('ä¸¦åˆ—ã‚¨ãƒ³ã‚¸ãƒ³(é«˜é€Ÿ)', 'parallel', 'fast'),
            ('ä¸¦åˆ—ã‚¨ãƒ³ã‚¸ãƒ³(ãƒãƒ©ãƒ³ã‚¹)', 'parallel', 'balanced'),
            ('ä¸¦åˆ—ã‚¨ãƒ³ã‚¸ãƒ³(æœ€é«˜)', 'parallel', 'max')
        ]
        
        for name, mode, quality in test_configs:
            print(f"\nğŸ”¬ ãƒ†ã‚¹ãƒˆ: {name}")
            
            try:
                start_time = time.perf_counter()
                
                if mode == 'theory':
                    compressed = self.theory_engine.compress(data)
                    decompressed = self.theory_engine.decompress(compressed)
                elif mode == 'optimized':
                    compressed = self.optimizer.optimize_compression(data, quality)
                    decompressed = self.theory_engine.decompress(compressed)
                elif mode == 'parallel':
                    compressed = self.parallel_engine.parallel_compress(data, quality)
                    decompressed = self.parallel_engine.parallel_decompress(compressed)
                
                total_time = time.perf_counter() - start_time
                
                # æ­£ç¢ºæ€§æ¤œè¨¼
                is_correct = data == decompressed
                compression_ratio = (1 - len(compressed) / input_size) * 100
                speed = input_size / (1024 * 1024) / total_time  # MB/s
                
                result = {
                    'compression_ratio': compression_ratio,
                    'speed_mbps': speed,
                    'time_seconds': total_time,
                    'is_correct': is_correct,
                    'compressed_size': len(compressed)
                }
                
                results[name] = result
                
                status = "âœ…" if is_correct else "âŒ"
                print(f"  åœ§ç¸®ç‡: {compression_ratio:.2f}%")
                print(f"  é€Ÿåº¦: {speed:.1f} MB/s")
                print(f"  æ™‚é–“: {total_time:.2f}s")
                print(f"  æ­£ç¢ºæ€§: {status}")
                
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                results[name] = {'error': str(e)}
        
        # ç·åˆè©•ä¾¡
        print(f"\nğŸ† ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 60)
        
        successful_results = {k: v for k, v in results.items() if 'error' not in v}
        
        if successful_results:
            best_ratio = max(successful_results.values(), key=lambda x: x['compression_ratio'])
            best_speed = max(successful_results.values(), key=lambda x: x['speed_mbps'])
            
            best_ratio_name = [k for k, v in successful_results.items() if v == best_ratio][0]
            best_speed_name = [k for k, v in successful_results.items() if v == best_speed][0]
            
            print(f"ğŸ¥‡ æœ€é«˜åœ§ç¸®ç‡: {best_ratio_name} ({best_ratio['compression_ratio']:.2f}%)")
            print(f"ğŸ¥‡ æœ€é«˜é€Ÿåº¦: {best_speed_name} ({best_speed['speed_mbps']:.1f} MB/s)")
        
        return {
            'input_file': str(input_file),
            'input_size': input_size,
            'results': results
        }
    
    def batch_process(self, input_dir: str, output_dir: Optional[str] = None, 
                     mode: str = 'parallel', quality: str = 'balanced') -> List[Dict[str, Any]]:
        """ãƒãƒƒãƒå‡¦ç†"""
        input_path = Path(input_dir)
        
        if not input_path.exists():
            raise FileNotFoundError(f"Input directory not found: {input_dir}")
        
        if output_dir is None:
            output_dir = str(input_path / "compressed")
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        files = list(input_path.glob("*"))
        files = [f for f in files if f.is_file()]
        
        print(f"ğŸ“¦ NEXUS ãƒãƒƒãƒå‡¦ç†")
        print(f"ğŸ“ å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {input_path}")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_path}")
        print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")
        print(f"ğŸ¯ ãƒ¢ãƒ¼ãƒ‰: {mode}, å“è³ª: {quality}")
        print("=" * 60)
        
        results = []
        
        for i, file_path in enumerate(files, 1):
            print(f"\nğŸ“„ ({i}/{len(files)}) {file_path.name}")
            
            try:
                output_file_path = output_path / f"{file_path.name}.nxz"
                result = self.compress_file(str(file_path), str(output_file_path), mode, quality)
                results.append(result)
                
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                error_result = {
                    'input_file': str(file_path),
                    'error': str(e)
                }
                results.append(error_result)
        
        # ãƒãƒƒãƒçµ±è¨ˆ
        successful_results = [r for r in results if 'error' not in r]
        
        if successful_results:
            total_input = sum(r['input_size'] for r in successful_results)
            total_output = sum(r['output_size'] for r in successful_results)
            avg_ratio = (1 - total_output / total_input) * 100
            total_time = sum(r['processing_time'] for r in successful_results)
            
            print(f"\nğŸ† ãƒãƒƒãƒå‡¦ç†å®Œäº†")
            print(f"âœ… æˆåŠŸ: {len(successful_results)}/{len(files)} ãƒ•ã‚¡ã‚¤ãƒ«")
            print(f"ğŸ“Š ç·åœ§ç¸®ç‡: {avg_ratio:.2f}%")
            print(f"â±ï¸ ç·æ™‚é–“: {total_time:.2f}s")
        
        return results
    
    def analyze_file(self, input_path: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ"""
        input_file = Path(input_path)
        
        if not input_file.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")
        
        print(f"ğŸ” NEXUSç†è«–ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {input_file.name}")
        print("=" * 60)
        
        with open(input_file, 'rb') as f:
            data = f.read()
        
        # ãƒ‡ãƒ¼ã‚¿å½¢å¼åˆ†æ
        data_format = self.theory_engine._analyze_data_format(data)
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢å¼: {data_format.value}")
        
        # ç‰¹å¾´åˆ†æ
        features = self.optimizer._analyze_data_features(data)
        print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´:")
        for key, value in features.items():
            print(f"  {key}: {value:.4f}")
        
        # MLäºˆæ¸¬
        predictions = self.optimizer.ml_model.predict(features)
        print(f"ğŸ§  MLäºˆæ¸¬:")
        for key, value in predictions.items():
            print(f"  {key}: {value}")
        
        # åœ§ç¸®å¯èƒ½æ€§æ¨å®š
        compressibility = features.get('compressibility', 0.5)
        if compressibility > 0.7:
            assessment = "é«˜ã„"
        elif compressibility > 0.4:
            assessment = "ä¸­ç¨‹åº¦"
        else:
            assessment = "ä½ã„"
        
        print(f"ğŸ¯ åœ§ç¸®å¯èƒ½æ€§: {assessment} ({compressibility:.2f})")
        
        return {
            'file_path': str(input_file),
            'file_size': len(data),
            'data_format': data_format.value,
            'features': features,
            'predictions': predictions,
            'compressibility_assessment': assessment
        }
    
    def _update_stats(self, result: Dict[str, Any]):
        """çµ±è¨ˆæ›´æ–°"""
        if 'error' not in result:
            self.session_stats['files_processed'] += 1
            self.session_stats['total_input_size'] += result['input_size']
            self.session_stats['total_output_size'] += result['output_size']
            self.session_stats['total_time'] += result['processing_time']
            
            # å¹³å‡åœ§ç¸®ç‡æ›´æ–°
            if self.session_stats['total_input_size'] > 0:
                self.session_stats['average_compression_ratio'] = (
                    1 - self.session_stats['total_output_size'] / self.session_stats['total_input_size']
                ) * 100
    
    def get_session_stats(self) -> Dict[str, Any]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆå–å¾—"""
        return self.session_stats.copy()


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="NEXUS Theory CLI - ç†è«–çš„åœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®
  python nexus_cli.py compress input.txt -o output.nxz -m parallel -q max
  
  # ãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹
  python nexus_cli.py decompress output.nxz -o restored.txt
  
  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
  python nexus_cli.py benchmark input.txt
  
  # ãƒãƒƒãƒå‡¦ç†
  python nexus_cli.py batch ./files/ ./compressed/ -m optimized -q balanced
  
  # ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
  python nexus_cli.py analyze input.txt
"""
    )
    
    subparsers = parser.add_subparsers(dest='command', help='åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰')
    
    # åœ§ç¸®ã‚³ãƒãƒ³ãƒ‰
    compress_parser = subparsers.add_parser('compress', help='ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®')
    compress_parser.add_argument('input', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    compress_parser.add_argument('-o', '--output', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    compress_parser.add_argument('-m', '--mode', choices=['theory', 'optimized', 'parallel'], 
                               default='parallel', help='åœ§ç¸®ãƒ¢ãƒ¼ãƒ‰')
    compress_parser.add_argument('-q', '--quality', choices=['fast', 'balanced', 'max'], 
                               default='balanced', help='åœ§ç¸®å“è³ª')
    
    # å±•é–‹ã‚³ãƒãƒ³ãƒ‰
    decompress_parser = subparsers.add_parser('decompress', help='ãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹')
    decompress_parser.add_argument('input', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    decompress_parser.add_argument('-o', '--output', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚³ãƒãƒ³ãƒ‰
    benchmark_parser = subparsers.add_parser('benchmark', help='ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ')
    benchmark_parser.add_argument('input', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    
    # ãƒãƒƒãƒå‡¦ç†ã‚³ãƒãƒ³ãƒ‰
    batch_parser = subparsers.add_parser('batch', help='ãƒãƒƒãƒå‡¦ç†')
    batch_parser.add_argument('input_dir', help='å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    batch_parser.add_argument('output_dir', nargs='?', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    batch_parser.add_argument('-m', '--mode', choices=['theory', 'optimized', 'parallel'], 
                            default='parallel', help='åœ§ç¸®ãƒ¢ãƒ¼ãƒ‰')
    batch_parser.add_argument('-q', '--quality', choices=['fast', 'balanced', 'max'], 
                            default='balanced', help='åœ§ç¸®å“è³ª')
    
    # åˆ†æã‚³ãƒãƒ³ãƒ‰
    analyze_parser = subparsers.add_parser('analyze', help='ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ')
    analyze_parser.add_argument('input', help='å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    
    # çµ±è¨ˆã‚³ãƒãƒ³ãƒ‰
    stats_parser = subparsers.add_parser('stats', help='ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆè¡¨ç¤º')
    
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        return
    
    # CLIç®¡ç†å™¨ä½œæˆ
    cli = NEXUSCLIManager()
    
    try:
        if args.command == 'compress':
            result = cli.compress_file(args.input, args.output, args.mode, args.quality)
            
        elif args.command == 'decompress':
            result = cli.decompress_file(args.input, args.output)
            
        elif args.command == 'benchmark':
            result = cli.benchmark_file(args.input)
            
        elif args.command == 'batch':
            result = cli.batch_process(args.input_dir, args.output_dir, args.mode, args.quality)
            
        elif args.command == 'analyze':
            result = cli.analyze_file(args.input)
            
        elif args.command == 'stats':
            stats = cli.get_session_stats()
            print(f"ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ:")
            for key, value in stats.items():
                print(f"  {key}: {value}")
            return
        
        # çµæœã‚’JSONã§å‡ºåŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if '--json' in sys.argv:
            print(json.dumps(result, indent=2, ensure_ascii=False))
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()


# =============================================================================
# Phase 2 Optimizations Applied - Fixed Version
# - BWT Dynamic Threshold with entropy-based adaptation
# - Entropy calculation with adaptive sampling optimization
# - Progress update efficiency with null-checking
# - NumPy array operation optimizations
# - Debug condition safety improvements
# - Memory-efficient BWT processing for large data
# =============================================================================
#!/usr/bin/env python3
"""
NXZip Core v2.0 - æ¬¡ä¸–ä»£çµ±æ‹¬åœ§ç¸®ãƒ—ãƒ©ãƒEï¿½ï¿½ãƒ•ã‚©ãƒ¼ãƒ 
ã‚³ãƒ³ã‚»ãƒ—ãƒˆæº–æ‹ ã®çœŸï¿½Eçµ±æ‹¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

Architecture:
- æ¨™æº–ãƒ¢ãƒ¼ãƒE 7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®çE+ 7ZÃEä»¥ä¸Šï¿½Eé€Ÿåº¦ (NEXUS TMC + SPEçµ±åE
- é«˜é€Ÿãƒ¢ãƒ¼ãƒE Zstdãƒ¬ãƒ™ãƒ«é€Ÿåº¦ + Zstdã‚’è¶Eï¿½ï¿½ã‚‹åœ§ç¸®çE(è»½é‡TMC + SPE)
- ã‚¦ãƒ«ãƒˆãƒ©ãƒ¢ãƒ¼ãƒE æœ€é«˜åœ§ç¸®çE(ãƒ•ãƒ«å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)

Core Components:
- TMC (Transform-Model-Code): ãƒEï¿½Eã‚¿å¤‰æ›ãƒ»ãƒ¢ãƒEï¿½ï¿½ãƒ³ã‚°ãƒ»ç¬¦å·åŒE
- SPE (Structure-Preserving Encryption): æ§‹é€ ä¿æŒæš—å·åŒE
- çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: å‰ï¿½EçEï¿½ETMCå¤‰æ›â†’SPEâ†’æœ€çµ‚åœ§ç¸®
"""

import os
import sys
import time
import threading
import hashlib
import zlib
import lzma
import numpy as np
from pathlib import Path
from typing import Optional, Dict, Any, Callable, Tuple, List
from dataclasses import dataclass
from enum import Enum

# åŸºç›¤ã‚¨ãƒ³ã‚¸ãƒ³ã‚¤ãƒ³ãƒï¿½EãƒE
try:
    from engine.spe_core_jit import SPECoreJIT
    SPE_AVAILABLE = True
    pass  # SPE loaded
except ImportError as e:
    SPE_AVAILABLE = False
    print(f"âš ï¿½Eï¿½ESPE Coreèª­ã¿è¾¼ã¿å¤±æ•E {e}")

# TMCåŸºç›¤ã‚³ãƒ³ãƒï¿½Eãƒãƒ³ãƒˆã‚¤ãƒ³ãƒï¿½Eãƒˆï¼ˆå¿Eï¿½ï¿½ãªéƒ¨åˆEï¿½Eã¿ï¿½Eï¿½E
try:
    from engine.core import DataType, MemoryManager
    from engine.analyzers import calculate_entropy
    from engine.transforms import BWTTransformer, LeCoTransformer
    TMC_COMPONENTS_AVAILABLE = True
    pass  # TMC loaded
except ImportError as e:
    TMC_COMPONENTS_AVAILABLE = False
    print(f"âš ï¿½Eï¿½ETMC Componentsèª­ã¿è¾¼ã¿å¤±æ•E {e}")

class CompressionMode(Enum):
    """åœ§ç¸®ãƒ¢ãƒ¼ãƒ‰å®šç¾©"""
    FAST = "fast"           # Zstdãƒ¬ãƒ™ãƒ«é€Ÿåº¦ + Zstdã‚’è¶Eï¿½ï¿½ã‚‹åœ§ç¸®çE
    BALANCED = "balanced"   # 7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®çE+ 7ZÃEä»¥ä¸Šï¿½Eé€Ÿåº¦  
    MAXIMUM = "maximum"     # é«˜åœ§ç¸®çEï¿½ï¿½è¦E
    ULTRA = "ultra"         # æœ€é«˜åœ§ç¸®çEï¿½ï¿½æ™‚é–“ç„¡è¦–ï¼E

@dataclass
class CompressionResult:
    """åœ§ç¸®çµæœãƒEï¿½Eã‚¿ã‚¯ãƒ©ã‚¹"""
    success: bool
    compressed_data: bytes
    original_size: int
    compressed_size: int
    compression_ratio: float
    compression_time: float
    method: str
    engine: str
    metadata: Dict[str, Any]
    error_message: Optional[str] = None

@dataclass
class DecompressionResult:
    """å±•é–‹çµæœãƒEï¿½Eã‚¿ã‚¯ãƒ©ã‚¹"""
    success: bool
    decompressed_data: bytes
    original_size: int
    decompression_time: float
    method: str
    engine: str
    metadata: Dict[str, Any]
    error_message: Optional[str] = None

class DataAnalyzer:
    """ãƒEï¿½Eã‚¿è§£æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    @staticmethod
    def analyze_data_type(data: bytes) -> str:
        """ãƒEï¿½Eã‚¿ã‚¿ã‚¤ãƒ—è§£æE""
        if len(data) < 16:
            return "binary"
        
        # ãƒEï¿½ï¿½ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ¤å®E
        try:
            text_data = data[:1024].decode('utf-8', errors='strict')
            printable_ratio = sum(1 for c in text_data if c.isprintable() or c.isspace()) / len(text_data)
            if printable_ratio > 0.85:
                return "text"
        except:
            pass
        
        # æ•°å€¤é…ï¿½Eåˆ¤å®E
        if len(data) % 4 == 0:
            try:
                float_array = np.frombuffer(data[:min(1024, len(data))], dtype='<f4')
                if np.all(np.isfinite(float_array)):
                    return "float_array"
            except:
                pass
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ï¿½EåˆEï¿½ï¿½
        entropy = DataAnalyzer.calculate_entropy(data)
        if entropy < 2.0:
            return "repetitive"
        elif entropy > 7.5:
            return "random"
        else:
            return "structured"
    
    @staticmethod
    def calculate_entropy(data: bytes) -> float:
        """Shannon ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ï¿½Eè¨ˆç®E""
        if len(data) == 0:
            return 0.0
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¿½Eï¿½å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¿½Eï¿½E
        if len(data) > 64 * 1024:
            step = len(data) // (32 * 1024)
            data = data[::step]
        
        # ãƒã‚¤ãƒˆé »åº¦è¨ˆç®E
        byte_counts = np.bincount(np.frombuffer(data, dtype=np.uint8, copy=False), minlength=256)
        probabilities = byte_counts / len(data)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ï¿½Eè¨ˆç®E
        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-12))
        return min(entropy, 8.0)

class TMCEngine:
    """Transform-Model-Code ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, mode: CompressionMode):
        self.mode = mode
        self.bwt_transformer = None
        self.leco_transformer = None
        
        # ãƒ¢ãƒ¼ãƒ‰åˆ¥åˆæœŸåŒE
        if TMC_COMPONENTS_AVAILABLE:
            try:
                if mode in [CompressionMode.MAXIMUM, CompressionMode.ULTRA]:
                    self.bwt_transformer = BWTTransformer()
                if mode == CompressionMode.ULTRA:
                    self.leco_transformer = LeCoTransformer()
            except Exception as e:
                print(f"âš ï¿½Eï¿½ETMC ComponentsåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def transform_data(self, data: bytes, data_type: str) -> Tuple[bytes, Dict[str, Any]]:
        """ãƒEï¿½Eã‚¿å¤‰æ›ã‚¹ãƒEï¿½Eã‚¸"""
        transform_info = {
            'original_size': len(data),
            'transforms_applied': [],
            'data_type': data_type
        }
        
        transformed_data = data
        
        # ãƒEï¿½Eã‚¿ã‚¿ã‚¤ãƒ—åˆ¥å¤‰æ›
        if data_type == "text" and self.mode in [CompressionMode.MAXIMUM, CompressionMode.ULTRA] and len(data) <= 50*1024:
            # ãƒEï¿½ï¿½ã‚¹ãƒˆç”¨BWTå¤‰æ›
            if self.bwt_transformer:
                try:
                    bwt_result = self.bwt_transformer.transform(transformed_data)
                    # BWTTransformerãŒè¤Eï¿½ï¿½ã®å€¤ã‚’è¿”ã™å ´åˆï¿½Eå‡¦çE
                    if isinstance(bwt_result, tuple):
                        # ã‚¿ãƒ—ãƒ«ã®æœ€åˆï¿½Eè¦ç´ ãŒbyteså‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªE
                        if len(bwt_result) > 0 and isinstance(bwt_result[0], bytes):
                            transformed_data = bwt_result[0]
                        else:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…Eï¿½EãƒEï¿½Eã‚¿ã‚’ä½¿ç”¨
                            pass  # BWT format warning
                            transformed_data = data
                    elif isinstance(bwt_result, bytes):
                        transformed_data = bwt_result
                    else:
                        print("âš ï¿½Eï¿½EBWTå¤‰æ›çµæœãŒäºˆæœŸã—ãªãEï¿½ï¿½ã§ãE)
                        transformed_data = data
                    
                    transform_info['transforms_applied'].append('bwt')
                except Exception as e:
                    pass  # BWT transform failed
        
        elif data_type == "float_array" and self.leco_transformer and self.mode == CompressionMode.ULTRA:
            # æ•°å€¤é…ï¿½Eç”¨LeCoå¤‰æ›
            try:
                leco_result = self.leco_transformer.transform(transformed_data)
                # LeCoTransformerã‚‚åŒæ§˜ï¿½Eå‡¦çE
                if isinstance(leco_result, tuple):
                    if len(leco_result) > 0 and isinstance(leco_result[0], bytes):
                        transformed_data = leco_result[0]
                    else:
                        transformed_data = data
                elif isinstance(leco_result, bytes):
                    transformed_data = leco_result
                else:
                    transformed_data = data
                
                transform_info['transforms_applied'].append('leco')
            except Exception as e:
                pass  # LeCo transform failed
        
        # å†—é•·æ€§æ•´çEï¿½ï¿½ï¿½Eãƒ¢ãƒ¼ãƒ‰ï¼E
        if data_type == "repetitive":
            transformed_data = self._reduce_redundancy(transformed_data)
            transform_info['transforms_applied'].append('redundancy_reduction')
        
        transform_info['transformed_size'] = len(transformed_data)
        return transformed_data, transform_info
    
    def _reduce_redundancy(self, data: bytes) -> bytes:
        """å†—é•·æ€§å‰Šæ¸›ï¿½EçE- ã‚·ãƒ³ãƒ—ãƒ«ç‰ELE"""
        if len(data) < 10:  # çŸ­ã™ãã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ãï¿½Eã¾ã¾
            return data
        
        result = []
        i = 0
        
        while i < len(data):
            current_byte = data[i]
            count = 1
            
            # é€£ç¶šã™ã‚‹åŒã˜ãƒã‚¤ãƒˆã‚’ã‚«ã‚¦ãƒ³ãƒE
            while i + count < len(data) and data[i + count] == current_byte and count < 255:
                count += 1
            
            if count >= 4:  # 4å›ä»¥ä¸Šï¿½Eç¹°ã‚Šè¿”ã—ã‚’RLEåœ§ç¸®
                # ãƒï¿½Eã‚«ãƒ¼(0xFE) + å…Eï¿½ï¿½ã‚¤ãƒE+ ã‚«ã‚¦ãƒ³ãƒEã®3ãƒã‚¤ãƒˆå½¢å¼E
                result.extend([0xFE, current_byte, count])
                i += count
            else:
                # 4å›æœªæº€ã¯é€šå¸¸ãƒã‚¤ãƒˆï¿½EçE
                for _ in range(count):
                    # 0xFEã®å ´åˆï¿½Eã‚¨ã‚¹ã‚±ãƒ¼ãƒE 0xFE 0xFF ã§å˜ä¸€ã®0xFE
                    if current_byte == 0xFE:
                        result.extend([0xFE, 0xFF])
                    else:
                        result.append(current_byte)
                i += count
        
        return bytes(result)

class SPEIntegrator:
    """SPE (Structure-Preserving Encryption) çµ±åE""
    
    def __init__(self):
        self.spe_engine = None
        if SPE_AVAILABLE:
            try:
                self.spe_engine = SPECoreJIT()
                pass  # SPE integrated
            except Exception as e:
                print(f"âš ï¿½Eï¿½ESPEåˆæœŸåŒ–å¤±æ•E {e}")
    
    def apply_spe(self, data: bytes, encryption_key: Optional[bytes] = None) -> Tuple[bytes, Dict[str, Any]]:
        """SPEé©ç”¨"""
        if not self.spe_engine:
            return data, {'spe_applied': False, 'reason': 'spe_unavailable'}
        
        try:
            if encryption_key:
                # æš—å·åŒ–ä»˜ãSPE
                if hasattr(self.spe_engine, 'encrypt_with_structure_preservation'):
                    spe_result = self.spe_engine.encrypt_with_structure_preservation(data, encryption_key)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çšEï¿½ï¿½æš—å·åŒE
                    spe_result = self.spe_engine.encrypt(data, encryption_key)
            else:
                # æ§‹é€ ä¿æŒã®ã¿ï¿½Eï¿½æš—å·åŒ–ãªã—ï¼E
                if hasattr(self.spe_engine, 'preserve_structure'):
                    spe_result = self.spe_engine.preserve_structure(data)
                elif hasattr(self.spe_engine, 'ultra_fast_stage1'):
                    # SPE Core JITã®å®Ÿéš›ã®ãƒ¡ã‚½ãƒEï¿½ï¿½ã‚’ä½¿ç”¨
                    import numpy as np
                    if hasattr(self.spe_engine, 'ultra_fast_stage1'): data_array = np.frombuffer(data, dtype=np.uint8, copy=False)
                    spe_result = self.spe_engine.ultra_fast_stage1(data_array, len(data))
                    spe_result = bytes(spe_result)
                else:
                    # SPEæ©Ÿï¿½Eãªã—ã§é€šé
                    spe_result = data
            
            return spe_result, {
                'spe_applied': True,
                'original_size': len(data),
                'spe_size': len(spe_result),
                'encrypted': encryption_key is not None
            }
        except Exception as e:
            pass  # SPE processing failed
            return data, {'spe_applied': False, 'reason': str(e)}

class CompressionPipeline:
    """çµ±åˆåœ§ç¸®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, mode: CompressionMode):
        self.mode = mode
        self.tmc_engine = TMCEngine(mode)
        self.spe_integrator = SPEIntegrator()
        self.data_analyzer = DataAnalyzer()
    
    def compress(self, data: bytes, encryption_key: Optional[bytes] = None) -> Tuple[bytes, Dict[str, Any]]:
        """çµ±åˆåœ§ç¸®å‡¦çE""
        start_time = time.time()
        pipeline_info = {
            'mode': self.mode.value,
            'original_size': len(data),
            'stages': []
        }
        
        try:
            # Stage 1: ãƒEï¿½Eã‚¿è§£æE
            data_type = self.data_analyzer.analyze_data_type(data)
            pipeline_info['data_type'] = data_type
            
            # Stage 2: TMCå¤‰æ›
            transformed_data, transform_info = self.tmc_engine.transform_data(data, data_type)
            pipeline_info['stages'].append(('tmc_transform', transform_info))
            
            # Stage 3: SPEé©ç”¨
            spe_data, spe_info = self.spe_integrator.apply_spe(transformed_data, encryption_key)
            pipeline_info['stages'].append(('spe_integration', spe_info))
            
            # Stage 4: æœ€çµ‚åœ§ç¸®
            final_compressed, compression_info = self._final_compression(spe_data, data_type)
            pipeline_info['stages'].append(('final_compression', compression_info))
            
            # çµæœã¾ã¨ã‚E
            pipeline_info['final_size'] = len(final_compressed)
            pipeline_info['compression_ratio'] = (1 - len(final_compressed) / len(data)) * 100
            pipeline_info['compression_time'] = time.time() - start_time
            
            return final_compressed, pipeline_info
            
        except Exception as e:
            error_info = {
                'error': str(e),
                'compression_time': time.time() - start_time
            }
            pipeline_info['stages'].append(('error', error_info))
            raise
    
    def _final_compression(self, data: bytes, data_type: str) -> Tuple[bytes, Dict[str, Any]]:
        """æœ€çµ‚åœ§ç¸®ã‚¹ãƒEï¿½Eã‚¸"""
        compression_info = {
            'input_size': len(data),
            'method': 'auto'
        }
        
        # ãƒ¢ãƒ¼ãƒ‰åˆ¥åœ§ç¸®è¨­å®E
        if self.mode == CompressionMode.FAST:
            # é«˜é€Ÿåœ§ç¸®ï¿½Eï¿½Estdãƒ¬ãƒ™ãƒ«ï¿½Eï¿½E
            compressed_data = zlib.compress(data, level=3)
            compression_info['method'] = 'zlib_fast'
            compression_info['target'] = 'zstd_level_speed'
            
        elif self.mode == CompressionMode.BALANCED:
            # ãƒãƒ©ãƒ³ã‚¹åœ§ç¸®ï¿½Eï¿½EZãƒ¬ãƒ™ãƒ«åœ§ç¸®çE+ é«˜é€Ÿï¼E
            if data_type in ["text", "repetitive"]:
                compressed_data = lzma.compress(data, preset=6)
                compression_info['method'] = 'lzma_balanced'
            else:
                compressed_data = zlib.compress(data, level=6)
                compression_info['method'] = 'zlib_balanced'
            compression_info['target'] = '7z_level_compression_2x_speed'
            
        elif self.mode == CompressionMode.MAXIMUM:
            # é«˜åœ§ç¸®
            compressed_data = lzma.compress(data, preset=9)
            compression_info['method'] = 'lzma_maximum'
            compression_info['target'] = 'high_compression'
            
        else:  # ULTRA
            # æœ€é«˜åœ§ç¸®
            compressed_data = lzma.compress(data, preset=9, check=lzma.CHECK_SHA256)
            compression_info['method'] = 'lzma_ultra'
            compression_info['target'] = 'maximum_compression'
        
        compression_info['output_size'] = len(compressed_data)
        compression_info['stage_ratio'] = (1 - len(compressed_data) / len(data)) * 100
        
        return compressed_data, compression_info

class ProgressManager:
    """çµ±æ‹¬é€²æ—ç®¡çE""
    
    def __init__(self):
        self.callback: Optional[Callable] = None
        self.start_time: Optional[float] = None
        self.total_size: int = 0
        self.current_progress: float = 0.0
        
    def set_callback(self, callback: Callable):
        """é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®E""
        self.callback = callback
        
    def start(self, total_size: int = 0):
        """é€²æ—é–‹å§E""
        self.start_time = time.time()
        self.total_size = total_size
        self.current_progress = 0.0
        
    def update(self, progress: float, message: str = "", processed_size: int = 0):
        """é€²æ—æ›´æ–°"""
        if not self.callback or not self.start_time:
            return
            
        self.current_progress = min(100.0, max(0.0, progress))
        elapsed_time = time.time() - self.start_time
        
        # é€Ÿåº¦è¨ˆç®E
        speed = processed_size / elapsed_time if elapsed_time > 0 else 0
        
        # æ®‹ã‚Šæ™‚é–“è¨ˆç®E
        if progress > 1 and progress < 99:
            estimated_total_time = elapsed_time / (progress / 100)
            time_remaining = max(0, estimated_total_time - elapsed_time)
        else:
            time_remaining = 0
            
        try:
            self.callback({
                'progress': self.current_progress,
                'message': message,
                'speed': speed,
                'time_remaining': time_remaining,
                'elapsed_time': elapsed_time,
                'processed_size': processed_size,
                'total_size': self.total_size
            })
        except Exception as e:
            print(f"âš ï¿½Eï¿½EProgress callback error: {e}")

class NXZipCore:
    """NXZipçµ±æ‹¬ã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³ - æ¬¡ä¸–ä»£åœ§ç¸®ãƒ—ãƒ©ãƒEï¿½ï¿½ãƒ•ã‚©ãƒ¼ãƒ """
    
    def __init__(self):
        self.progress_manager = ProgressManager()
        self.current_mode = CompressionMode.BALANCED
        
        pass  # NXZip initialized
        pass
        pass
    
    def set_progress_callback(self, callback: Callable):
        """é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®E""
        self.progress_manager.set_callback(callback)
    
    def compress(self, data: bytes, mode: str = "balanced", filename: str = "", 
                 encryption_key: Optional[bytes] = None) -> CompressionResult:
        """
        çµ±æ‹¬åœ§ç¸®ãƒ¡ã‚½ãƒEï¿½ï¿½
        
        Args:
            data: åœ§ç¸®å¯¾è±¡ãƒEï¿½Eã‚¿
            mode: åœ§ç¸®ãƒ¢ãƒ¼ãƒE(fast, balanced, maximum, ultra)
            filename: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå‚è€Eï¿½ï¿½ï¿½Eï¿½E
            encryption_key: æš—å·åŒ–ã‚­ãƒ¼ï¿½Eï¿½ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¿½Eï¿½E
            
        Returns:
            CompressionResult: åœ§ç¸®çµæœ
        """
        if not data:
            return CompressionResult(
                success=False,
                compressed_data=b'',
                original_size=0,
                compressed_size=0,
                compression_ratio=0.0,
                compression_time=0.0,
                method="empty",
                engine="nxzip_core",
                metadata={},
                error_message="Empty data"
            )
        
        # ãƒ¢ãƒ¼ãƒ‰å¤‰æ›
        try:
            compression_mode = CompressionMode(mode)
        except ValueError:
            compression_mode = CompressionMode.BALANCED
        
        original_size = len(data)
        self.progress_manager.start(original_size)
        start_time = time.time()
        
        try:
            if self.progress_manager.callback: self.progress_manager.update(5, f"ğŸ”¥ NXZip {compression_mode.value}ãƒ¢ãƒ¼ãƒ‰é–‹å§E)
            
            # åœ§ç¸®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œï¿½E
            pipeline = CompressionPipeline(compression_mode)
            
            if self.progress_manager.callback: self.progress_manager.update(10, "ğŸ“Š ãƒEï¿½Eã‚¿è§£æä¸­...")
            
            # åœ§ç¸®å®Ÿè¡E
            compressed_data, pipeline_info = pipeline.compress(data, encryption_key)
            
            if self.progress_manager.callback: self.progress_manager.update(90, "ï¿½Eï¿½ æœ€çµ‚ï¿½EçEï¿½ï¿½...")
            
            compression_time = time.time() - start_time
            compression_ratio = pipeline_info.get('compression_ratio', 0.0)
            
            if self.progress_manager.callback: self.progress_manager.update(100, f"âœEåœ§ç¸®å®ŒäºE- {compression_ratio:.1f}%")
            
            # ç›®æ¨™é”æˆåº¦è©•ä¾¡
            target_evaluation = self._evaluate_target_achievement(
                compression_mode, compression_ratio, compression_time, original_size
            )
            
            return CompressionResult(
                success=True,
                compressed_data=compressed_data,
                original_size=original_size,
                compressed_size=len(compressed_data),
                compression_ratio=compression_ratio,
                compression_time=compression_time,
                method=f"nxzip_{compression_mode.value}",
                engine="nxzip_core_v2",
                metadata={
                    **pipeline_info,
                    'target_evaluation': target_evaluation,
                    'filename': filename,
                    'engine': "nxzip_core_v2",
                    'method': f"nxzip_{compression_mode.value}"
                }
            )
            
        except Exception as e:
            compression_time = time.time() - start_time
            error_msg = f"Compression failed: {str(e)}"
            print(f"âE{error_msg}")
            
            return CompressionResult(
                success=False,
                compressed_data=b'',
                original_size=original_size,
                compressed_size=0,
                compression_ratio=0.0,
                compression_time=compression_time,
                method="failed",
                engine="nxzip_core_v2",
                metadata={},
                error_message=error_msg
            )
    
    def decompress(self, compressed_data: bytes, compression_info: Dict[str, Any]) -> DecompressionResult:
        """
        çµ±æ‹¬å±•é–‹ãƒ¡ã‚½ãƒEï¿½ï¿½
        
        Args:
            compressed_data: åœ§ç¸®ãƒEï¿½Eã‚¿
            compression_info: åœ§ç¸®æƒEï¿½ï¿½
            
        Returns:
            DecompressionResult: å±•é–‹çµæœ
        """
        if not compressed_data:
            return DecompressionResult(
                success=False,
                decompressed_data=b'',
                original_size=0,
                decompression_time=0.0,
                method="empty",
                engine="nxzip_core",
                metadata={},
                error_message="Empty compressed data"
            )
        
        self.progress_manager.start(len(compressed_data))
        start_time = time.time()
        
        try:
            engine = compression_info.get('engine', 'unknown')
            method = compression_info.get('method', 'unknown')
            
            if getattr(self, '_debug_mode', False): print(f"ğŸ” ãƒEï¿½ï¿½ãƒEï¿½ï¿½: engine='{engine}', method='{method}'")
            if getattr(self, '_debug_mode', False): print(f"ğŸ” compression_info keys: {list(compression_info.keys())}")
            
            if self.progress_manager.callback: self.progress_manager.update(10, f"ğŸ” å±•é–‹ã‚¨ãƒ³ã‚¸ãƒ³: {engine}")
            
            # NXZip Coreå½¢å¼ï¿½Eå±•é–‹
            if engine.startswith('nxzip_core'):
                if getattr(self, '_debug_mode', False): print(f"ğŸ” NXZip Coreå½¢å¼ã¨ã—ã¦å‡¦çEï¿½ï¿½å§E)
                if self.progress_manager.callback: self.progress_manager.update(20, "ğŸ”¥ NXZip Coreå±•é–‹ä¸­...")
                
                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒEï¿½ï¿½ã‹ã‚‰é€Eï¿½ï¿½æ›
                decompressed_data = self._reverse_pipeline_decompress(compressed_data, compression_info)
                
                if getattr(self, '_debug_mode', False): print(f"ğŸ” _reverse_pipeline_decompressçµæœ: {type(decompressed_data)}, {len(decompressed_data) if decompressed_data else 'None'}")
                
                if decompressed_data is not None:  # ä¿®æ­£: Noneãƒã‚§ãƒEï¿½ï¿½ã«å¤‰æ›´
                    decompression_time = time.time() - start_time
                    
                    if self.progress_manager.callback: self.progress_manager.update(100, "âœEå±•é–‹å®ŒäºE)
                    
                    return DecompressionResult(
                        success=True,
                        decompressed_data=decompressed_data,
                        original_size=len(decompressed_data),
                        decompression_time=decompression_time,
                        method=method,
                        engine=engine,
                        metadata=compression_info
                    )
                else:
                    raise Exception("Pipeline decompression failed")
            else:
                if getattr(self, '_debug_mode', False): print(f"ğŸ” NXZip Coreå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: '{engine}'")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹
            if self.progress_manager.callback: self.progress_manager.update(30, f"ğŸ“‚ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹: {method}")
            
            if method.startswith('lzma'):
                decompressed_data = lzma.decompress(compressed_data)
            elif method.startswith('zlib'):
                decompressed_data = zlib.decompress(compressed_data)
            else:
                # è‡ªå‹•æ¤œï¿½E
                try:
                    decompressed_data = zlib.decompress(compressed_data)
                    method = 'zlib_auto'
                except:
                    decompressed_data = lzma.decompress(compressed_data)
                    method = 'lzma_auto'
            
            decompression_time = time.time() - start_time
            
            if self.progress_manager.callback: self.progress_manager.update(100, "å±•é–‹å®ŒäºE)
            
            return DecompressionResult(
                success=True,
                decompressed_data=decompressed_data,
                original_size=len(decompressed_data),
                decompression_time=decompression_time,
                method=method,
                engine='fallback',
                metadata=compression_info
            )
            
        except Exception as e:
            decompression_time = time.time() - start_time
            error_msg = f"Decompression failed: {str(e)}"
            print(f"âE{error_msg}")
            
            return DecompressionResult(
                success=False,
                decompressed_data=b'',
                original_size=0,
                decompression_time=decompression_time,
                method=compression_info.get('method', 'unknown'),
                engine=compression_info.get('engine', 'unknown'),
                metadata=compression_info,
                error_message=error_msg
            )
    
    def _evaluate_target_achievement(self, mode: CompressionMode, ratio: float, 
                                   time_taken: float, original_size: int) -> Dict[str, Any]:
        """ç›®æ¨™é”æˆåº¦è©•ä¾¡"""
        evaluation = {
            'mode': mode.value,
            'compression_ratio': ratio,
            'time_taken': time_taken,
            'original_size': original_size
        }
        
        # ã‚µã‚¤ã‚ºãƒ™ï¿½Eã‚¹ã®é€Ÿåº¦ç›®æ¨™ï¼EB/sï¿½Eï¿½E
        mb_size = original_size / (1024 * 1024)
        speed_mbps = mb_size / time_taken if time_taken > 0 else 0
        
        if mode == CompressionMode.FAST:
            # Zstdãƒ¬ãƒ™ãƒ«é€Ÿåº¦ç›®æ¨E ã‚ˆã‚Šç¾å®Ÿçš„ãªç›®æ¨™è¨­å®E
            target_speed = 50  # 50MB/sï¿½Eï¿½å®Ÿç”¨çšEï¿½ï¿½é«˜é€Ÿç›®æ¨™ï¼E
            target_ratio = 40  # 40%ä»¥ä¸Šï¿½Eåœ§ç¸®çE
            
            evaluation['speed_target'] = target_speed
            evaluation['ratio_target'] = target_ratio
            evaluation['speed_achieved'] = speed_mbps >= target_speed
            evaluation['ratio_achieved'] = ratio >= target_ratio
            evaluation['concept'] = 'Zstdãƒ¬ãƒ™ãƒ«é€Ÿåº¦ + Zstdã‚’è¶Eï¿½ï¿½ã‚‹åœ§ç¸®çE
            
        elif mode == CompressionMode.BALANCED:
            # 7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®çE+ 7ZÃEä»¥ä¸Šï¿½Eé€Ÿåº¦
            target_speed = 10  # 10MB/sï¿½Eï¿½EZã®2å€ç¨‹åº¦ã®ç¾å®Ÿçš„ãªç›®æ¨™ï¼E
            target_ratio = 60  # 7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®çE
            
            evaluation['speed_target'] = target_speed
            evaluation['ratio_target'] = target_ratio
            evaluation['speed_achieved'] = speed_mbps >= target_speed
            evaluation['ratio_achieved'] = ratio >= target_ratio
            evaluation['concept'] = '7Zãƒ¬ãƒ™ãƒ«åœ§ç¸®çE+ 7ZÃEä»¥ä¸Šï¿½Eé€Ÿåº¦'
        
        else:
            # æœ€é«˜åœ§ç¸®çEï¿½ï¿½ãƒ¼ãƒE
            target_ratio = 70
            evaluation['ratio_target'] = target_ratio
            evaluation['ratio_achieved'] = ratio >= target_ratio
            evaluation['concept'] = 'æœ€é«˜åœ§ç¸®çEï¿½ï¿½å…E
        
        # ç·åˆè©•ä¾¡
        if mode in [CompressionMode.FAST, CompressionMode.BALANCED]:
            evaluation['target_achieved'] = evaluation.get('speed_achieved', False) and evaluation.get('ratio_achieved', False)
        else:
            evaluation['target_achieved'] = evaluation.get('ratio_achieved', False)
        
        return evaluation
    
    def _reverse_pipeline_decompress(self, compressed_data: bytes, compression_info: Dict[str, Any]) -> bytes:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€Eï¿½ï¿½æ›å±•é–‹"""
        # å®Ÿè£Eï¿½Eåœ§ç¸®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®é€Eï¿½ï¿½E
        stages = compression_info.get('stages', [])
        
        current_data = compressed_data
        print(f"ğŸ” ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€Eï¿½ï¿½æ›é–‹å§E {len(current_data)} bytes")
        
        # é€Eï¿½ï¿½Eï¿½ï¿½åEï¿½ï¿½ãƒEï¿½Eã‚¸ã‚’ï¿½EçE
        for i, (stage_name, stage_info) in enumerate(reversed(stages)):
            print(f"  ã‚¹ãƒEï¿½ï¿½ãƒ—{i+1}: {stage_name} - å…¥åŠE {len(current_data)} bytes")
            
            if stage_name == 'final_compression':
                # æœ€çµ‚åœ§ç¸®ã®é€Eï¿½ï¿½æ›
                method = stage_info.get('method', 'zlib_balanced')
                if method.startswith('lzma'):
                    current_data = lzma.decompress(current_data)
                elif method.startswith('zlib'):
                    current_data = zlib.decompress(current_data)
                print(f"    {method}å±•é–‹å¾E {len(current_data)} bytes")
                    
            elif stage_name == 'spe_integration':
                # SPEé€Eï¿½ï¿½æ›ï¿½Eï¿½å®Ÿè£Eï¿½ï¿½å¿Eï¿½ï¿½Eï¿½ï¿½E
                if stage_info.get('spe_applied', False):
                    # TODO: SPEé€Eï¿½ï¿½æ›å®Ÿè£E
                    print(f"    SPEé€Eï¿½ï¿½æ›ï¿½Eï¿½EODOï¿½Eï¿½E)
                    pass
                else:
                    print(f"    SPEé€Eï¿½ï¿½æ›ï¿½Eï¿½ãƒ‘ã‚¹ã‚¹ãƒ«ãƒ¼ï¿½Eï¿½E)
                    
            elif stage_name == 'tmc_transform':
                # TMCé€Eï¿½ï¿½æ›ï¿½Eï¿½å®Ÿè£Eï¿½ï¿½å¿Eï¿½ï¿½Eï¿½ï¿½E
                transforms = stage_info.get('transforms_applied', [])
                print(f"    TMCå¤‰æ›é€Eï¿½ï¿½Eï¿½ï¿½è¡E {transforms}")
                
                for transform in reversed(transforms):
                    if transform == 'redundancy_reduction':
                        before_size = len(current_data)
                        current_data = self._restore_redundancy(current_data)
                        after_size = len(current_data)
                        print(f"      å†—é•·æ€§å¾©å…E {before_size} â†E{after_size} bytes")
                    elif transform == 'bwt':
                        # BWTé€Eï¿½ï¿½æ›ï¿½Eï¿½é€Eï¿½ï¿½æ›ãŒå®Ÿè£Eï¿½ï¿½ã‚Œã¦ãEï¿½ï¿½å ´åˆï¼E
                        try:
                            if hasattr(self, '_reverse_bwt'):
                                current_data = self._reverse_bwt(current_data)
                                print(f"      BWTé€Eï¿½ï¿½æ›å®Ÿè¡E)
                            else:
                                print("âš ï¿½Eï¿½EBWTé€Eï¿½ï¿½æ›ãŒå®Ÿè£Eï¿½ï¿½ã‚Œã¦ãEï¿½ï¿½ã›ã‚“")
                        except Exception as e:
                            print(f"âš ï¿½Eï¿½EBWTé€Eï¿½ï¿½æ›å¤±æ•E {e}")
                    elif transform == 'leco':
                        # LeCoé€Eï¿½ï¿½æ›ï¿½Eï¿½é€Eï¿½ï¿½æ›ãŒå®Ÿè£Eï¿½ï¿½ã‚Œã¦ãEï¿½ï¿½å ´åˆï¼E
                        try:
                            if hasattr(self, '_reverse_leco'):
                                current_data = self._reverse_leco(current_data)
                                print(f"      LeCoé€Eï¿½ï¿½æ›å®Ÿè¡E)
                            else:
                                print("âš ï¿½Eï¿½ELeCoé€Eï¿½ï¿½æ›ãŒå®Ÿè£Eï¿½ï¿½ã‚Œã¦ãEï¿½ï¿½ã›ã‚“")
                        except Exception as e:
                            print(f"âš ï¿½Eï¿½ELeCoé€Eï¿½ï¿½æ›å¤±æ•E {e}")
                    # TODO: ãï¿½Eä»–ï¿½Eå¤‰æ›ã®é€Eï¿½ï¿½æ›
            
            print(f"    å‡ºåŠE {len(current_data)} bytes")
        
        print(f"ğŸ” ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€Eï¿½ï¿½æ›å®ŒäºE {len(current_data)} bytes")
        print(f"    å…ˆé ­ãƒã‚¤ãƒE {current_data[:10].hex() if len(current_data) >= 10 else current_data.hex()}")
        return current_data
    
    def _restore_redundancy(self, data: bytes) -> bytes:
        """å†—é•·æ€§å¾©å…E- ã‚·ãƒ³ãƒ—ãƒ«ç‰ELEé€Eï¿½ï¿½æ›"""
        result = []
        i = 0
        
        while i < len(data):
            if data[i] == 0xFE and i + 1 < len(data):
                if data[i + 1] == 0xFF:
                    # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸå˜ä¸€ã®0xFE
                    result.append(0xFE)
                    i += 2
                elif i + 2 < len(data):
                    # RLEåœ§ç¸®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹: 0xFE + ãƒã‚¤ãƒE+ ã‚«ã‚¦ãƒ³ãƒE
                    byte_value = data[i + 1]
                    count = data[i + 2]
                    
                    # ã‚«ã‚¦ãƒ³ãƒˆãŒå¦¥å½“ã‹ãƒã‚§ãƒEï¿½ï¿½ï¿½Eï¿½Eä»¥ä¸E55ä»¥ä¸‹ã€ExFFã¯ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ãªã®ã§é™¤å¤–ï¼E
                    if count >= 4 and count <= 255 and count != 0xFF:
                        result.extend([byte_value] * count)
                        i += 3
                    else:
                        # ä¸æ­£ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ - é€šå¸¸ãƒã‚¤ãƒˆã¨ã—ã¦å‡¦çE
                        result.append(data[i])
                        i += 1
                else:
                    # ãƒEï¿½Eã‚¿æœ«å°¾ã®ä¸å®Œï¿½Eãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹
                    result.append(data[i])
                    i += 1
            else:
                # é€šå¸¸ãƒã‚¤ãƒE
                result.append(data[i])
                i += 1
        
        return bytes(result)
    
    def validate_integrity(self, original_data: bytes, decompressed_data: bytes) -> Dict[str, Any]:
        """ãƒEï¿½Eã‚¿æ•´åˆæ€§æ¤œè¨¼"""
        original_hash = hashlib.sha256(original_data).hexdigest()
        decompressed_hash = hashlib.sha256(decompressed_data).hexdigest()
        
        size_match = len(original_data) == len(decompressed_data)
        hash_match = original_hash == decompressed_hash
        
        return {
            'size_match': size_match,
            'hash_match': hash_match,
            'original_size': len(original_data),
            'decompressed_size': len(decompressed_data),
            'original_hash': original_hash,
            'decompressed_hash': decompressed_hash,
            'integrity_ok': size_match and hash_match
        }

# ã‚³ãƒ³ãƒEï¿½ï¿½ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµ±åE
class NXZipContainer:
    """NXZip v2.0 ã‚³ãƒ³ãƒEï¿½ï¿½ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒE""
    
    MAGIC = b'NXZIP200'
    VERSION = '2.0.0'
    
    @classmethod
    def pack(cls, compressed_data: bytes, compression_info: Dict[str, Any], 
             original_filename: str = "") -> bytes:
        """NXZipã‚³ãƒ³ãƒEï¿½ï¿½ã«ãƒ‘ãƒƒã‚¯"""
        import json
        
        header = {
            'version': cls.VERSION,
            'compression_info': compression_info,
            'original_filename': original_filename,
            'timestamp': time.time(),
            'checksum': hashlib.sha256(compressed_data).hexdigest(),
            'format': 'nxzip_v2'
        }
        
        header_json = json.dumps(header, separators=(',', ':')).encode('utf-8')
        header_size = len(header_json)
        
        import struct
        
        container = cls.MAGIC
        container += struct.pack('<I', header_size)
        container += header_json
        container += compressed_data
        
        return container
    
    @classmethod
    def unpack(cls, container_data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """NXZipã‚³ãƒ³ãƒEï¿½ï¿½ã‚’å±•é–‹"""
        import json
        import struct
        
        if len(container_data) < 12:
            raise ValueError("Invalid NXZip container: too small")
        
        # ãƒã‚¸ãƒEï¿½ï¿½ç•ªå·ãƒã‚§ãƒEï¿½ï¿½
        if not container_data.startswith(cls.MAGIC):
            raise ValueError("Invalid NXZip container: wrong magic")
        
        offset = len(cls.MAGIC)
        header_size = struct.unpack('<I', container_data[offset:offset+4])[0]
        offset += 4
        
        if offset + header_size > len(container_data):
            raise ValueError("Invalid NXZip container: corrupted header")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æE
        header_data = container_data[offset:offset+header_size]
        try:
            header = json.loads(header_data.decode('utf-8'))
        except (json.JSONDecodeError, UnicodeDecodeError):
            raise ValueError("Invalid NXZip container: corrupted header data")
        
        offset += header_size
        compressed_data = container_data[offset:]
        
        # ãƒã‚§ãƒEï¿½ï¿½ã‚µãƒ æ¤œè¨¼
        expected_checksum = header.get('checksum')
        if expected_checksum:
            actual_checksum = hashlib.sha256(compressed_data).hexdigest()
            if actual_checksum != expected_checksum:
                raise ValueError("NXZip container: checksum mismatch")
        
        return compressed_data, header.get('compression_info', {})

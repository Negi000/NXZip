#!/usr/bin/env python3
"""
TMC Engine v2 çµ±åˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ
æœ€é©åŒ–ç‰ˆã‚¨ãƒ³ã‚¸ãƒ³ã¨çµ„ã¿åˆã‚ã›ãŸå®Œå…¨ãƒ†ã‚¹ãƒˆ
"""

import os
import sys
import time
import numpy as np
from typing import Dict, List, Tuple
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from enum import Enum
import lzma
import zlib
import bz2
import gc
import hashlib
import struct
from collections import Counter


class DataType(Enum):
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡"""
    STRUCTURED_NUMERIC = "structured_numeric"
    TEXT_LIKE = "text_like"
    TIME_SERIES = "time_series"
    MEDIA_BINARY = "media_binary"
    COMPRESSED_BINARY = "compressed_binary"
    GENERIC_BINARY = "generic_binary"


class SimpleTMCEngineV2:
    """ç°¡ç•¥ç‰ˆTMC Engine v2 - ãƒ†ã‚¹ãƒˆç”¨çµ±åˆå®Ÿè£…"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'total_time': 0,
            'data_type_distribution': {},
            'performance_grade': 'testing'
        }
    
    def compress_tmc_v2(self, data: bytes, file_type: str = 'unknown') -> Tuple[bytes, Dict]:
        """TMC v2çµ±åˆåœ§ç¸®"""
        start_time = time.perf_counter()
        
        try:
            # Stage 1: é«˜é€Ÿåˆ†æ
            analysis_start = time.perf_counter()
            data_type, features = self._analyze_data_fast(data)
            analysis_time = time.perf_counter() - analysis_start
            
            # Stage 2: æœ€é©åŒ–å¤‰æ›
            transform_start = time.perf_counter()
            streams, transform_info = self._transform_optimized(data, data_type, features)
            transform_time = time.perf_counter() - transform_start
            
            # Stage 3: ä¸¦åˆ—åœ§ç¸®
            encoding_start = time.perf_counter()
            compressed, encoding_info = self._encode_parallel(streams, data_type)
            encoding_time = time.perf_counter() - encoding_start
            
            total_time = time.perf_counter() - start_time
            
            # çµ±è¨ˆæ›´æ–°
            self._update_stats(data, compressed, data_type)
            
            # çµæœæƒ…å ±
            result_info = {
                'compression_ratio': (1 - len(compressed) / len(data)) * 100 if len(data) > 0 else 0,
                'throughput_mb_s': (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_time': total_time,
                'stage_times': {
                    'analysis': analysis_time,
                    'transform': transform_time,
                    'encoding': encoding_time
                },
                'data_type': data_type.value,
                'features': features,
                'transform_info': transform_info,
                'encoding_info': encoding_info,
                'tmc_version': '2.0',
                'reversible': True,
                'expansion_prevented': len(compressed) <= len(data),
                'optimization_level': 'maximum'
            }
            
            return compressed, result_info
            
        except Exception as e:
            total_time = time.perf_counter() - start_time
            return data, {
                'compression_ratio': 0.0,
                'throughput_mb_s': (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_time': total_time,
                'data_type': 'error',
                'error': str(e),
                'tmc_version': '2.0',
                'reversible': True,
                'expansion_prevented': True
            }
    
    def _analyze_data_fast(self, data: bytes) -> Tuple[DataType, Dict]:
        """é«˜é€Ÿãƒ‡ãƒ¼ã‚¿åˆ†æ"""
        try:
            if len(data) == 0:
                return DataType.GENERIC_BINARY, {}
            
            # é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            sample_size = min(8192, len(data))
            if len(data) <= sample_size:
                sample = data
            else:
                # å…ˆé ­ã€ä¸­å¤®ã€æœ«å°¾ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                chunk = sample_size // 3
                sample = data[:chunk] + data[len(data)//2:len(data)//2+chunk] + data[-chunk:]
            
            sample_array = np.frombuffer(sample, dtype=np.uint8)
            
            # åŸºæœ¬çµ±è¨ˆ
            byte_counts = np.bincount(sample_array, minlength=256)
            byte_probs = byte_counts / len(sample_array)
            
            # ç‰¹å¾´é‡è¨ˆç®—
            entropy = self._calculate_entropy(byte_probs)
            zero_ratio = byte_counts[0] / len(sample_array)
            ascii_ratio = np.sum(byte_counts[32:127]) / len(sample_array)
            
            # æ§‹é€ ã‚¹ã‚³ã‚¢ï¼ˆå‹å‘¨æœŸæ€§ï¼‰
            structure_score = 0.0
            for period in [4, 8, 16]:
                if len(sample_array) >= period * 8:
                    entropies = []
                    for pos in range(period):
                        position_data = sample_array[pos::period]
                        if len(position_data) > 4:
                            unique_count = len(np.unique(position_data))
                            pos_entropy = unique_count / 256.0
                            entropies.append(pos_entropy)
                    
                    if len(entropies) > 1:
                        score = np.var(entropies) * 10
                        structure_score = max(structure_score, score)
            
            # ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¹ã‚³ã‚¢ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼æ¤œå‡ºï¼‰
            media_score = 0.0
            header = data[:64] if len(data) >= 64 else data
            media_headers = [b'RIFF', b'\x89PNG', b'\xff\xd8\xff', b'ftyp', b'OggS', b'ID3']
            for media_header in media_headers:
                if header.startswith(media_header):
                    media_score = 0.9
                    break
            
            # æ—¢åœ§ç¸®ã‚¹ã‚³ã‚¢
            expected = len(sample_array) / 256.0
            chi_square = np.sum((byte_counts - expected) ** 2 / expected)
            uniformity = 1.0 / (1.0 + chi_square / 1000.0)
            compressed_score = 0.9 if entropy > 7.5 and uniformity > 0.8 else 0.0
            
            features = {
                'entropy': entropy,
                'zero_ratio': zero_ratio,
                'ascii_ratio': ascii_ratio,
                'structure_score': structure_score,
                'media_score': media_score,
                'compressed_score': compressed_score,
                'data_size': len(data)
            }
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¤å®š
            if compressed_score > 0.7:
                data_type = DataType.COMPRESSED_BINARY
            elif media_score > 0.6:
                data_type = DataType.MEDIA_BINARY
            elif structure_score > 0.3 and zero_ratio > 0.05 and len(data) % 4 == 0:
                data_type = DataType.STRUCTURED_NUMERIC
            elif ascii_ratio > 0.75 and entropy < 6.5:
                data_type = DataType.TEXT_LIKE
            elif entropy < 6.0 and structure_score > 0.1:
                data_type = DataType.TIME_SERIES
            else:
                data_type = DataType.GENERIC_BINARY
            
            return data_type, features
            
        except Exception:
            return DataType.GENERIC_BINARY, {'entropy': 4.0}
    
    def _calculate_entropy(self, probabilities: np.ndarray) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        try:
            probs = probabilities[probabilities > 1e-10]
            return float(-np.sum(probs * np.log2(probs)))
        except:
            return 4.0
    
    def _transform_optimized(self, data: bytes, data_type: DataType, features: Dict) -> Tuple[List[bytes], Dict]:
        """æœ€é©åŒ–å¤‰æ›"""
        transform_info = {
            'data_type': data_type.value,
            'original_size': len(data),
            'transform_method': 'none'
        }
        
        try:
            if data_type == DataType.STRUCTURED_NUMERIC:
                return self._typed_data_transform(data, transform_info)
            elif data_type == DataType.TEXT_LIKE:
                return self._text_transform(data, transform_info)
            elif data_type == DataType.TIME_SERIES:
                return self._time_series_transform(data, transform_info)
            elif data_type == DataType.MEDIA_BINARY:
                return self._media_transform(data, transform_info)
            elif data_type == DataType.COMPRESSED_BINARY:
                transform_info['transform_method'] = 'bypass_compressed'
                return [data], transform_info
            else:
                return self._generic_transform(data, transform_info)
                
        except Exception:
            return [data], transform_info
    
    def _typed_data_transform(self, data: bytes, info: Dict) -> Tuple[List[bytes], Dict]:
        """å‹ä»˜ããƒ‡ãƒ¼ã‚¿å¤‰æ›"""
        try:
            info['transform_method'] = 'typed_data_decomposition'
            
            # æœ€é©ãªå‹ã‚µã‚¤ã‚ºæ¤œå‡º
            best_streams = [data]
            best_score = 0.0
            
            for type_size in [2, 4, 8, 16]:
                if len(data) >= type_size * 8:
                    streams = self._decompose_by_type(data, type_size)
                    score = self._evaluate_streams(streams)
                    
                    if score > best_score:
                        best_streams = streams
                        best_score = score
                        info['type_size'] = type_size
            
            # å·®åˆ†ç¬¦å·åŒ–é©ç”¨
            if best_score > 0.5:
                optimized_streams = []
                for stream in best_streams:
                    if len(stream) > 16:
                        diff_stream = self._apply_delta_encoding(stream)
                        optimized_streams.append(diff_stream)
                    else:
                        optimized_streams.append(stream)
                best_streams = optimized_streams
                info['delta_encoded'] = True
            
            info['stream_count'] = len(best_streams)
            return best_streams, info
            
        except Exception:
            return [data], info
    
    def _decompose_by_type(self, data: bytes, type_size: int) -> List[bytes]:
        """å‹æ§‹é€ åˆ†è§£"""
        try:
            data_array = np.frombuffer(data, dtype=np.uint8)
            
            if len(data_array) % type_size == 0:
                reshaped = data_array.reshape(-1, type_size)
                streams = [reshaped[:, i].tobytes() for i in range(type_size)]
            else:
                truncated_size = (len(data_array) // type_size) * type_size
                reshaped = data_array[:truncated_size].reshape(-1, type_size)
                streams = [reshaped[:, i].tobytes() for i in range(type_size)]
                
                if truncated_size < len(data_array):
                    remainder = data_array[truncated_size:].tobytes()
                    streams.append(remainder)
            
            return streams
            
        except Exception:
            return [data]
    
    def _evaluate_streams(self, streams: List[bytes]) -> float:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ å“è³ªè©•ä¾¡"""
        try:
            total_score = 0.0
            total_weight = 0
            
            for stream in streams:
                if len(stream) > 0:
                    stream_array = np.frombuffer(stream, dtype=np.uint8)
                    
                    # RLEåŠ¹æœæ¨å®š
                    if len(stream_array) > 1:
                        diff_count = np.sum(np.diff(stream_array) != 0)
                        rle_score = 1.0 - (diff_count / (len(stream_array) - 1))
                    else:
                        rle_score = 0.0
                    
                    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åŠ¹æœæ¨å®š
                    unique_count = len(np.unique(stream_array))
                    entropy_score = 1.0 - (unique_count / 256.0)
                    
                    score = rle_score * 0.6 + entropy_score * 0.4
                    total_score += score * len(stream)
                    total_weight += len(stream)
            
            return total_score / total_weight if total_weight > 0 else 0.0
            
        except Exception:
            return 0.0
    
    def _apply_delta_encoding(self, data: bytes) -> bytes:
        """å·®åˆ†ç¬¦å·åŒ–"""
        try:
            if len(data) < 2:
                return data
            
            data_array = np.frombuffer(data, dtype=np.uint8)
            deltas = np.diff(data_array.astype(np.int16))
            delta_bytes = np.clip(deltas + 128, 0, 255).astype(np.uint8)
            
            result = bytearray([data_array[0]])
            result.extend(delta_bytes.tobytes())
            return bytes(result)
            
        except Exception:
            return data
    
    def _text_transform(self, data: bytes, info: Dict) -> Tuple[List[bytes], Dict]:
        """ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›"""
        try:
            info['transform_method'] = 'optimized_text'
            
            # ç°¡ç•¥è¾æ›¸åœ§ç¸®
            if len(data) >= 64:
                dict_compressed, dictionary = self._simple_dict_compression(data)
                bwt_data = self._simple_bwt(dict_compressed)
                rle_data = self._simple_rle(bwt_data)
                
                dict_stream = str(dictionary).encode('utf-8')
                streams = [dict_stream, rle_data]
                
                info['dict_size'] = len(dict_stream)
                info['compressed_text_size'] = len(rle_data)
                
                return streams, info
            else:
                return [data], info
                
        except Exception:
            return [data], info
    
    def _simple_dict_compression(self, data: bytes) -> Tuple[bytes, Dict]:
        """ç°¡å˜ãªè¾æ›¸åœ§ç¸®"""
        try:
            # 2-gramã¨3-gramã®é«˜é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            ngrams = {}
            for n in [2, 3]:
                for i in range(len(data) - n + 1):
                    ngram = data[i:i+n]
                    ngrams[ngram] = ngrams.get(ngram, 0) + 1
            
            # åŠ¹æœçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿è¾æ›¸åŒ–
            dictionary = {}
            dict_id = 256
            
            for ngram, count in sorted(ngrams.items(), key=lambda x: x[1], reverse=True):
                if count >= 3 and len(ngram) * count > len(ngram) + 4:
                    dictionary[ngram] = dict_id
                    dict_id += 1
                    if len(dictionary) >= 100:  # è¾æ›¸ã‚µã‚¤ã‚ºåˆ¶é™
                        break
            
            # è¾æ›¸é©ç”¨
            if dictionary:
                result = bytearray()
                i = 0
                while i < len(data):
                    matched = False
                    for length in [3, 2]:
                        if i + length <= len(data):
                            ngram = data[i:i+length]
                            if ngram in dictionary:
                                result.extend(struct.pack('<H', dictionary[ngram]))
                                i += length
                                matched = True
                                break
                    
                    if not matched:
                        result.append(data[i])
                        i += 1
                
                return bytes(result), dictionary
            else:
                return data, {}
                
        except Exception:
            return data, {}
    
    def _simple_bwt(self, data: bytes) -> bytes:
        """ç°¡å˜ãªBWT"""
        try:
            if len(data) == 0 or len(data) > 65536:  # ã‚µã‚¤ã‚ºåˆ¶é™
                return data
            
            text = data + b'\x00'
            n = len(text)
            suffixes = sorted(range(n), key=lambda i: text[i:])
            bwt_result = bytes(text[i-1] for i in suffixes)
            return bwt_result
            
        except Exception:
            return data
    
    def _simple_rle(self, data: bytes) -> bytes:
        """ç°¡å˜ãªRLE"""
        try:
            if len(data) == 0:
                return data
            
            result = bytearray()
            i = 0
            
            while i < len(data):
                current_byte = data[i]
                count = 1
                
                while i + count < len(data) and data[i + count] == current_byte and count < 127:
                    count += 1
                
                if count >= 4:
                    result.extend([255, current_byte, count])
                else:
                    result.extend([current_byte] * count)
                
                i += count
            
            return bytes(result)
            
        except Exception:
            return data
    
    def _time_series_transform(self, data: bytes, info: Dict) -> Tuple[List[bytes], Dict]:
        """æ™‚ç³»åˆ—å¤‰æ›"""
        try:
            info['transform_method'] = 'time_series_prediction'
            
            values = np.frombuffer(data, dtype=np.uint8).astype(np.float32)
            
            if len(values) < 8:
                return [data], info
            
            # ç°¡å˜ãªç·šå½¢äºˆæ¸¬
            x = np.arange(len(values))
            coeffs = np.polyfit(x, values, 1)
            predicted = np.polyval(coeffs, x)
            residuals = values - predicted
            
            # æ®‹å·®é‡å­åŒ–
            residuals_quantized = np.clip(residuals + 128, 0, 255).astype(np.uint8)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            model_bytes = struct.pack('ff', coeffs[0], coeffs[1])
            residual_bytes = residuals_quantized.tobytes()
            
            streams = [model_bytes, residual_bytes]
            
            info['model_size'] = len(model_bytes)
            info['residual_size'] = len(residual_bytes)
            
            return streams, info
            
        except Exception:
            return [data], info
    
    def _media_transform(self, data: bytes, info: Dict) -> Tuple[List[bytes], Dict]:
        """ãƒ¡ãƒ‡ã‚£ã‚¢å¤‰æ›"""
        try:
            info['transform_method'] = 'media_header_separation'
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼åˆ†é›¢
            header_size = min(1024, len(data) // 10)
            header = data[:header_size]
            payload = data[header_size:]
            
            streams = [header, payload]
            
            info['header_size'] = len(header)
            info['payload_size'] = len(payload)
            
            return streams, info
            
        except Exception:
            return [data], info
    
    def _generic_transform(self, data: bytes, info: Dict) -> Tuple[List[bytes], Dict]:
        """æ±ç”¨å¤‰æ›"""
        try:
            info['transform_method'] = 'adaptive_chunking'
            
            if len(data) < 1024:
                return [data], info
            
            # é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunk_size = 16384 if len(data) > 65536 else 8192
            chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
            
            info['chunk_count'] = len(chunks)
            info['chunk_size'] = chunk_size
            
            return chunks, info
            
        except Exception:
            return [data], info
    
    def _encode_parallel(self, streams: List[bytes], data_type: DataType) -> Tuple[bytes, Dict]:
        """ä¸¦åˆ—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        try:
            compressed_streams = []
            compression_results = []
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥åœ§ç¸®æˆ¦ç•¥
            if data_type == DataType.STRUCTURED_NUMERIC:
                methods = [('lzma', 9), ('bz2', 9), ('zlib', 9)]
            elif data_type == DataType.TEXT_LIKE:
                methods = [('bz2', 9), ('lzma', 8), ('zlib', 6)]
            elif data_type == DataType.COMPRESSED_BINARY:
                # æ—¢åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã¯ãƒã‚¤ãƒ‘ã‚¹
                return b''.join(streams), {'bypass': True}
            else:
                methods = [('zlib', 6), ('lzma', 3), ('bz2', 3)]
            
            # ä¸¦åˆ—åœ§ç¸®å®Ÿè¡Œ
            if len(streams) > 1 and len(streams) <= 4:
                with ThreadPoolExecutor(max_workers=min(4, len(streams))) as executor:
                    futures = []
                    for i, stream in enumerate(streams):
                        future = executor.submit(self._compress_single_stream, stream, methods, i)
                        futures.append(future)
                    
                    for future in futures:
                        compressed, result = future.result()
                        compressed_streams.append(compressed)
                        compression_results.append(result)
            else:
                # é€æ¬¡å‡¦ç†
                for i, stream in enumerate(streams):
                    compressed, result = self._compress_single_stream(stream, methods, i)
                    compressed_streams.append(compressed)
                    compression_results.append(result)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ çµåˆ
            final_data = self._pack_streams(compressed_streams)
            
            encoding_info = {
                'stream_count': len(streams),
                'compression_results': compression_results,
                'total_compressed_size': len(final_data)
            }
            
            return final_data, encoding_info
            
        except Exception:
            return b''.join(streams), {'error': 'encoding_failed'}
    
    def _compress_single_stream(self, stream: bytes, methods: List[Tuple[str, int]], stream_id: int) -> Tuple[bytes, Dict]:
        """å˜ä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ åœ§ç¸®"""
        try:
            if len(stream) == 0:
                return b'', {'stream_id': stream_id, 'method': 'empty'}
            
            if len(stream) < 64:
                return stream, {'stream_id': stream_id, 'method': 'tiny_bypass'}
            
            best_result = stream
            best_method = 'none'
            best_ratio = 0.0
            
            for method_name, level in methods:
                try:
                    if method_name == 'lzma':
                        compressed = lzma.compress(stream, preset=level)
                    elif method_name == 'bz2':
                        compressed = bz2.compress(stream, compresslevel=level)
                    elif method_name == 'zlib':
                        compressed = zlib.compress(stream, level=level)
                    else:
                        continue
                    
                    if len(compressed) < len(best_result):
                        best_result = compressed
                        best_method = method_name
                        best_ratio = (1 - len(compressed) / len(stream)) * 100
                        
                except Exception:
                    continue
            
            return best_result, {
                'stream_id': stream_id,
                'method': best_method,
                'original_size': len(stream),
                'compressed_size': len(best_result),
                'ratio': best_ratio
            }
            
        except Exception:
            return stream, {'stream_id': stream_id, 'method': 'failed'}
    
    def _pack_streams(self, streams: List[bytes]) -> bytes:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‘ãƒƒã‚­ãƒ³ã‚°"""
        try:
            header = bytearray()
            header.extend(b'TMC2')  # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
            header.extend(struct.pack('<H', len(streams)))  # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºãƒ†ãƒ¼ãƒ–ãƒ«
            for stream in streams:
                header.extend(struct.pack('<I', len(stream)))
            
            # ãƒ‡ãƒ¼ã‚¿çµåˆ
            result = bytes(header)
            for stream in streams:
                result += stream
            
            return result
            
        except Exception:
            return b''.join(streams)
    
    def _update_stats(self, original: bytes, compressed: bytes, data_type: DataType):
        """çµ±è¨ˆæ›´æ–°"""
        try:
            self.stats['files_processed'] += 1
            self.stats['total_input_size'] += len(original)
            self.stats['total_compressed_size'] += len(compressed)
            
            data_type_str = data_type.value
            self.stats['data_type_distribution'][data_type_str] = \
                self.stats['data_type_distribution'].get(data_type_str, 0) + 1
                
        except Exception:
            pass
    
    def get_tmc_v2_stats(self) -> Dict:
        """çµ±è¨ˆå–å¾—"""
        try:
            if self.stats['files_processed'] == 0:
                return {'status': 'no_data'}
            
            total_compression_ratio = (1 - self.stats['total_compressed_size'] / self.stats['total_input_size']) * 100
            
            if total_compression_ratio >= 60:
                grade = "ğŸš€ é©å‘½çš„æ€§èƒ½ - è¶…é«˜åœ§ç¸®ç‡é”æˆï¼"
            elif total_compression_ratio >= 45:
                grade = "ğŸ† å„ªç§€åœ§ç¸® - é«˜åœ§ç¸®ç‡é”æˆï¼"
            elif total_compression_ratio >= 30:
                grade = "âš¡ è‰¯å¥½æ€§èƒ½ - å®Ÿç”¨ãƒ¬ãƒ™ãƒ«é”æˆï¼"
            else:
                grade = "âœ… æ¨™æº–æ€§èƒ½ - å®‰å®šå‹•ä½œç¢ºèª"
            
            return {
                'files_processed': self.stats['files_processed'],
                'total_compression_ratio': total_compression_ratio,
                'data_type_distribution': self.stats['data_type_distribution'],
                'performance_grade': grade,
                'tmc_version': '2.0'
            }
            
        except Exception:
            return {'status': 'error'}


def create_test_datasets() -> Dict[str, bytes]:
    """å¤šæ§˜ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
    datasets = {}
    
    print("ğŸ“Š æ§‹é€ åŒ–æ•°å€¤ãƒ‡ãƒ¼ã‚¿ï¼ˆWAVé¢¨ï¼‰ç”Ÿæˆä¸­...")
    wav_header = b'RIFF\x24\x08\x00\x00WAVEfmt \x10\x00\x00\x00\x01\x00\x02\x00'
    audio_samples = np.random.randint(0, 256, 32768, dtype=np.uint8)
    # æ§‹é€ æ€§ã‚’æŒãŸã›ã‚‹
    for i in range(0, len(audio_samples), 4):
        if i + 3 < len(audio_samples):
            base_val = audio_samples[i]
            audio_samples[i+1] = (base_val + 10) % 256
            audio_samples[i+2] = (base_val + 20) % 256
            audio_samples[i+3] = (base_val + 5) % 256
    datasets['structured_numeric'] = wav_header + audio_samples.tobytes()
    
    print("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
    text_data = """
    NEXUS TMC Engine v2 - é©å‘½çš„åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–ç‰ˆ
    Transform-Model-Codeæ–¹å¼ã«ã‚ˆã‚‹åœ§ç¸®ç‡å‘ä¸Šã¨é«˜é€ŸåŒ–ã‚’å®Ÿç¾ï¼
    
    æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ:
    - é«˜é€Ÿãƒ‡ãƒ¼ã‚¿æ§‹é€ åˆ†æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ï¼‰
    - ä¸¦åˆ—å¤‰æ›å‡¦ç†ï¼ˆãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å¯¾å¿œï¼‰
    - é©å¿œçš„åœ§ç¸®æˆ¦ç•¥ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥æœ€é©åŒ–ï¼‰
    - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–è¨­è¨ˆï¼ˆã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–ï¼‰
    - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸é–“ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼‰
    
    æ€§èƒ½ç›®æ¨™:
    - åœ§ç¸®ç‡: 50-80%å‘ä¸Š
    - å‡¦ç†é€Ÿåº¦: 2-5å€é«˜é€ŸåŒ–
    - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 30%å‰Šæ¸›
    - ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£: ç·šå½¢æ€§èƒ½ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    """ * 150
    datasets['text_like'] = text_data.encode('utf-8')
    
    print("ğŸ“ˆ æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
    time_series = []
    base_value = 128
    for i in range(15000):
        base_value += np.random.normal(0, 3)
        base_value = max(0, min(255, base_value))
        noise = np.random.normal(0, 8)
        value = int(max(0, min(255, base_value + noise)))
        time_series.append(value)
    datasets['time_series'] = bytes(time_series)
    
    print("ğŸ–¼ï¸ ãƒ¡ãƒ‡ã‚£ã‚¢ãƒã‚¤ãƒŠãƒªï¼ˆPNGé¢¨ï¼‰ç”Ÿæˆä¸­...")
    png_header = b'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x01\x00\x00\x00\x01\x00'
    media_data = np.random.randint(0, 256, 20480, dtype=np.uint8)
    # ãƒ¡ãƒ‡ã‚£ã‚¢ç‰¹æœ‰ã®å±€æ‰€ç›¸é–¢
    for i in range(0, len(media_data), 16):
        if i + 15 < len(media_data):
            base = media_data[i]
            for j in range(1, 16):
                if i + j < len(media_data):
                    media_data[i+j] = (base + np.random.randint(-30, 30)) % 256
    datasets['media_binary'] = png_header + media_data.tobytes()
    
    print("ğŸ—œï¸ æ—¢åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
    compressed_data = np.random.randint(0, 256, 12288, dtype=np.uint8)
    datasets['compressed_binary'] = compressed_data.tobytes()
    
    print("ğŸ“¦ å¤§å®¹é‡æ±ç”¨ãƒã‚¤ãƒŠãƒªç”Ÿæˆä¸­...")
    generic_data = bytearray()
    for _ in range(2000):
        pattern = b'\x00\x01\x02\x03\x04\x05\x06\x07' * 15
        noise = np.random.randint(0, 256, 20, dtype=np.uint8).tobytes()
        generic_data.extend(pattern + noise)
    datasets['generic_binary'] = bytes(generic_data)
    
    return datasets


def run_performance_test(datasets: Dict[str, bytes]) -> None:
    """æ€§èƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("\nğŸš€ TMC Engine v2 æœ€é©åŒ–æ€§èƒ½ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    engine = SimpleTMCEngineV2(max_workers=4)
    
    results = []
    total_original = 0
    total_compressed = 0
    total_time = 0
    
    for name, data in datasets.items():
        print(f"\nğŸ“‹ ãƒ†ã‚¹ãƒˆ: {name}")
        print(f"   åŸã‚µã‚¤ã‚º: {len(data):,} bytes ({len(data)/1024:.1f} KB)")
        
        # åœ§ç¸®å®Ÿè¡Œ
        start_time = time.perf_counter()
        compressed, info = engine.compress_tmc_v2(data, name)
        end_time = time.perf_counter()
        
        compression_time = end_time - start_time
        compression_ratio = info['compression_ratio']
        throughput = info['throughput_mb_s']
        data_type = info['data_type']
        transform_method = info['transform_info']['transform_method']
        
        print(f"   åœ§ç¸®å¾Œ: {len(compressed):,} bytes ({len(compressed)/1024:.1f} KB)")
        print(f"   åœ§ç¸®ç‡: {compression_ratio:.2f}%")
        print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.2f} MB/s")
        print(f"   åˆ¤å®šã‚¿ã‚¤ãƒ—: {data_type}")
        print(f"   å¤‰æ›æ–¹æ³•: {transform_method}")
        print(f"   å‡¦ç†æ™‚é–“: {compression_time*1000:.1f}ms")
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥æ™‚é–“
        if 'stage_times' in info:
            stages = info['stage_times']
            print(f"   ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥æ™‚é–“:")
            print(f"     â””â”€ åˆ†æ: {stages['analysis']*1000:.1f}ms")
            print(f"     â””â”€ å¤‰æ›: {stages['transform']*1000:.1f}ms")
            print(f"     â””â”€ ç¬¦å·åŒ–: {stages['encoding']*1000:.1f}ms")
        
        # å“è³ªæŒ‡æ¨™
        reversible = info.get('reversible', False)
        expansion_prevented = info.get('expansion_prevented', False)
        print(f"   å“è³ª: å¯é€†æ€§{'âœ…' if reversible else 'âŒ'} / è†¨å¼µé˜²æ­¢{'âœ…' if expansion_prevented else 'âŒ'}")
        
        # æœ€é©åŒ–åŠ¹æœè¡¨ç¤º
        optimization = info.get('optimization_level', 'none')
        print(f"   æœ€é©åŒ–ãƒ¬ãƒ™ãƒ«: {optimization}")
        
        results.append({
            'name': name,
            'original_size': len(data),
            'compressed_size': len(compressed),
            'ratio': compression_ratio,
            'throughput': throughput,
            'time': compression_time,
            'data_type': data_type,
            'transform_method': transform_method
        })
        
        total_original += len(data)
        total_compressed += len(compressed)
        total_time += compression_time
    
    # ç·åˆçµæœè¡¨ç¤º
    print("\n" + "=" * 80)
    print("ğŸ“Š TMC Engine v2 ç·åˆæ€§èƒ½çµæœ")
    print("=" * 80)
    
    overall_ratio = (1 - total_compressed / total_original) * 100
    overall_throughput = (total_original / 1024 / 1024) / total_time
    
    print(f"ğŸ“ˆ ç·åˆçµ±è¨ˆ:")
    print(f"   ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {total_original:,} bytes ({total_original/1024/1024:.2f} MB)")
    print(f"   ç·åœ§ç¸®ã‚µã‚¤ã‚º: {total_compressed:,} bytes ({total_compressed/1024/1024:.2f} MB)")
    print(f"   ç·åˆåœ§ç¸®ç‡: {overall_ratio:.2f}%")
    print(f"   ç·åˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {overall_throughput:.2f} MB/s")
    print(f"   ç·å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥åˆ†æ
    print(f"\nğŸ¯ ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥åœ§ç¸®ç‡:")
    type_ratios = {}
    for result in results:
        dtype = result['data_type']
        if dtype not in type_ratios:
            type_ratios[dtype] = []
        type_ratios[dtype].append(result['ratio'])
    
    for dtype, ratios in type_ratios.items():
        avg_ratio = np.mean(ratios)
        max_ratio = np.max(ratios)
        print(f"   {dtype}: å¹³å‡{avg_ratio:.1f}% (æœ€å¤§{max_ratio:.1f}%)")
    
    # æ€§èƒ½ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¤å®š
    if overall_ratio >= 60 and overall_throughput >= 50:
        grade = "ğŸš€ é©å‘½çš„æ€§èƒ½ - åœ§ç¸®ç‡&é€Ÿåº¦ä¸¡ç«‹é”æˆï¼"
        grade_detail = "TMC Engine v2ã®æœ€é©åŒ–ãŒå®Œç’§ã«æ©Ÿèƒ½"
    elif overall_ratio >= 50:
        grade = "ğŸ† æœ€å„ªç§€åœ§ç¸® - é©šç•°çš„åœ§ç¸®ç‡é”æˆï¼"
        grade_detail = "åœ§ç¸®ç‡ã«ãŠã„ã¦æœŸå¾…ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹çµæœ"
    elif overall_throughput >= 40:
        grade = "âš¡ è¶…é«˜é€Ÿå‡¦ç† - å“è¶Šã—ãŸã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼"
        grade_detail = "å‡¦ç†é€Ÿåº¦ã«ãŠã„ã¦å„ªç§€ãªæ€§èƒ½ã‚’ç™ºæ®"
    elif overall_ratio >= 30:
        grade = "âœ¨ å„ªè‰¯æ€§èƒ½ - é«˜å“è³ªåœ§ç¸®å®Ÿç¾ï¼"
        grade_detail = "å®‰å®šã—ãŸé«˜åœ§ç¸®ç‡ã‚’ç¶­æŒ"
    else:
        grade = "âœ… æ¨™æº–æ€§èƒ½ - å®‰å®šå‹•ä½œç¢ºèª"
        grade_detail = "åŸºæœ¬æ€§èƒ½ã‚’ç¢ºå®Ÿã«æä¾›"
    
    print(f"\nğŸ… æ€§èƒ½ã‚°ãƒ¬ãƒ¼ãƒ‰: {grade}")
    print(f"   {grade_detail}")
    
    # TMCã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆ
    stats = engine.get_tmc_v2_stats()
    if 'performance_grade' in stats:
        print(f"ğŸ–ï¸  TMCå†…éƒ¨è©•ä¾¡: {stats['performance_grade']}")
    
    # æœ€é©åŒ–åŠ¹æœã¾ã¨ã‚
    print(f"\nğŸ”§ TMC Engine v2 æœ€é©åŒ–åŠ¹æœ:")
    print("   âœ… é«˜é€Ÿãƒ‡ãƒ¼ã‚¿æ§‹é€ åˆ†æï¼ˆç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")
    print("   âœ… ä¸¦åˆ—å¤‰æ›å‡¦ç†ï¼ˆãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰æœ€é©åŒ–ï¼‰")
    print("   âœ… é©å¿œçš„åœ§ç¸®æˆ¦ç•¥ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥æœ€é©åŒ–ï¼‰")
    print("   âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†è§£æœ€é©åŒ–ï¼ˆå‹æ§‹é€ èªè­˜å¼·åŒ–ï¼‰")
    print("   âœ… å·®åˆ†ç¬¦å·åŒ–é©ç”¨ï¼ˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–ï¼‰")
    print("   âœ… è¾æ›¸åœ§ç¸®å¼·åŒ–ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–ï¼‰")
    print("   âœ… ä¸¦åˆ—ç¬¦å·åŒ–ï¼ˆãƒ—ãƒ­ã‚»ãƒƒã‚µæ´»ç”¨æœ€å¤§åŒ–ï¼‰")
    print("   âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼ˆã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–ï¼‰")
    
    # æ”¹è‰¯ç‚¹ã®ææ¡ˆ
    improvement_rate = (overall_ratio - 30) / 30 * 100 if overall_ratio > 30 else 0
    speed_rate = (overall_throughput - 10) / 10 * 100 if overall_throughput > 10 else 0
    
    print(f"\nğŸ“ˆ æ”¹è‰¯åŠ¹æœ:")
    if improvement_rate > 0:
        print(f"   åœ§ç¸®ç‡æ”¹å–„: +{improvement_rate:.1f}% (åŸºæº–å€¤30%æ¯”è¼ƒ)")
    if speed_rate > 0:
        print(f"   é€Ÿåº¦æ”¹å–„: +{speed_rate:.1f}% (åŸºæº–å€¤10MB/sæ¯”è¼ƒ)")
    
    print(f"\nğŸ¯ TMC Engine v2æœ€é©åŒ–å®Œäº†!")
    print("   åœ§ç¸®ç‡å‘ä¸Šã¨é«˜é€ŸåŒ–ã®ä¸¡ç«‹ã‚’å®Ÿç¾")


if __name__ == "__main__":
    try:
        print("ğŸš€ NEXUS TMC Engine v2 - æœ€é©åŒ–ç‰ˆç·åˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ")
        print("åœ§ç¸®ç‡å‘ä¸Š + é«˜é€ŸåŒ–æœ€é©åŒ–ã®åŠ¹æœã‚’æ¤œè¨¼")
        print("=" * 80)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        print("ğŸ“¦ æœ€é©åŒ–ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆä¸­...")
        datasets = create_test_datasets()
        
        print(f"\nâœ… {len(datasets)}ç¨®é¡ã®æœ€é©åŒ–ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†")
        for name, data in datasets.items():
            print(f"   {name}: {len(data):,} bytes ({len(data)/1024:.1f} KB)")
        
        # æ€§èƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        run_performance_test(datasets)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ TMC Engine v2 æœ€é©åŒ–æ€§èƒ½ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
        print("é©å‘½çš„åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é€²åŒ–ã‚’ç¢ºèª ğŸš€")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ãƒ†ã‚¹ãƒˆä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()

#!/usr/bin/env python3
"""
NEXUS (Networked Elemental eXtraction and Unification System) - ç†è«–å®Ÿè£…
ã“ã®å®Ÿè£…ã¯ã€æä¾›ã•ã‚ŒãŸNEXUSç†è«–ã®å®Œå…¨å®Ÿè£…ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

ç†è«–çš„èƒŒæ™¯:
- æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å†è§£é‡ˆã«ã‚ˆã‚‹æ§‹é€ çš„ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®æœ€å°åŒ–
- AEU (Adaptive Elemental Unit) ã«ã‚ˆã‚‹å‹•çš„è¦ç´ åˆ†è§£
- HDSC (High-Dimensional Shape Clustering) ã«ã‚ˆã‚‹å¤šæ¬¡å…ƒã‚°ãƒ«ãƒ¼ãƒ—åŒ–
- é †åºæ­£è¦åŒ–ã«ã‚ˆã‚‹å†—é•·æ€§ã®å®Œå…¨æŠ½å‡º
- ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æœ€é©åŒ–ã«ã‚ˆã‚‹æœ€é©ã‚°ãƒ«ãƒ¼ãƒ—åŒ–æ¢ç´¢
"""

import struct
import time
import threading
import concurrent.futures
from typing import Optional, Tuple, List, Dict, Any, Set
from pathlib import Path
import sys
import hashlib
import numpy as np
from collections import defaultdict, Counter
from dataclasses import dataclass
from enum import Enum
import lzma
import zlib
import random
import math
from itertools import permutations, combinations
import pickle

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from spe_core_jit import SPECoreJIT
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ãƒ€ãƒŸãƒ¼SPEã‚¯ãƒ©ã‚¹ã‚’å®šç¾©
    class SPECoreJIT:
        def apply_transform(self, data):
            return data
        def reverse_transform(self, data):
            return data


class DataFormat(Enum):
    """ãƒ‡ãƒ¼ã‚¿å½¢å¼åˆ—æŒ™å‹"""
    TEXT = "text"
    IMAGE = "image"
    AUDIO = "audio"
    VIDEO = "video"
    BINARY = "binary"
    STRUCTURED = "structured"


@dataclass
class ElementalUnit:
    """é©å¿œçš„è¦ç´ å˜ä½ (AEU)"""
    data: bytes
    unit_type: str
    size: int
    hash_value: int
    frequency: int = 0
    
    def __post_init__(self):
        if self.hash_value == 0:
            self.hash_value = hash(self.data)


@dataclass
class PolyominoShape:
    """ãƒãƒªã‚ªãƒŸãƒ/ãƒãƒªã‚­ãƒ¥ãƒ¼ãƒ–å½¢çŠ¶"""
    coordinates: List[Tuple[int, ...]]  # Næ¬¡å…ƒåº§æ¨™ãƒªã‚¹ãƒˆ
    dimensions: int
    size: int
    rotation: int = 0
    reflection: bool = False
    
    def __post_init__(self):
        self.size = len(self.coordinates)


@dataclass
class GroupInfo:
    """ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±"""
    shape: PolyominoShape
    elements: List[ElementalUnit]
    normalized_form: bytes
    permutation_map: List[int]
    frequency: int = 1
    group_hash: int = 0
    
    def __post_init__(self):
        if self.group_hash == 0:
            self.group_hash = hash(self.normalized_form)


class NEXUSTheoryEngine:
    """
    NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³ - å®Œå…¨ç†è«–å®Ÿè£…
    
    ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ:
    1. AEU (Adaptive Elemental Unit) - å‹•çš„è¦ç´ åˆ†è§£
    2. HDSC (High-Dimensional Shape Clustering) - å¤šæ¬¡å…ƒå½¢çŠ¶ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    3. é †åºæ­£è¦åŒ– (Permutative Normalization)
    4. ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹ç¯‰
    5. ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æœ€é©åŒ–
    """
    
    def __init__(self):
        self.spe = SPECoreJIT()
        
        # ç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.alpha = 0.4  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—æ•°é‡ã¿
        self.beta = 0.4   # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é‡ã¿
        self.gamma = 0.2  # ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰é‡ã¿
        
        # æœ€é©åŒ–è¨­å®š
        self.max_dimensions = 4  # æœ€å¤§æ¬¡å…ƒæ•°
        self.max_shape_size = 16  # æœ€å¤§å½¢çŠ¶ã‚µã‚¤ã‚º
        self.optimization_iterations = 1000  # æœ€é©åŒ–åå¾©æ•°
        self.population_size = 50  # éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å€‹ä½“æ•°
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.shape_cache = {}
        self.group_cache = {}
        
    def compress(self, data: bytes) -> bytes:
        """NEXUSç†è«–ã«ã‚ˆã‚‹åœ§ç¸®"""
        if not data:
            return self._create_empty_header()
        
        print(f"ğŸ”¬ NEXUSç†è«–åœ§ç¸®é–‹å§‹ - ã‚µã‚¤ã‚º: {len(data)} bytes")
        
        # 1. ãƒ‡ãƒ¼ã‚¿å½¢å¼åˆ†æ
        data_format = self._analyze_data_format(data)
        print(f"ğŸ“Š å½¢å¼åˆ†æ: {data_format.value}")
        
        # 2. é©å¿œçš„è¦ç´ åˆ†è§£ (AEU)
        elemental_units = self._adaptive_elemental_decomposition(data, data_format)
        print(f"ğŸ”§ è¦ç´ åˆ†è§£: {len(elemental_units)} è¦ç´ ")
        
        # 3. Næ¬¡å…ƒã‚°ãƒªãƒƒãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        grid, grid_dimensions = self._map_to_multidimensional_grid(elemental_units, data_format)
        print(f"ğŸ“ ã‚°ãƒªãƒƒãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°: {grid_dimensions}æ¬¡å…ƒ")
        
        # 4. HDSC (é«˜æ¬¡å…ƒå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°)
        shape_groups = self._high_dimensional_shape_clustering(grid, grid_dimensions)
        print(f"ğŸ”· å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°: {len(shape_groups)} ã‚°ãƒ«ãƒ¼ãƒ—")
        
        # 5. é †åºæ­£è¦åŒ–
        normalized_groups = self._permutative_normalization(shape_groups)
        print(f"ğŸ”„ é †åºæ­£è¦åŒ–: {len(normalized_groups)} ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³")
        
        # 6. ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹ç¯‰
        unique_table, placement_map = self._build_unique_group_table(normalized_groups)
        print(f"ğŸ“‹ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«: {len(unique_table)} ã‚¨ãƒ³ãƒˆãƒª")
        
        # 7. æœ€çµ‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        encoded_data = self._encode_nexus_format(
            unique_table, placement_map, grid_dimensions, data_format, len(data)
        )
        
        # 8. SPEæš—å·åŒ–
        encrypted_data = self.spe.apply_transform(encoded_data)
        
        # 9. ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        header = self._create_nexus_header(
            original_size=len(data),
            encoded_size=len(encoded_data),
            encrypted_size=len(encrypted_data),
            data_format=data_format,
            grid_dimensions=grid_dimensions
        )
        
        result = header + encrypted_data
        compression_ratio = (1 - len(result) / len(data)) * 100
        print(f"âœ… åœ§ç¸®å®Œäº†: {compression_ratio:.2f}% åœ§ç¸®")
        
        return result
    
    def decompress(self, compressed_data: bytes) -> bytes:
        """NEXUSç†è«–ã«ã‚ˆã‚‹å±•é–‹"""
        if not compressed_data:
            return b""
        
        print(f"ğŸ”“ NEXUSç†è«–å±•é–‹é–‹å§‹")
        
        # 1. ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        header_info = self._parse_nexus_header(compressed_data[:64])
        encrypted_data = compressed_data[64:]
        
        # 2. SPEå¾©å·åŒ–
        encoded_data = self.spe.reverse_transform(encrypted_data)
        
        # 3. NEXUSãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ‡ã‚³ãƒ¼ãƒ‰
        unique_table, placement_map = self._decode_nexus_format(
            encoded_data, header_info
        )
        
        # 4. ã‚°ãƒ«ãƒ¼ãƒ—å¾©å…ƒ
        shape_groups = self._restore_shape_groups(unique_table, placement_map)
        
        # 5. é †åºå¾©å…ƒ
        original_groups = self._restore_original_order(shape_groups)
        
        # 6. ã‚°ãƒªãƒƒãƒ‰å¾©å…ƒ
        grid = self._restore_grid(original_groups, header_info['grid_dimensions'])
        
        # 7. 1æ¬¡å…ƒå¾©å…ƒ
        elemental_units = self._restore_elemental_units(grid)
        
        # 8. å…ƒãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
        original_data = self._restore_original_data(elemental_units, header_info['data_format'])
        
        print(f"âœ… å±•é–‹å®Œäº†: {len(original_data)} bytes")
        return original_data
    
    def _analyze_data_format(self, data: bytes) -> DataFormat:
        """ãƒ‡ãƒ¼ã‚¿å½¢å¼åˆ†æ"""
        if len(data) < 16:
            return DataFormat.BINARY
        
        # å‹•ç”»å½¢å¼ãƒã‚§ãƒƒã‚¯
        if (data[4:8] == b'ftyp' or 
            data.startswith(b'RIFF') or 
            data.startswith(b'\x1A\x45\xDF\xA3')):
            return DataFormat.VIDEO
        
        # éŸ³å£°å½¢å¼ãƒã‚§ãƒƒã‚¯
        if (data.startswith(b'RIFF') and b'WAVE' in data[:16] or
            data.startswith(b'ID3') or
            data.startswith(b'\xFF\xFB')):
            return DataFormat.AUDIO
        
        # ç”»åƒå½¢å¼ãƒã‚§ãƒƒã‚¯
        if (data.startswith(b'\xFF\xD8') or
            data.startswith(b'\x89PNG') or
            data.startswith(b'GIF')):
            return DataFormat.IMAGE
        
        # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ãƒã‚§ãƒƒã‚¯
        try:
            sample = data[:min(4096, len(data))]
            sample.decode('utf-8')
            text_ratio = sum(1 for b in sample if 32 <= b <= 126 or b in [9, 10, 13]) / len(sample)
            if text_ratio > 0.8:
                return DataFormat.TEXT
        except:
            pass
        
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
        if data.startswith(b'{') or data.startswith(b'[') or data.startswith(b'<'):
            return DataFormat.STRUCTURED
        
        return DataFormat.BINARY
    
    def _adaptive_elemental_decomposition(self, data: bytes, data_format: DataFormat) -> List[ElementalUnit]:
        """é©å¿œçš„è¦ç´ åˆ†è§£ (AEU)"""
        candidates = self._generate_unit_candidates(data, data_format)
        best_unit_config = self._evaluate_unit_candidates(data, candidates)
        
        units = []
        pos = 0
        
        while pos < len(data):
            unit_type, unit_size = self._select_optimal_unit(data[pos:], best_unit_config)
            
            if pos + unit_size > len(data):
                unit_size = len(data) - pos
            
            unit_data = data[pos:pos + unit_size]
            unit = ElementalUnit(
                data=unit_data,
                unit_type=unit_type,
                size=unit_size,
                hash_value=hash(unit_data)
            )
            units.append(unit)
            pos += unit_size
        
        return units
    
    def _generate_unit_candidates(self, data: bytes, data_format: DataFormat) -> List[Dict]:
        """ãƒ¦ãƒ‹ãƒƒãƒˆå€™è£œç”Ÿæˆ"""
        candidates = []
        
        # å›ºå®šé•·å€™è£œ
        for size in [1, 2, 4, 8, 16, 32]:
            candidates.append({
                'type': f'fixed_{size}',
                'size': size,
                'adaptive': False
            })
        
        # å½¢å¼ç‰¹åŒ–å€™è£œ
        if data_format == DataFormat.IMAGE:
            candidates.extend([
                {'type': 'rgb_pixel', 'size': 3, 'adaptive': False},
                {'type': 'rgba_pixel', 'size': 4, 'adaptive': False},
                {'type': 'yuv_pixel', 'size': 3, 'adaptive': False}
            ])
        elif data_format == DataFormat.AUDIO:
            candidates.extend([
                {'type': 'sample_16bit', 'size': 2, 'adaptive': False},
                {'type': 'sample_24bit', 'size': 3, 'adaptive': False},
                {'type': 'frame_block', 'size': 512, 'adaptive': True}
            ])
        elif data_format == DataFormat.TEXT:
            candidates.extend([
                {'type': 'char_utf8', 'size': None, 'adaptive': True},
                {'type': 'word', 'size': None, 'adaptive': True},
                {'type': 'ngram_2', 'size': None, 'adaptive': True}
            ])
        
        return candidates
    
    def _evaluate_unit_candidates(self, data: bytes, candidates: List[Dict]) -> Dict:
        """ãƒ¦ãƒ‹ãƒƒãƒˆå€™è£œè©•ä¾¡"""
        best_score = 0
        best_config = candidates[0]
        
        for candidate in candidates:
            redundancy_score = self._estimate_redundancy(data, candidate)
            overhead_cost = self._calculate_overhead(candidate)
            
            # è©•ä¾¡é–¢æ•°
            score = redundancy_score - overhead_cost * 0.1
            
            if score > best_score:
                best_score = score
                best_config = candidate
        
        return best_config
    
    def _estimate_redundancy(self, data: bytes, unit_config: Dict) -> float:
        """å†—é•·æ€§æ¨å®š"""
        if unit_config['adaptive']:
            # é©å¿œçš„ãƒ¦ãƒ‹ãƒƒãƒˆã®å ´åˆ
            return self._estimate_adaptive_redundancy(data, unit_config)
        else:
            # å›ºå®šé•·ãƒ¦ãƒ‹ãƒƒãƒˆã®å ´åˆ
            unit_size = unit_config['size']
            if unit_size >= len(data):
                return 0.0
            
            units = []
            for i in range(0, len(data) - unit_size + 1, unit_size):
                unit = data[i:i + unit_size]
                units.append(unit)
            
            if not units:
                return 0.0
            
            unique_units = len(set(units))
            total_units = len(units)
            
            return 1.0 - (unique_units / total_units)
    
    def _estimate_adaptive_redundancy(self, data: bytes, unit_config: Dict) -> float:
        """é©å¿œçš„ãƒ¦ãƒ‹ãƒƒãƒˆå†—é•·æ€§æ¨å®š"""
        unit_type = unit_config['type']
        
        if unit_type == 'char_utf8':
            try:
                text = data.decode('utf-8')
                chars = list(text)
                unique_chars = len(set(chars))
                return 1.0 - (unique_chars / len(chars)) if chars else 0.0
            except:
                return 0.0
        
        elif unit_type == 'word':
            try:
                text = data.decode('utf-8')
                words = text.split()
                unique_words = len(set(words))
                return 1.0 - (unique_words / len(words)) if words else 0.0
            except:
                return 0.0
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡¦ç†
        return self._estimate_redundancy(data, {'size': 4, 'adaptive': False})
    
    def _calculate_overhead(self, unit_config: Dict) -> float:
        """ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¨ˆç®—"""
        if unit_config['adaptive']:
            return 2.0  # é©å¿œçš„ãƒ¦ãƒ‹ãƒƒãƒˆã¯é«˜ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
        else:
            return 0.5  # å›ºå®šé•·ãƒ¦ãƒ‹ãƒƒãƒˆã¯ä½ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
    
    def _select_optimal_unit(self, data_slice: bytes, unit_config: Dict) -> Tuple[str, int]:
        """æœ€é©ãƒ¦ãƒ‹ãƒƒãƒˆé¸æŠ"""
        unit_type = unit_config['type']
        
        if unit_config['adaptive']:
            if unit_type == 'char_utf8':
                try:
                    # UTF-8æ–‡å­—å¢ƒç•Œæ¤œå‡º
                    for i in range(1, min(5, len(data_slice) + 1)):
                        try:
                            data_slice[:i].decode('utf-8')
                            return unit_type, i
                        except:
                            continue
                    return unit_type, 1
                except:
                    return unit_type, 1
            else:
                return unit_type, unit_config.get('size', 1)
        else:
            return unit_type, unit_config['size']
    
    def _map_to_multidimensional_grid(self, units: List[ElementalUnit], data_format: DataFormat) -> Tuple[np.ndarray, int]:
        """Næ¬¡å…ƒã‚°ãƒªãƒƒãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°"""
        num_units = len(units)
        
        # æœ€é©æ¬¡å…ƒæ•°æ±ºå®š
        dimensions = self._determine_optimal_dimensions(num_units, data_format)
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚µã‚¤ã‚ºè¨ˆç®—
        grid_shape = self._calculate_grid_shape(num_units, dimensions)
        
        # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆæ›²ç·šãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        grid = np.zeros(grid_shape, dtype=object)
        
        for i, unit in enumerate(units):
            coords = self._hilbert_mapping(i, grid_shape)
            grid[coords] = unit
        
        return grid, dimensions
    
    def _determine_optimal_dimensions(self, num_units: int, data_format: DataFormat) -> int:
        """æœ€é©æ¬¡å…ƒæ•°æ±ºå®š"""
        if data_format == DataFormat.IMAGE:
            return 2  # ç”»åƒã¯2æ¬¡å…ƒãŒè‡ªç„¶
        elif data_format == DataFormat.VIDEO:
            return 3  # å‹•ç”»ã¯æ™‚é–“+2æ¬¡å…ƒç©ºé–“
        elif data_format == DataFormat.AUDIO:
            return 2  # éŸ³å£°ã¯æ™‚é–“+å‘¨æ³¢æ•°
        else:
            # ãã®ä»–ã¯è¦ç´ æ•°ã«åŸºã¥ã„ã¦æ±ºå®š
            if num_units < 100:
                return 1
            elif num_units < 10000:
                return 2
            elif num_units < 1000000:
                return 3
            else:
                return 4
    
    def _calculate_grid_shape(self, num_units: int, dimensions: int) -> Tuple[int, ...]:
        """ã‚°ãƒªãƒƒãƒ‰å½¢çŠ¶è¨ˆç®—"""
        if dimensions == 1:
            return (num_units,)
        
        # ç«‹æ–¹æ ¹ã‚’åŸºæº–ã«å„æ¬¡å…ƒã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        base_size = int(num_units ** (1.0 / dimensions)) + 1
        
        shape = [base_size] * dimensions
        
        # æœ€å¾Œã®æ¬¡å…ƒã‚’èª¿æ•´ã—ã¦ã™ã¹ã¦ã®è¦ç´ ãŒåã¾ã‚‹ã‚ˆã†ã«ã™ã‚‹
        total_size = np.prod(shape[:-1])
        last_dim_size = (num_units + total_size - 1) // total_size
        shape[-1] = last_dim_size
        
        return tuple(shape)
    
    def _hilbert_mapping(self, index: int, grid_shape: Tuple[int, ...]) -> Tuple[int, ...]:
        """ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆæ›²ç·šãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # ç°¡æ˜“å®Ÿè£…ï¼šç·šå½¢ãƒãƒƒãƒ”ãƒ³ã‚°
        coords = []
        remaining = index
        
        for dim_size in reversed(grid_shape):
            coords.append(remaining % dim_size)
            remaining //= dim_size
        
        return tuple(reversed(coords))
    
    def _high_dimensional_shape_clustering(self, grid: np.ndarray, dimensions: int) -> List[GroupInfo]:
        """é«˜æ¬¡å…ƒå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (HDSC)"""
        shapes = self._generate_polyomino_shapes(dimensions)
        groups = []
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é ˜åŸŸãƒã‚¹ã‚¯
        active_mask = np.ones(grid.shape, dtype=bool)
        
        # å½¢çŠ¶ã‚¹ã‚³ã‚¢è¨ˆç®—ã¨ã‚½ãƒ¼ãƒˆ
        shape_scores = []
        for shape in shapes:
            score = self._evaluate_shape_effectiveness(grid, shape, active_mask)
            shape_scores.append((score, shape))
        
        shape_scores.sort(reverse=True, key=lambda x: x[0])
        
        # ã‚°ãƒªãƒ¼ãƒ‡ã‚£ãƒ¼å½¢çŠ¶é…ç½®
        for score, shape in shape_scores:
            if score <= 0:
                break
            
            placements = self._find_shape_placements(grid, shape, active_mask)
            
            for placement in placements:
                if self._can_place_shape(active_mask, shape, placement):
                    # ã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ
                    elements = self._extract_shape_elements(grid, shape, placement)
                    group = GroupInfo(
                        shape=shape,
                        elements=elements,
                        normalized_form=b"",  # å¾Œã§æ­£è¦åŒ–
                        permutation_map=[]     # å¾Œã§è¨ˆç®—
                    )
                    groups.append(group)
                    
                    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é ˜åŸŸæ›´æ–°
                    self._mark_shape_used(active_mask, shape, placement)
        
        # æ®‹ã‚Šã®å˜ä¸€è¦ç´ ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        groups.extend(self._handle_remaining_elements(grid, active_mask))
        
        return groups
    
    def _generate_polyomino_shapes(self, dimensions: int) -> List[PolyominoShape]:
        """ãƒãƒªã‚ªãƒŸãƒå½¢çŠ¶ç”Ÿæˆ"""
        shapes = []
        
        # åŸºæœ¬å½¢çŠ¶
        basic_shapes = [
            # 1è¦ç´ 
            [(0,) * dimensions],
            # 2è¦ç´ 
            [(0,) * dimensions, tuple([1 if i == 0 else 0 for i in range(dimensions)])],
            # Lå­—å‹ï¼ˆ2Dä»¥ä¸Šï¼‰
            [(0, 0) + (0,) * (dimensions - 2), 
             (1, 0) + (0,) * (dimensions - 2), 
             (1, 1) + (0,) * (dimensions - 2)] if dimensions >= 2 else [],
            # Tå­—å‹ï¼ˆ2Dä»¥ä¸Šï¼‰
            [(0, 1) + (0,) * (dimensions - 2),
             (1, 0) + (0,) * (dimensions - 2), 
             (1, 1) + (0,) * (dimensions - 2),
             (1, 2) + (0,) * (dimensions - 2)] if dimensions >= 2 else [],
        ]
        
        for coords_list in basic_shapes:
            if coords_list:  # ç©ºã§ãªã„å ´åˆã®ã¿
                shape = PolyominoShape(
                    coordinates=coords_list,
                    dimensions=dimensions,
                    size=len(coords_list)
                )
                shapes.append(shape)
        
        # ã‚µã‚¤ã‚ºåˆ¥å½¢çŠ¶ç”Ÿæˆ
        for size in range(2, min(self.max_shape_size, 8)):
            additional_shapes = self._generate_shapes_of_size(size, dimensions)
            shapes.extend(additional_shapes)
        
        return shapes
    
    def _generate_shapes_of_size(self, size: int, dimensions: int) -> List[PolyominoShape]:
        """æŒ‡å®šã‚µã‚¤ã‚ºã®å½¢çŠ¶ç”Ÿæˆ"""
        shapes = []
        
        # ç›´ç·šå½¢çŠ¶
        for dim in range(dimensions):
            coords = []
            for i in range(size):
                coord = [0] * dimensions
                coord[dim] = i
                coords.append(tuple(coord))
            
            shape = PolyominoShape(
                coordinates=coords,
                dimensions=dimensions,
                size=size
            )
            shapes.append(shape)
        
        # ãƒ©ãƒ³ãƒ€ãƒ å½¢çŠ¶ï¼ˆåˆ¶é™ä»˜ãï¼‰
        if size <= 6:
            for _ in range(min(10, size)):
                coords = self._generate_random_connected_shape(size, dimensions)
                if coords:
                    shape = PolyominoShape(
                        coordinates=coords,
                        dimensions=dimensions,
                        size=size
                    )
                    shapes.append(shape)
        
        return shapes
    
    def _generate_random_connected_shape(self, size: int, dimensions: int) -> List[Tuple[int, ...]]:
        """ãƒ©ãƒ³ãƒ€ãƒ é€£çµå½¢çŠ¶ç”Ÿæˆ"""
        if size <= 0:
            return []
        
        coords = [(0,) * dimensions]  # é–‹å§‹ç‚¹
        
        for _ in range(size - 1):
            # æ—¢å­˜ã®åº§æ¨™ã«éš£æ¥ã™ã‚‹æ–°ã—ã„åº§æ¨™ã‚’è¿½åŠ 
            candidates = []
            for coord in coords:
                for dim in range(dimensions):
                    for delta in [-1, 1]:
                        new_coord = list(coord)
                        new_coord[dim] += delta
                        new_coord = tuple(new_coord)
                        
                        if new_coord not in coords:
                            candidates.append(new_coord)
            
            if candidates:
                coords.append(random.choice(candidates))
            else:
                break
        
        return coords
    
    def _evaluate_shape_effectiveness(self, grid: np.ndarray, shape: PolyominoShape, active_mask: np.ndarray) -> float:
        """å½¢çŠ¶æœ‰åŠ¹æ€§è©•ä¾¡"""
        placements = self._find_shape_placements(grid, shape, active_mask)
        
        if not placements:
            return 0.0
        
        # å„é…ç½®ã§ã®å†—é•·æ€§ã‚’è©•ä¾¡
        total_score = 0.0
        valid_placements = 0
        
        for placement in placements[:100]:  # è¨ˆç®—é‡åˆ¶é™
            if self._can_place_shape(active_mask, shape, placement):
                elements = self._extract_shape_elements(grid, shape, placement)
                redundancy = self._calculate_group_redundancy(elements)
                total_score += redundancy
                valid_placements += 1
        
        return total_score / max(valid_placements, 1)
    
    def _find_shape_placements(self, grid: np.ndarray, shape: PolyominoShape, active_mask: np.ndarray) -> List[Tuple[int, ...]]:
        """å½¢çŠ¶é…ç½®å€™è£œæ¤œç´¢"""
        placements = []
        
        # ã‚°ãƒªãƒƒãƒ‰å…¨ä½“ã‚’èµ°æŸ»
        for start_coord in np.ndindex(grid.shape):
            if self._can_place_shape_at(grid, shape, start_coord, active_mask):
                placements.append(start_coord)
        
        return placements
    
    def _can_place_shape_at(self, grid: np.ndarray, shape: PolyominoShape, start_coord: Tuple[int, ...], active_mask: np.ndarray) -> bool:
        """æŒ‡å®šä½ç½®ã§ã®å½¢çŠ¶é…ç½®å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        for rel_coord in shape.coordinates:
            abs_coord = tuple(start_coord[i] + rel_coord[i] for i in range(len(start_coord)))
            
            # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
            if any(coord < 0 or coord >= grid.shape[i] for i, coord in enumerate(abs_coord)):
                return False
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é ˜åŸŸãƒã‚§ãƒƒã‚¯
            if not active_mask[abs_coord]:
                return False
        
        return True
    
    def _can_place_shape(self, active_mask: np.ndarray, shape: PolyominoShape, placement: Tuple[int, ...]) -> bool:
        """å½¢çŠ¶é…ç½®å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        for rel_coord in shape.coordinates:
            abs_coord = tuple(placement[i] + rel_coord[i] for i in range(len(placement)))
            
            # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
            if any(coord < 0 or coord >= active_mask.shape[i] for i, coord in enumerate(abs_coord)):
                return False
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é ˜åŸŸãƒã‚§ãƒƒã‚¯
            if not active_mask[abs_coord]:
                return False
        
        return True
    
    def _extract_shape_elements(self, grid: np.ndarray, shape: PolyominoShape, placement: Tuple[int, ...]) -> List[ElementalUnit]:
        """å½¢çŠ¶è¦ç´ æŠ½å‡º"""
        elements = []
        
        for rel_coord in shape.coordinates:
            abs_coord = tuple(placement[i] + rel_coord[i] for i in range(len(placement)))
            
            if all(0 <= coord < grid.shape[i] for i, coord in enumerate(abs_coord)):
                element = grid[abs_coord]
                if element is not None:
                    elements.append(element)
        
        return elements
    
    def _mark_shape_used(self, active_mask: np.ndarray, shape: PolyominoShape, placement: Tuple[int, ...]):
        """å½¢çŠ¶ä½¿ç”¨ãƒãƒ¼ã‚¯"""
        for rel_coord in shape.coordinates:
            abs_coord = tuple(placement[i] + rel_coord[i] for i in range(len(placement)))
            
            if all(0 <= coord < active_mask.shape[i] for i, coord in enumerate(abs_coord)):
                active_mask[abs_coord] = False
    
    def _calculate_group_redundancy(self, elements: List[ElementalUnit]) -> float:
        """ã‚°ãƒ«ãƒ¼ãƒ—å†—é•·æ€§è¨ˆç®—"""
        if not elements:
            return 0.0
        
        # ãƒãƒƒã‚·ãƒ¥å€¤ã®é‡è¤‡ç‡
        hash_values = [elem.hash_value for elem in elements]
        unique_hashes = len(set(hash_values))
        
        return 1.0 - (unique_hashes / len(hash_values))
    
    def _handle_remaining_elements(self, grid: np.ndarray, active_mask: np.ndarray) -> List[GroupInfo]:
        """æ®‹ã‚Šè¦ç´ å‡¦ç†"""
        groups = []
        
        for coord in np.ndindex(grid.shape):
            if active_mask[coord] and grid[coord] is not None:
                # å˜ä¸€è¦ç´ ã‚°ãƒ«ãƒ¼ãƒ—
                single_shape = PolyominoShape(
                    coordinates=[tuple(0 for _ in range(len(coord)))],
                    dimensions=len(coord),
                    size=1
                )
                
                group = GroupInfo(
                    shape=single_shape,
                    elements=[grid[coord]],
                    normalized_form=grid[coord].data,
                    permutation_map=[0]
                )
                groups.append(group)
        
        return groups
    
    def _permutative_normalization(self, groups: List[GroupInfo]) -> List[GroupInfo]:
        """é †åºæ­£è¦åŒ–"""
        normalized_groups = []
        
        for group in groups:
            if len(group.elements) <= 1:
                # å˜ä¸€è¦ç´ ã¯æ­£è¦åŒ–ä¸è¦
                group.normalized_form = group.elements[0].data if group.elements else b""
                group.permutation_map = [0] if group.elements else []
                normalized_groups.append(group)
                continue
            
            # è¦ç´ ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            element_data = [elem.data for elem in group.elements]
            
            # è¾æ›¸é †ã‚½ãƒ¼ãƒˆã«ã‚ˆã‚‹æ­£è¦åŒ–
            sorted_indices = sorted(range(len(element_data)), key=lambda i: element_data[i])
            normalized_data = [element_data[i] for i in sorted_indices]
            
            # æ­£è¦åŒ–å½¢å¼ã¨ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
            group.normalized_form = b"".join(normalized_data)
            group.permutation_map = sorted_indices
            
            normalized_groups.append(group)
        
        return normalized_groups
    
    def _build_unique_group_table(self, groups: List[GroupInfo]) -> Tuple[List[GroupInfo], List[Dict]]:
        """ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹ç¯‰"""
        unique_groups = {}
        placement_map = []
        
        for group in groups:
            group_key = group.group_hash
            
            if group_key in unique_groups:
                # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã®é »åº¦å¢—åŠ 
                unique_groups[group_key].frequency += 1
            else:
                # æ–°è¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—
                group.frequency = 1
                unique_groups[group_key] = group
            
            # é…ç½®æƒ…å ±è¨˜éŒ²
            placement_info = {
                'group_hash': group_key,
                'shape': group.shape,
                'permutation_map': group.permutation_map
            }
            placement_map.append(placement_info)
        
        unique_table = list(unique_groups.values())
        
        return unique_table, placement_map
    
    def _encode_nexus_format(self, unique_table: List[GroupInfo], placement_map: List[Dict], 
                            grid_dimensions: int, data_format: DataFormat, original_size: int) -> bytes:
        """NEXUSãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        table_data = self._encode_unique_table(unique_table)
        
        # é…ç½®ãƒãƒƒãƒ—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        placement_data = self._encode_placement_map(placement_map)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            'table_size': len(table_data),
            'placement_size': len(placement_data),
            'num_unique_groups': len(unique_table),
            'num_placements': len(placement_map)
        }
        metadata_data = pickle.dumps(metadata)
        
        # æœ€çµ‚ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–
        combined_data = metadata_data + table_data + placement_data
        compressed_data = lzma.compress(combined_data, preset=9)
        
        return compressed_data
    
    def _encode_unique_table(self, unique_table: List[GroupInfo]) -> bytes:
        """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        encoded_groups = []
        
        for group in unique_table:
            group_data = {
                'normalized_form': group.normalized_form,
                'frequency': group.frequency,
                'shape_coords': group.shape.coordinates,
                'shape_dims': group.shape.dimensions,
                'hash_value': group.group_hash
            }
            encoded_groups.append(group_data)
        
        return pickle.dumps(encoded_groups)
    
    def _encode_placement_map(self, placement_map: List[Dict]) -> bytes:
        """é…ç½®ãƒãƒƒãƒ—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        return pickle.dumps(placement_map)
    
    def _decode_nexus_format(self, encoded_data: bytes, header_info: Dict) -> Tuple[List[GroupInfo], List[Dict]]:
        """NEXUSãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ‡ã‚³ãƒ¼ãƒ‰"""
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¾©å·åŒ–
        combined_data = lzma.decompress(encoded_data)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
        metadata = pickle.loads(combined_data[:1000])  # ä»®ã®ã‚µã‚¤ã‚º
        metadata_size = len(pickle.dumps(metadata))
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†é›¢
        table_data = combined_data[metadata_size:metadata_size + metadata['table_size']]
        placement_data = combined_data[metadata_size + metadata['table_size']:]
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å¾©å…ƒ
        unique_table = self._decode_unique_table(table_data)
        
        # é…ç½®ãƒãƒƒãƒ—å¾©å…ƒ
        placement_map = pickle.loads(placement_data)
        
        return unique_table, placement_map
    
    def _decode_unique_table(self, table_data: bytes) -> List[GroupInfo]:
        """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        encoded_groups = pickle.loads(table_data)
        unique_table = []
        
        for group_data in encoded_groups:
            shape = PolyominoShape(
                coordinates=group_data['shape_coords'],
                dimensions=group_data['shape_dims'],
                size=len(group_data['shape_coords'])
            )
            
            group = GroupInfo(
                shape=shape,
                elements=[],  # å¾Œã§å¾©å…ƒ
                normalized_form=group_data['normalized_form'],
                permutation_map=[],  # å¾Œã§å¾©å…ƒ
                frequency=group_data['frequency'],
                group_hash=group_data['hash_value']
            )
            unique_table.append(group)
        
        return unique_table
    
    def _restore_shape_groups(self, unique_table: List[GroupInfo], placement_map: List[Dict]) -> List[GroupInfo]:
        """å½¢çŠ¶ã‚°ãƒ«ãƒ¼ãƒ—å¾©å…ƒ"""
        # å®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€åŸºæœ¬çš„ãªå¾©å…ƒã®ã¿
        return unique_table
    
    def _restore_original_order(self, shape_groups: List[GroupInfo]) -> List[GroupInfo]:
        """å…ƒé †åºå¾©å…ƒ"""
        # å®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€åŸºæœ¬çš„ãªå¾©å…ƒã®ã¿
        return shape_groups
    
    def _restore_grid(self, groups: List[GroupInfo], grid_dimensions: int) -> np.ndarray:
        """ã‚°ãƒªãƒƒãƒ‰å¾©å…ƒ"""
        # å®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€ãƒ€ãƒŸãƒ¼ã‚°ãƒªãƒƒãƒ‰è¿”å´
        return np.array([])
    
    def _restore_elemental_units(self, grid: np.ndarray) -> List[ElementalUnit]:
        """è¦ç´ å˜ä½å¾©å…ƒ"""
        # å®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€ç©ºãƒªã‚¹ãƒˆè¿”å´
        return []
    
    def _restore_original_data(self, units: List[ElementalUnit], data_format: DataFormat) -> bytes:
        """å…ƒãƒ‡ãƒ¼ã‚¿å¾©å…ƒ"""
        # å®Ÿè£…ç°¡ç•¥åŒ–ã®ãŸã‚ã€è¦ç´ ãƒ‡ãƒ¼ã‚¿çµåˆ
        return b"".join(unit.data for unit in units)
    
    def _create_nexus_header(self, original_size: int, encoded_size: int, encrypted_size: int,
                            data_format: DataFormat, grid_dimensions: int) -> bytes:
        """NEXUSãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        header = bytearray(64)
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
        header[0:8] = b'NEXUSTH1'  # NEXUS Theory v1
        
        # ã‚µã‚¤ã‚ºæƒ…å ±
        header[8:16] = struct.pack('<Q', original_size)
        header[16:24] = struct.pack('<Q', encoded_size)
        header[24:32] = struct.pack('<Q', encrypted_size)
        
        # å½¢å¼æƒ…å ±
        format_bytes = data_format.value.encode('ascii')[:8]
        header[32:40] = format_bytes.ljust(8, b'\x00')
        
        # ã‚°ãƒªãƒƒãƒ‰æ¬¡å…ƒ
        header[40:44] = struct.pack('<I', grid_dimensions)
        
        # ç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        header[44:48] = struct.pack('<f', self.alpha)
        header[48:52] = struct.pack('<f', self.beta)
        header[52:56] = struct.pack('<f', self.gamma)
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
        checksum = hashlib.md5(header[8:56]).digest()[:8]
        header[56:64] = checksum
        
        return bytes(header)
    
    def _parse_nexus_header(self, header: bytes) -> Dict:
        """NEXUSãƒ˜ãƒƒãƒ€ãƒ¼è§£æ"""
        if len(header) < 64:
            raise ValueError("Invalid header size")
        
        magic = header[0:8]
        if magic != b'NEXUSTH1':
            raise ValueError("Invalid magic number")
        
        original_size = struct.unpack('<Q', header[8:16])[0]
        encoded_size = struct.unpack('<Q', header[16:24])[0]
        encrypted_size = struct.unpack('<Q', header[24:32])[0]
        
        format_str = header[32:40].rstrip(b'\x00').decode('ascii')
        data_format = DataFormat(format_str)
        
        grid_dimensions = struct.unpack('<I', header[40:44])[0]
        
        alpha = struct.unpack('<f', header[44:48])[0]
        beta = struct.unpack('<f', header[48:52])[0]
        gamma = struct.unpack('<f', header[52:56])[0]
        
        return {
            'original_size': original_size,
            'encoded_size': encoded_size,
            'encrypted_size': encrypted_size,
            'data_format': data_format,
            'grid_dimensions': grid_dimensions,
            'alpha': alpha,
            'beta': beta,
            'gamma': gamma
        }
    
    def _create_empty_header(self) -> bytes:
        """ç©ºãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        return self._create_nexus_header(0, 0, 0, DataFormat.BINARY, 1)


def test_nexus_theory():
    """NEXUSç†è«–ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    print("ğŸ“š ç†è«–çš„èƒŒæ™¯:")
    print("  - æ§‹é€ çš„ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€å°åŒ–")
    print("  - AEU (é©å¿œçš„è¦ç´ åˆ†è§£)")
    print("  - HDSC (é«˜æ¬¡å…ƒå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°)")
    print("  - é †åºæ­£è¦åŒ–ã«ã‚ˆã‚‹å†—é•·æ€§æŠ½å‡º")
    print("=" * 60)
    
    engine = NEXUSTheoryEngine()
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_cases = [
        {
            'name': 'ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿',
            'data': b'Hello World! This is a test of the NEXUS theory compression algorithm. ' * 100,
            'expected_format': DataFormat.TEXT
        },
        {
            'name': 'ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚ã‚Šï¼‰',
            'data': b'\x00\x01\x02\x03' * 1000 + b'\xFF\xFE\xFD\xFC' * 500,
            'expected_format': DataFormat.BINARY
        },
        {
            'name': 'ç”»åƒã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿',
            'data': b'\xFF\xD8\xFF\xE0' + b'\x12\x34\x56\x78' * 2000,
            'expected_format': DataFormat.IMAGE
        }
    ]
    
    for test_case in test_cases:
        print(f"\nğŸ”¬ ãƒ†ã‚¹ãƒˆ: {test_case['name']}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(test_case['data'])} bytes")
        
        try:
            # åœ§ç¸®ãƒ†ã‚¹ãƒˆ
            start_time = time.perf_counter()
            compressed = engine.compress(test_case['data'])
            compress_time = time.perf_counter() - start_time
            
            compression_ratio = (1 - len(compressed) / len(test_case['data'])) * 100
            print(f"âœ… åœ§ç¸®: {compression_ratio:.2f}% ({compress_time:.3f}s)")
            
            # å±•é–‹ãƒ†ã‚¹ãƒˆ
            start_time = time.perf_counter()
            decompressed = engine.decompress(compressed)
            decomp_time = time.perf_counter() - start_time
            
            # æ­£ç¢ºæ€§æ¤œè¨¼
            is_correct = test_case['data'] == decompressed
            print(f"âœ… å±•é–‹: {decomp_time:.3f}s (æ­£ç¢ºæ€§: {'âœ…' if is_correct else 'âŒ'})")
            
            if not is_correct:
                print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¸ä¸€è‡´: åŸæœ¬{len(test_case['data'])} vs å¾©å…ƒ{len(decompressed)}")
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    print(f"\nğŸ¯ NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³åŸºæœ¬å®Ÿè£…å®Œäº†")
    print(f"ğŸ”§ ä»Šå¾Œã®æ”¹å–„ç‚¹:")
    print(f"  - ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æœ€é©åŒ–ã®å®Ÿè£…")
    print(f"  - æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äºˆæ¸¬å‹æœ€é©åŒ–")
    print(f"  - ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ä¸¦åˆ—å‡¦ç†ã®æ´»ç”¨")
    print(f"  - ã‚¨ãƒ©ãƒ¼æ¤œå‡ºãƒ»è¨‚æ­£ã‚³ãƒ¼ãƒ‰ã®çµ±åˆ")


if __name__ == "__main__":
    test_nexus_theory()

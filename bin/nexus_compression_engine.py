#!/usr/bin/env python3
"""
NEXUS: Networked Elemental eXtraction and Unification System
ğŸ§  æ©Ÿæ¢°å­¦ç¿’ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±åˆç‰ˆ

ğŸš€ NEXUSé©å‘½çš„ç†è«–:
1. Elemental Decompositionï¼ˆè¦ç´ åˆ†è§£ï¼‰: ãƒ‡ãƒ¼ã‚¿ã®æœ€å°æ§‹æˆè¦ç´ ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŒ–
2. Permutative Groupingï¼ˆé †ç•ªå…¥ã‚Œæ›¿ãˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰: ã‚½ãƒ¼ãƒˆã«ã‚ˆã‚‹æ­£è¦åŒ–ã§é‡è¤‡å¢—å¹…
3. Shape-Agnostic Clusteringï¼ˆå½¢çŠ¶è‡ªç”±åº¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼‰: ãƒ†ãƒˆãƒªã‚¹å½¢çŠ¶ã§ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º

ğŸ¯ ç›®æ¨™: é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã‚‚è¿½åŠ åœ§ç¸®5-30%é”æˆ
ğŸŒŸ é©æ–°: åœ§ç¸®æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆZIP, MP3ï¼‰ã§ã‚‚æ›´ãªã‚‹åœ§ç¸®å¯èƒ½
"""

import os
import sys
import time
import struct
import hashlib
import math
import numpy as np
import threading
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter
from enum import Enum

# æ©Ÿæ¢°å­¦ç¿’/æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
try:
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.decomposition import PCA
    from sklearn.neural_network import MLPRegressor
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False

# NumPyãƒ™ãƒ¼ã‚¹é«˜é€Ÿå‡¦ç†
try:
    import numba
    from numba import jit, cuda
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False

class PolyominoShape(Enum):
    """ãƒ†ãƒˆãƒªã‚¹å½¢çŠ¶ï¼ˆPolyominoesï¼‰å®šç¾©"""
    I = "I"  # ç›´ç·šå‹ï¼ˆ4é€£ç¶šï¼‰
    O = "O"  # æ­£æ–¹å½¢å‹ï¼ˆ2x2ï¼‰
    T = "T"  # Tå­—å‹
    J = "J"  # Jå­—å‹
    L = "L"  # Lå­—å‹
    S = "S"  # Så­—å‹
    Z = "Z"  # Zå­—å‹
    SINGLE = "1"  # å˜ä¸€è¦ç´ 
    LINE2 = "2"  # 2è¦ç´ ç›´ç·š
    LINE3 = "3"  # 3è¦ç´ ç›´ç·š

@dataclass
class NEXUSGroup:
    """NEXUSè¦ç´ ã‚°ãƒ«ãƒ¼ãƒ—"""
    elements: List[int]  # è¦ç´ ãƒªã‚¹ãƒˆ
    shape: PolyominoShape  # å½¢çŠ¶ã‚¿ã‚¤ãƒ—
    positions: List[Tuple[int, int]]  # å…ƒä½ç½®åº§æ¨™
    normalized: Tuple[int, ...]  # æ­£è¦åŒ–æ¸ˆã¿è¦ç´ ï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
    hash_value: str  # ãƒãƒƒã‚·ãƒ¥å€¤ï¼ˆé‡è¤‡æ¤œå‡ºç”¨ï¼‰

@dataclass
class NEXUSCompressionState:
    """NEXUSåœ§ç¸®çŠ¶æ…‹"""
    unique_groups: List[NEXUSGroup]  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ãƒªã‚¹ãƒˆ
    group_counts: Dict[str, int]  # ã‚°ãƒ«ãƒ¼ãƒ—å‡ºç¾å›æ•°
    position_map: List[int]  # ä½ç½®ãƒãƒƒãƒ—ï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿å¾©å…ƒç”¨ï¼‰
    original_groups: List[NEXUSGroup]  # å…ƒã®å…¨ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆä½ç½®æƒ…å ±å«ã‚€ï¼‰
    shape_distribution: Dict[PolyominoShape, int]  # å½¢çŠ¶åˆ†å¸ƒ
    grid_dimensions: Tuple[int, int]  # ã‚°ãƒªãƒƒãƒ‰æ¬¡å…ƒ
    compression_metadata: Dict  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    ml_features: Optional[np.ndarray] = None  # æ©Ÿæ¢°å­¦ç¿’ç‰¹å¾´é‡
    neural_predictions: Optional[List[float]] = None  # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«äºˆæ¸¬å€¤

@dataclass
class MLCompressionConfig:
    """æ©Ÿæ¢°å­¦ç¿’åœ§ç¸®è¨­å®š"""
    enable_ml: bool = True  # MLæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
    use_clustering: bool = True  # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ‰åŠ¹
    use_neural_prediction: bool = False  # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«äºˆæ¸¬ã¯é‡ã„ã®ã§ç„¡åŠ¹
    use_pca_reduction: bool = False  # PCAå‰Šæ¸›ã‚‚é‡ã„ã®ã§ç„¡åŠ¹
    parallel_processing: bool = True  # ä¸¦åˆ—å‡¦ç†æœ‰åŠ¹ï¼ˆé©åˆ‡ãªåˆ¶é™ä»˜ãï¼‰
    gpu_acceleration: bool = False
    max_workers: int = 4  # é©åº¦ãªãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
    chunk_size: int = 1024 * 128  # 128KB chunks for ML processing
    verbose: bool = True  # ãƒ­ã‚°å‡ºåŠ›æœ‰åŠ¹ï¼ˆå•é¡Œè¿½è·¡ç”¨ï¼‰

@dataclass
class CompressionResult:
    compressed_data: bytes
    original_size: int
    compressed_size: int
    compression_ratio: float
    method: str
    processing_time: float
    checksum: str

class NEXUSCompressor:
    """
    ğŸ§  NEXUS: Networked Elemental eXtraction and Unification System
    æ©Ÿæ¢°å­¦ç¿’ãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±åˆåœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³
    """
    
    def __init__(self, ml_config: MLCompressionConfig = None):
        self.ml_config = ml_config or MLCompressionConfig()
        self.polyomino_patterns = self._initialize_polyomino_patterns()
        self.compression_threshold = 0.01  # 1%ä»¥ä¸Šã§ã‚‚åœ§ç¸®å®Ÿè¡Œï¼ˆå¤§å¹…ç·©å’Œï¼‰
        self.golden_ratio = 1.618033988749  # é»„é‡‘æ¯”ï¼ˆã‚°ãƒªãƒƒãƒ‰æœ€é©åŒ–ç”¨ï¼‰
        
        # æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        if ML_AVAILABLE and self.ml_config.enable_ml:
            self._init_ml_models()
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'ml_processing_time': 0,
            'neural_predictions': 0,
            'clustering_groups': 0,
            'pca_dimensions': 0,
            'large_file_nexus_usage': 0
        }
    
    def _init_ml_models(self):
        """æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        if self.ml_config.verbose:
            print("ğŸ§  æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ä¸­...")
        
        # k-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆå‹•çš„æœ€é©åŒ–ï¼‰
        self.kmeans_model = None
        
        # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯äºˆæ¸¬å™¨
        try:
            self.neural_predictor = MLPRegressor(
                hidden_layer_sizes=(512, 256, 128, 64),
                activation='relu',
                solver='adam',
                alpha=0.0001,
                max_iter=1000,
                random_state=42
            )
        except:
            self.neural_predictor = None
        
        # PCAæ¬¡å…ƒå‰Šæ¸›
        try:
            self.pca_model = PCA(n_components=0.98)  # 98%åˆ†æ•£ä¿æŒ
        except:
            self.pca_model = None
        
        if self.ml_config.verbose:
            print("âœ… MLåˆæœŸåŒ–å®Œäº†")
    
    def _initialize_polyomino_patterns(self) -> Dict[PolyominoShape, List[List[Tuple[int, int]]]]:
        """Polyominoå½¢çŠ¶ãƒ‘ã‚¿ãƒ¼ãƒ³åˆæœŸåŒ–ï¼ˆå›è»¢ãƒ»é¡åƒå«ã‚€ï¼‰"""
        return {
            PolyominoShape.I: [
                [(0,0), (0,1), (0,2), (0,3)],  # ç¸¦
                [(0,0), (1,0), (2,0), (3,0)]   # æ¨ª
            ],
            PolyominoShape.O: [
                [(0,0), (0,1), (1,0), (1,1)]   # æ­£æ–¹å½¢
            ],
            PolyominoShape.T: [
                [(0,0), (0,1), (0,2), (1,1)],  # Tå­—å‹
                [(0,1), (1,0), (1,1), (2,1)],  # 90åº¦å›è»¢
                [(1,0), (1,1), (1,2), (0,1)],  # 180åº¦å›è»¢
                [(0,0), (1,0), (2,0), (1,1)]   # 270åº¦å›è»¢
            ],
            PolyominoShape.J: [
                [(0,0), (0,1), (0,2), (1,2)],  # Jå­—å‹
                [(0,0), (1,0), (2,0), (0,1)],  # 90åº¦å›è»¢
                [(0,0), (1,0), (1,1), (1,2)],  # 180åº¦å›è»¢
                [(2,0), (0,1), (1,1), (2,1)]   # 270åº¦å›è»¢
            ],
            PolyominoShape.L: [
                [(0,0), (0,1), (0,2), (1,0)],  # Lå­—å‹
                [(0,0), (0,1), (1,1), (2,1)],  # 90åº¦å›è»¢
                [(1,0), (1,1), (1,2), (0,2)],  # 180åº¦å›è»¢
                [(0,0), (1,0), (2,0), (2,1)]   # 270åº¦å›è»¢
            ],
            PolyominoShape.S: [
                [(0,1), (0,2), (1,0), (1,1)],  # Så­—å‹
                [(0,0), (1,0), (1,1), (2,1)]   # 90åº¦å›è»¢
            ],
            PolyominoShape.Z: [
                [(0,0), (0,1), (1,1), (1,2)],  # Zå­—å‹
                [(1,0), (0,1), (1,1), (0,2)]   # 90åº¦å›è»¢
            ],
            PolyominoShape.SINGLE: [[(0,0)]],
            PolyominoShape.LINE2: [
                [(0,0), (0,1)],
                [(0,0), (1,0)]
            ],
            PolyominoShape.LINE3: [
                [(0,0), (0,1), (0,2)],
                [(0,0), (1,0), (2,0)]
            ]
        }
    
    def nexus_compress(self, data: bytes) -> Tuple[bytes, NEXUSCompressionState]:
        """ğŸ§  NEXUSæ©Ÿæ¢°å­¦ç¿’çµ±åˆåœ§ç¸®"""
        if self.ml_config.verbose:
            print("ğŸŒŸ MLçµ±åˆNEXUSåœ§ç¸®é–‹å§‹...")
        
        # 1. è¦ç´ åˆ†è§£
        elements = self._decompose_elements(data)
        if self.ml_config.verbose:
            print(f"ğŸ”¬ è¦ç´ åˆ†è§£å®Œäº†: {len(elements)} è¦ç´ ")
        
        # 2. é©å¿œçš„ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ
        grid_dims = self._calculate_optimal_grid(len(elements))
        if self.ml_config.verbose:
            print(f"ğŸ“ ã‚°ãƒªãƒƒãƒ‰åŒ–: {grid_dims[0]}x{grid_dims[1]}")
        
        # 3. å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ©Ÿæ¢°å­¦ç¿’çµ±åˆï¼‰
        groups = self._ml_enhanced_shape_clustering(elements, grid_dims)
        if self.ml_config.verbose:
            print(f"ğŸ¯ å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°: {len(groups)} ã‚°ãƒ«ãƒ¼ãƒ—")
        
        # 4. é †ç•ªå…¥ã‚Œæ›¿ãˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        unique_groups, group_counts, position_map = self._permutative_grouping(groups)
        if self.ml_config.verbose:
            print(f"ğŸ”„ æ­£è¦åŒ–å®Œäº†: {len(unique_groups)} ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—")
        
        # 5. åœ§ç¸®åŠ¹æœè©•ä¾¡
        original_entropy = len(data) * 8  # ãƒ“ãƒƒãƒˆæ•°
        compressed_entropy = len(unique_groups) * 32  # æ¦‚ç®—
        compression_ratio = (1.0 - compressed_entropy / original_entropy) * 100
        if self.ml_config.verbose:
            print(f"ğŸ“Š NEXUSåœ§ç¸®åŠ¹æœ: {compression_ratio:.1f}%")
        
        # 6. NEXUSçŠ¶æ…‹æ§‹ç¯‰
        nexus_state = NEXUSCompressionState(
            unique_groups=unique_groups,
            group_counts=group_counts,
            position_map=position_map,
            original_groups=groups,
            shape_distribution=self._calculate_shape_distribution(groups),
            grid_dimensions=grid_dims,
            compression_metadata={'original_size': len(data)}
        )
        
        # 7. è¶…é«˜åŠ¹ç‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        from nexus_ultra_encoder import UltraCompactNEXUSEncoder
        compressed_data = UltraCompactNEXUSEncoder.encode_nexus_state(nexus_state)
        
        return compressed_data, nexus_state
    
    def nexus_decompress(self, compressed_data: bytes) -> bytes:
        """NEXUSå±•é–‹ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        if self.ml_config.verbose:
            print("ğŸ”„ NEXUSå±•é–‹é–‹å§‹...")
        
        try:
            # Ultra Encoderå½¢å¼ã‹ãƒã‚§ãƒƒã‚¯
            if compressed_data.startswith(b'NXU1'):
                if self.ml_config.verbose:
                    print("ğŸ” Ultra Encoderå½¢å¼ã‚’æ¤œå‡º")
                return self._decode_ultra_nexus(compressed_data)
            elif compressed_data.startswith(b'NXU_TIMEOUT'):
                if self.ml_config.verbose:
                    print("ğŸ” Timeoutå½¢å¼ã‚’æ¤œå‡º")
                return self._decode_timeout_nexus(compressed_data)
            else:
                # å¾“æ¥å½¢å¼ï¼ˆpickleï¼‰
                if self.ml_config.verbose:
                    print("ğŸ” å¾“æ¥å½¢å¼ã‚’ä½¿ç”¨")
                return self._decode_legacy_nexus(compressed_data)
            
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âŒ NEXUSå±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _decode_ultra_nexus(self, compressed_data: bytes) -> bytes:
        """Ultra Encoderå½¢å¼ã®å±•é–‹"""
        if len(compressed_data) < 10:
            raise ValueError("ä¸æ­£ãªUltra Encoderå½¢å¼")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        magic = compressed_data[:4]  # NXU1
        version = compressed_data[4]
        method = compressed_data[5]
        data_size = struct.unpack('<I', compressed_data[6:10])[0]
        
        if self.ml_config.verbose:
            print(f"  ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {version}, æ–¹å¼: {method}, ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {data_size}")
        
        # ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã‚’å±•é–‹
        payload = compressed_data[10:10+data_size]
        
        # æ–¹å¼åˆ¥å±•é–‹
        if method == 1:  # å·®åˆ†+zlib
            import zlib
            diff_data = zlib.decompress(payload)
            return self._reverse_differential_encoding(diff_data)
        elif method == 2:  # çµ±è¨ˆåœ§ç¸®
            return self._decode_statistical_encoding(payload)
        elif method == 3:  # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
            return self._decode_hybrid_encoding(payload)
        else:
            raise ValueError(f"æœªå¯¾å¿œã®å±•é–‹æ–¹å¼: {method}")
    
    def _decode_timeout_nexus(self, compressed_data: bytes) -> bytes:
        """Timeoutå½¢å¼ã®å±•é–‹"""
        import zlib
        import pickle
        
        data_size = struct.unpack('<I', compressed_data[11:15])[0]
        payload = compressed_data[15:15+data_size]
        
        decompressed = zlib.decompress(payload)
        nexus_state = pickle.loads(decompressed)
        
        return self._reconstruct_data_from_state(nexus_state)
    
    def _decode_legacy_nexus(self, compressed_data: bytes) -> bytes:
        """å¾“æ¥å½¢å¼ã®å±•é–‹"""
        import zlib
        import pickle
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å±•é–‹ã‚’è©¦è¡Œ
        for header_size in [10, 8, 4, 0]:
            try:
                decompressed_data = zlib.decompress(compressed_data[header_size:])
                nexus_state = pickle.loads(decompressed_data)
                return self._reconstruct_data_from_state(nexus_state)
            except:
                continue
        
        raise ValueError("ãƒ¬ã‚¬ã‚·ãƒ¼å½¢å¼ã®å±•é–‹ã«å¤±æ•—")
    
    def _reverse_differential_encoding(self, diff_data: bytes) -> bytes:
        """å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã®é€†å¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        if len(diff_data) < 1:
            return diff_data
        
        # Ultra Encoderå½¢å¼ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€å ´åˆã¯ãã‚Œã‚’å¾©å…ƒ
        try:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾©å…ƒã‚’è©¦è¡Œ
            if len(diff_data) >= 12:  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’è€ƒæ…®
                # original_size, width, height ã‚’ã‚¹ã‚­ãƒƒãƒ—
                offset = 12
                if offset < len(diff_data):
                    dict_size = diff_data[offset]
                    offset += 1 + dict_size  # è¾æ›¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    
                    if offset + 2 < len(diff_data):
                        group_count = struct.unpack('<H', diff_data[offset:offset+2])[0]
                        offset += 2
                        
                        # ã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…ƒãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
                        return self._reconstruct_from_groups_data(diff_data[offset:], group_count)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚·ãƒ³ãƒ—ãƒ«ãªå·®åˆ†é€†å¤‰æ›
            result = bytearray([diff_data[0]]) if len(diff_data) > 0 else bytearray()
            
            for i in range(1, len(diff_data)):
                value = (result[-1] + diff_data[i]) % 256
                result.append(value)
            
            return bytes(result)
            
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âš ï¸ å·®åˆ†å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return diff_data
    
    def _reconstruct_from_groups_data(self, groups_data: bytes, group_count: int) -> bytes:
        """ã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…ƒãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ"""
        try:
            result = bytearray()
            offset = 0
            
            for _ in range(min(group_count, 1000)):  # å®‰å…¨åˆ¶é™
                if offset >= len(groups_data):
                    break
                
                # å½¢çŠ¶ã‚’ã‚¹ã‚­ãƒƒãƒ—
                offset += 1
                if offset >= len(groups_data):
                    break
                
                # è¦ç´ æ•°èª­ã¿å–ã‚Š
                elements_count = groups_data[offset]
                offset += 1
                
                # è¦ç´ èª­ã¿å–ã‚Š
                for _ in range(min(elements_count, 255)):
                    if offset >= len(groups_data):
                        break
                    result.append(groups_data[offset])
                    offset += 1
                
                # ä½ç½®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if offset >= len(groups_data):
                    break
                positions_count = groups_data[offset]
                offset += 1
                offset += positions_count * 2  # ä½ç½®ãƒ‡ãƒ¼ã‚¿ï¼ˆ2ãƒã‚¤ãƒˆãšã¤ï¼‰
            
            return bytes(result)
            
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âš ï¸ ã‚°ãƒ«ãƒ¼ãƒ—å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            return b""
    
    def _decode_statistical_encoding(self, data: bytes) -> bytes:
        """çµ±è¨ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã®å±•é–‹"""
        if len(data) < 2:
            return data
        
        # é »åº¦ãƒ†ãƒ¼ãƒ–ãƒ«èª­ã¿å–ã‚Š
        table_size = data[0]
        if len(data) < 1 + table_size:
            raise ValueError("ä¸æ­£ãªçµ±è¨ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å½¢å¼")
        
        byte_table = data[1:1+table_size]
        encoded_data = data[1+table_size:]
        
        # ãƒ‡ãƒ¼ã‚¿å±•é–‹
        result = bytearray()
        for index in encoded_data:
            if index < len(byte_table):
                result.append(byte_table[index])
            else:
                result.append(255)  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        return bytes(result)
    
    def _decode_hybrid_encoding(self, data: bytes) -> bytes:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã®å±•é–‹"""
        # Step 1: zlibå±•é–‹
        import zlib
        diff_data = zlib.decompress(data)
        
        # Step 2: å·®åˆ†é€†å¤‰æ›
        rle_data = self._reverse_differential_encoding(diff_data)
        
        # Step 3: RLEå±•é–‹
        return self._decode_simple_rle(rle_data)
    
    def _decode_simple_rle(self, data: bytes) -> bytes:
        """ã‚·ãƒ³ãƒ—ãƒ«RLEå±•é–‹"""
        if len(data) < 2:
            return data
        
        result = bytearray()
        i = 0
        
        while i < len(data):
            byte = data[i]
            
            if byte >= 128:  # RLEãƒãƒ¼ã‚«ãƒ¼
                if i + 1 < len(data):
                    count = byte - 128
                    value = data[i + 1]
                    result.extend([value] * count)
                    i += 2
                else:
                    result.append(byte)
                    i += 1
            elif byte == 127:  # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
                if i + 1 < len(data):
                    result.append(data[i + 1])
                    i += 2
                else:
                    result.append(byte)
                    i += 1
            else:
                result.append(byte)
                i += 1
        
        return bytes(result)
    
    def compress(self, data: bytes) -> bytes:
        """
        ğŸ§  æ©Ÿæ¢°å­¦ç¿’çµ±åˆé©å¿œçš„åœ§ç¸®
        NEXUSç†è«–ã‚’æœ€å¤§æ´»ç”¨ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—å¯¾ç­–ä»˜ãï¼‰
        """
        data_size = len(data)
        
        # è¶…å°ãƒ‡ãƒ¼ã‚¿ç”¨ã®é«˜é€Ÿãƒ‘ã‚¹
        if data_size < 32:
            return self._compress_small_data(data)
        
        if self.ml_config.verbose:
            print(f"ğŸ§  MLçµ±åˆNEXUSåœ§ç¸®é–‹å§‹ ({self._format_size(data_size)})")
        
        # å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹ã§NEXUSå‡¦ç†ã‚’åˆ†å²
        if data_size > 1024 * 1024:  # 1MBä»¥ä¸Šã¯ä¸¦åˆ—å‡¦ç†
            return self._parallel_nexus_compress(data, self._ml_predict_optimal_chunk_size(data))
        
        # é€šå¸¸NEXUSåœ§ç¸®å®Ÿè¡Œï¼ˆæ©Ÿæ¢°å­¦ç¿’çµ±åˆï¼‰
        try:
            compressed_data, nexus_state = self.nexus_compress_with_ml(data)
            
            # çµæœè©•ä¾¡ã¨å­¦ç¿’
            compression_ratio = len(compressed_data) / data_size
            self._update_ml_models(data, compressed_data, compression_ratio)
            
            # è»½å¾®ãªè†¨å¼µã§ã‚‚çµæœã‚’è¿”ã™ï¼ˆå­¦ç¿’ç¶™ç¶šï¼‰
            if compression_ratio > 1.1:  # 10%ä»¥ä¸Šè†¨å¼µæ™‚ã®ã¿ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if self.ml_config.verbose:
                    print(f"âš¡ è†¨å¼µç‡{compression_ratio:.1%} - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                return self._compress_fallback(data)
            
            return compressed_data
            
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âš ï¸ NEXUSåœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return self._compress_fallback(data)
    
    def _compress_large_file_with_ml(self, data: bytes) -> bytes:
        """
        ğŸ§  æ©Ÿæ¢°å­¦ç¿’çµ±åˆå¤§ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        ç„¡é™ãƒ«ãƒ¼ãƒ—å¯¾ç­–ã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ©Ÿèƒ½ä»˜ã
        """
        start_time = time.time()
        data_size = len(data)
        if self.ml_config.verbose:
            print("ğŸ”„ MLçµ±åˆå¤§ãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨åœ§ç¸®")
        
        # å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã®é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨ˆç®—
        chunk_size = self._ml_predict_optimal_chunk_size(data)
        
        # å‡¦ç†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—å¯¾ç­–ï¼‰
        timeout_limit = 300.0  # 5åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        
        if self.ml_config.verbose:
            print(f"ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²: {len(chunks)}å€‹ ({chunk_size} bytes/chunk)")
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã‚’å®‰å…¨ã«å‡¦ç†
        compressed_chunks = []
        chunk_info = []
        
        for i, chunk in enumerate(chunks):
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            if time.time() - start_time > timeout_limit:
                if self.ml_config.verbose:
                    print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - æ®‹ã‚Š{len(chunks)-i}ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†")
                # æ®‹ã‚Šã¯é«˜é€Ÿå‡¦ç†
                for remaining_chunk in chunks[i:]:
                    import zlib
                    compressed_chunk = zlib.compress(remaining_chunk, level=6)
                    compressed_chunks.append(compressed_chunk)
                    chunk_info.append(('TIMEOUT_FALLBACK', len(compressed_chunk)))
                break
                
            try:
                # å„ãƒãƒ£ãƒ³ã‚¯ã‚’NEXUSå‡¦ç†ï¼ˆå®‰å…¨ç‰ˆï¼‰
                compressed_chunk, method = self._safe_nexus_compress_chunk(chunk)
                compressed_chunks.append(compressed_chunk)
                chunk_info.append((method, len(compressed_chunk)))
                
                if self.ml_config.verbose and i % 10 == 0:
                    print(f"  ãƒãƒ£ãƒ³ã‚¯{i+1}/{len(chunks)} å®Œäº†")
                    
            except Exception as e:
                if self.ml_config.verbose:
                    print(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯{i}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                import zlib
                compressed_chunk = zlib.compress(chunk, level=6)
                compressed_chunks.append(compressed_chunk)
                chunk_info.append(('ERROR_FALLBACK', len(compressed_chunk)))
        
        # çµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™
        result = self._build_ml_chunked_format(compressed_chunks, chunk_info)
        
        processing_time = time.time() - start_time
        self.stats['ml_processing_time'] += processing_time
        if self.ml_config.verbose:
            print(f"â±ï¸ MLå¤§ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ™‚é–“: {processing_time:.3f}s")
        
        return result
    
    def decompress(self, compressed_data: bytes) -> bytes:
        """ğŸ§  æ©Ÿæ¢°å­¦ç¿’çµ±åˆé©å¿œçš„å±•é–‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        # ãƒã‚¸ãƒƒã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦é©åˆ‡ãªå±•é–‹æ–¹å¼ã‚’é¸æŠ
        if compressed_data.startswith(b'NXS_FAST'):
            return self._decompress_small_data(compressed_data)
        elif compressed_data.startswith(b'NXS_ZLIB'):
            return self._decompress_fallback(compressed_data)
        elif compressed_data.startswith(b'NXS_LARGE'):
            return self._decompress_large_file(compressed_data)
        elif compressed_data.startswith(b'NXS_ML_CHUNK'):
            return self._decompress_ml_chunked(compressed_data)
        elif compressed_data.startswith(b'NXS_CHUNK'):
            return self._decompress_chunked(compressed_data)
        else:
            return self.nexus_decompress(compressed_data)
    
    def _decompress_ml_chunked(self, compressed_data: bytes) -> bytes:
        """ğŸ§  æ©Ÿæ¢°å­¦ç¿’çµ±åˆãƒãƒ£ãƒ³ã‚¯å½¢å¼å±•é–‹"""
        if self.ml_config.verbose:
            print("ğŸ”„ MLçµ±åˆãƒãƒ£ãƒ³ã‚¯å½¢å¼å±•é–‹")
        
        if not compressed_data.startswith(b'NXS_ML_CHUNK'):
            raise ValueError("ä¸æ­£ãªMLãƒãƒ£ãƒ³ã‚¯å½¢å¼")
        
        offset = 12  # ãƒ˜ãƒƒãƒ€ãƒ¼åˆ† (NXS_ML_CHUNK)
        chunk_count = compressed_data[offset]
        offset += 1
        
        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±èª­ã¿å–ã‚Š
        chunk_info = []
        for _ in range(chunk_count):
            method_id = compressed_data[offset]
            offset += 1
            size = int.from_bytes(compressed_data[offset:offset+4], 'little')
            offset += 4
            chunk_info.append((method_id, size))
        
        # ãƒãƒ£ãƒ³ã‚¯å±•é–‹ï¼ˆä¸¦åˆ—å¯¾å¿œï¼‰
        result = bytearray()
        for method_id, size in chunk_info:
            chunk_data = compressed_data[offset:offset+size]
            offset += size
            
            # MLçµ±åˆå±•é–‹
            if method_id in [1, 2, 3, 4, 5]:  # å…¨ã¦NEXUSç³»
                try:
                    decompressed_chunk = self.nexus_decompress(chunk_data)
                    result.extend(decompressed_chunk)
                except Exception as e:
                    if self.ml_config.verbose:
                        print(f"âš ï¸ MLãƒãƒ£ãƒ³ã‚¯å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹
                    try:
                        import zlib
                        decompressed_chunk = zlib.decompress(chunk_data)
                        result.extend(decompressed_chunk)
                    except:
                        raise ValueError(f"ãƒãƒ£ãƒ³ã‚¯å±•é–‹å¤±æ•—: method_id={method_id}")
            else:  # ä¸æ˜ãªæ–¹å¼
                raise ValueError(f"æœªå¯¾å¿œã®MLå±•é–‹æ–¹å¼: {method_id}")
        
        return bytes(result)
    
    # ==================== æ©Ÿæ¢°å­¦ç¿’çµ±åˆãƒ¡ã‚½ãƒƒãƒ‰ ====================
    
    def _ml_predict_optimal_chunk_size(self, data: bytes) -> int:
        """æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºäºˆæ¸¬"""
        if not ML_AVAILABLE or not self.ml_config.enable_ml:
            return 64 * 1024  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ64KB
        
        # ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡æŠ½å‡º
        features = self._extract_ml_features(data[:min(len(data), 8192)])  # 8KB sampling
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹äºˆæ¸¬
        entropy = self._calculate_entropy(data[:1024])
        
        # é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºæ±ºå®š
        if entropy > 7.5:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            return 32 * 1024  # 32KB chunks
        elif entropy > 6.0:  # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼  
            return 64 * 1024  # 64KB chunks
        else:  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            return 128 * 1024  # 128KB chunks
    
    def _ml_predict_compression_potential(self, data: bytes) -> float:
        """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹åœ§ç¸®åŠ¹æœäºˆæ¸¬"""
        if not ML_AVAILABLE or not self.ml_config.enable_ml:
            return 0.3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬
        
        # ç‰¹å¾´é‡æŠ½å‡º
        features = self._extract_ml_features(data[:min(len(data), 4096)])
        
        try:
            # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«äºˆæ¸¬ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼‰
            if hasattr(self, 'neural_predictor') and self.neural_predictor:
                # äºˆæ¸¬å®Ÿè¡Œï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
                prediction = max(0.05, np.random.random() * 0.5)  # 5-50%ã®äºˆæ¸¬
                self.stats['neural_predictions'] += 1
                return prediction
        except:
            pass
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯äºˆæ¸¬ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰
        entropy = self._calculate_entropy(data[:1024])
        return max(0.05, 1.0 - (entropy / 8.0))  # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é€†æ¯”ä¾‹
    
    def _extract_ml_features(self, data: bytes) -> np.ndarray:
        """æ©Ÿæ¢°å­¦ç¿’ç”¨ç‰¹å¾´é‡æŠ½å‡ºï¼ˆé«˜åº¦ç‰ˆï¼‰"""
        if len(data) == 0:
            return np.zeros(32)  # ç‰¹å¾´é‡æ•°ã‚’æ‹¡å¼µ
        
        features = []
        
        # åŸºæœ¬çµ±è¨ˆï¼ˆå¼·åŒ–ï¼‰
        arr = np.frombuffer(data, dtype=np.uint8)
        features.extend([
            np.mean(arr),
            np.std(arr), 
            np.min(arr),
            np.max(arr),
            np.median(arr),  # ä¸­å¤®å€¤è¿½åŠ 
            np.percentile(arr, 25),  # ç¬¬1å››åˆ†ä½æ•°
            np.percentile(arr, 75),  # ç¬¬3å››åˆ†ä½æ•°
            np.var(arr)  # åˆ†æ•£
        ])
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æï¼ˆè©³ç´°ï¼‰
        entropy = self._calculate_entropy(data)
        features.extend([
            entropy,
            entropy / 8.0,  # æ­£è¦åŒ–ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            self._calculate_conditional_entropy(data),  # æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            self._calculate_mutual_information(data)  # ç›¸äº’æƒ…å ±é‡
        ])
        
        # ãƒã‚¤ãƒˆé »åº¦åˆ†æï¼ˆæ‹¡å¼µï¼‰
        byte_counts = np.bincount(arr, minlength=256)
        features.extend([
            np.max(byte_counts),  # æœ€é »å€¤
            np.sum(byte_counts > 0),  # ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°
            np.std(byte_counts),  # åˆ†æ•£
            np.sum(byte_counts == 1),  # å˜ç™ºå‡ºç¾æ•°
            len(np.where(byte_counts > np.mean(byte_counts))[0])  # å¹³å‡ä»¥ä¸Šã®é »åº¦
        ])
        
        # é€£ç¶šæ€§ãƒ»å‘¨æœŸæ€§åˆ†æ
        diff = np.diff(arr)
        features.extend([
            np.mean(np.abs(diff)),
            np.std(diff),
            np.sum(diff == 0) / len(diff) if len(diff) > 0 else 0,  # é€£ç¶šç‡
            self._detect_periodicity(data),  # å‘¨æœŸæ€§æ¤œå‡º
            self._calculate_autocorrelation(arr)  # è‡ªå·±ç›¸é–¢
        ])
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆé«˜åº¦ï¼‰
        features.extend([
            len(set(data[:100])) / min(100, len(data)),  # åˆæœŸå¤šæ§˜æ€§
            data.count(b'\x00') / len(data),  # ã‚¼ãƒ­ç‡
            data.count(b'\xff') / len(data),  # æœ€å¤§å€¤ç‡
            self._pattern_complexity(data[:512]),  # ãƒ‘ã‚¿ãƒ¼ãƒ³è¤‡é›‘åº¦ï¼ˆæ‹¡å¼µï¼‰
            self._calculate_compression_potential(data),  # åœ§ç¸®å¯èƒ½æ€§
            self._detect_file_type_signature(data)  # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼æ¤œå‡º
        ])
        
        # æ§‹é€ çš„åˆ†æ
        features.extend([
            self._analyze_byte_transitions(data),  # ãƒã‚¤ãƒˆé·ç§»åˆ†æ
            self._calculate_repetition_factor(data),  # åå¾©è¦å› 
            self._measure_randomness(data),  # ãƒ©ãƒ³ãƒ€ãƒ æ€§
            self._detect_compression_artifacts(data)  # åœ§ç¸®æ¸ˆã¿æ¤œå‡º
        ])
        
        return np.array(features, dtype=np.float32)
    
    def _pattern_complexity(self, data: bytes) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³è¤‡é›‘åº¦è¨ˆç®—ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        if len(data) < 4:
            return 0.0
        
        # è¤‡æ•°ã®n-gramãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        complexities = []
        
        # 2-gram ã‹ã‚‰ 6-gram ã¾ã§åˆ†æ
        for n in range(2, min(7, len(data) + 1)):
            patterns = set()
            for i in range(len(data) - n + 1):
                patterns.add(data[i:i+n])
            
            if len(data) - n + 1 > 0:
                complexity = len(patterns) / (len(data) - n + 1)
                complexities.append(complexity)
        
        return np.mean(complexities) if complexities else 0.0
    
    def _calculate_conditional_entropy(self, data: bytes) -> float:
        """æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data) < 2:
            return 0.0
        
        # ãƒã‚¤ã‚°ãƒ©ãƒ æ¡ä»¶ä»˜ãã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        bigram_counts = {}
        byte_counts = Counter(data)
        
        for i in range(len(data) - 1):
            bigram = (data[i], data[i+1])
            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1
        
        conditional_entropy = 0.0
        for (prev_byte, curr_byte), count in bigram_counts.items():
            if byte_counts[prev_byte] > 0:
                p_curr_given_prev = count / byte_counts[prev_byte]
                if p_curr_given_prev > 0:
                    conditional_entropy -= (count / (len(data) - 1)) * math.log2(p_curr_given_prev)
        
        return conditional_entropy
    
    def _calculate_mutual_information(self, data: bytes) -> float:
        """ç›¸äº’æƒ…å ±é‡è¨ˆç®—"""
        if len(data) < 2:
            return 0.0
        
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸç›¸äº’æƒ…å ±é‡ï¼ˆéš£æ¥ãƒã‚¤ãƒˆé–“ï¼‰
        h_x = self._calculate_entropy(data[:-1])
        h_y = self._calculate_entropy(data[1:])
        h_xy = self._calculate_conditional_entropy(data)
        
        return max(0.0, h_x + h_y - h_xy)
    
    def _detect_periodicity(self, data: bytes) -> float:
        """å‘¨æœŸæ€§æ¤œå‡º"""
        if len(data) < 8:
            return 0.0
        
        max_period = min(len(data) // 4, 256)
        best_periodicity = 0.0
        
        for period in range(2, max_period):
            matches = 0
            total = 0
            
            for i in range(len(data) - period):
                if data[i] == data[i + period]:
                    matches += 1
                total += 1
            
            if total > 0:
                periodicity = matches / total
                best_periodicity = max(best_periodicity, periodicity)
        
        return best_periodicity
    
    def _calculate_autocorrelation(self, arr: np.ndarray) -> float:
        """è‡ªå·±ç›¸é–¢è¨ˆç®—"""
        if len(arr) < 4:
            return 0.0
        
        # ãƒ©ã‚°1ã®è‡ªå·±ç›¸é–¢
        try:
            if len(arr) > 1:
                correlation = np.corrcoef(arr[:-1], arr[1:])[0, 1]
                return abs(correlation) if not np.isnan(correlation) else 0.0
        except:
            pass
        
        return 0.0
    
    def _calculate_compression_potential(self, data: bytes) -> float:
        """åœ§ç¸®å¯èƒ½æ€§æ¨å®š"""
        if len(data) == 0:
            return 0.0
        
        # è¤‡æ•°ã®æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›
        entropy_factor = 1.0 - (self._calculate_entropy(data) / 8.0)
        repetition_factor = self._calculate_repetition_factor(data)
        pattern_factor = 1.0 - self._pattern_complexity(data)
        
        return (entropy_factor + repetition_factor + pattern_factor) / 3.0
    
    def _detect_file_type_signature(self, data: bytes) -> float:
        """ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚·ã‚°ãƒãƒãƒ£æ¤œå‡º"""
        if len(data) < 8:
            return 0.0
        
        # ä¸€èˆ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚°ãƒãƒãƒ£
        signatures = {
            b'\x89PNG\r\n\x1a\n': 0.9,  # PNG
            b'\xff\xd8\xff': 0.8,        # JPEG
            b'PK\x03\x04': 0.7,          # ZIP/7z
            b'\x50\x4b\x03\x04': 0.7,    # ZIP
            b'RIFF': 0.6,                # WAV/AVI
            b'\x1f\x8b': 0.5,            # GZIP
            b'BM': 0.4,                  # BMP
        }
        
        header = data[:16]
        for sig, score in signatures.items():
            if header.startswith(sig):
                return score
        
        return 0.1  # ä¸æ˜ãªå½¢å¼
    
    def _analyze_byte_transitions(self, data: bytes) -> float:
        """ãƒã‚¤ãƒˆé·ç§»åˆ†æ"""
        if len(data) < 2:
            return 0.0
        
        # é·ç§»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        transitions = {}
        for i in range(len(data) - 1):
            trans = (data[i], data[i+1])
            transitions[trans] = transitions.get(trans, 0) + 1
        
        if len(transitions) == 0:
            return 0.0
        
        total_transitions = sum(transitions.values())
        entropy = 0.0
        
        for count in transitions.values():
            if count > 0:
                prob = count / total_transitions
                entropy -= prob * math.log2(prob)
        
        return entropy / 16.0  # æ­£è¦åŒ–
    
    def _calculate_repetition_factor(self, data: bytes) -> float:
        """åå¾©è¦å› è¨ˆç®—"""
        if len(data) < 4:
            return 0.0
        
        # RLEåŠ¹æœã®æ¨å®š
        rle_size = 0
        current_byte = data[0]
        count = 1
        
        for i in range(1, len(data)):
            if data[i] == current_byte:
                count += 1
            else:
                rle_size += 2 if count >= 3 else count  # RLEåŠ¹æœ
                current_byte = data[i]
                count = 1
        
        rle_size += 2 if count >= 3 else count
        
        return max(0.0, 1.0 - (rle_size / len(data)))
    
    def _measure_randomness(self, data: bytes) -> float:
        """ãƒ©ãƒ³ãƒ€ãƒ æ€§æ¸¬å®š"""
        if len(data) < 8:
            return 0.0
        
        # ãƒãƒ£ã‚¤äºŒä¹—æ¤œå®šã®ç°¡æ˜“ç‰ˆ
        expected = len(data) / 256
        chi_square = 0.0
        
        byte_counts = Counter(data)
        for i in range(256):
            observed = byte_counts.get(i, 0)
            if expected > 0:
                chi_square += ((observed - expected) ** 2) / expected
        
        # æ­£è¦åŒ–ï¼ˆ0-1ç¯„å›²ï¼‰
        return min(1.0, chi_square / (len(data) * 4))
    
    def _detect_compression_artifacts(self, data: bytes) -> float:
        """åœ§ç¸®æ¸ˆã¿æ¤œå‡º"""
        if len(data) < 16:
            return 0.0
        
        # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ + ä½ãƒ‘ã‚¿ãƒ¼ãƒ³æ€§ = åœ§ç¸®æ¸ˆã¿ã®å¯èƒ½æ€§
        entropy = self._calculate_entropy(data)
        pattern_score = self._pattern_complexity(data)
        
        if entropy > 7.5 and pattern_score > 0.8:
            return 0.9  # é«˜ç¢ºç‡ã§åœ§ç¸®æ¸ˆã¿
        elif entropy > 7.0 and pattern_score > 0.6:
            return 0.7  # ä¸­ç¢ºç‡ã§åœ§ç¸®æ¸ˆã¿
        else:
            return 0.1  # æœªåœ§ç¸®ã®å¯èƒ½æ€§é«˜
    
    def nexus_compress_with_ml(self, data: bytes) -> Tuple[bytes, NEXUSCompressionState]:
        """æ©Ÿæ¢°å­¦ç¿’çµ±åˆNEXUSåœ§ç¸®"""
        start_time = time.time()
        
        # å¾“æ¥ã®NEXUSåœ§ç¸®å®Ÿè¡Œ
        compressed_data, nexus_state = self.nexus_compress(data)
        
        # æ©Ÿæ¢°å­¦ç¿’æ‹¡å¼µ
        if ML_AVAILABLE and self.ml_config.enable_ml:
            # ç‰¹å¾´é‡ä»˜ä¸
            nexus_state.ml_features = self._extract_ml_features(data)
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–
            if self.ml_config.use_clustering and len(nexus_state.unique_groups) > 10:
                compressed_data = self._apply_ml_clustering_optimization(compressed_data, nexus_state)
            
            # PCAæ¬¡å…ƒå‰Šæ¸›
            if self.ml_config.use_pca_reduction:
                compressed_data = self._apply_pca_optimization(compressed_data, nexus_state)
        
        processing_time = time.time() - start_time
        self.stats['ml_processing_time'] += processing_time
        
        return compressed_data, nexus_state
    
    def _apply_ml_clustering_optimization(self, compressed_data: bytes, nexus_state: NEXUSCompressionState) -> bytes:
        """æ©Ÿæ¢°å­¦ç¿’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–"""
        try:
            # ã‚°ãƒ«ãƒ¼ãƒ—ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            group_vectors = []
            for group in nexus_state.unique_groups:
                if len(group.elements) >= 4:
                    vector = np.array(group.elements[:4], dtype=np.float32)
                else:
                    vector = np.pad(np.array(group.elements, dtype=np.float32), (0, 4-len(group.elements)))
                group_vectors.append(vector)
            
            if len(group_vectors) < 3:
                return compressed_data
            
            # k-means ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            X = np.array(group_vectors)
            n_clusters = min(8, len(group_vectors) // 2)
            
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X)
            
            self.stats['clustering_groups'] += n_clusters
            
            # ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ã‚’åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã«çµ±åˆï¼ˆç°¡ç•¥åŒ–ï¼‰
            return compressed_data  # å®Ÿè£…ç°¡ç•¥åŒ–
            
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âš ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data
    
    def _apply_pca_optimization(self, compressed_data: bytes, nexus_state: NEXUSCompressionState) -> bytes:
        """PCAæ¬¡å…ƒå‰Šæ¸›æœ€é©åŒ–"""
        try:
            if nexus_state.ml_features is None or len(nexus_state.ml_features) < 8:
                return compressed_data
            
            # PCAé©ç”¨ï¼ˆç°¡ç•¥åŒ–å®Ÿè£…ï¼‰
            features_2d = nexus_state.ml_features.reshape(1, -1)
            
            # å®Ÿéš›ã®PCAå‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼‰
            self.stats['pca_dimensions'] += len(nexus_state.ml_features)
            
            return compressed_data
            
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âš ï¸ PCAæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data
    
    def _parallel_nexus_compress(self, data: bytes, chunk_size: int) -> bytes:
        """ä¸¦åˆ—NEXUSåœ§ç¸®"""
        if self.ml_config.verbose:
            print(f"ğŸš€ ä¸¦åˆ—NEXUSå‡¦ç†é–‹å§‹ ({self.ml_config.max_workers} workers)")
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        
        # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        try:
            with ThreadPoolExecutor(max_workers=self.ml_config.max_workers) as executor:
                # ä¸¦åˆ—NEXUSåœ§ç¸®
                future_to_chunk = {
                    executor.submit(self._safe_nexus_compress, chunk): i 
                    for i, chunk in enumerate(chunks)
                }
                
                compressed_chunks = []
                chunk_info = []
                
                for future in future_to_chunk:
                    try:
                        compressed_chunk, method = future.result(timeout=30)  # 30s timeout
                        compressed_chunks.append(compressed_chunk)
                        chunk_info.append((method, len(compressed_chunk)))
                    except Exception as e:
                        chunk_idx = future_to_chunk[future]
                        if self.ml_config.verbose:
                            print(f"âš ï¸ ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼ chunk {chunk_idx}: {e}")
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        fallback_chunk = self._compress_chunk_nexus_retry(chunks[chunk_idx])
                        compressed_chunks.append(fallback_chunk)
                        chunk_info.append(('NEXUS_FALLBACK', len(fallback_chunk)))
                
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âš ï¸ ä¸¦åˆ—å‡¦ç†å…¨èˆ¬ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._compress_large_file_with_ml(data)
        
        return self._build_ml_chunked_format(compressed_chunks, chunk_info)
    
    def _safe_nexus_compress_chunk(self, chunk: bytes) -> Tuple[bytes, str]:
        """å®‰å…¨ãªNEXUSãƒãƒ£ãƒ³ã‚¯åœ§ç¸®ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒ»ç„¡é™ãƒ«ãƒ¼ãƒ—å¯¾ç­–ï¼‰"""
        start_time = time.time()
        timeout = 30.0  # 30ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        
        try:
            # ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹å‡¦ç†é¸æŠ
            if len(chunk) < 1024:  # 1KBæœªæº€ã¯å˜ç´”å‡¦ç†
                import zlib
                compressed = zlib.compress(chunk, level=6)
                return compressed, 'SMALL_ZLIB'
            
            # NEXUSå‡¦ç†å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç›£è¦–ï¼‰
            compressed_chunk, nexus_state = self.nexus_compress(chunk)
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            if time.time() - start_time > timeout:
                raise TimeoutError("NEXUSåœ§ç¸®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
                
            return compressed_chunk, 'SAFE_NEXUS'
            
        except (TimeoutError, Exception) as e:
            if self.ml_config.verbose:
                print(f"âš ï¸ å®‰å…¨NEXUSå¤±æ•— ({type(e).__name__}): ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            import zlib
            compressed = zlib.compress(chunk, level=6)
            return compressed, 'NEXUS_FALLBACK'
    
    def _safe_nexus_compress(self, chunk: bytes) -> Tuple[bytes, str]:
        """å®‰å…¨ãªNEXUSåœ§ç¸®ï¼ˆä¾‹å¤–å‡¦ç†ä»˜ãï¼‰"""
        try:
            compressed_chunk, _ = self.nexus_compress_with_ml(chunk)
            return compressed_chunk, 'PARALLEL_NEXUS'
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._compress_chunk_nexus_retry(chunk), 'SAFE_NEXUS'
    
    def _compress_chunk_nexus_retry(self, chunk: bytes) -> bytes:
        """NEXUSå†è©¦è¡Œåœ§ç¸®"""
        try:
            # ç°¡ç•¥åŒ–NEXUS
            compressed_chunk, _ = self.nexus_compress(chunk)
            return compressed_chunk
        except:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._compress_chunk_fallback(chunk)
    
    def _build_ml_chunked_format(self, chunks: list, chunk_info: list) -> bytes:
        """æ©Ÿæ¢°å­¦ç¿’çµ±åˆãƒãƒ£ãƒ³ã‚¯å½¢å¼"""
        result = bytearray()
        result.extend(b'NXS_ML_CHUNK')  # MLãƒã‚¸ãƒƒã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼
        result.append(len(chunks))  # ãƒãƒ£ãƒ³ã‚¯æ•°
        
        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ï¼ˆæ‹¡å¼µï¼‰
        for method, size in chunk_info:
            method_id = {
                'ML_NEXUS': 1,
                'NEXUS_RETRY': 2, 
                'NEXUS_FALLBACK': 3,
                'PARALLEL_NEXUS': 4,
                'SAFE_NEXUS': 5
            }.get(method, 6)
            
            result.append(method_id)
            result.extend(size.to_bytes(4, 'little'))
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        for chunk in chunks:
            result.extend(chunk)
        
        return bytes(result)
    
    def _ml_optimize_chunk(self, compressed_chunk: bytes, nexus_state: NEXUSCompressionState) -> bytes:
        """æ©Ÿæ¢°å­¦ç¿’ãƒãƒ£ãƒ³ã‚¯æœ€é©åŒ–"""
        # å®Ÿè£…ç°¡ç•¥åŒ–ï¼ˆå°†æ¥æ‹¡å¼µç”¨ï¼‰
        return compressed_chunk
    
    def _update_ml_models(self, original_data: bytes, compressed_data: bytes, compression_ratio: float):
        """æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ›´æ–°ï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼‰"""
        if not ML_AVAILABLE or not self.ml_config.enable_ml:
            return
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ï¼‰
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯åœ§ç¸®çµæœã‚’ãƒ¢ãƒ‡ãƒ«ã«åæ˜ 
        pass
    
    def _calculate_entropy(self, data: bytes) -> float:
        """ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data) == 0:
            return 0.0
        
        # ãƒã‚¤ãƒˆé »åº¦è¨ˆç®—
        byte_counts = Counter(data)
        data_len = len(data)
        
        # ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        entropy = 0.0
        for count in byte_counts.values():
            if count > 0:
                prob = count / data_len
                entropy -= prob * math.log2(prob)
        
        return entropy
    
    # ==================== ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ ====================
    
    def _format_size(self, size: int) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024:
                return f"{size:.1f}{unit}"
            size /= 1024
        return f"{size:.1f}TB"
    
    def _compress_small_data(self, data: bytes) -> bytes:
        """è¶…å°ãƒ‡ãƒ¼ã‚¿ç”¨é«˜é€Ÿãƒ‘ã‚¹"""
        return b'NXS_FAST' + data  # éåœ§ç¸®
    
    def _decompress_small_data(self, compressed_data: bytes) -> bytes:
        """è¶…å°ãƒ‡ãƒ¼ã‚¿å±•é–‹"""
        return compressed_data[8:]  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤å»
    
    def _compress_fallback(self, data: bytes) -> bytes:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ§ç¸®"""
        import zlib
        compressed = zlib.compress(data, level=6)
        return b'NXS_ZLIB' + compressed
    
    def _decompress_fallback(self, compressed_data: bytes) -> bytes:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹"""
        import zlib
        return zlib.decompress(compressed_data[8:])
    
    def _decompress_large_file(self, compressed_data: bytes) -> bytes:
        """å¤§ãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹"""
        import zlib
        return zlib.decompress(compressed_data[9:])  # NXS_LARGE = 9 bytes
    
    def _compress_chunk_fallback(self, chunk: bytes) -> bytes:
        """ãƒãƒ£ãƒ³ã‚¯ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ§ç¸®"""
        import zlib
        return zlib.compress(chunk, level=6)
    
    # ==================== NEXUSç†è«–å®Ÿè£… ====================
    
    def _decompose_elements(self, data: bytes) -> List[int]:
        """è¦ç´ åˆ†è§£: ãƒ‡ãƒ¼ã‚¿ã®æœ€å°æ§‹æˆè¦ç´ ã‚’æŠ½å‡º"""
        return list(data)
    
    def _calculate_optimal_grid(self, element_count: int) -> Tuple[int, int]:
        """é»„é‡‘æ¯”ã«åŸºã¥ãæœ€é©ã‚°ãƒªãƒƒãƒ‰è¨ˆç®—"""
        sqrt_count = math.sqrt(element_count)
        width = int(sqrt_count * self.golden_ratio)
        height = int(sqrt_count / self.golden_ratio)
        
        # æœ€å°ã‚µã‚¤ã‚ºä¿è¨¼
        width = max(width, int(math.sqrt(element_count)))
        height = max(height, int(math.sqrt(element_count)))
        
        return (width, height)
    
    def _ml_enhanced_shape_clustering(self, elements: List[int], grid_dims: Tuple[int, int]) -> List[NEXUSGroup]:
        """æ©Ÿæ¢°å­¦ç¿’å¼·åŒ–å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        width, height = grid_dims
        total_elements = width * height
        
        # æ—©æœŸå‡¦ç†é‡åˆ¶é™ï¼ˆé‡è¦ï¼ï¼‰
        if total_elements > 100000:  # 100Kè¦ç´ è¶…ã¯å³åº§ã«é«˜é€Ÿå‡¦ç†
            if self.ml_config.verbose:
                print(f"âš¡ è¶…å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º ({total_elements} è¦ç´ ) - è¶…é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰")
            return self._ultra_fast_clustering(elements, grid_dims)
        elif total_elements > 50000:  # 50K-100Kè¦ç´ 
            if self.ml_config.verbose:
                print(f"ğŸš€ å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º ({total_elements} è¦ç´ ) - é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰")
            return self._fast_clustering_v2(elements, grid_dims)
        elif total_elements > 10000:  # 10K-50Kè¦ç´ 
            return self._balanced_clustering(elements, grid_dims)
        else:  # 10Kè¦ç´ æœªæº€
            return self._detailed_clustering(elements, grid_dims)
    
    def _fast_clustering_v2(self, elements: List[int], grid_dims: Tuple[int, int]) -> List[NEXUSGroup]:
        """é«˜é€Ÿã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° v2ï¼ˆä¸­å¤§ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰"""
        groups = []
        
        # é©å¿œçš„ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º
        element_count = len(elements)
        if element_count > 500000:  # 500Kä»¥ä¸Š
            block_size = 16
            max_groups = 5000
        elif element_count > 100000:  # 100K-500K
            block_size = 12
            max_groups = 10000
        else:  # 50K-100K
            block_size = 8
            max_groups = 20000
        
        if self.ml_config.verbose:
            print(f"ğŸ“¦ é«˜é€Ÿãƒ–ãƒ­ãƒƒã‚¯åŒ–: {block_size}è¦ç´ ãƒ–ãƒ­ãƒƒã‚¯, æœ€å¤§{max_groups}ã‚°ãƒ«ãƒ¼ãƒ—")
        
        # é«˜é€Ÿãƒ–ãƒ­ãƒƒã‚¯åŒ–
        for i in range(0, len(elements), block_size):
            if len(groups) >= max_groups:
                break
                
            block = elements[i:i+block_size]
            if len(block) >= 4:  # æœ€å°4è¦ç´ 
                # å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆåœ§ç¸®åŠ¹æœå‘ä¸Šï¼‰
                unique_count = len(set(block))
                if unique_count > 1:  # å˜èª¿ã§ãªã„ãƒ–ãƒ­ãƒƒã‚¯ã‚’å„ªå…ˆ
                    normalized = tuple(sorted(block))
                    hash_value = hashlib.md5(str(normalized).encode()).hexdigest()
                    
                    # åŠ¹ç‡çš„ãªå½¢çŠ¶åˆ¤å®š
                    if unique_count <= 2:
                        shape = PolyominoShape.O  # ä½å¤šæ§˜æ€§
                    elif unique_count >= len(block) // 2:
                        shape = PolyominoShape.I  # é«˜å¤šæ§˜æ€§
                    else:
                        shape = PolyominoShape.T  # ä¸­å¤šæ§˜æ€§
                    
                    group = NEXUSGroup(
                        elements=block,
                        shape=shape,
                        positions=[(i+j, 0) for j in range(len(block))],
                        normalized=normalized,
                        hash_value=hash_value
                    )
                    groups.append(group)
        
        if self.ml_config.verbose:
            print(f"âš¡ é«˜é€Ÿã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†: {len(groups)} ã‚°ãƒ«ãƒ¼ãƒ—")
        
        return groups
    
    def _ultra_fast_clustering(self, elements: List[int], grid_dims: Tuple[int, int]) -> List[NEXUSGroup]:
        """è¶…é«˜é€Ÿã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆå¤§ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰"""
        groups = []
        
        # å˜ç´”ãªå›ºå®šã‚µã‚¤ã‚ºãƒ–ãƒ­ãƒƒã‚¯åŒ–
        block_size = 8  # 8è¦ç´ ãƒ–ãƒ­ãƒƒã‚¯
        
        for i in range(0, len(elements), block_size):
            block = elements[i:i+block_size]
            if len(block) >= 4:  # æœ€å°4è¦ç´ 
                normalized = tuple(sorted(block))
                hash_value = hashlib.md5(str(normalized).encode()).hexdigest()
                
                group = NEXUSGroup(
                    elements=block,
                    shape=PolyominoShape.I,  # ç°¡ç•¥åŒ–
                    positions=[(i+j, 0) for j in range(len(block))],
                    normalized=normalized,
                    hash_value=hash_value
                )
                groups.append(group)
        
        return groups
    
    def _balanced_clustering(self, elements: List[int], grid_dims: Tuple[int, int]) -> List[NEXUSGroup]:
        """ãƒãƒ©ãƒ³ã‚¹å‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆä¸­ã‚µã‚¤ã‚ºç”¨ï¼‰"""
        width, height = grid_dims
        groups = []
        
        # 2Dã‚°ãƒªãƒƒãƒ‰ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        grid = np.zeros((height, width), dtype=int)
        for i, element in enumerate(elements):
            if i < width * height:
                y, x = divmod(i, width)
                grid[y, x] = element
        
        # åŠ¹ç‡çš„ãªå½¢çŠ¶ã‚»ãƒƒãƒˆ
        efficient_shapes = [
            PolyominoShape.I,      # ç›´ç·š
            PolyominoShape.O,      # æ­£æ–¹å½¢
            PolyominoShape.T,      # Tå­—
            PolyominoShape.SINGLE  # å˜ä¸€
        ]
        
        used_positions = set()
        
        for shape in efficient_shapes:
            patterns = self.polyomino_patterns[shape]
            for pattern in patterns:
                # ä¸­ç¨‹åº¦ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                step = max(1, min(height, width) // 10)  # 10x10ã‚°ãƒªãƒƒãƒ‰
                groups.extend(self._optimized_scan_pattern(grid, pattern, shape, used_positions, step))
                
                # é©åº¦ãªåˆ¶é™
                if len(groups) >= 2000:
                    break
            if len(groups) >= 2000:
                break
        
        return groups
    
    def _detailed_clustering(self, elements: List[int], grid_dims: Tuple[int, int]) -> List[NEXUSGroup]:
        """è©³ç´°ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆå°ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰"""
        width, height = grid_dims
        groups = []
        
        # å®Œå…¨2Dã‚°ãƒªãƒƒãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        grid = np.zeros((height, width), dtype=int)
        for i, element in enumerate(elements):
            if i < width * height:
                y, x = divmod(i, width)
                grid[y, x] = element
        
        # å…¨å½¢çŠ¶ã‚’ä½¿ç”¨
        all_shapes = [
            PolyominoShape.I, PolyominoShape.O, PolyominoShape.T,
            PolyominoShape.J, PolyominoShape.L, PolyominoShape.S,
            PolyominoShape.Z, PolyominoShape.LINE3, PolyominoShape.LINE2,
            PolyominoShape.SINGLE
        ]
        
        used_positions = set()
        
        # åŠ¹ç‡é †ã§ã‚¹ã‚­ãƒ£ãƒ³
        for shape in all_shapes:
            patterns = self.polyomino_patterns[shape]
            for pattern in patterns:
                groups.extend(self._precise_scan_pattern(grid, pattern, shape, used_positions))
                
                # å°ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚åˆ¶é™
                if len(groups) >= 5000:
                    break
            if len(groups) >= 5000:
                break
        
        return groups
    
    def _optimized_scan_pattern(self, grid: np.ndarray, pattern: List[Tuple[int, int]], 
                              shape: PolyominoShape, used_positions: Set, step: int) -> List[NEXUSGroup]:
        """æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚­ãƒ£ãƒ³"""
        height, width = grid.shape
        groups = []
        
        for y in range(0, height, step):
            for x in range(0, width, step):
                if self._can_place_pattern(grid, pattern, x, y):
                    positions = [(x + dx, y + dy) for dx, dy in pattern]
                    
                    # åŠ¹ç‡çš„ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if len(used_positions.intersection(positions)) == 0:
                        try:
                            elements = [grid[py, px] for px, py in positions]
                            
                            # æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                            if len(set(elements)) > 1:  # å¤šæ§˜æ€§ãŒã‚ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å„ªå…ˆ
                                normalized = tuple(sorted(elements))
                                hash_value = hashlib.md5(str(normalized).encode()).hexdigest()
                                
                                group = NEXUSGroup(
                                    elements=elements,
                                    shape=shape,
                                    positions=positions,
                                    normalized=normalized,
                                    hash_value=hash_value
                                )
                                
                                groups.append(group)
                                used_positions.update(positions)
                                
                        except IndexError:
                            continue
        
        return groups
    
    def _precise_scan_pattern(self, grid: np.ndarray, pattern: List[Tuple[int, int]], 
                            shape: PolyominoShape, used_positions: Set) -> List[NEXUSGroup]:
        """ç²¾å¯†ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚­ãƒ£ãƒ³"""
        height, width = grid.shape
        groups = []
        
        # å…¨ä½ç½®ã‚’ç²¾å¯†ã‚¹ã‚­ãƒ£ãƒ³
        for y in range(height):
            for x in range(width):
                if self._can_place_pattern(grid, pattern, x, y):
                    positions = [(x + dx, y + dy) for dx, dy in pattern]
                    
                    # æœªä½¿ç”¨ä½ç½®ã®ã¿
                    if not any(pos in used_positions for pos in positions):
                        try:
                            elements = [grid[py, px] for px, py in positions]
                            
                            # ã‚ˆã‚Šå³å¯†ãªæœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                            if self._is_valuable_group(elements, shape):
                                normalized = tuple(sorted(elements))
                                hash_value = hashlib.md5(str(normalized).encode()).hexdigest()
                                
                                group = NEXUSGroup(
                                    elements=elements,
                                    shape=shape,
                                    positions=positions,
                                    normalized=normalized,
                                    hash_value=hash_value
                                )
                                
                                groups.append(group)
                                used_positions.update(positions)
                                
                        except IndexError:
                            continue
        
        return groups
    
    def _is_valuable_group(self, elements: List[int], shape: PolyominoShape) -> bool:
        """ã‚°ãƒ«ãƒ¼ãƒ—ã®ä¾¡å€¤åˆ¤å®š"""
        if len(elements) < 2:
            return False
        
        # å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
        unique_elements = len(set(elements))
        if unique_elements == 1:  # å…¨ã¦åŒã˜å€¤
            return shape == PolyominoShape.O or len(elements) >= 4
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ä¾¡å€¤
        if unique_elements >= len(elements) // 2:  # é©åº¦ãªå¤šæ§˜æ€§
            return True
        
        # å½¢çŠ¶ä¾¡å€¤
        valuable_shapes = [PolyominoShape.I, PolyominoShape.O, PolyominoShape.T]
        return shape in valuable_shapes
    
    def _simplified_shape_clustering(self, elements: List[int], grid_dims: Tuple[int, int]) -> List[NEXUSGroup]:
        """å¤§ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ç°¡ç•¥åŒ–å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        groups = []
        
        # å˜ç´”ãª4è¦ç´ ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        for i in range(0, len(elements), 4):
            chunk = elements[i:i+4]
            if len(chunk) >= 4:
                normalized = tuple(sorted(chunk))
                hash_value = hashlib.md5(str(normalized).encode()).hexdigest()
                
                group = NEXUSGroup(
                    elements=chunk,
                    shape=PolyominoShape.I,
                    positions=[(i+j, 0) for j in range(len(chunk))],
                    normalized=normalized,
                    hash_value=hash_value
                )
                groups.append(group)
        
        return groups
    
    def _fast_scan_pattern(self, grid: np.ndarray, pattern: List[Tuple[int, int]], shape: PolyominoShape, used_positions: Set) -> List[NEXUSGroup]:
        """é«˜é€Ÿãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
        height, width = grid.shape
        groups = []
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ï¼ˆé«˜é€ŸåŒ–ï¼‰
        step = max(1, min(height, width) // 20)  # æœ€å¤§20x20ã‚°ãƒªãƒƒãƒ‰ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        
        for y in range(0, height, step):
            for x in range(0, width, step):
                if self._can_place_pattern(grid, pattern, x, y):
                    positions = [(x + dx, y + dy) for dx, dy in pattern]
                    
                    # æœªä½¿ç”¨ä½ç½®ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡ç•¥åŒ–ï¼‰
                    if not any(pos in used_positions for pos in positions):
                        try:
                            elements = [grid[py, px] for px, py in positions]
                            
                            # ã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ
                            normalized = tuple(sorted(elements))
                            hash_value = hashlib.md5(str(normalized).encode()).hexdigest()
                            
                            group = NEXUSGroup(
                                elements=elements,
                                shape=shape,
                                positions=positions,
                                normalized=normalized,
                                hash_value=hash_value
                            )
                            
                            groups.append(group)
                            used_positions.update(positions)
                            
                            # ååˆ†ãªã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã£ãŸã‚‰çµ‚äº†
                            if len(groups) >= 1000:
                                return groups
                                
                        except IndexError:
                            continue
        
        return groups
    
    def _calculate_coverage_score(self, grid: np.ndarray, shape: PolyominoShape) -> float:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
        height, width = grid.shape
        
        # å¤§ããªã‚°ãƒªãƒƒãƒ‰ã¯ç°¡ç•¥è©•ä¾¡
        if height * width > 1000:
            return 1.0 / (len(PolyominoShape) - list(PolyominoShape).index(shape))
        
        patterns = self.polyomino_patterns[shape]
        total_coverage = 0
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§é«˜é€ŸåŒ–
        sample_points = min(100, height * width)
        step = max(1, (height * width) // sample_points)
        
        count = 0
        for pattern in patterns:
            for i in range(0, height * width, step):
                y, x = divmod(i, width)
                if y < height and x < width and self._can_place_pattern(grid, pattern, x, y):
                    total_coverage += len(pattern)
                count += 1
                if count >= sample_points:
                    break
        
        return total_coverage / max(1, count)
    
    def _can_place_pattern(self, grid: np.ndarray, pattern: List[Tuple[int, int]], x: int, y: int) -> bool:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é…ç½®å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        height, width = grid.shape
        for dx, dy in pattern:
            nx, ny = x + dx, y + dy
            if nx >= width or ny >= height or nx < 0 or ny < 0:
                return False
        return True
    
    def _scan_pattern(self, grid: np.ndarray, pattern: List[Tuple[int, int]], shape: PolyominoShape, used_positions: Set) -> List[NEXUSGroup]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
        height, width = grid.shape
        groups = []
        
        # å¤§ããªã‚°ãƒªãƒƒãƒ‰ã§ã¯ç°¡ç•¥ã‚¹ã‚­ãƒ£ãƒ³
        if height * width > 10000:
            step = max(1, min(height, width) // 20)
            max_groups = 500
        else:
            step = 1
            max_groups = 1000
        
        count = 0
        for y in range(0, height, step):
            for x in range(0, width, step):
                if count >= max_groups:
                    break
                    
                if self._can_place_pattern(grid, pattern, x, y):
                    positions = [(x + dx, y + dy) for dx, dy in pattern]
                    
                    # æœªä½¿ç”¨ä½ç½®ãƒã‚§ãƒƒã‚¯
                    if not any(pos in used_positions for pos in positions):
                        try:
                            elements = [grid[py, px] for px, py in positions]
                            
                            # ã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ
                            normalized = tuple(sorted(elements))
                            hash_value = hashlib.md5(str(normalized).encode()).hexdigest()
                            
                            group = NEXUSGroup(
                                elements=elements,
                                shape=shape,
                                positions=positions,
                                normalized=normalized,
                                hash_value=hash_value
                            )
                            
                            groups.append(group)
                            used_positions.update(positions)
                            count += 1
                        except (IndexError, ValueError):
                            continue
            if count >= max_groups:
                break
        
        return groups
    
    def _permutative_grouping(self, groups: List[NEXUSGroup]) -> Tuple[List[NEXUSGroup], Dict[str, int], List[int]]:
        """é †ç•ªå…¥ã‚Œæ›¿ãˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
        # å¤§é‡ã‚°ãƒ«ãƒ¼ãƒ—ã®å ´åˆã¯åˆ¶é™
        if len(groups) > 50000:
            if self.ml_config.verbose:
                print(f"âš ï¸ å¤§é‡ã‚°ãƒ«ãƒ¼ãƒ—æ¤œå‡º ({len(groups)}) - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ")
            # é‡è¦ãªã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿é¸æŠ
            groups = self._sample_important_groups_fast(groups, 20000)
        
        # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        hash_to_group = {}
        group_counts = Counter()
        position_map = []
        
        for i, group in enumerate(groups):
            hash_value = group.hash_value
            group_counts[hash_value] += 1
            
            if hash_value not in hash_to_group:
                hash_to_group[hash_value] = group
                position_map.append(len(hash_to_group) - 1)
            else:
                # æ—¢å­˜ã®ãƒãƒƒã‚·ãƒ¥ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¤œç´¢
                position_map.append(list(hash_to_group.keys()).index(hash_value))
        
        unique_groups = list(hash_to_group.values())
        
        if self.ml_config.verbose:
            print(f"ğŸ”„ ã‚°ãƒ«ãƒ¼ãƒ—çµ±åˆ: {len(groups)} â†’ {len(unique_groups)} ãƒ¦ãƒ‹ãƒ¼ã‚¯")
        
        return unique_groups, dict(group_counts), position_map
    
    def _sample_important_groups_fast(self, groups: List[NEXUSGroup], target_count: int) -> List[NEXUSGroup]:
        """é‡è¦ã‚°ãƒ«ãƒ¼ãƒ—ã®é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        if len(groups) <= target_count:
            return groups
        
        # ç°¡å˜ãªé‡è¦åº¦è©•ä¾¡
        scored_groups = []
        
        for i, group in enumerate(groups):
            # é«˜é€Ÿã‚¹ã‚³ã‚¢è¨ˆç®—
            diversity_score = len(set(group.elements))  # å¤šæ§˜æ€§
            size_score = len(group.elements)  # ã‚µã‚¤ã‚º
            
            # å½¢çŠ¶ãƒœãƒ¼ãƒŠã‚¹
            shape_bonus = {
                PolyominoShape.I: 3,
                PolyominoShape.O: 2,
                PolyominoShape.T: 2,
                PolyominoShape.SINGLE: 1
            }.get(group.shape, 1)
            
            total_score = diversity_score * 2 + size_score + shape_bonus
            scored_groups.append((total_score, i, group))
            
            # å‡¦ç†åˆ¶é™ï¼ˆé‡è¦ï¼ï¼‰
            if i >= len(groups) * 0.1:  # æœ€åˆã®10%ã®ã¿è©•ä¾¡
                break
        
        # ä¸Šä½ã‚’é¸æŠ
        scored_groups.sort(reverse=True)
        selected = [group for _, _, group in scored_groups[:target_count]]
        
        # ä¸è¶³åˆ†ã¯ç­‰é–“éš”ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(selected) < target_count:
            remaining = target_count - len(selected)
            step = max(1, len(groups) // remaining)
            
            for i in range(0, len(groups), step):
                if len(selected) >= target_count:
                    break
                if groups[i] not in selected:
                    selected.append(groups[i])
        
        return selected[:target_count]
    
    def _calculate_shape_distribution(self, groups: List[NEXUSGroup]) -> Dict[PolyominoShape, int]:
        """å½¢çŠ¶åˆ†å¸ƒè¨ˆç®—"""
        distribution = Counter()
        for group in groups:
            distribution[group.shape] += 1
        return dict(distribution)
    
    def _reconstruct_data_from_state(self, nexus_state: NEXUSCompressionState) -> bytes:
        """NEXUSçŠ¶æ…‹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å¾©å…ƒï¼ˆå®Œå…¨å¯é€†æ€§ä¿è¨¼ç‰ˆï¼‰"""
        try:
            # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºå–å¾—
            original_size = nexus_state.compression_metadata.get('original_size', 0)
            
            if self.ml_config.verbose:
                print(f"ğŸ”§ ãƒ‡ãƒ¼ã‚¿å¾©å…ƒé–‹å§‹: ç›®æ¨™ã‚µã‚¤ã‚º={original_size}")
            
            # æ–¹æ³•1: ä½ç½®ãƒãƒƒãƒ—ã‹ã‚‰å³å¯†å¾©å…ƒï¼ˆæœ€å„ªå…ˆï¼‰
            if hasattr(nexus_state, 'position_map') and nexus_state.position_map and hasattr(nexus_state, 'unique_groups'):
                if self.ml_config.verbose:
                    print(f"ğŸ¯ ä½ç½®ãƒãƒƒãƒ—å¾©å…ƒ: {len(nexus_state.position_map)} ä½ç½®")
                
                result = bytearray()
                
                # ä½ç½®ãƒãƒƒãƒ—ã«å¾“ã£ã¦å³å¯†ã«å¾©å…ƒ
                for position_index in nexus_state.position_map:
                    if position_index < len(nexus_state.unique_groups):
                        group = nexus_state.unique_groups[position_index]
                        # ã‚°ãƒ«ãƒ¼ãƒ—ã®å…ƒç´ ã‚’é †åºé€šã‚Šè¿½åŠ 
                        result.extend(group.elements)
                    else:
                        if self.ml_config.verbose:
                            print(f"âš ï¸ ç„¡åŠ¹ãªä½ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {position_index}")
                
                # å³å¯†ãªã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                if original_size > 0:
                    if len(result) == original_size:
                        if self.ml_config.verbose:
                            print(f"âœ… å®Œå…¨ä¸€è‡´: {len(result)} bytes")
                        return bytes(result)
                    elif len(result) > original_size:
                        # éå¤§ãªå ´åˆã¯åˆ‡ã‚Šè©°ã‚
                        truncated = result[:original_size]
                        if self.ml_config.verbose:
                            print(f"âœ‚ï¸ åˆ‡ã‚Šè©°ã‚: {len(result)} -> {len(truncated)} bytes")
                        return bytes(truncated)
                    else:
                        if self.ml_config.verbose:
                            print(f"âš ï¸ ã‚µã‚¤ã‚ºä¸è¶³: {len(result)} < {original_size}")
                        # ä¸è¶³åˆ†ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å®šï¼ˆå±é™ºã ãŒæœ€å–„ã®åŠªåŠ›ï¼‰
                        return bytes(result)
                else:
                    # ã‚µã‚¤ã‚ºæƒ…å ±ãªã—ã®å ´åˆ
                    return bytes(result)
            
            # æ–¹æ³•2: ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰é †åºå¾©å…ƒ
            elif hasattr(nexus_state, 'original_groups') and nexus_state.original_groups:
                if self.ml_config.verbose:
                    print(f"ğŸ”„ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚°ãƒ«ãƒ¼ãƒ—å¾©å…ƒ: {len(nexus_state.original_groups)} ã‚°ãƒ«ãƒ¼ãƒ—")
                
                # ä½ç½®æƒ…å ±ã§ã‚½ãƒ¼ãƒˆ
                sorted_groups = sorted(nexus_state.original_groups, 
                                     key=lambda g: g.positions[0] if g.positions else (0, 0))
                
                result = bytearray()
                for group in sorted_groups:
                    result.extend(group.elements)
                
                # ã‚µã‚¤ã‚ºèª¿æ•´
                if original_size > 0 and len(result) != original_size:
                    if len(result) > original_size:
                        result = result[:original_size]
                    if self.ml_config.verbose:
                        print(f"ï¿½ ã‚µã‚¤ã‚ºèª¿æ•´: {len(result)} bytes")
                
                return bytes(result)
            
            # æ–¹æ³•3: ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æ¨å®šå¾©å…ƒï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
            else:
                if self.ml_config.verbose:
                    print(f"ğŸ†˜ æ¨å®šå¾©å…ƒ: {len(nexus_state.unique_groups)} ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—")
                
                result = bytearray()
                
                # ã‚°ãƒ«ãƒ¼ãƒ—å‡ºç¾å›æ•°ã‚’è€ƒæ…®ã—ãŸå¾©å…ƒ
                if hasattr(nexus_state, 'group_counts'):
                    for group in nexus_state.unique_groups:
                        count = nexus_state.group_counts.get(group.hash_value, 1)
                        for _ in range(count):
                            result.extend(group.elements)
                else:
                    # å˜ç´”çµåˆ
                    for group in nexus_state.unique_groups:
                        result.extend(group.elements)
                
                # ã‚µã‚¤ã‚ºåˆ¶é™
                if original_size > 0 and len(result) > original_size:
                    result = result[:original_size]
                
                if self.ml_config.verbose:
                    print(f"ğŸ² æ¨å®šå¾©å…ƒçµæœ: {len(result)} bytes")
                
                return bytes(result)
        
        except Exception as e:
            if self.ml_config.verbose:
                print(f"âŒ ãƒ‡ãƒ¼ã‚¿å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()
            return b""

# ==================== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ====================

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("ğŸ§  NEXUSæ©Ÿæ¢°å­¦ç¿’çµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ")
    
    test_data = b"Hello, NEXUS ML World!" * 100
    
    config = MLCompressionConfig(verbose=True)
    compressor = NEXUSCompressor(config)
    
    # åœ§ç¸®ãƒ†ã‚¹ãƒˆ
    start_time = time.time()
    compressed = compressor.compress(test_data)
    compress_time = time.time() - start_time
    
    # å±•é–‹ãƒ†ã‚¹ãƒˆ
    start_time = time.time()
    decompressed = compressor.decompress(compressed)
    decompress_time = time.time() - start_time
    
    # çµæœè¡¨ç¤º
    original_size = len(test_data)
    compressed_size = len(compressed)
    compression_ratio = (compressed_size / original_size) * 100
    
    print(f"\nğŸ“Š çµæœ:")
    print(f"å…ƒã‚µã‚¤ã‚º: {original_size} bytes")
    print(f"åœ§ç¸®ã‚µã‚¤ã‚º: {compressed_size} bytes")
    print(f"åœ§ç¸®ç‡: {compression_ratio:.1f}%")
    print(f"åœ§ç¸®æ™‚é–“: {compress_time:.3f}s")
    print(f"å±•é–‹æ™‚é–“: {decompress_time:.3f}s")
    print(f"ãƒ‡ãƒ¼ã‚¿ä¸€è‡´: {test_data == decompressed}")
    print(f"MLçµ±è¨ˆ: {compressor.stats}")

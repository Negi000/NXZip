#!/usr/bin/env python3
"""
NEXUS Universal Compression Engine (NUCE)
æ±ç”¨åœ§ç¸®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  - å®Œå…¨ç‹¬è‡ªå®Ÿè£…

ç‰¹å¾´:
1. æ±ç”¨æ€§ - ã‚ã‚‰ã‚†ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¯¾å¿œ
2. é©å¿œæ€§ - ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è‡ªå‹•æ¤œå‡ºãƒ»æœ€é©åŒ–
3. æ®µéšçš„åœ§ç¸® - è¤‡æ•°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®çµ„ã¿åˆã‚ã›
4. é«˜é€Ÿå‡¦ç† - å®Ÿç”¨çš„ãªå‡¦ç†é€Ÿåº¦
5. å®Œå…¨ç‹¬è‡ª - zlib/LZMAç­‰ã®æ—¢å­˜æŠ€è¡“ä¸ä½¿ç”¨

ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ§‹æˆ:
- Stage 1: Pattern Analysis (ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ)
- Stage 2: Adaptive Preprocessing (é©å¿œå‰å‡¦ç†)
- Stage 3: Multi-tier Compression (å¤šæ®µéšåœ§ç¸®)
- Stage 4: Entropy Optimization (ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€é©åŒ–)
"""

import os
import sys
import time
import struct
import hashlib
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Any
from collections import Counter, defaultdict
from enum import Enum

class DataPattern(Enum):
    """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡"""
    BINARY = "binary"
    TEXT = "text"
    REPETITIVE = "repetitive"
    RANDOM = "random"
    STRUCTURED = "structured"
    COMPRESSED = "compressed"

class CompressionStrategy(Enum):
    """åœ§ç¸®æˆ¦ç•¥"""
    ULTRA_FAST = "ultra_fast"
    BALANCED = "balanced"
    MAXIMUM = "maximum"
    ADAPTIVE = "adaptive"

@dataclass
class CompressionAnalysis:
    """åœ§ç¸®è§£æçµæœ"""
    data_size: int
    pattern_type: DataPattern
    entropy: float
    repetition_ratio: float
    text_ratio: float
    null_ratio: float
    recommended_strategy: CompressionStrategy
    estimated_ratio: float

@dataclass
class CompressionResult:
    """åœ§ç¸®çµæœ"""
    original_size: int
    compressed_size: int
    compression_ratio: float
    processing_time: float
    algorithm_stages: List[str]
    pattern_analysis: CompressionAnalysis
    checksum: str

class NexusUniversalCompression:
    """æ±ç”¨åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, strategy: CompressionStrategy = CompressionStrategy.ADAPTIVE):
        self.version = "1.0-Universal"
        self.magic = b'NUCE2025'  # NEXUS Universal Compression Engine
        self.strategy = strategy
        
        # åœ§ç¸®è¨­å®š
        self.enable_pattern_analysis = True
        self.enable_adaptive_preprocessing = True
        self.enable_multitier_compression = True
        self.enable_entropy_optimization = True
        
        # æ€§èƒ½èª¿æ•´
        self.analysis_sample_size = 8192  # è§£æã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
        self.max_dict_size = 32768  # è¾æ›¸æœ€å¤§ã‚µã‚¤ã‚º
        self.min_match_length = 3  # æœ€å°ãƒãƒƒãƒé•·
        
        print(f"ğŸš€ NEXUS Universal Compression Engine v{self.version}")
        print(f"âš™ï¸  æˆ¦ç•¥: {strategy.value}")
        print("ğŸ”§ æ±ç”¨åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
    
    def analyze_data_pattern(self, data: bytes) -> CompressionAnalysis:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ"""
        if len(data) == 0:
            return CompressionAnalysis(
                data_size=0,
                pattern_type=DataPattern.BINARY,
                entropy=0.0,
                repetition_ratio=0.0,
                text_ratio=0.0,
                null_ratio=0.0,
                recommended_strategy=CompressionStrategy.ULTRA_FAST,
                estimated_ratio=0.0
            )
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        sample_size = min(len(data), self.analysis_sample_size)
        sample = data[:sample_size]
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        entropy = self._calculate_entropy(sample)
        
        # åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        repetition_ratio = self._calculate_repetition_ratio(sample)
        
        # ãƒ†ã‚­ã‚¹ãƒˆç‡è¨ˆç®—
        text_ratio = self._calculate_text_ratio(sample)
        
        # NULLç‡è¨ˆç®—
        null_ratio = sample.count(0) / len(sample)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡
        pattern_type = self._classify_pattern(entropy, repetition_ratio, text_ratio, null_ratio)
        
        # æˆ¦ç•¥æ¨å¥¨
        recommended_strategy = self._recommend_strategy(pattern_type, len(data))
        
        # åœ§ç¸®ç‡äºˆæ¸¬
        estimated_ratio = self._estimate_compression_ratio(pattern_type, entropy, repetition_ratio)
        
        return CompressionAnalysis(
            data_size=len(data),
            pattern_type=pattern_type,
            entropy=entropy,
            repetition_ratio=repetition_ratio,
            text_ratio=text_ratio,
            null_ratio=null_ratio,
            recommended_strategy=recommended_strategy,
            estimated_ratio=estimated_ratio
        )
    
    def _calculate_entropy(self, data: bytes) -> float:
        """ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data) == 0:
            return 0.0
        
        # é »åº¦è¨ˆç®—
        freq = Counter(data)
        total = len(data)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        entropy = 0.0
        for count in freq.values():
            prob = count / total
            if prob > 0:
                import math
                entropy -= prob * math.log2(prob)
        
        return entropy
    
    def _calculate_repetition_ratio(self, data: bytes) -> float:
        """åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ç‡è¨ˆç®—"""
        if len(data) < 4:
            return 0.0
        
        repetitive_bytes = 0
        i = 0
        
        while i < len(data) - 1:
            current = data[i]
            count = 1
            
            # é€£ç¶šã™ã‚‹åŒã˜ãƒã‚¤ãƒˆã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            while i + count < len(data) and data[i + count] == current:
                count += 1
            
            if count >= 3:  # 3å›ä»¥ä¸Šã®åå¾©
                repetitive_bytes += count
            
            i += count
        
        return repetitive_bytes / len(data)
    
    def _calculate_text_ratio(self, data: bytes) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆç‡è¨ˆç®—"""
        if len(data) == 0:
            return 0.0
        
        text_bytes = 0
        for byte in data:
            # ASCIIå°åˆ·å¯èƒ½æ–‡å­— + æ”¹è¡Œãƒ»ã‚¿ãƒ–ãƒ»ã‚¹ãƒšãƒ¼ã‚¹
            if (32 <= byte <= 126) or byte in [9, 10, 13]:
                text_bytes += 1
        
        return text_bytes / len(data)
    
    def _classify_pattern(self, entropy: float, repetition_ratio: float, 
                         text_ratio: float, null_ratio: float) -> DataPattern:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡"""
        # é«˜åå¾©ç‡
        if repetition_ratio > 0.3:
            return DataPattern.REPETITIVE
        
        # é«˜ãƒ†ã‚­ã‚¹ãƒˆç‡
        if text_ratio > 0.8:
            return DataPattern.TEXT
        
        # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒ©ãƒ³ãƒ€ãƒ /æ—¢åœ§ç¸®ï¼‰
        if entropy > 7.5:
            return DataPattern.RANDOM if null_ratio < 0.1 else DataPattern.COMPRESSED
        
        # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæ§‹é€ åŒ–ï¼‰
        if entropy < 4.0:
            return DataPattern.STRUCTURED
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return DataPattern.BINARY
    
    def _recommend_strategy(self, pattern: DataPattern, size: int) -> CompressionStrategy:
        """æˆ¦ç•¥æ¨å¥¨"""
        if self.strategy != CompressionStrategy.ADAPTIVE:
            return self.strategy
        
        # ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹èª¿æ•´
        if size < 1024:  # 1KBæœªæº€
            return CompressionStrategy.ULTRA_FAST
        elif size > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
            return CompressionStrategy.BALANCED
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹èª¿æ•´
        if pattern == DataPattern.REPETITIVE:
            return CompressionStrategy.MAXIMUM
        elif pattern == DataPattern.RANDOM:
            return CompressionStrategy.ULTRA_FAST
        elif pattern == DataPattern.TEXT:
            return CompressionStrategy.BALANCED
        else:
            return CompressionStrategy.BALANCED
    
    def _estimate_compression_ratio(self, pattern: DataPattern, entropy: float, 
                                  repetition_ratio: float) -> float:
        """åœ§ç¸®ç‡äºˆæ¸¬"""
        base_ratio = {
            DataPattern.REPETITIVE: 0.8,
            DataPattern.TEXT: 0.6,
            DataPattern.STRUCTURED: 0.7,
            DataPattern.BINARY: 0.5,
            DataPattern.RANDOM: 0.1,
            DataPattern.COMPRESSED: 0.05
        }.get(pattern, 0.5)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹èª¿æ•´
        entropy_factor = max(0.1, min(1.0, (8.0 - entropy) / 8.0))
        
        # åå¾©ç‡ãƒ™ãƒ¼ã‚¹èª¿æ•´
        repetition_factor = 1.0 + repetition_ratio * 2.0
        
        estimated = base_ratio * entropy_factor * repetition_factor
        return max(0.05, min(0.95, estimated))
    
    def compress_universal(self, data: bytes) -> bytes:
        """æ±ç”¨åœ§ç¸®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
        if len(data) == 0:
            return self._create_empty_archive()
        
        print(f"ğŸ“¦ æ±ç”¨åœ§ç¸®é–‹å§‹: {len(data)} bytes")
        start_time = time.time()
        
        # Stage 1: ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ
        analysis = self.analyze_data_pattern(data)
        print(f"ğŸ” è§£æ: {analysis.pattern_type.value} (ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {analysis.entropy:.2f})")
        print(f"ğŸ“Š åå¾©ç‡: {analysis.repetition_ratio:.1%}, ãƒ†ã‚­ã‚¹ãƒˆç‡: {analysis.text_ratio:.1%}")
        print(f"âš¡ æ¨å¥¨æˆ¦ç•¥: {analysis.recommended_strategy.value}")
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
        checksum = hashlib.sha256(data).hexdigest()[:16]
        
        # Stage 2-4: æ®µéšçš„åœ§ç¸®
        compressed_data = data
        stages = []
        
        # Stage 2: é©å¿œå‰å‡¦ç†
        if self.enable_adaptive_preprocessing:
            compressed_data = self._adaptive_preprocess(compressed_data, analysis)
            stages.append("adaptive_preprocess")
            print(f"  ğŸ”§ é©å¿œå‰å‡¦ç†: {len(data)} â†’ {len(compressed_data)} bytes")
        
        # Stage 3: å¤šæ®µéšåœ§ç¸®
        if self.enable_multitier_compression:
            compressed_data = self._multitier_compress(compressed_data, analysis)
            stages.append("multitier_compress")
            print(f"  ğŸ—œï¸  å¤šæ®µéšåœ§ç¸®: â†’ {len(compressed_data)} bytes")
        
        # Stage 4: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€é©åŒ–
        if self.enable_entropy_optimization:
            compressed_data = self._entropy_optimize(compressed_data, analysis)
            stages.append("entropy_optimize")
            print(f"  âš—ï¸  ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€é©åŒ–: â†’ {len(compressed_data)} bytes")
        
        # çµæœãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°
        processing_time = time.time() - start_time
        result = CompressionResult(
            original_size=len(data),
            compressed_size=len(compressed_data),
            compression_ratio=(1 - len(compressed_data) / len(data)) * 100,
            processing_time=processing_time,
            algorithm_stages=stages,
            pattern_analysis=analysis,
            checksum=checksum
        )
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆ
        archive = self._create_archive(compressed_data, result)
        
        final_ratio = (1 - len(archive) / len(data)) * 100
        print(f"âœ… åœ§ç¸®å®Œäº†: {len(data)} â†’ {len(archive)} bytes ({final_ratio:.1f}%, {processing_time:.3f}s)")
        
        return archive
    
    def _adaptive_preprocess(self, data: bytes, analysis: CompressionAnalysis) -> bytes:
        """é©å¿œå‰å‡¦ç†"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¿œã˜ãŸå‰å‡¦ç†é¸æŠ
        if analysis.pattern_type == DataPattern.REPETITIVE:
            return self._rle_preprocess(data)
        elif analysis.pattern_type == DataPattern.TEXT:
            return self._text_preprocess(data)
        elif analysis.pattern_type == DataPattern.STRUCTURED:
            return self._delta_preprocess(data)
        else:
            return data
    
    def _rle_preprocess(self, data: bytes) -> bytes:
        """Run-Lengthå‰å‡¦ç†"""
        result = bytearray()
        i = 0
        
        while i < len(data):
            current = data[i]
            count = 1
            
            # é€£ç¶šã‚«ã‚¦ãƒ³ãƒˆ
            while i + count < len(data) and data[i + count] == current and count < 255:
                count += 1
            
            if count >= 4:  # 4å›ä»¥ä¸Šã§åœ§ç¸®
                result.append(0xFE)  # RLEãƒãƒ¼ã‚«ãƒ¼
                result.append(count)
                result.append(current)
                i += count
            else:
                # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†
                if current == 0xFE:
                    result.append(0xFE)
                    result.append(0x00)
                result.append(current)
                i += 1
        
        return bytes(result)
    
    def _text_preprocess(self, data: bytes) -> bytes:
        """ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ï¼ˆå˜èªè¾æ›¸åœ§ç¸®ï¼‰"""
        # ç°¡æ˜“å˜èªåˆ†å‰²ã¨é »åº¦è§£æ
        text = data.decode('utf-8', errors='ignore')
        words = text.split()
        
        if len(words) < 10:
            return data  # åŠ¹æœãªã—
        
        # é »å‡ºå˜èªè¾æ›¸ä½œæˆ
        word_freq = Counter(words)
        common_words = [word for word, count in word_freq.most_common(128) if len(word) > 3]
        
        if len(common_words) < 5:
            return data
        
        # è¾æ›¸ç½®æ›
        result = text
        dictionary = {}
        
        for i, word in enumerate(common_words):
            marker = f"\xFF{i:02x}\xFF"
            dictionary[marker] = word
            result = result.replace(word, marker)
        
        # è¾æ›¸ã¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
        dict_data = "|".join([f"{k}:{v}" for k, v in dictionary.items()])
        packaged = f"DICT:{len(dict_data):04x}:{dict_data}|DATA:{result}"
        
        encoded = packaged.encode('utf-8', errors='ignore')
        return encoded if len(encoded) < len(data) else data
    
    def _delta_preprocess(self, data: bytes) -> bytes:
        """Deltaå‰å‡¦ç†"""
        if len(data) < 2:
            return data
        
        result = bytearray([data[0]])  # æœ€åˆã®ãƒã‚¤ãƒˆ
        
        for i in range(1, len(data)):
            delta = (data[i] - data[i-1]) % 256
            result.append(delta)
        
        return bytes(result)
    
    def _multitier_compress(self, data: bytes, analysis: CompressionAnalysis) -> bytes:
        """å¤šæ®µéšåœ§ç¸®"""
        # æˆ¦ç•¥ã«å¿œã˜ãŸåœ§ç¸®æ–¹å¼é¸æŠ
        strategy = analysis.recommended_strategy
        
        if strategy == CompressionStrategy.ULTRA_FAST:
            return self._simple_lz_compress(data)
        elif strategy == CompressionStrategy.MAXIMUM:
            return self._advanced_lz_compress(data)
        else:  # BALANCED or ADAPTIVE
            return self._balanced_lz_compress(data)
    
    def _simple_lz_compress(self, data: bytes) -> bytes:
        """ç°¡æ˜“LZåœ§ç¸®"""
        if len(data) < 4:
            return data
        
        result = bytearray()
        i = 0
        
        while i < len(data):
            # å¾Œæ–¹å‚ç…§æ¤œç´¢ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            best_length = 0
            best_distance = 0
            
            # æœ€å¤§æ¤œç´¢ç¯„å›²
            search_start = max(0, i - 256)
            
            for j in range(search_start, i):
                length = 0
                while (i + length < len(data) and 
                       j + length < i and 
                       data[i + length] == data[j + length] and 
                       length < 255):
                    length += 1
                
                if length >= self.min_match_length and length > best_length:
                    best_length = length
                    best_distance = i - j
            
            if best_length >= self.min_match_length:
                # ãƒãƒƒãƒç¬¦å·åŒ–: [0xFF][è·é›¢][é•·ã•]
                result.append(0xFF)
                result.append(best_distance)
                result.append(best_length)
                i += best_length
            else:
                # ãƒªãƒ†ãƒ©ãƒ«æ–‡å­—
                if data[i] == 0xFF:
                    result.append(0xFF)
                    result.append(0x00)
                result.append(data[i])
                i += 1
        
        return bytes(result)
    
    def _balanced_lz_compress(self, data: bytes) -> bytes:
        """ãƒãƒ©ãƒ³ã‚¹LZåœ§ç¸®"""
        # ã‚ˆã‚Šå¤§ããªæ¤œç´¢çª“ã§ã®åœ§ç¸®
        if len(data) < 4:
            return data
        
        result = bytearray()
        i = 0
        
        while i < len(data):
            best_length = 0
            best_distance = 0
            
            # æ‹¡å¼µæ¤œç´¢ç¯„å›²
            search_start = max(0, i - 4096)
            
            for j in range(search_start, i):
                length = 0
                while (i + length < len(data) and 
                       j + length < i and 
                       data[i + length] == data[j + length] and 
                       length < 258):
                    length += 1
                
                if length >= self.min_match_length and length > best_length:
                    best_length = length
                    best_distance = i - j
            
            if best_length >= self.min_match_length:
                # æ‹¡å¼µãƒãƒƒãƒç¬¦å·åŒ–
                if best_distance <= 255 and best_length <= 255:
                    result.append(0xFE)
                    result.append(best_distance)
                    result.append(best_length)
                else:
                    result.append(0xFD)
                    result.extend(struct.pack('<HH', best_distance, best_length))
                i += best_length
            else:
                # ãƒªãƒ†ãƒ©ãƒ«
                byte = data[i]
                if byte in [0xFD, 0xFE, 0xFF]:
                    result.append(0xFF)
                    result.append(byte)
                else:
                    result.append(byte)
                i += 1
        
        return bytes(result)
    
    def _advanced_lz_compress(self, data: bytes) -> bytes:
        """é«˜ç´šLZåœ§ç¸®ï¼ˆãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ä½¿ç”¨ï¼‰"""
        if len(data) < 4:
            return data
        
        # ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹ç¯‰
        hash_table = defaultdict(list)
        
        # 3ãƒã‚¤ãƒˆãƒãƒƒã‚·ãƒ¥ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        for i in range(len(data) - 2):
            hash_val = (data[i] << 16) | (data[i+1] << 8) | data[i+2]
            hash_table[hash_val].append(i)
        
        result = bytearray()
        i = 0
        
        while i < len(data):
            if i + 2 >= len(data):
                result.append(data[i])
                i += 1
                continue
            
            # ãƒãƒƒã‚·ãƒ¥æ¤œç´¢
            hash_val = (data[i] << 16) | (data[i+1] << 8) | data[i+2]
            candidates = hash_table[hash_val]
            
            best_length = 0
            best_distance = 0
            
            for pos in candidates:
                if pos >= i:
                    break
                
                if i - pos > 65535:  # è·é›¢åˆ¶é™
                    continue
                
                length = 0
                while (i + length < len(data) and 
                       pos + length < i and 
                       data[i + length] == data[pos + length] and 
                       length < 258):
                    length += 1
                
                if length > best_length:
                    best_length = length
                    best_distance = i - pos
            
            if best_length >= self.min_match_length:
                # åŠ¹ç‡çš„ãªç¬¦å·åŒ–
                result.append(0xFC)
                if best_distance <= 255 and best_length <= 255:
                    result.append(0x01)  # çŸ­è·é›¢ãƒ»çŸ­é•·
                    result.append(best_distance)
                    result.append(best_length)
                else:
                    result.append(0x02)  # é•·è·é›¢ãƒ»é•·é•·
                    result.extend(struct.pack('<HH', best_distance, best_length))
                i += best_length
            else:
                byte = data[i]
                if byte == 0xFC:
                    result.append(0xFC)
                    result.append(0x00)
                result.append(byte)
                i += 1
        
        return bytes(result)
    
    def _entropy_optimize(self, data: bytes, analysis: CompressionAnalysis) -> bytes:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€é©åŒ–"""
        if len(data) < 16:
            return data
        
        # é »åº¦è§£æ
        freq = Counter(data)
        
        if len(freq) <= 1:
            return data
        
        # é »åº¦é †ã‚½ãƒ¼ãƒˆ
        sorted_symbols = sorted(freq.items(), key=lambda x: x[1], reverse=True)
        
        # ç°¡æ˜“é©å¿œç¬¦å·åŒ–
        if len(sorted_symbols) <= 16:
            return self._nibble_encode(data, sorted_symbols)
        else:
            return self._huffman_like_encode(data, sorted_symbols)
    
    def _nibble_encode(self, data: bytes, sorted_symbols: List[Tuple[int, int]]) -> bytes:
        """4ãƒ“ãƒƒãƒˆç¬¦å·åŒ–"""
        # æœ€é »å‡º16ã‚·ãƒ³ãƒœãƒ«ã‚’4ãƒ“ãƒƒãƒˆã§ç¬¦å·åŒ–
        encode_table = {}
        decode_table = {}
        
        for i, (symbol, _) in enumerate(sorted_symbols[:16]):
            encode_table[symbol] = i
            decode_table[i] = symbol
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        encoded_bits = []
        escaped_symbols = []
        
        for byte in data:
            if byte in encode_table:
                encoded_bits.append(encode_table[byte])
            else:
                encoded_bits.append(15)  # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
                escaped_symbols.append(byte)
        
        # ãƒ“ãƒƒãƒˆãƒ‘ãƒƒã‚­ãƒ³ã‚°
        packed = bytearray()
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
        packed.append(len(decode_table))
        for i in range(len(decode_table)):
            packed.append(decode_table[i])
        
        # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚·ãƒ³ãƒœãƒ«æ•°
        packed.extend(struct.pack('<I', len(escaped_symbols)))
        packed.extend(escaped_symbols)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
        for i in range(0, len(encoded_bits), 2):
            if i + 1 < len(encoded_bits):
                packed_byte = (encoded_bits[i] << 4) | encoded_bits[i + 1]
            else:
                packed_byte = encoded_bits[i] << 4
            packed.append(packed_byte)
        
        return bytes(packed) if len(packed) < len(data) else data
    
    def _huffman_like_encode(self, data: bytes, sorted_symbols: List[Tuple[int, int]]) -> bytes:
        """Huffmané¢¨ç¬¦å·åŒ–"""
        # ç°¡æ˜“å¯å¤‰é•·ç¬¦å·ä½œæˆ
        code_table = {}
        
        # é »åº¦ã«åŸºã¥ãç¬¦å·é•·æ±ºå®š
        total_freq = sum(freq for _, freq in sorted_symbols)
        
        for i, (symbol, freq) in enumerate(sorted_symbols):
            if i < 2:
                code_length = 2
            elif i < 6:
                code_length = 3
            elif i < 14:
                code_length = 4
            elif i < 30:
                code_length = 5
            else:
                code_length = 8
            
            # ç¬¦å·ç”Ÿæˆï¼ˆç°¡æ˜“ï¼‰
            code = i & ((1 << code_length) - 1)
            code_table[symbol] = (code, code_length)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        bit_stream = []
        
        for byte in data:
            if byte in code_table:
                code, length = code_table[byte]
                for i in range(length):
                    bit_stream.append((code >> (length - 1 - i)) & 1)
            else:
                # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
                for i in range(8):
                    bit_stream.append((byte >> (7 - i)) & 1)
        
        # ãƒ‘ãƒƒã‚­ãƒ³ã‚°
        packed = bytearray()
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
        packed.append(len(code_table))
        for symbol, (code, length) in code_table.items():
            packed.append(symbol)
            packed.append(length)
            packed.append(code)
        
        # ãƒ“ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ 
        packed.extend(struct.pack('<I', len(bit_stream)))
        
        for i in range(0, len(bit_stream), 8):
            byte = 0
            for j in range(8):
                if i + j < len(bit_stream):
                    byte |= bit_stream[i + j] << (7 - j)
            packed.append(byte)
        
        return bytes(packed) if len(packed) < len(data) else data
    
    def _create_archive(self, compressed_data: bytes, result: CompressionResult) -> bytes:
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆ"""
        archive = bytearray()
        
        # ãƒã‚¸ãƒƒã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼
        archive.extend(self.magic)
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        archive.append(1)
        
        # çµæœãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = self._serialize_result(result)
        archive.extend(struct.pack('<I', len(metadata)))
        archive.extend(metadata)
        
        # åœ§ç¸®ãƒ‡ãƒ¼ã‚¿
        archive.extend(struct.pack('<I', len(compressed_data)))
        archive.extend(compressed_data)
        
        return bytes(archive)
    
    def _serialize_result(self, result: CompressionResult) -> bytes:
        """çµæœã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        data = bytearray()
        
        # åŸºæœ¬æƒ…å ±
        data.extend(struct.pack('<I', result.original_size))
        data.extend(struct.pack('<I', result.compressed_size))
        data.extend(struct.pack('<f', result.compression_ratio))
        data.extend(struct.pack('<f', result.processing_time))
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
        checksum_bytes = result.checksum.encode('utf-8')
        data.append(len(checksum_bytes))
        data.extend(checksum_bytes)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸æƒ…å ±
        data.append(len(result.algorithm_stages))
        for stage in result.algorithm_stages:
            stage_bytes = stage.encode('utf-8')
            data.append(len(stage_bytes))
            data.extend(stage_bytes)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³è§£ææƒ…å ±
        analysis = result.pattern_analysis
        data.append(ord(analysis.pattern_type.value[0]))  # æœ€åˆã®æ–‡å­—
        data.extend(struct.pack('<f', analysis.entropy))
        data.extend(struct.pack('<f', analysis.repetition_ratio))
        data.extend(struct.pack('<f', analysis.text_ratio))
        
        return bytes(data)
    
    def _create_empty_archive(self) -> bytes:
        """ç©ºã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä½œæˆ"""
        archive = bytearray()
        archive.extend(self.magic)
        archive.append(1)
        archive.extend(struct.pack('<I', 0))  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
        archive.extend(struct.pack('<I', 0))  # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
        return bytes(archive)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    if len(sys.argv) < 2:
        print("ğŸš€ NEXUS Universal Compression Engine")
        print("æ±ç”¨åœ§ç¸®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  - å®Œå…¨ç‹¬è‡ªå®Ÿè£…")
        print()
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python nexus_universal_compression.py compress <ãƒ•ã‚¡ã‚¤ãƒ«> [æˆ¦ç•¥]")
        print("  python nexus_universal_compression.py analyze <ãƒ•ã‚¡ã‚¤ãƒ«>")
        print("  python nexus_universal_compression.py test")
        print()
        print("æˆ¦ç•¥ã‚ªãƒ—ã‚·ãƒ§ãƒ³:")
        print("  ultra_fast - è¶…é«˜é€Ÿåœ§ç¸®")
        print("  balanced   - ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰")
        print("  maximum    - æœ€å¤§åœ§ç¸®")
        print("  adaptive   - é©å¿œå‹")
        print()
        print("ç‰¹å¾´:")
        print("  ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ - ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§è‡ªå‹•æ¤œå‡º")
        print("  ğŸ”§ é©å¿œå‡¦ç† - æœ€é©ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é¸æŠ")
        print("  ğŸ—œï¸  å¤šæ®µéšåœ§ç¸® - è¤‡æ•°æ‰‹æ³•çµ„ã¿åˆã‚ã›")
        print("  âš—ï¸  ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€é©åŒ– - ç†è«–é™ç•Œè¿½æ±‚")
        return
    
    command = sys.argv[1].lower()
    
    if command == "test":
        print("ğŸ§ª Universal Compression ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        
        # å„ç¨®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        test_cases = [
            ("åå¾©ãƒ‡ãƒ¼ã‚¿", b"ABCD" * 1000),
            ("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿", "Hello World! " * 200),
            ("æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿", bytes(range(256)) * 20),
            ("ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿", os.urandom(2048)),
        ]
        
        for name, test_data in test_cases:
            if isinstance(test_data, str):
                test_data = test_data.encode('utf-8')
            
            print(f"\nğŸ“Š {name}ãƒ†ã‚¹ãƒˆ: {len(test_data)} bytes")
            
            compressor = NexusUniversalCompression(CompressionStrategy.ADAPTIVE)
            compressed = compressor.compress_universal(test_data)
            
            ratio = (1 - len(compressed) / len(test_data)) * 100
            print(f"çµæœ: {len(test_data)} â†’ {len(compressed)} bytes ({ratio:.1f}%)")
    
    elif command == "analyze" and len(sys.argv) >= 3:
        file_path = sys.argv[2]
        
        if not os.path.exists(file_path):
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return
        
        with open(file_path, 'rb') as f:
            data = f.read()
        
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«è§£æ: {file_path}")
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(data)} bytes")
        
        compressor = NexusUniversalCompression()
        analysis = compressor.analyze_data_pattern(data)
        
        print(f"\nğŸ” è§£æçµæœ:")
        print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³: {analysis.pattern_type.value}")
        print(f"  ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {analysis.entropy:.3f}")
        print(f"  åå¾©ç‡: {analysis.repetition_ratio:.1%}")
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆç‡: {analysis.text_ratio:.1%}")
        print(f"  NULLç‡: {analysis.null_ratio:.1%}")
        print(f"  æ¨å¥¨æˆ¦ç•¥: {analysis.recommended_strategy.value}")
        print(f"  äºˆæƒ³åœ§ç¸®ç‡: {analysis.estimated_ratio:.1%}")
    
    elif command == "compress" and len(sys.argv) >= 3:
        file_path = sys.argv[2]
        strategy_name = sys.argv[3] if len(sys.argv) >= 4 else "adaptive"
        
        # æˆ¦ç•¥ãƒ‘ãƒ¼ã‚¹
        strategy_map = {
            "ultra_fast": CompressionStrategy.ULTRA_FAST,
            "balanced": CompressionStrategy.BALANCED,
            "maximum": CompressionStrategy.MAXIMUM,
            "adaptive": CompressionStrategy.ADAPTIVE
        }
        
        strategy = strategy_map.get(strategy_name, CompressionStrategy.ADAPTIVE)
        
        if not os.path.exists(file_path):
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return
        
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®: {file_path}")
        
        with open(file_path, 'rb') as f:
            data = f.read()
        
        compressor = NexusUniversalCompression(strategy)
        compressed = compressor.compress_universal(data)
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
        base_name = os.path.splitext(file_path)[0]
        output_path = f"{base_name}.nuce"
        
        with open(output_path, 'wb') as f:
            f.write(compressed)
        
        ratio = (1 - len(compressed) / len(data)) * 100
        print(f"âœ… åœ§ç¸®å®Œäº†!")
        print(f"ğŸ“ å‡ºåŠ›: {output_path}")
        print(f"ğŸ“Š åœ§ç¸®ç‡: {ratio:.1f}%")
    
    else:
        print("âŒ ç„¡åŠ¹ãªã‚³ãƒãƒ³ãƒ‰ã§ã™ã€‚")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
NEXUS Enhanced Engine v5.0 - å¯é€†æ€§ä¿è¨¼ & é«˜åœ§ç¸®ç‡ç‰ˆ
å¯é€†æ€§å•é¡Œã‚’å®Œå…¨è§£æ±ºã—ã€åœ§ç¸®ç‡ã‚’å¤§å¹…æ”¹å–„
"""

import numpy as np
import lzma
import zlib
import bz2
import struct
import time
import hashlib
from typing import Dict, List, Tuple, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from dataclasses import dataclass
from pathlib import Path
import io


@dataclass
class EnhancedConfig:
    """æ‹¡å¼µè¨­å®š"""
    # åŸºæœ¬è¨­å®š
    max_threads: int = 4
    chunk_size_mb: float = 1.0
    memory_limit_gb: float = 6.0
    
    # å¯é€†æ€§ä¿è¨¼
    ensure_reversibility: bool = True  # å¯é€†æ€§å¼·åˆ¶ä¿è¨¼
    strict_mode: bool = True  # å³æ ¼ãƒ¢ãƒ¼ãƒ‰
    
    # é«˜åœ§ç¸®è¨­å®š
    aggressive_compression: bool = True  # ç©æ¥µçš„åœ§ç¸®
    multi_pass_compression: bool = True  # å¤šæ®µåœ§ç¸®
    adaptive_algorithms: bool = True  # é©å¿œçš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    
    # å“è³ªè¨­å®š
    compression_level: int = 9  # æœ€é«˜åœ§ç¸®
    enable_preprocessing: bool = True
    enable_entropy_coding: bool = True


class IntelligentPatternAnalyzer:
    """çŸ¥çš„ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æå™¨ - å¯é€†æ€§ä¿è¨¼ä»˜ã"""
    
    def __init__(self):
        self.analysis_cache = {}
        self.pattern_database = self._build_pattern_database()
    
    def safe_analyze(self, data: bytes, file_type: str) -> Dict[str, Any]:
        """å®‰å…¨è§£æ - å¯é€†æ€§ä¿è¨¼"""
        return {
            'file_type': file_type,
            'size': len(data),
            'entropy': self._calculate_entropy(data),
            'compression_strategy': self._select_safe_strategy(file_type, len(data)),
            'optimization_potential': self._estimate_safe_potential(data, file_type),
            'pattern_complexity': self._analyze_pattern_complexity(data),
            'redundancy_level': self._detect_redundancy(data)
        }
    
    def _build_pattern_database(self) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"""
        return {
            'text_patterns': {
                'repeated_chars': b'\x00\x20\xFF',
                'line_endings': [b'\r\n', b'\n', b'\r'],
                'common_words': [b'the', b'and', b'for', b'are', b'but', b'not']
            },
            'binary_patterns': {
                'padding_bytes': [b'\x00', b'\xFF', b'\x20'],
                'alignment_patterns': [4, 8, 16, 32, 64, 128, 256]
            },
            'compression_signatures': {
                'gzip': b'\x1f\x8b',
                'zip': b'PK',
                '7z': b'7z\xbc\xaf\x27\x1c',
                'rar': b'Rar!',
                'bzip2': b'BZ'
            }
        }
    
    def _calculate_entropy(self, data: bytes) -> float:
        """æ­£ç¢ºãªã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if len(data) == 0:
            return 0.0
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼‰
        sample_size = min(64 * 1024, len(data))
        if len(data) > sample_size:
            step = len(data) // sample_size
            sample = data[::step][:sample_size]
        else:
            sample = data
        
        # é »åº¦è¨ˆç®—
        byte_counts = np.bincount(np.frombuffer(sample, dtype=np.uint8), minlength=256)
        probabilities = byte_counts / len(sample)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        entropy = 0.0
        for p in probabilities:
            if p > 0:
                entropy -= p * np.log2(p)
        
        return entropy / 8.0  # 0-1æ­£è¦åŒ–
    
    def _select_safe_strategy(self, file_type: str, size: int) -> str:
        """å®‰å…¨ãªåœ§ç¸®æˆ¦ç•¥é¸æŠ - å¯é€†æ€§ä¿è¨¼"""
        if file_type in ['åœ§ç¸®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–']:
            return 'smart_minimal'  # æ—¢åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã‚¹ãƒãƒ¼ãƒˆæœ€å°åœ§ç¸®
        elif file_type in ['ãƒ†ã‚­ã‚¹ãƒˆ']:
            return 'text_advanced'  # ãƒ†ã‚­ã‚¹ãƒˆé«˜åº¦åœ§ç¸®
        elif file_type in ['ç”»åƒ']:
            return 'lossless_image'  # ç„¡æå¤±ç”»åƒåœ§ç¸®
        elif file_type in ['éŸ³æ¥½']:
            return 'audio_lossless'  # ç„¡æå¤±éŸ³æ¥½åœ§ç¸®
        elif file_type in ['å‹•ç”»']:
            return 'video_lossless'  # ç„¡æå¤±å‹•ç”»åœ§ç¸®
        else:
            return 'universal_safe'  # æ±ç”¨å®‰å…¨åœ§ç¸®
    
    def _estimate_safe_potential(self, data: bytes, file_type: str) -> float:
        """å®‰å…¨ãªæœ€é©åŒ–ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«æ¨å®š"""
        entropy = self._calculate_entropy(data)
        redundancy = self._detect_redundancy(data)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥èª¿æ•´
        type_multipliers = {
            'ãƒ†ã‚­ã‚¹ãƒˆ': 1.2,
            'ç”»åƒ': 0.7,  # æ—¢åœ§ç¸®ã®å¯èƒ½æ€§
            'éŸ³æ¥½': 0.5,  # é€šå¸¸æ—¢åœ§ç¸®
            'å‹•ç”»': 0.4,  # é€šå¸¸æ—¢åœ§ç¸®
            'åœ§ç¸®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–': 0.1,  # æ—¢åœ§ç¸®
            'ãã®ä»–': 0.8
        }
        
        base_potential = (1.0 - entropy) * redundancy
        multiplier = type_multipliers.get(file_type, 0.8)
        
        return min(0.95, base_potential * multiplier)
    
    def _analyze_pattern_complexity(self, data: bytes) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³è¤‡é›‘åº¦åˆ†æ"""
        if len(data) < 1024:
            return 0.5
        
        sample = data[:4096]  # å…ˆé ­4KBåˆ†æ
        
        # ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        pattern_scores = []
        for pattern_len in [2, 4, 8, 16, 32]:
            if len(sample) >= pattern_len * 10:
                pattern_count = 0
                for i in range(0, len(sample) - pattern_len, pattern_len):
                    pattern = sample[i:i + pattern_len]
                    if sample[i + pattern_len:i + pattern_len * 2] == pattern:
                        pattern_count += 1
                
                pattern_ratio = pattern_count / (len(sample) // pattern_len)
                pattern_scores.append(pattern_ratio)
        
        return 1.0 - (sum(pattern_scores) / len(pattern_scores) if pattern_scores else 0.5)
    
    def _detect_redundancy(self, data: bytes) -> float:
        """å†—é•·æ€§æ¤œå‡º"""
        if len(data) < 256:
            return 0.5
        
        sample = data[:8192]  # å…ˆé ­8KBåˆ†æ
        
        # ãƒã‚¤ãƒˆé »åº¦åˆ†æ
        byte_counts = np.bincount(np.frombuffer(sample, dtype=np.uint8), minlength=256)
        max_frequency = np.max(byte_counts)
        redundancy_ratio = max_frequency / len(sample)
        
        # é€£ç¶šåŒä¸€ãƒã‚¤ãƒˆæ¤œå‡º
        consecutive_count = 0
        max_consecutive = 0
        prev_byte = None
        
        for byte_val in sample:
            if byte_val == prev_byte:
                consecutive_count += 1
                max_consecutive = max(max_consecutive, consecutive_count)
            else:
                consecutive_count = 1
            prev_byte = byte_val
        
        consecutive_ratio = max_consecutive / len(sample)
        
        return min(1.0, redundancy_ratio + consecutive_ratio)


class AdvancedCompressionEngine:
    """é«˜åº¦åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³ - å¯é€†æ€§ä¿è¨¼ä»˜ã"""
    
    def __init__(self, config: EnhancedConfig):
        self.config = config
        self.analyzer = IntelligentPatternAnalyzer()
    
    def safe_compress_chunk(self, chunk_data: bytes, strategy: str, chunk_id: int) -> bytes:
        """å®‰å…¨ãƒãƒ£ãƒ³ã‚¯åœ§ç¸® - å¯é€†æ€§ä¿è¨¼"""
        try:
            # æˆ¦ç•¥åˆ¥åœ§ç¸®
            if strategy == 'smart_minimal':
                return self._smart_minimal_compression(chunk_data)
            elif strategy == 'text_advanced':
                return self._text_advanced_compression(chunk_data)
            elif strategy == 'lossless_image':
                return self._lossless_image_compression(chunk_data)
            elif strategy == 'audio_lossless':
                return self._audio_lossless_compression(chunk_data)
            elif strategy == 'video_lossless':
                return self._video_lossless_compression(chunk_data)
            else:
                return self._universal_safe_compression(chunk_data)
                
        except Exception as e:
            # å®Œå…¨å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._guaranteed_safe_compression(chunk_data)
    
    def _smart_minimal_compression(self, data: bytes) -> bytes:
        """ã‚¹ãƒãƒ¼ãƒˆæœ€å°åœ§ç¸® - æ—¢åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ç”¨"""
        # è¤‡æ•°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æœ€å°ã‚µã‚¤ã‚ºã‚’é¸æŠ
        candidates = []
        
        # LZMA preset 0 (æœ€é«˜é€Ÿ)
        try:
            lzma_result = lzma.compress(data, preset=0)
            candidates.append(('LZMA0', lzma_result))
        except:
            pass
        
        # ZLIB ãƒ¬ãƒ™ãƒ«1
        try:
            zlib_result = zlib.compress(data, level=1)
            candidates.append(('ZLIB1', zlib_result))
        except:
            pass
        
        # æœ€å°ã‚µã‚¤ã‚ºã‚’é¸æŠ
        if candidates:
            best_method, best_result = min(candidates, key=lambda x: len(x[1]))
            return best_method.encode('ascii').ljust(8, b'\x00') + best_result
        else:
            return b'RAW\x00\x00\x00\x00\x00' + data
    
    def _text_advanced_compression(self, data: bytes) -> bytes:
        """ãƒ†ã‚­ã‚¹ãƒˆé«˜åº¦åœ§ç¸®"""
        try:
            # å‰å‡¦ç†ï¼ˆå¯é€†ï¼‰
            processed_data = data
            processing_flags = 0
            
            # æ”¹è¡Œçµ±ä¸€ï¼ˆè¨˜éŒ²ï¼‰
            if b'\r\n' in data:
                processed_data = processed_data.replace(b'\r\n', b'\n')
                processing_flags |= 0x01
            
            # æœ«å°¾ç©ºç™½é™¤å»ï¼ˆè¨˜éŒ²ï¼‰
            if data.endswith(b' ') or data.endswith(b'\t'):
                original_end = len(data)
                processed_data = processed_data.rstrip(b' \t')
                trailing_spaces = original_end - len(processed_data)
                processing_flags |= 0x02
            else:
                trailing_spaces = 0
            
            # æœ€é«˜åœ§ç¸®LZMA
            compressed = lzma.compress(processed_data, preset=9)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
            header = struct.pack('<BH', processing_flags, trailing_spaces)
            
            return b'TXTADV\x00\x00' + header + compressed
            
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return b'TXTFAIL\x00' + lzma.compress(data, preset=6)
    
    def _lossless_image_compression(self, data: bytes) -> bytes:
        """ç„¡æå¤±ç”»åƒåœ§ç¸® - ãƒ‡ãƒ¼ã‚¿æ”¹å¤‰ãªã—"""
        try:
            # æœ€é«˜åœ§ç¸®è¨­å®šã‚’è©¦è¡Œ
            candidates = []
            
            # LZMA preset 9
            try:
                lzma_result = lzma.compress(data, preset=9)
                candidates.append(('LZMA9', lzma_result))
            except:
                pass
            
            # BZIP2 æœ€é«˜åœ§ç¸®
            try:
                bz2_result = bz2.compress(data, compresslevel=9)
                candidates.append(('BZIP29', bz2_result))
            except:
                pass
            
            # ZLIB æœ€é«˜åœ§ç¸®
            try:
                zlib_result = zlib.compress(data, level=9)
                candidates.append(('ZLIB9', zlib_result))
            except:
                pass
            
            # æœ€å°ã‚µã‚¤ã‚ºã‚’é¸æŠ
            if candidates:
                best_method, best_result = min(candidates, key=lambda x: len(x[1]))
                return best_method.encode('ascii').ljust(8, b'\x00') + best_result
            else:
                return b'IMGRAW\x00\x00' + data
                
        except:
            return b'IMGFAIL\x00' + data
    
    def _audio_lossless_compression(self, data: bytes) -> bytes:
        """ç„¡æå¤±éŸ³æ¥½åœ§ç¸®"""
        try:
            # éŸ³æ¥½ãƒ•ã‚¡ã‚¤ãƒ«å‘ã‘æœ€é©åŒ–ï¼ˆãƒ‡ãƒ¼ã‚¿æ”¹å¤‰ãªã—ï¼‰
            # ID3ã‚¿ã‚°ä½ç½®ã®è¨˜éŒ²ï¼ˆé™¤å»ã§ã¯ãªãåœ§ç¸®æœ€é©åŒ–ï¼‰
            
            # WAVãƒ˜ãƒƒãƒ€ãƒ¼æ¤œå‡º
            if data[:4] == b'RIFF' and data[8:12] == b'WAVE':
                return self._compress_wav_lossless(data)
            # MP3æ¤œå‡º
            elif data[:3] == b'ID3' or (data[0] == 0xFF and (data[1] & 0xE0) == 0xE0):
                return self._compress_mp3_lossless(data)
            else:
                # ä¸€èˆ¬éŸ³æ¥½ãƒ•ã‚¡ã‚¤ãƒ«
                return b'AUDGEN\x00\x00' + lzma.compress(data, preset=9)
                
        except:
            return b'AUDFAIL\x00' + lzma.compress(data, preset=6)
    
    def _compress_wav_lossless(self, data: bytes) -> bytes:
        """WAVç„¡æå¤±åœ§ç¸®"""
        try:
            # WAVæ§‹é€ è§£æï¼ˆæ”¹å¤‰ãªã—ï¼‰
            # PCMãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã®é«˜åŠ¹ç‡åœ§ç¸®
            
            # å˜ç´”ã ãŒåŠ¹æœçš„ï¼šLZMAæœ€é«˜åœ§ç¸®
            compressed = lzma.compress(data, preset=9)
            return b'WAVLZMA9' + compressed
            
        except:
            return b'WAVFAIL\x00' + data
    
    def _compress_mp3_lossless(self, data: bytes) -> bytes:
        """MP3ç„¡æå¤±åœ§ç¸®"""
        try:
            # MP3ã¯æ—¢ã«åœ§ç¸®æ¸ˆã¿ãªã®ã§è»½å¾®ãªå‡¦ç†
            # ãƒ•ãƒ¬ãƒ¼ãƒ å¢ƒç•Œã‚’è€ƒæ…®ã—ãŸåˆ†å‰²åœ§ç¸®
            
            chunk_size = 32768  # 32KBå˜ä½
            compressed_chunks = []
            
            for i in range(0, len(data), chunk_size):
                chunk = data[i:i + chunk_size]
                # å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã¯ZLIBã®æ–¹ãŒåŠ¹ç‡çš„
                if len(chunk) < 1024:
                    compressed_chunk = zlib.compress(chunk, level=9)
                else:
                    compressed_chunk = lzma.compress(chunk, preset=3)
                compressed_chunks.append(compressed_chunk)
            
            # ãƒãƒ£ãƒ³ã‚¯æ•°è¨˜éŒ²
            result = b'MP3CHUNK' + struct.pack('<I', len(compressed_chunks))
            for chunk in compressed_chunks:
                result += struct.pack('<I', len(chunk)) + chunk
            
            return result
            
        except:
            return b'MP3FAIL\x00' + data
    
    def _video_lossless_compression(self, data: bytes) -> bytes:
        """ç„¡æå¤±å‹•ç”»åœ§ç¸®"""
        try:
            # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¯é€šå¸¸æ—¢åœ§ç¸®ãªã®ã§æœ€å°å‡¦ç†
            return b'VIDMIN\x00\x00' + lzma.compress(data, preset=1)
        except:
            return b'VIDFAIL\x00' + data
    
    def _universal_safe_compression(self, data: bytes) -> bytes:
        """æ±ç”¨å®‰å…¨åœ§ç¸®"""
        try:
            # å¤šæ®µåœ§ç¸®ãƒ†ã‚¹ãƒˆ
            candidates = []
            
            # LZMAå„ãƒ—ãƒªã‚»ãƒƒãƒˆ
            for preset in [9, 6, 3]:
                try:
                    result = lzma.compress(data, preset=preset)
                    candidates.append((f'LZMA{preset}', result))
                except:
                    continue
            
            # BZIP2
            try:
                result = bz2.compress(data, compresslevel=9)
                candidates.append(('BZIP2', result))
            except:
                pass
            
            # ZLIB
            try:
                result = zlib.compress(data, level=9)
                candidates.append(('ZLIB', result))
            except:
                pass
            
            # æœ€é©é¸æŠ
            if candidates:
                best_method, best_result = min(candidates, key=lambda x: len(x[1]))
                return best_method.encode('ascii').ljust(8, b'\x00') + best_result
            else:
                return b'UNIFAIL\x00' + data
                
        except:
            return b'SAFEFAIL' + data
    
    def _guaranteed_safe_compression(self, data: bytes) -> bytes:
        """å®Œå…¨å®‰å…¨åœ§ç¸® - æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        try:
            # æœ€ã‚‚å®‰å…¨ãªZLIB
            return b'SAFE\x00\x00\x00\x00' + zlib.compress(data, level=1)
        except:
            # æœ€çµ‚æ‰‹æ®µï¼šç„¡åœ§ç¸®
            return b'NONE\x00\x00\x00\x00' + data


class SafeThreadPoolManager:
    """å®‰å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ç®¡ç†å™¨"""
    
    def __init__(self, max_threads: int):
        self.max_threads = max_threads
        self.executor = None
        self.active_futures = []
        self.shutdown_timeout = 5.0
    
    def __enter__(self):
        self.executor = ThreadPoolExecutor(max_workers=self.max_threads)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.executor:
            # å®‰å…¨ãªã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
            for future in self.active_futures:
                try:
                    future.result(timeout=0.1)
                except:
                    future.cancel()
            
            # Python 3.8ä»¥ä¸‹å¯¾å¿œ
            try:
                self.executor.shutdown(wait=True, timeout=self.shutdown_timeout)
            except TypeError:
                # timeout ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç„¡ã„å ´åˆã®å¯¾å¿œ
                self.executor.shutdown(wait=True)
            self.executor = None
        self.active_futures.clear()
    
    def submit_task(self, func, *args, **kwargs):
        """å®‰å…¨ã‚¿ã‚¹ã‚¯æŠ•å…¥"""
        if self.executor:
            future = self.executor.submit(func, *args, **kwargs)
            self.active_futures.append(future)
            return future
        return None


class NEXUSEnhancedEngine:
    """NEXUSæ‹¡å¼µã‚¨ãƒ³ã‚¸ãƒ³ v5.0 - å¯é€†æ€§ä¿è¨¼ & é«˜åœ§ç¸®ç‡"""
    
    def __init__(self, config: EnhancedConfig = None):
        self.config = config or EnhancedConfig()
        self.analyzer = IntelligentPatternAnalyzer()
        self.compressor = AdvancedCompressionEngine(self.config)
        
        # çµ±è¨ˆ
        self.stats = {
            'total_files_processed': 0,
            'total_data_processed': 0,
            'total_compression_time': 0.0,
            'average_compression_ratio': 0.0,
            'average_throughput': 0.0,
            'reversibility_success_rate': 0.0
        }
        
        print(f"ğŸš€ NEXUSæ‹¡å¼µã‚¨ãƒ³ã‚¸ãƒ³ v5.0 åˆæœŸåŒ–")
        print(f"   ğŸ”’ å¯é€†æ€§ä¿è¨¼: {'æœ‰åŠ¹' if self.config.ensure_reversibility else 'ç„¡åŠ¹'}")
        print(f"   âš¡ ç©æ¥µçš„åœ§ç¸®: {'æœ‰åŠ¹' if self.config.aggressive_compression else 'ç„¡åŠ¹'}")
        print(f"   ğŸ§µ ã‚¹ãƒ¬ãƒƒãƒ‰æ•°: {self.config.max_threads}")
        print(f"   ğŸ’¾ ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {self.config.chunk_size_mb}MB")
        print(f"   ğŸ¯ åœ§ç¸®ãƒ¬ãƒ™ãƒ«: {self.config.compression_level}")
    
    def enhanced_compress(self, data: bytes, file_type: str, quality: str = 'maximum') -> bytes:
        """æ‹¡å¼µåœ§ç¸® - å¯é€†æ€§ä¿è¨¼ä»˜ã"""
        start_time = time.perf_counter()
        
        print(f"ğŸ”¥ NEXUSæ‹¡å¼µåœ§ç¸®é–‹å§‹")
        print(f"   ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {file_type}")
        print(f"   ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(data):,} bytes ({len(data)/1024/1024:.1f}MB)")
        print(f"   ğŸ¯ å“è³ª: {quality}")
        print(f"   ğŸ”’ å¯é€†æ€§ä¿è¨¼: {'æœ‰åŠ¹' if self.config.ensure_reversibility else 'ç„¡åŠ¹'}")
        
        # çŸ¥çš„è§£æ
        print(f"   ğŸ§  çŸ¥çš„ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ...")
        analysis = self.analyzer.safe_analyze(data, file_type)
        strategy = analysis['compression_strategy']
        potential = analysis['optimization_potential']
        print(f"      æ¨å¥¨æˆ¦ç•¥: {strategy}")
        print(f"      æœ€é©åŒ–ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«: {potential:.3f}")
        print(f"      ãƒ‘ã‚¿ãƒ¼ãƒ³è¤‡é›‘åº¦: {analysis['pattern_complexity']:.3f}")
        print(f"      å†—é•·æ€§ãƒ¬ãƒ™ãƒ«: {analysis['redundancy_level']:.3f}")
        
        # é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        chunk_size = self._calculate_optimal_chunk_size(len(data), analysis)
        chunks = self._split_to_chunks(data, chunk_size)
        print(f"   ğŸ”· é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²: {len(chunks)} ãƒãƒ£ãƒ³ã‚¯ (å¹³å‡{chunk_size/1024:.0f}KB)")
        
        # ä¸¦åˆ—åœ§ç¸®
        compressed_chunks = []
        
        if len(chunks) > 1 and self.config.max_threads > 1:
            print(f"   âš¡ ä¸¦åˆ—é«˜åº¦åœ§ç¸®å®Ÿè¡Œ...")
            compressed_chunks = self._parallel_safe_compress(chunks, strategy)
        else:
            print(f"   ğŸ”§ ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«é«˜åº¦åœ§ç¸®å®Ÿè¡Œ...")
            for i, chunk in enumerate(chunks):
                compressed_chunk = self.compressor.safe_compress_chunk(chunk, strategy, i)
                compressed_chunks.append(compressed_chunk)
        
        # çµæœçµ±åˆ
        result = self._create_enhanced_format(compressed_chunks, len(data), file_type, analysis)
        
        # å¯é€†æ€§æ¤œè¨¼ï¼ˆå³æ ¼ãƒ¢ãƒ¼ãƒ‰ï¼‰
        if self.config.ensure_reversibility:
            print(f"   ğŸ” å¯é€†æ€§æ¤œè¨¼...")
            try:
                decompressed = simulate_enhanced_decompression(result)
                if len(decompressed) != len(data) or decompressed != data:
                    print(f"      âŒ å¯é€†æ€§æ¤œè¨¼å¤±æ•— - å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ")
                    result = self._create_safe_fallback(data, file_type)
                else:
                    print(f"      âœ… å¯é€†æ€§æ¤œè¨¼æˆåŠŸ")
            except Exception as e:
                print(f"      âš ï¸ å¯é€†æ€§æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ - å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ: {e}")
                result = self._create_safe_fallback(data, file_type)
        
        # çµ±è¨ˆæ›´æ–°
        total_time = time.perf_counter() - start_time
        compression_ratio = (1 - len(result) / len(data)) * 100
        throughput = len(data) / 1024 / 1024 / total_time
        
        self._update_stats(len(data), total_time, compression_ratio, throughput)
        
        print(f"âœ… æ‹¡å¼µåœ§ç¸®å®Œäº†!")
        print(f"   ğŸ“ˆ åœ§ç¸®ç‡: {compression_ratio:.2f}%")
        print(f"   âš¡ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.2f}MB/s")
        print(f"   â±ï¸ å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
        print(f"   ğŸ”’ å¯é€†æ€§: ä¿è¨¼æ¸ˆã¿")
        
        return result
    
    def _calculate_optimal_chunk_size(self, data_size: int, analysis: Dict[str, Any]) -> int:
        """æœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨ˆç®—"""
        base_chunk_size = int(self.config.chunk_size_mb * 1024 * 1024)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³è¤‡é›‘åº¦ã«åŸºã¥ãèª¿æ•´
        complexity = analysis['pattern_complexity']
        if complexity > 0.8:  # é«˜è¤‡é›‘åº¦
            chunk_size = base_chunk_size // 2  # å°ã•ãªãƒãƒ£ãƒ³ã‚¯
        elif complexity < 0.3:  # ä½è¤‡é›‘åº¦
            chunk_size = base_chunk_size * 2  # å¤§ããªãƒãƒ£ãƒ³ã‚¯
        else:
            chunk_size = base_chunk_size
        
        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«åŸºã¥ãèª¿æ•´
        if data_size < chunk_size:
            chunk_size = data_size
        elif data_size > chunk_size * 20:  # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«
            chunk_size = min(chunk_size * 2, data_size // 10)
        
        return max(64 * 1024, chunk_size)  # æœ€å°64KB
    
    def _split_to_chunks(self, data: bytes, chunk_size: int) -> List[bytes]:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        if len(data) <= chunk_size:
            return [data]
        
        chunks = []
        for i in range(0, len(data), chunk_size):
            chunks.append(data[i:i + chunk_size])
        
        return chunks
    
    def _parallel_safe_compress(self, chunks: List[bytes], strategy: str) -> List[bytes]:
        """ä¸¦åˆ—å®‰å…¨åœ§ç¸®"""
        compressed_chunks = [None] * len(chunks)
        
        with SafeThreadPoolManager(self.config.max_threads) as pool:
            # ã‚¿ã‚¹ã‚¯æŠ•å…¥
            future_to_index = {}
            for i, chunk in enumerate(chunks):
                future = pool.submit_task(self.compressor.safe_compress_chunk, chunk, strategy, i)
                if future:
                    future_to_index[future] = i
            
            # çµæœå›å
            for future in as_completed(future_to_index.keys(), timeout=120):
                try:
                    index = future_to_index[future]
                    compressed_chunks[index] = future.result()
                except Exception as e:
                    print(f"      âš ï¸ ãƒãƒ£ãƒ³ã‚¯{future_to_index.get(future, '?')}åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
                    # å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    index = future_to_index[future]
                    compressed_chunks[index] = b'FAIL\x00\x00\x00\x00' + chunks[index]
        
        # Noneè¦ç´ ã®å‡¦ç†
        for i, chunk in enumerate(compressed_chunks):
            if chunk is None:
                compressed_chunks[i] = b'NONE\x00\x00\x00\x00' + chunks[i]
        
        return compressed_chunks
    
    def _create_enhanced_format(self, compressed_chunks: List[bytes], original_size: int, 
                               file_type: str, analysis: Dict[str, Any]) -> bytes:
        """æ‹¡å¼µãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä½œæˆ"""
        # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆï¼ˆ256ãƒã‚¤ãƒˆï¼‰
        header = bytearray(256)
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
        header[0:8] = b'NXENH500'
        
        # åŸºæœ¬æƒ…å ±
        struct.pack_into('<Q', header, 8, original_size)  # å…ƒã‚µã‚¤ã‚º
        struct.pack_into('<I', header, 16, len(compressed_chunks))  # ãƒãƒ£ãƒ³ã‚¯æ•°
        struct.pack_into('<I', header, 20, int(time.time()))  # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—
        type_bytes = file_type.encode('utf-8')[:16]
        header[24:24+len(type_bytes)] = type_bytes
        
        # è§£ææƒ…å ±
        struct.pack_into('<f', header, 40, analysis['entropy'])
        struct.pack_into('<f', header, 44, analysis['optimization_potential'])
        struct.pack_into('<f', header, 48, analysis['pattern_complexity'])
        struct.pack_into('<f', header, 52, analysis['redundancy_level'])
        
        # æˆ¦ç•¥æƒ…å ±
        strategy_bytes = analysis['compression_strategy'].encode('utf-8')[:16]
        header[56:56+len(strategy_bytes)] = strategy_bytes
        
        # è¨­å®šæƒ…å ±
        header[72] = self.config.compression_level
        header[73] = 1 if self.config.ensure_reversibility else 0
        header[74] = 1 if self.config.aggressive_compression else 0
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
        header_checksum = zlib.crc32(header[8:128])
        struct.pack_into('<I', header, 128, header_checksum)
        
        # ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†
        result = bytes(header)
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        for i, chunk in enumerate(compressed_chunks):
            # ãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ (32ãƒã‚¤ãƒˆ)
            chunk_header = struct.pack('<IIII', i, len(chunk), zlib.crc32(chunk), 0)
            chunk_header += b'\x00' * 16  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            
            result += chunk_header + chunk
        
        return result
    
    def _create_safe_fallback(self, data: bytes, file_type: str) -> bytes:
        """å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½œæˆ"""
        try:
            # æœ€ã‚‚å®‰å…¨ãªZLIBåœ§ç¸®
            compressed = zlib.compress(data, level=6)
            
            # ç°¡å˜ãªãƒ˜ãƒƒãƒ€ãƒ¼
            header = b'NXSAFE50' + struct.pack('<QI', len(data), int(time.time()))
            header += file_type.encode('utf-8')[:16].ljust(16, b'\x00')
            header += b'\x00' * (128 - len(header))
            
            return header + compressed
        except:
            # æœ€çµ‚æ‰‹æ®µï¼šç„¡åœ§ç¸®
            header = b'NXRAW500' + struct.pack('<QI', len(data), int(time.time()))
            header += file_type.encode('utf-8')[:16].ljust(16, b'\x00')
            header += b'\x00' * (128 - len(header))
            
            return header + data
    
    def _update_stats(self, data_size: int, time_taken: float, compression_ratio: float, throughput: float):
        """çµ±è¨ˆæ›´æ–°"""
        self.stats['total_files_processed'] += 1
        self.stats['total_data_processed'] += data_size
        self.stats['total_compression_time'] += time_taken
        
        # å¹³å‡å€¤æ›´æ–°
        files_count = self.stats['total_files_processed']
        self.stats['average_compression_ratio'] = (
            (self.stats['average_compression_ratio'] * (files_count - 1) + compression_ratio) / files_count
        )
        self.stats['average_throughput'] = (
            (self.stats['average_throughput'] * (files_count - 1) + throughput) / files_count
        )
    
    def get_enhanced_report(self) -> Dict[str, Any]:
        """æ‹¡å¼µãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        return {
            'engine_version': 'NEXUS Enhanced v5.0',
            'configuration': {
                'max_threads': self.config.max_threads,
                'chunk_size_mb': self.config.chunk_size_mb,
                'ensure_reversibility': self.config.ensure_reversibility,
                'aggressive_compression': self.config.aggressive_compression,
                'compression_level': self.config.compression_level
            },
            'performance_stats': self.stats.copy(),
            'features': {
                'reversibility_guarantee': True,
                'intelligent_pattern_analysis': True,
                'adaptive_chunking': True,
                'multi_algorithm_compression': True,
                'safe_fallback': True
            }
        }


def simulate_enhanced_decompression(compressed_data: bytes) -> bytes:
    """æ‹¡å¼µè§£å‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    try:
        if len(compressed_data) < 256:
            return compressed_data
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        header = compressed_data[:256]
        
        if header[:8] == b'NXENH500':
            return decompress_enhanced_format(compressed_data)
        elif header[:8] == b'NXSAFE50':
            return decompress_safe_format(compressed_data)
        elif header[:8] == b'NXRAW500':
            return decompress_raw_format(compressed_data)
        else:
            return compressed_data
            
    except Exception as e:
        return compressed_data


def decompress_enhanced_format(compressed_data: bytes) -> bytes:
    """æ‹¡å¼µãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡"""
    header = compressed_data[:256]
    original_size = struct.unpack('<Q', header[8:16])[0]
    chunk_count = struct.unpack('<I', header[16:20])[0]
    
    # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿è§£å‡
    decompressed_chunks = []
    current_pos = 256
    
    for _ in range(chunk_count):
        if current_pos + 32 > len(compressed_data):
            break
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼
        chunk_header = compressed_data[current_pos:current_pos + 32]
        chunk_id, chunk_size, chunk_crc = struct.unpack('<III', chunk_header[:12])
        current_pos += 32
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        if current_pos + chunk_size > len(compressed_data):
            chunk_size = len(compressed_data) - current_pos
        
        chunk_data = compressed_data[current_pos:current_pos + chunk_size]
        current_pos += chunk_size
        
        # è§£å‡
        decompressed_chunk = decompress_enhanced_chunk(chunk_data)
        decompressed_chunks.append((chunk_id, decompressed_chunk))
    
    # çµåˆ
    decompressed_chunks.sort(key=lambda x: x[0])
    result = b''.join(chunk[1] for chunk in decompressed_chunks)
    
    return result


def decompress_enhanced_chunk(chunk_data: bytes) -> bytes:
    """æ‹¡å¼µãƒãƒ£ãƒ³ã‚¯è§£å‡"""
    if len(chunk_data) < 8:
        return chunk_data
    
    method_prefix = chunk_data[:8].rstrip(b'\x00')
    
    try:
        if method_prefix in [b'LZMA0', b'LZMA3', b'LZMA6', b'LZMA9']:
            return lzma.decompress(chunk_data[8:])
        elif method_prefix in [b'ZLIB1', b'ZLIB', b'ZLIB9']:
            return zlib.decompress(chunk_data[8:])
        elif method_prefix in [b'BZIP2', b'BZIP29']:
            return bz2.decompress(chunk_data[8:])
        elif method_prefix == b'TXTADV':
            return decompress_text_advanced(chunk_data[8:])
        elif method_prefix in [b'IMGRAW', b'AUDGEN', b'VIDMIN']:
            return chunk_data[8:]
        elif method_prefix in [b'WAVLZMA9']:
            return lzma.decompress(chunk_data[8:])
        elif method_prefix == b'MP3CHUNK':
            return decompress_mp3_chunk(chunk_data[8:])
        elif method_prefix in [b'RAW', b'NONE', b'FAIL']:
            return chunk_data[8:]
        elif method_prefix == b'SAFE':
            return zlib.decompress(chunk_data[8:])
        else:
            # æ¨™æº–LZMAè©¦è¡Œ
            return lzma.decompress(chunk_data)
    except:
        try:
            return zlib.decompress(chunk_data[8:])
        except:
            return chunk_data[8:]


def decompress_text_advanced(data: bytes) -> bytes:
    """ãƒ†ã‚­ã‚¹ãƒˆé«˜åº¦è§£å‡"""
    if len(data) < 3:
        return data
    
    processing_flags, trailing_spaces = struct.unpack('<BH', data[:3])
    compressed_data = data[3:]
    
    # LZMAè§£å‡
    decompressed = lzma.decompress(compressed_data)
    
    # å¾Œå‡¦ç†ï¼ˆé€†é †ï¼‰
    if processing_flags & 0x02:  # æœ«å°¾ç©ºç™½å¾©å…ƒ
        decompressed += b' ' * trailing_spaces
    
    if processing_flags & 0x01:  # æ”¹è¡Œå¾©å…ƒ
        decompressed = decompressed.replace(b'\n', b'\r\n')
    
    return decompressed


def decompress_mp3_chunk(data: bytes) -> bytes:
    """MP3ãƒãƒ£ãƒ³ã‚¯è§£å‡"""
    if len(data) < 4:
        return data
    
    chunk_count = struct.unpack('<I', data[:4])[0]
    pos = 4
    
    chunks = []
    for _ in range(chunk_count):
        if pos + 4 > len(data):
            break
        
        chunk_size = struct.unpack('<I', data[pos:pos+4])[0]
        pos += 4
        
        if pos + chunk_size > len(data):
            chunk_size = len(data) - pos
        
        chunk_data = data[pos:pos+chunk_size]
        pos += chunk_size
        
        try:
            decompressed_chunk = lzma.decompress(chunk_data)
        except:
            try:
                decompressed_chunk = zlib.decompress(chunk_data)
            except:
                decompressed_chunk = chunk_data
        
        chunks.append(decompressed_chunk)
    
    return b''.join(chunks)


def decompress_safe_format(compressed_data: bytes) -> bytes:
    """å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡"""
    header = compressed_data[:128]
    original_size = struct.unpack('<Q', header[8:16])[0]
    compressed = compressed_data[128:]
    
    return zlib.decompress(compressed)


def decompress_raw_format(compressed_data: bytes) -> bytes:
    """ç”Ÿãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡"""
    return compressed_data[128:]  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤å»


if __name__ == "__main__":
    # é«˜æ€§èƒ½è¨­å®š
    config = EnhancedConfig(
        max_threads=4,
        chunk_size_mb=1.0,
        ensure_reversibility=True,
        aggressive_compression=True,
        compression_level=9
    )
    
    engine = NEXUSEnhancedEngine(config)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_data = b"This is a test data for NEXUS Enhanced Engine v5.0 with reversibility guarantee" * 100
    
    # åœ§ç¸®ãƒ†ã‚¹ãƒˆ
    compressed = engine.enhanced_compress(test_data, 'ãƒ†ã‚­ã‚¹ãƒˆ', 'maximum')
    
    # è§£å‡ãƒ†ã‚¹ãƒˆ
    decompressed = simulate_enhanced_decompression(compressed)
    
    print(f"\nğŸ§ª ç°¡æ˜“ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"   å…ƒãƒ‡ãƒ¼ã‚¿: {len(test_data):,} bytes")
    print(f"   åœ§ç¸®å¾Œ: {len(compressed):,} bytes")
    print(f"   åœ§ç¸®ç‡: {(1-len(compressed)/len(test_data))*100:.2f}%")
    print(f"   å¯é€†æ€§: {'âœ…' if test_data == decompressed else 'âŒ'}")
    print(f"   ã‚µã‚¤ã‚ºä¸€è‡´: {'âœ…' if len(test_data) == len(decompressed) else 'âŒ'}")
    
    if test_data == decompressed:
        print(f"   ğŸ† å®Œå…¨å¯é€†æ€§ç¢ºèª!")
    else:
        print(f"   âŒ å¯é€†æ€§å•é¡Œç™ºç”Ÿ")

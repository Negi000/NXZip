#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NEXUS + ZSTANDARD ULTIMATE COMPRESSION ENGINE
Perfect reversibility + Maximum compression ratio
"""

import struct
import math
import time
import random
import os
import collections
import json
from typing import List, Tuple, Dict, Any

try:
    import zstandard as zstd
    ZSTD_AVAILABLE = True
except ImportError:
    print("âš ï¸ Zstandard not found, installing...")
    import subprocess
    subprocess.check_call(['pip', 'install', 'zstandard'])
    import zstandard as zstd
    ZSTD_AVAILABLE = True

# Polyomino shapes optimized for NEXUS theory
POLYOMINO_SHAPES = {
    "I-1": [(0, 0)],
    "I-2": [(0, 0), (0, 1)],
    "I-3": [(0, 0), (0, 1), (0, 2)],
    "I-4": [(0, 0), (0, 1), (0, 2), (0, 3)],
    "I-5": [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4)],
    "O-4": [(0, 0), (0, 1), (1, 0), (1, 1)],
    "T-4": [(0, 0), (0, 1), (0, 2), (1, 1)],
    "L-4": [(0, 0), (0, 1), (0, 2), (1, 0)],
    "R-6": [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)],
}


class NexusZstandardEngine:
    """NEXUS + Zstandard Ultimate Compression Engine"""
    
    def __init__(self, compression_level: int = 3):
        self.compression_level = compression_level
        self.compressor = zstd.ZstdCompressor(level=compression_level)
        self.decompressor = zstd.ZstdDecompressor()
    
    def _select_optimal_shape(self, data: bytes) -> str:
        """Advanced shape selection based on data patterns"""
        if len(data) <= 512:
            return "I-1"  # Smallest files
        elif len(data) <= 2048:
            return "I-2"
        elif len(data) <= 8192:
            return "I-3"
        elif len(data) <= 32768:
            return "I-4"
        else:
            # Analyze data entropy for larger files
            sample_size = min(len(data), 4096)
            sample_data = data[:sample_size]
            
            # Calculate entropy
            counts = collections.Counter(sample_data)
            entropy = 0
            total = len(sample_data)
            for count in counts.values():
                p_x = count / total
                entropy -= p_x * math.log2(p_x)
            
            # Entropy-based selection
            if entropy < 2.0:  # Low entropy (repetitive)
                return "R-6"  # Rectangular blocks for patterns
            elif entropy < 4.0:  # Medium entropy
                return "O-4"  # Square blocks
            elif entropy < 6.0:  # Higher entropy
                return "I-5"  # Long lines
            else:  # High entropy (random)
                return "I-3"  # Balanced approach
    
    def _get_perfect_blocks(self, data: bytes, grid_width: int, shape_coords: Tuple[Tuple[int, int], ...]) -> Tuple[List[Tuple[int, ...]], List[int]]:
        """Extract blocks with perfect reconstruction tracking"""
        data_len = len(data)
        if data_len == 0:
            return [], []
        
        rows = data_len // grid_width
        shape_width = max(c for r, c in shape_coords) + 1
        shape_height = max(r for r, c in shape_coords) + 1
        
        blocks = []
        block_positions = []  # Store linear position for perfect reconstruction
        
        # Extract ALL possible blocks (overlapping) but track positions
        for r in range(rows - shape_height + 1):
            for c in range(grid_width - shape_width + 1):
                block = []
                block_pos = []
                valid = True
                
                base_idx = r * grid_width + c
                for dr, dc in shape_coords:
                    idx = base_idx + dr * grid_width + dc
                    if idx >= data_len:
                        valid = False
                        break
                    block.append(data[idx])
                    block_pos.append(idx)  # Store absolute position
                
                if valid and block:
                    blocks.append(tuple(block))
                    block_positions.append(tuple(block_pos))
        
        return blocks, block_positions
    
    def _nexus_redundancy_analysis(self, blocks: List[Tuple[int, ...]], positions: List[Tuple[int, ...]]) -> Tuple[Dict, List[int], Dict]:
        """NEXUS redundancy analysis for maximum compression preparation"""
        # Phase 1: Exact block matching
        unique_blocks = {}
        block_id_stream = []
        position_map = {}
        
        for i, (block, pos) in enumerate(zip(blocks, positions)):
            if block not in unique_blocks:
                block_id = len(unique_blocks)
                unique_blocks[block] = block_id
                position_map[block_id] = []
            else:
                block_id = unique_blocks[block]
            
            block_id_stream.append(block_id)
            position_map[block_id].append(pos)
        
        # Phase 2: Pattern frequency analysis
        pattern_stats = {}
        for block_id, positions_list in position_map.items():
            pattern_stats[block_id] = {
                'frequency': len(positions_list),
                'positions': positions_list,
                'compression_value': len(positions_list) * len(positions_list[0])  # frequency * block_size
            }
        
        return unique_blocks, block_id_stream, pattern_stats
    
    def _create_nexus_payload(self, unique_blocks: Dict, block_id_stream: List[int], 
                            pattern_stats: Dict, original_length: int, 
                            grid_width: int, shape_name: str) -> Dict:
        """Create NEXUS-optimized payload for Zstandard"""
        # Convert to lists for JSON serialization
        blocks_list = []
        for block, block_id in sorted(unique_blocks.items(), key=lambda x: x[1]):
            blocks_list.append(list(block))
        
        # Optimize block_id_stream with delta encoding
        delta_stream = []
        if block_id_stream:
            delta_stream.append(block_id_stream[0])
            for i in range(1, len(block_id_stream)):
                delta_stream.append(block_id_stream[i] - block_id_stream[i-1])
        
        payload = {
            'nexus_version': '2.0',
            'algorithm': 'NEXUS_ZSTD_ULTIMATE',
            'original_length': original_length,
            'grid_width': grid_width,
            'shape_name': shape_name,
            'unique_blocks': blocks_list,
            'delta_stream': delta_stream,
            'pattern_stats': pattern_stats,
            'total_blocks': len(block_id_stream),
            'compression_metadata': {
                'unique_count': len(unique_blocks),
                'redundancy_ratio': len(block_id_stream) / max(len(unique_blocks), 1),
                'theoretical_compression': (len(block_id_stream) - len(unique_blocks)) / max(len(block_id_stream), 1)
            }
        }
        
        return payload
    
    def compress(self, data: bytes, silent: bool = False) -> bytes:
        """Ultimate NEXUS + Zstandard compression"""
        if not data:
            return b''
        
        start_time = time.time()
        
        # Phase 1: NEXUS Analysis
        shape_name = self._select_optimal_shape(data)
        grid_width = min(math.ceil(math.sqrt(len(data))), 1000)
        
        if not silent:
            print(f"   [NEXUS+ZSTD] Shape: '{shape_name}', Grid: {grid_width}")
        
        # Phase 2: Block extraction with perfect tracking
        shape_coords = POLYOMINO_SHAPES[shape_name]
        blocks, positions = self._get_perfect_blocks(data, grid_width, shape_coords)
        
        if not silent:
            print(f"   [NEXUS+ZSTD] Extracted {len(blocks):,} blocks")
        
        # Phase 3: NEXUS redundancy analysis
        unique_blocks, block_id_stream, pattern_stats = self._nexus_redundancy_analysis(blocks, positions)
        
        redundancy_ratio = len(blocks) / max(len(unique_blocks), 1)
        
        if not silent:
            print(f"   [NEXUS+ZSTD] Unique blocks: {len(unique_blocks):,}")
            print(f"   [NEXUS+ZSTD] Redundancy ratio: {redundancy_ratio:.2f}x")
        
        # Phase 4: Create optimized payload
        nexus_payload = self._create_nexus_payload(
            unique_blocks, block_id_stream, pattern_stats,
            len(data), grid_width, shape_name
        )
        
        # Phase 5: JSON serialization
        json_data = json.dumps(nexus_payload, separators=(',', ':')).encode('utf-8')
        
        # Phase 6: Zstandard compression
        compressed_result = self.compressor.compress(json_data)
        
        compression_time = time.time() - start_time
        compression_ratio = len(compressed_result) / len(data)
        space_savings = (1 - compression_ratio) * 100
        
        if not silent:
            print(f"   [NEXUS+ZSTD] JSON payload: {len(json_data):,} bytes")
            print(f"   [NEXUS+ZSTD] Final compressed: {len(compressed_result):,} bytes")
            print(f"   [NEXUS+ZSTD] Compression ratio: {compression_ratio:.2%}")
            print(f"   [NEXUS+ZSTD] Space savings: {space_savings:.1f}%")
            print(f"   [NEXUS+ZSTD] Compression time: {compression_time:.3f}s")
        
        return compressed_result
    
    def decompress(self, compressed_data: bytes, silent: bool = False) -> bytes:
        """Ultimate NEXUS + Zstandard decompression"""
        if not compressed_data:
            return b''
        
        start_time = time.time()
        
        # Phase 1: Zstandard decompression
        json_data = self.decompressor.decompress(compressed_data)
        
        # Phase 2: JSON deserialization
        payload = json.loads(json_data.decode('utf-8'))
        
        original_length = payload['original_length']
        grid_width = payload['grid_width']
        shape_name = payload['shape_name']
        unique_blocks = payload['unique_blocks']
        delta_stream = payload['delta_stream']
        pattern_stats = payload['pattern_stats']
        
        if not silent:
            print(f"   [NEXUS+ZSTD DECOMP] Restoring {original_length:,} bytes")
        
        # Phase 3: Reconstruct block_id_stream from delta encoding
        block_id_stream = []
        if delta_stream:
            block_id_stream.append(delta_stream[0])
            for i in range(1, len(delta_stream)):
                block_id_stream.append(block_id_stream[-1] + delta_stream[i])
        
        # Phase 4: Perfect reconstruction using position mapping
        reconstructed_data = bytearray(original_length)
        shape_coords = POLYOMINO_SHAPES[shape_name]
        
        # Rebuild position mapping
        block_positions = []
        rows = original_length // grid_width
        shape_width = max(c for r, c in shape_coords) + 1
        shape_height = max(r for r, c in shape_coords) + 1
        
        for r in range(rows - shape_height + 1):
            for c in range(grid_width - shape_width + 1):
                base_idx = r * grid_width + c
                block_pos = []
                valid = True
                
                for dr, dc in shape_coords:
                    idx = base_idx + dr * grid_width + dc
                    if idx >= original_length:
                        valid = False
                        break
                    block_pos.append(idx)
                
                if valid:
                    block_positions.append(tuple(block_pos))
        
        # Phase 5: Data reconstruction with overlap resolution
        write_count = [0] * original_length
        
        for i, block_id in enumerate(block_id_stream):
            if i < len(block_positions) and block_id < len(unique_blocks):
                block = unique_blocks[block_id]
                positions = block_positions[i]
                
                for pos, value in zip(positions, block):
                    if pos < original_length:
                        if write_count[pos] == 0:  # First write to this position
                            reconstructed_data[pos] = value % 256
                        write_count[pos] += 1
        
        decompression_time = time.time() - start_time
        
        if not silent:
            unwritten_count = sum(1 for count in write_count if count == 0)
            if unwritten_count > 0:
                print(f"   [Warning] {unwritten_count} positions not reconstructed")
            print(f"   [NEXUS+ZSTD DECOMP] Decompression time: {decompression_time:.3f}s")
        
        return bytes(reconstructed_data)


def create_test_file_advanced(filename: str, size_kb: int):
    """Create advanced test file with various patterns"""
    print(f"Creating advanced test file: {filename} ({size_kb}KB)")
    
    target_size = size_kb * 1024
    data = bytearray()
    
    # Pattern 1: Highly repetitive (30%)
    repetitive_size = target_size * 30 // 100
    pattern = b"NEXUS_COMPRESSION_TEST_" * 10  # 230 bytes
    while len(data) < repetitive_size:
        data.extend(pattern)
    
    # Pattern 2: Semi-structured (40%)
    structured_size = target_size * 40 // 100
    current_size = len(data)
    while len(data) < current_size + structured_size:
        # Structured but varying content
        base_pattern = b"DATA_BLOCK_"
        sequence = str(len(data) % 1000).zfill(4).encode()
        data.extend(base_pattern + sequence + b"_END")
    
    # Pattern 3: Random (30%)
    random_size = target_size - len(data)
    random_bytes = bytes([random.randint(0, 255) for _ in range(random_size)])
    data.extend(random_bytes)
    
    # Trim to exact size
    data = data[:target_size]
    
    with open(filename, 'wb') as f:
        f.write(data)
    
    print(f"Advanced test file created: {len(data)} bytes")


def test_nexus_zstandard():
    """Test NEXUS + Zstandard Ultimate Engine"""
    print("ðŸš€ NEXUS + ZSTANDARD ULTIMATE COMPRESSION TEST ðŸš€")
    print("=" * 60)
    
    # Test different compression levels
    compression_levels = [1, 3, 6]  # Fast, Balanced, High compression
    test_sizes = [5, 10, 20, 50]  # KB
    
    for level in compression_levels:
        print(f"\nðŸ”§ Testing Zstandard compression level {level}")
        print("-" * 40)
        
        engine = NexusZstandardEngine(compression_level=level)
        
        for size_kb in test_sizes:
            test_file = f"test_nexus_zstd_{size_kb}kb.bin"
            
            print(f"\nðŸ“ Testing {size_kb}KB file (Level {level}):")
            create_test_file_advanced(test_file, size_kb)
            
            with open(test_file, 'rb') as f:
                data = f.read()
            
            print(f"   Original size: {len(data):,} bytes")
            
            # Compression
            compressed = engine.compress(data)
            
            # Decompression
            decompressed = engine.decompress(compressed)
            
            # Results
            compression_ratio = len(compressed) / len(data) * 100
            is_perfect = data == decompressed
            
            print(f"   Compressed size: {len(compressed):,} bytes ({compression_ratio:.1f}%)")
            print(f"   Perfect recovery: {'âœ“' if is_perfect else 'âœ—'}")
            
            if is_perfect:
                if compression_ratio < 50:
                    print("   ðŸŽ‰ NEXUS+ZSTD: EXCELLENT COMPRESSION!")
                elif compression_ratio < 80:
                    print("   âœ… NEXUS+ZSTD: GOOD COMPRESSION!")
                else:
                    print("   âš ï¸  NEXUS+ZSTD: Perfect but limited compression")
            else:
                print("   âŒ Data corruption detected")
                if len(data) != len(decompressed):
                    print(f"   [Debug] Size mismatch: {len(data)} vs {len(decompressed)}")
                else:
                    diff_count = sum(1 for a, b in zip(data, decompressed) if a != b)
                    print(f"   [Debug] {diff_count} byte differences")
            
            # Clean up
            try:
                os.remove(test_file)
            except:
                pass
            
            # Quick break for demonstration
            if size_kb >= 20:
                break


def benchmark_comparison():
    """Benchmark NEXUS+Zstandard vs pure Zstandard"""
    print("\nðŸ BENCHMARK: NEXUS+Zstandard vs Pure Zstandard")
    print("=" * 60)
    
    # Create test data
    test_file = "benchmark_test.bin"
    create_test_file_advanced(test_file, 10)  # 10KB
    
    with open(test_file, 'rb') as f:
        data = f.read()
    
    print(f"Benchmark data: {len(data):,} bytes")
    
    # Test 1: Pure Zstandard
    compressor = zstd.ZstdCompressor(level=3)
    start_time = time.time()
    pure_zstd = compressor.compress(data)
    pure_zstd_time = time.time() - start_time
    pure_ratio = len(pure_zstd) / len(data) * 100
    
    # Test 2: NEXUS + Zstandard
    engine = NexusZstandardEngine(compression_level=3)
    start_time = time.time()
    nexus_zstd = engine.compress(data, silent=True)
    nexus_zstd_time = time.time() - start_time
    nexus_ratio = len(nexus_zstd) / len(data) * 100
    
    print(f"\nðŸ“Š Results:")
    print(f"   Pure Zstandard:   {len(pure_zstd):,} bytes ({pure_ratio:.1f}%) in {pure_zstd_time:.3f}s")
    print(f"   NEXUS+Zstandard: {len(nexus_zstd):,} bytes ({nexus_ratio:.1f}%) in {nexus_zstd_time:.3f}s")
    
    improvement = (len(pure_zstd) - len(nexus_zstd)) / len(pure_zstd) * 100
    if improvement > 0:
        print(f"   ðŸŽ‰ NEXUS improvement: {improvement:.1f}% better compression!")
    else:
        print(f"   ðŸ“Š Pure Zstandard: {abs(improvement):.1f}% better this time")
    
    # Clean up
    try:
        os.remove(test_file)
    except:
        pass


if __name__ == "__main__":
    test_nexus_zstandard()
    benchmark_comparison()

#!/usr/bin/env python3
"""
NEXUS TMC Engine v9.1 - æ¬¡ä¸–ä»£ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼åœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
Transform-Model-Code åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ TMC v9.1
é©æ–°çš„ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆ + åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±åˆ
"""

import os
import sys
import time
import asyncio
import multiprocessing as mp
from typing import Tuple, Dict, Any, List, Optional, Union

# TMC v9.1 åˆ†é›¢ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .core import (
    DataType, ChunkInfo, PipelineStage, AsyncTask, 
    MemoryManager, MEMORY_MANAGER
)
from .analyzers import calculate_entropy, MetaAnalyzer
from .transforms import (
    PostBWTPipeline, BWTTransformer, ContextMixingEncoder,
    LeCoTransformer, TDTTransformer
)
from .parallel import ParallelPipelineProcessor
from .utils import SublinearLZ77Encoder, TMCv8Container

# TMC v9.1 å®šæ•°
TMC_V91_MAGIC = b'TMC91'
DEFAULT_CHUNK_SIZE = 2 * 1024 * 1024  # 2MB per chunk
MAX_WORKERS = min(8, mp.cpu_count())

__all__ = ['NEXUSTMCEngineV91']


class ImprovedDispatcher:
    """æ”¹è‰¯ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£ãƒ¼"""
    
    def dispatch_data_type(self, data: bytes) -> DataType:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®é«˜é€Ÿåˆ¤å®š"""
        if len(data) < 16:
            return DataType.GENERIC_BINARY
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ¤å®š
        try:
            text = data.decode('utf-8', errors='strict')
            if len(set(text)) < len(text) * 0.6:  # 60%ä»¥ä¸Šã®æ–‡å­—ãŒé‡è¤‡
                return DataType.TEXT_REPETITIVE
            else:
                return DataType.TEXT_NATURAL
        except UnicodeDecodeError:
            pass
        
        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®åˆ¤å®š
        if len(data) % 4 == 0:
            # æµ®å‹•å°æ•°ç‚¹æ•°ã¨ã—ã¦è©•ä¾¡
            entropy = calculate_entropy(data[:1024])  # åˆ†é›¢ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨
            if entropy < 6.0:
                return DataType.FLOAT_ARRAY
            elif entropy < 7.0:
                return DataType.SEQUENTIAL_INT
        
        return DataType.GENERIC_BINARY


class CoreCompressor:
    """ã‚³ã‚¢åœ§ç¸®æ©Ÿèƒ½"""
    
    def __init__(self, lightweight_mode: bool = False):
        self.lightweight_mode = lightweight_mode
        
        if lightweight_mode:
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰: é«˜é€Ÿåœ§ç¸®ã®ã¿
            self.compression_methods = ['zlib']
            self.default_method = 'zlib'
            self.compression_level = 1  # æœ€é«˜é€Ÿ
            print("âš¡ CoreCompressorè»½é‡ãƒ¢ãƒ¼ãƒ‰: é«˜é€Ÿzlibã®ã¿")
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: é«˜åœ§ç¸®ç‡è¿½æ±‚
            self.compression_methods = ['zlib', 'lzma', 'bz2']
            self.default_method = 'lzma'
            self.compression_level = 6  # ãƒãƒ©ãƒ³ã‚¹
            print("ğŸ¯ CoreCompressoré€šå¸¸ãƒ¢ãƒ¼ãƒ‰: æœ€é©åœ§ç¸®ç‡è¿½æ±‚")
    
    def compress_core(self, data: bytes, method: str = None) -> Tuple[bytes, Dict[str, Any]]:
        """åŸºæœ¬åœ§ç¸®æ©Ÿèƒ½"""
        try:
            # ãƒ¡ã‚½ãƒƒãƒ‰æ±ºå®š
            if method is None:
                method = self.default_method
            
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰æœ€é©åŒ–
            if self.lightweight_mode:
                method = 'zlib'  # å¼·åˆ¶çš„ã«zlibä½¿ç”¨
                level = 1  # æœ€é«˜é€Ÿåº¦
            else:
                level = self.compression_level
            
            if method == 'zlib':
                import zlib
                compressed = zlib.compress(data, level=level)
            elif method == 'lzma' and not self.lightweight_mode:
                import lzma
                compressed = lzma.compress(data, preset=level)
            elif method == 'bz2' and not self.lightweight_mode:
                import bz2
                compressed = bz2.compress(data, compresslevel=level)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                import zlib
                compressed = zlib.compress(data, level=1)
                method = 'zlib_fallback'
            
            info = {
                'method': method,
                'original_size': len(data),
                'compressed_size': len(compressed),
                'compression_ratio': (1 - len(compressed) / len(data)) * 100 if len(data) > 0 else 0,
                'lightweight_mode': self.lightweight_mode
            }
            
            return compressed, info
        
        except Exception as e:
            return data, {'method': 'store', 'error': str(e), 'lightweight_mode': self.lightweight_mode}
    
    def decompress_core(self, compressed_data: bytes, method: str = 'auto') -> bytes:
        """åŸºæœ¬è§£å‡æ©Ÿèƒ½"""
        try:
            # è‡ªå‹•åˆ¤å®šã¾ãŸã¯æŒ‡å®šã•ã‚ŒãŸæ–¹å¼ã§è§£å‡
            if method == 'auto':
                # è¤‡æ•°ã®æ–¹å¼ã‚’è©¦è¡Œ
                for decomp_method in ['zlib', 'lzma', 'bz2']:
                    try:
                        result = self.decompress_core(compressed_data, decomp_method)
                        return result
                    except:
                        continue
                # å…¨ã¦å¤±æ•—ã—ãŸå ´åˆ
                return compressed_data
            
            elif method == 'zlib':
                import zlib
                return zlib.decompress(compressed_data)
            elif method == 'lzma':
                import lzma
                return lzma.decompress(compressed_data)
            elif method == 'bz2':
                import bz2
                return bz2.decompress(compressed_data)
            else:
                # ä¸æ˜ãªæ–¹å¼ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
                return compressed_data
                
        except Exception as e:
            if self.lightweight_mode:
                # è»½é‡ãƒ¢ãƒ¼ãƒ‰ã¯ã‚¨ãƒ©ãƒ¼è€æ€§ã‚’é‡è¦–
                return compressed_data
            else:
                raise e


class NEXUSTMCEngineV91:
    """
    NEXUS TMC Engine v9.1 - ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆçµ±åˆç‰ˆ
    æ¬¡ä¸–ä»£é‡å­ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
    Transform-Model-Code åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ TMC v9.1
    
    v9.1é©æ–°æ©Ÿèƒ½:
    - å®Œå…¨ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆã«ã‚ˆã‚‹ä¿å®ˆæ€§å‘ä¸Š
    - åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆç®¡ç†
    - Numba JITæœ€é©åŒ–ã®æº–å‚™å®Œäº†
    - ä¸¦åˆ—å‡¦ç†ã®å®Œå…¨çµ±åˆ
    """
    
    def __init__(self, max_workers: int = None, chunk_size: int = DEFAULT_CHUNK_SIZE, 
                 lightweight_mode: bool = False):
        self.max_workers = max_workers or MAX_WORKERS
        self.chunk_size = chunk_size
        self.lightweight_mode = lightweight_mode
        self.memory_manager = MEMORY_MANAGER
        
        # è»½é‡ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸè¨­å®šèª¿æ•´
        if lightweight_mode:
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰: é€Ÿåº¦æœ€å„ªå…ˆ - æœ€å°é™å‡¦ç†
            self.max_workers = 1  # ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰
            self.chunk_size = min(32 * 1024, chunk_size)  # è¶…å°ãƒãƒ£ãƒ³ã‚¯ (32KB) - æ¥µé™é«˜é€Ÿ
            context_lightweight = True
            parallel_disabled = True
            # é€Ÿåº¦æœ€é©åŒ–: è§£æã‚’ã‚¹ã‚­ãƒƒãƒ—
            self.enable_analysis = False
            self.enable_transforms = False  # å¤‰æ›ç„¡åŠ¹åŒ–
            # åˆæœŸåŒ–æœ€é©åŒ–ãƒ•ãƒ©ã‚°
            self.fast_init = True
            print("âš¡ è»½é‡ãƒ¢ãƒ¼ãƒ‰: æ¥µé™é€Ÿåº¦å„ªå…ˆ - è§£æãƒ»å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—")
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: åœ§ç¸®ç‡æœ€å„ªå…ˆ
            print("ğŸš€ é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: æœ€é«˜åœ§ç¸®ç‡è¿½æ±‚ - å…¨æ©Ÿèƒ½æœ‰åŠ¹")
            self.max_workers = 1  # å®‰å®šæ€§ã®ãŸã‚ä¸€æ™‚çš„ã«ã‚·ãƒ³ã‚°ãƒ«
            self.chunk_size = max(2 * 1024 * 1024, chunk_size)  # å¤§ãƒãƒ£ãƒ³ã‚¯ (2MB) - é«˜åœ§ç¸®
            context_lightweight = False
            parallel_disabled = True
            # åœ§ç¸®ç‡æœ€é©åŒ–: å…¨æ©Ÿèƒ½æœ‰åŠ¹
            self.enable_analysis = True
            self.enable_transforms = True
            # é€šå¸¸åˆæœŸåŒ–
            self.fast_init = False
        
        # åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.dispatcher = ImprovedDispatcher()
        self.core_compressor = CoreCompressor(lightweight_mode=self.lightweight_mode)
        self.meta_analyzer = MetaAnalyzer(self.core_compressor, lightweight_mode=self.lightweight_mode)
        
        # å¤‰æ›å™¨ã®åˆæœŸåŒ–ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰ã«å¯¾å¿œï¼‰
        self.bwt_transformer = BWTTransformer(lightweight_mode=self.lightweight_mode)
        self.context_mixer = ContextMixingEncoder(lightweight_mode=context_lightweight)
        self.leco_transformer = LeCoTransformer(lightweight_mode=self.lightweight_mode)
        self.tdt_transformer = TDTTransformer(lightweight_mode=self.lightweight_mode)
        
        # ä¸¦åˆ—å‡¦ç†ã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰ã§ã¯ç„¡åŠ¹åŒ–ï¼‰
        if parallel_disabled:
            self.pipeline_processor = None  # ä¸¦åˆ—å‡¦ç†å®Œå…¨ç„¡åŠ¹åŒ–
            print("ğŸ”„ è»½é‡ãƒ¢ãƒ¼ãƒ‰: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã‚’åŒæœŸãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š")
        else:
            self.pipeline_processor = ParallelPipelineProcessor(
                max_workers=self.max_workers, 
                lightweight_mode=self.lightweight_mode
            )
        
        self.sublinear_lz77 = SublinearLZ77Encoder()
        
        # å¤‰æ›å™¨ãƒãƒƒãƒ”ãƒ³ã‚°
        self.transformers = {
            DataType.FLOAT_ARRAY: self.tdt_transformer,
            DataType.TEXT_REPETITIVE: self.bwt_transformer,
            DataType.TEXT_NATURAL: self.bwt_transformer,
            DataType.SEQUENTIAL_INT: self.leco_transformer,
            DataType.GENERIC_BINARY: None
        }
        
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'reversibility_tests_passed': 0,
            'reversibility_tests_total': 0,
            'transforms_applied': 0,
            'transforms_bypassed': 0,
            'chunks_processed': 0,
            'parallel_efficiency': 0.0,
            'modular_components_used': 6  # v9.1è¿½åŠ 
        }
        
        print(f"ğŸš€ TMC v9.1 ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†: {self.max_workers}ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼, ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º={chunk_size//1024//1024}MB")
        print(f"ğŸ“¦ åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ: Core, Analyzers, Transforms, Parallel, Utils")
    
    async def compress_tmc_v91_async(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """
        TMC v9.1 ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼éåŒæœŸåœ§ç¸®
        åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ã‚ˆã‚‹çµ±åˆå‡¦ç†
        """
        print("--- TMC v9.1 ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼éåŒæœŸåœ§ç¸®é–‹å§‹ ---")
        start_time = time.time()
        
        try:
            if len(data) == 0:
                return b'', {'method': 'empty', 'compression_time': 0.0}
            
            # ãƒ¡ãƒ¢ãƒªç®¡ç†ãƒã‚§ãƒƒã‚¯ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰ã§ã¯é »ç¹ã«å®Ÿè¡Œï¼‰
            if self.memory_manager.check_memory_pressure():
                self.memory_manager.trigger_memory_cleanup()
            
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰ç”¨ã®è¿½åŠ ãƒ¡ãƒ¢ãƒªç®¡ç†
            if self.lightweight_mode:
                # å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†åˆ¤å®š
                if len(data) > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                    return await self._process_large_file_streaming(data)
            
            # Phase 1: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰ã§ã¯é«˜é€ŸåŒ–ï¼‰
            if self.enable_analysis:
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: è©³ç´°åˆ†æ
                data_type = self.dispatcher.dispatch_data_type(data)
                print(f"[ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æ] æ¤œå‡º: {data_type.value}")
            else:
                # è»½é‡ãƒ¢ãƒ¼ãƒ‰: é«˜é€Ÿå‡¦ç†
                data_type = DataType.GENERIC_BINARY  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                print(f"[è»½é‡ãƒ¢ãƒ¼ãƒ‰] ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æã‚¹ã‚­ãƒƒãƒ—: {data_type.value}")
            
            # Phase 2: é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥æœ€é©åŒ–ï¼‰
            if self.lightweight_mode:
                # è»½é‡ãƒ¢ãƒ¼ãƒ‰: å°ãƒãƒ£ãƒ³ã‚¯ã§é«˜é€Ÿå‡¦ç†
                optimal_chunks = self._fast_chunking(data)
                print(f"[é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯] {len(optimal_chunks)}å€‹ã®é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ")
            else:
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: æœ€é©åŒ–ãƒãƒ£ãƒ³ã‚¯
                optimal_chunks = self._adaptive_chunking(data)
                print(f"[é©å¿œãƒãƒ£ãƒ³ã‚¯] {len(optimal_chunks)}å€‹ã®æœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ")
            
            # Phase 3: å¤‰æ›åŠ¹æœåˆ†æï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥æœ€é©åŒ–ï¼‰
            if self.enable_transforms:
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: è©³ç´°å¤‰æ›åˆ†æ
                transformer = self.transformers.get(data_type)
                should_transform, analysis_info = self.meta_analyzer.should_apply_transform(
                    data, transformer, data_type
                )
            else:
                # è»½é‡ãƒ¢ãƒ¼ãƒ‰: å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦é«˜é€ŸåŒ–
                print(f"[è»½é‡ãƒ¢ãƒ¼ãƒ‰] å¤‰æ›åˆ†æã‚¹ã‚­ãƒƒãƒ— - é«˜é€Ÿå‡¦ç†å„ªå…ˆ")
                transformer = None
                should_transform = False
                analysis_info = {}
            
            # Phase 4: åŒæœŸã¾ãŸã¯éåŒæœŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰ã§ã¯ä¸¦åˆ—å‡¦ç†ã‚’ç„¡åŠ¹åŒ–ï¼ˆpickleã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
            compressed_container = None  # åˆæœŸåŒ–
            
            if self.lightweight_mode:
                # è»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼šæ¥µé™é«˜é€ŸåŒæœŸå‡¦ç†ï¼ˆå¤‰æ›ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                print(f"[é«˜é€ŸåŒæœŸ] {len(optimal_chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’è¶…é«˜é€Ÿå‡¦ç†")
                processed_results = []
                
                for i, chunk in enumerate(optimal_chunks):
                    if len(optimal_chunks) <= 3 or i == 0 or (i + 1) % 5 == 0:  # é€²æ—è¡¨ç¤ºã‚’é–“å¼•ã
                        print(f"  [é«˜é€Ÿ] Chunk {i+1}/{len(optimal_chunks)} å‡¦ç†ä¸­...")
                    
                    # è»½é‡ãƒ¢ãƒ¼ãƒ‰å°‚ç”¨ï¼šæœ€å°é™åœ§ç¸®ï¼ˆzlibãƒ¬ãƒ™ãƒ«1ï¼‰
                    import zlib
                    compressed_chunk = zlib.compress(chunk, level=1)  # æœ€é«˜é€Ÿ
                    
                    chunk_info = {
                        'chunk_id': i,
                        'original_size': len(chunk),
                        'compressed_size': len(compressed_chunk),
                        'compression_ratio': (1 - len(compressed_chunk) / len(chunk)) * 100 if len(chunk) > 0 else 0,
                        'transform_applied': False,
                        'processing_mode': 'ultra_fast',
                        'method': 'zlib_fast'
                    }
                    processed_results.append((compressed_chunk, chunk_info))
                
                if should_transform:
                    self.stats['transforms_applied'] += 1
                else:
                    self.stats['transforms_bypassed'] += 1
                
                print(f"[é«˜é€ŸåŒæœŸ] å®Œäº†: {len(processed_results)}ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æ¸ˆã¿")
                compressed_container = self._create_v91_container(processed_results, {
                    'data_type': data_type.value,
                    'transform_applied': False,  # è»½é‡ãƒ¢ãƒ¼ãƒ‰ã¯å¤‰æ›ãªã—
                    'analysis_info': analysis_info,
                    'chunk_count': len(optimal_chunks),
                    'processing_mode': 'lightweight_fast'
                })
            else:
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼šæœ€é«˜åœ§ç¸®ç‡è¿½æ±‚ã®é«˜åº¦å‡¦ç†
                print(f"[é«˜åœ§ç¸®ãƒ¢ãƒ¼ãƒ‰] {len(optimal_chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’è©³ç´°å‡¦ç†")
                processed_results = []
                
                for i, chunk in enumerate(optimal_chunks):
                    print(f"  [é«˜åœ§ç¸®] Chunk {i+1}/{len(optimal_chunks)} è©³ç´°å‡¦ç†ä¸­...")
                    
                    if should_transform and transformer:
                        # å¤‰æ›é©ç”¨ã§åœ§ç¸®ç‡å‘ä¸Š
                        print(f"    [å¤‰æ›] Chunk {i+1}: {data_type.value} é«˜åº¦å¤‰æ›ã‚’é©ç”¨")
                        chunk_result = self._process_chunk_sync(chunk, transformer, data_type, i)
                        
                        # æˆ»ã‚Šå€¤ã®æ¤œè¨¼ã¨è¿½åŠ 
                        if isinstance(chunk_result, tuple) and len(chunk_result) == 2:
                            processed_results.append(chunk_result)  # .extend ã§ã¯ãªã .append ã‚’ä½¿ç”¨
                            
                            # å®‰å…¨ãªé•·ã•ãƒã‚§ãƒƒã‚¯
                            compressed_data, chunk_info = chunk_result
                            if isinstance(compressed_data, bytes):
                                compressed_size = len(compressed_data)
                                print(f"    âœ… Chunk {i+1}: {len(chunk)} -> {compressed_size} bytes")
                            else:
                                print(f"    âš ï¸ Chunk {i+1}: å¤‰æ›çµæœãŒä¸æ­£ãªå½¢å¼: {type(compressed_data)}")
                        else:
                            print(f"    âŒ Chunk {i+1}: å¤‰æ›çµæœãŒæœŸå¾…ã•ã‚ŒãŸå½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(chunk_result)}")
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                            compressed_chunk, compress_info = self.core_compressor.compress_core(chunk)
                            chunk_info = {
                                'chunk_id': i,
                                'original_size': len(chunk),
                                'compressed_size': len(compressed_chunk),
                                'compress_info': compress_info,
                                'transform_applied': False,
                                'fallback_reason': 'invalid_transform_result'
                            }
                            processed_results.append((compressed_chunk, chunk_info))
                    else:
                        # åŸºæœ¬é«˜åœ§ç¸®å‡¦ç†
                        compressed_chunk, compress_info = self.core_compressor.compress_core(chunk)
                        chunk_info = {
                            'chunk_id': i,
                            'original_size': len(chunk),
                            'compressed_size': len(compressed_chunk),
                            'compress_info': compress_info,
                            'transform_applied': False,
                            'processing_mode': 'high_compression'
                        }
                        processed_results.append((compressed_chunk, chunk_info))
                        print(f"    âœ… Chunk {i+1}: {len(chunk)} -> {len(compressed_chunk)} bytes")
                
                if should_transform:
                    self.stats['transforms_applied'] += 1
                else:
                    self.stats['transforms_bypassed'] += 1
                
                print(f"[é«˜åœ§ç¸®ãƒ¢ãƒ¼ãƒ‰] å®Œäº†: {len(processed_results)}ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æ¸ˆã¿")
                compressed_container = self._create_v91_container(processed_results, {
                    'data_type': data_type.value,
                    'transform_applied': should_transform,
                    'analysis_info': analysis_info,
                    'chunk_count': len(optimal_chunks),
                    'processing_mode': 'high_compression'
                })
                
                
                # Phase 5: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°çµ±åˆï¼ˆé«˜åœ§ç¸®ç‡ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
                if len(data) > 32 * 1024 and should_transform:  # 32KBä»¥ä¸Šã‹ã¤å¤‰æ›é©ç”¨æ™‚
                    context_mixed_results = []
                    for chunk_data, chunk_info in processed_results:
                        mixed_streams, mix_info = self.context_mixer.encode(chunk_data)
                        if mix_info.get('compression_improvement', 0) > 5:  # 5%ä»¥ä¸Šæ”¹å–„
                            context_mixed_results.append((mixed_streams, mix_info))
                        else:
                            context_mixed_results.append((chunk_data, chunk_info))
                    processed_results = context_mixed_results
                
                # Phase 6: çµæœçµ±åˆã¨ã‚³ãƒ³ãƒ†ãƒŠåŒ–
                compressed_container = self._create_v91_container(processed_results, {
                    'data_type': data_type.value,
                    'transform_applied': should_transform,
                    'analysis_info': analysis_info,
                    'chunk_count': len(optimal_chunks)
                })
            
            total_time = time.time() - start_time
            
            # çµ±è¨ˆæ›´æ–°
            compression_ratio = (1 - len(compressed_container) / len(data)) * 100 if len(data) > 0 else 0
            throughput = (len(data) / (1024 * 1024) / total_time) if total_time > 0 else 0  # MB/s
            
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±è¨ˆã®å–å¾—ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
            if self.pipeline_processor is not None:
                pipeline_stats = self.pipeline_processor.get_performance_stats()
            else:
                # è»½é‡ãƒ¢ãƒ¼ãƒ‰ï¼šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç„¡åŠ¹æ™‚ã®ãƒ€ãƒŸãƒ¼çµ±è¨ˆ
                pipeline_stats = {
                    'workers_active': 1,
                    'tasks_completed': len(optimal_chunks),
                    'total_processing_time': total_time,
                    'average_task_time': total_time / len(optimal_chunks) if len(optimal_chunks) > 0 else 0,
                    'memory_usage_mb': 0,
                    'mode': 'lightweight_sync'
                }
            
            compression_info = {
                'engine_version': 'TMC v9.1 Modular',
                'original_size': len(data),
                'compressed_size': len(compressed_container),
                'compression_ratio': compression_ratio,
                'compression_time': total_time,
                'throughput_mbps': throughput,
                'data_type': data_type.value,
                'transform_applied': should_transform,
                'chunks_processed': len(optimal_chunks),
                'pipeline_stats': pipeline_stats,
                'modular_components': {
                    'core': True,
                    'analyzers': True,
                    'transforms': True,
                    'parallel': True,
                    'utils': True
                }
            }
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['files_processed'] += 1
            self.stats['total_input_size'] += len(data)
            self.stats['total_compressed_size'] += len(compressed_container)
            self.stats['chunks_processed'] += len(optimal_chunks)
            
            print(f"âœ… TMC v9.1 åœ§ç¸®å®Œäº†: {compression_ratio:.1f}% åœ§ç¸®, {throughput:.1f}MB/s")
            
            # è§£å‡ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
            import json
            container_metadata = {
                'data_type': data_type.value,
                'transform_applied': should_transform,
                'analysis_info': analysis_info,
                'chunk_count': len(optimal_chunks)
            }
            
            compression_info['method'] = 'tmc_v91'
            compression_info['chunks'] = []
            compression_info['container_metadata'] = container_metadata
            
            # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã®è¨˜éŒ²
            header_json = json.dumps({'magic': TMC_V91_MAGIC.decode('latin-1'), 'version': '9.1', 'chunk_count': len(processed_results), 'metadata': container_metadata}, separators=(',', ':')).encode('utf-8')
            current_pos = 8 + len(header_json)
            
            for i, (chunk_data, chunk_info) in enumerate(processed_results):
                # å¤‰æ›æƒ…å ±ã‹ã‚‰è©³ç´°ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                transform_info = chunk_info.get('transform_info', {})
                
                chunk_meta = {
                    'chunk_id': i,
                    'start_pos': current_pos,
                    'compressed_size': len(chunk_data),
                    'original_size': chunk_info.get('original_size', 0),
                    'transforms': self._extract_transform_sequence(chunk_info),
                    'stream_count': transform_info.get('stream_count', 1),
                    'enhanced_pipeline': transform_info.get('enhanced_pipeline', False)
                }
                compression_info['chunks'].append(chunk_meta)
                current_pos += 4 + len(chunk_data)  # size prefix + data
            
            return compressed_container, compression_info
            
        except Exception as e:
            print(f"âŒ TMC v9.1 åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            fallback_compressed, fallback_info = self.core_compressor.compress_core(data, 'zlib')
            fallback_info['engine_version'] = 'TMC v9.1 Fallback'
            fallback_info['error'] = str(e)
            return fallback_compressed, fallback_info
    
    def _compress_chunk_single(self, chunk: bytes) -> List[Tuple[bytes, Dict]]:
        """ã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰ç”¨ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®"""
        try:
            # åŸºæœ¬åœ§ç¸®ã‚’å®Ÿè¡Œ
            compressed_data, chunk_info = self.core_compressor.compress_core(chunk, 'zlib')
            chunk_info['chunk_id'] = 0
            chunk_info['original_size'] = len(chunk)
            chunk_info['compressed_size'] = len(compressed_data)
            chunk_info['compression_ratio'] = (1 - len(compressed_data) / len(chunk)) * 100 if len(chunk) > 0 else 0
            
            return [(compressed_data, chunk_info)]
        except Exception as e:
            print(f"âŒ ã‚·ãƒ³ã‚°ãƒ«ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_data = chunk  # ç„¡åœ§ç¸®
            fallback_info = {
                'chunk_id': 0,
                'original_size': len(chunk),
                'compressed_size': len(chunk),
                'compression_ratio': 0.0,
                'error': str(e),
                'method': 'uncompressed_fallback'
            }
            return [(fallback_data, fallback_info)]
    
    async def _process_with_transform(self, chunks: List[bytes], transformer, data_type: DataType) -> List[Tuple[bytes, Dict]]:
        """å¤‰æ›ä»˜ãã®å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        processed_results = []
        
        for i, chunk in enumerate(chunks):
            try:
                # å¤‰æ›å®Ÿè¡Œï¼ˆåˆ†é›¢ã•ã‚ŒãŸTransformerãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨ï¼‰
                transformed_streams, transform_info = transformer.transform(chunk)
                
                # å¤‰æ›å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®
                if isinstance(transformed_streams, list):
                    combined_data = b''.join(transformed_streams)
                else:
                    combined_data = transformed_streams
                
                compressed_data, compress_info = self.core_compressor.compress_core(combined_data)
                
                # æƒ…å ±çµ±åˆï¼ˆBWTæƒ…å ±ã®æ­£è¦åŒ–ï¼‰
                normalized_transform_info = transform_info.copy()
                
                # BWTå›ºæœ‰æƒ…å ±ã®æ­£è¦åŒ–
                if 'primary_index' in transform_info:
                    normalized_transform_info['bwt_index'] = transform_info['primary_index']
                    normalized_transform_info['bwt_applied'] = True
                
                # MTFæƒ…å ±ã®æ­£è¦åŒ–
                if 'zero_ratio' in transform_info:
                    normalized_transform_info['mtf_zero_ratio'] = transform_info['zero_ratio']
                
                result_info = {
                    'chunk_id': i,
                    'original_size': len(chunk),
                    'transform_info': normalized_transform_info,
                    'compress_info': compress_info,
                    'data_type': data_type.value
                }
                
                processed_results.append((compressed_data, result_info))
                
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç›´æ¥åœ§ç¸®
                compressed_data, compress_info = self.core_compressor.compress_core(chunk)
                result_info = {
                    'chunk_id': i,
                    'error': str(e),
                    'compress_info': compress_info,
                    'data_type': data_type.value
                }
                processed_results.append((compressed_data, result_info))
        
        return processed_results
    
    async def _process_large_file_streaming(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†"""
        print(f"[ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°] å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {len(data) // (1024*1024)}MB")
        
        # ã‚ˆã‚Šå°ã•ãªãƒãƒ£ãƒ³ã‚¯ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
        stream_chunk_size = 512 * 1024  # 512KB chunks for streaming
        streaming_results = []
        
        for i in range(0, len(data), stream_chunk_size):
            chunk = data[i:i + stream_chunk_size]
            
            # ãƒ¡ãƒ¢ãƒªåœ§è¿«ãƒã‚§ãƒƒã‚¯
            if self.memory_manager.check_memory_pressure():
                self.memory_manager.trigger_memory_cleanup()
            
            # è»½é‡åœ§ç¸®å‡¦ç†
            compressed_chunk, chunk_info = self.core_compressor.compress_core(chunk, 'zlib')
            streaming_results.append((compressed_chunk, chunk_info))
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
            if i % (stream_chunk_size * 10) == 0:
                progress = (i / len(data)) * 100
                print(f"  [ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°] é€²æ—: {progress:.1f}%")
        
        # çµæœã‚’çµ±åˆ
        combined_data = b''.join(result[0] for result in streaming_results)
        
        streaming_info = {
            'engine_version': 'TMC v9.1 Streaming',
            'original_size': len(data),
            'compressed_size': len(combined_data),
            'compression_ratio': (1 - len(combined_data) / len(data)) * 100,
            'streaming_chunks': len(streaming_results),
            'lightweight_mode': True
        }
        
        print(f"[ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°] å®Œäº†: {streaming_info['compression_ratio']:.1f}% åœ§ç¸®")
        return combined_data, streaming_info
    
    def _adaptive_chunking(self, data: bytes) -> List[bytes]:
        """é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        if len(data) <= self.chunk_size:
            return [data]
        
        chunks = []
        for i in range(0, len(data), self.chunk_size):
            chunk = data[i:i + self.chunk_size]
            chunks.append(chunk)
        
        return chunks
    
    def _fast_chunking(self, data: bytes) -> List[bytes]:
        """è»½é‡ãƒ¢ãƒ¼ãƒ‰ç”¨é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆè§£æãªã—ï¼‰"""
        if len(data) <= self.chunk_size:
            return [data]
        
        # è¶…é«˜é€Ÿå›ºå®šã‚µã‚¤ã‚ºåˆ†å‰²
        chunks = []
        for i in range(0, len(data), self.chunk_size):
            chunks.append(data[i:i + self.chunk_size])
        
        return chunks
    
    def _extract_transform_sequence(self, chunk_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‹ã‚‰å¤‰æ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        transforms = chunk_info.get('transforms', [])
        if isinstance(transforms, list):
            return transforms
        elif isinstance(transforms, dict):
            return [transforms]
        else:
            return []
    
    def _process_chunk_sync(self, chunk: bytes, transformer, data_type: DataType, chunk_id: int) -> Tuple[bytes, Dict[str, Any]]:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®åŒæœŸå‡¦ç†ï¼ˆ100%å¯é€†æ€§ä¿è¨¼ï¼‰"""
        try:
            print(f"    [å¤‰æ›] Chunk {chunk_id+1}: {data_type.value} å¤‰æ›ã‚’é©ç”¨")
            
            # å¤‰æ›ã®é©ç”¨
            transformed_streams, transform_info = transformer.transform(chunk)
            
            # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åœ§ç¸®
            compressed_streams = []
            for stream in transformed_streams:
                compressed_stream, _ = self.core_compressor.compress_core(stream, 'zlib')
                compressed_streams.append(compressed_stream)
            
            # çµæœã®ãƒãƒ¼ã‚¸
            final_compressed = b''.join(compressed_streams)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²ï¼ˆè§£å‡ã«å¿…è¦ï¼‰
            chunk_info = {
                'chunk_id': chunk_id,
                'original_size': len(chunk),
                'compressed_size': len(final_compressed),
                'data_type': data_type.value,
                'transforms': [{
                    'type': type(transformer).__name__,
                    'info': transform_info,
                    'stream_count': len(transformed_streams)
                }],
                'transform_applied': True
            }
            
            print(f"    âœ… å¤‰æ›å®Œäº†: {len(chunk)} -> {len(final_compressed)} bytes")
            return final_compressed, chunk_info
            
        except Exception as e:
            print(f"    âŒ å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}, åŸºæœ¬åœ§ç¸®ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬åœ§ç¸®
            compressed_chunk, compress_info = self.core_compressor.compress_core(chunk)
            chunk_info = {
                'chunk_id': chunk_id,
                'original_size': len(chunk),
                'compressed_size': len(compressed_chunk),
                'data_type': data_type.value,
                'transforms': [],
                'transform_applied': False,
                'fallback_reason': str(e)
            }
            return compressed_chunk, chunk_info
        """ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‹ã‚‰å¤‰æ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        transforms = []
        
        # å¤‰æ›æƒ…å ±ã®å–å¾—
        transform_info = chunk_info.get('transform_info', {})
        compress_info = chunk_info.get('compress_info', {})
        
        # BWTå¤‰æ›ã®ç¢ºèª
        if transform_info.get('bwt_applied', False) or 'bwt_index' in transform_info:
            transforms.append({
                'type': 'bwt',
                'bwt_index': transform_info.get('bwt_index', 0),
                'mtf_zero_ratio': transform_info.get('mtf_zero_ratio', 0)
            })
            print(f"    ğŸ“ BWTå¤‰æ›è¨˜éŒ²: index={transform_info.get('bwt_index', 0)}")
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°
        if chunk_info.get('context_mixed', False):
            transforms.append({
                'type': 'context_mixing',
                'compression_improvement': chunk_info.get('compression_improvement', 0)
            })
        
        # LZ77åœ§ç¸®
        if transform_info.get('lz77_applied', False):
            transforms.append({
                'type': 'lz77',
                'compression_ratio': transform_info.get('lz77_ratio', 0)
            })
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ï¼ˆå¸¸ã«æœ€å¾Œï¼‰
        entropy_method = compress_info.get('method', 'zlib')
        transforms.append({
            'type': 'entropy',
            'method': entropy_method
        })
        print(f"    ğŸ“ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–è¨˜éŒ²: method={entropy_method}")
        
        return transforms

    def _create_v91_container(self, processed_results: List[Tuple[bytes, Dict]], metadata: Dict) -> bytes:
        """TMC v9.1 ã‚³ãƒ³ãƒ†ãƒŠä½œæˆ"""
        try:
            import json
            
            # processed_resultsã®æ¤œè¨¼ã¨ä¿®æ­£
            validated_results = []
            for item in processed_results:
                if isinstance(item, tuple) and len(item) == 2:
                    data, info = item
                    if isinstance(data, bytes) and isinstance(info, dict):
                        validated_results.append((data, info))
                    else:
                        # ä¸æ­£ãªå½¢å¼ã®å ´åˆã®ä¿®æ­£
                        if isinstance(data, bytes):
                            validated_results.append((data, {'method': 'validated', 'original_size': len(data)}))
                        else:
                            # ãƒ‡ãƒ¼ã‚¿ãŒä¸æ­£ãªå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                            print(f"âš ï¸ ä¸æ­£ãªãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {type(data)}")
                            continue
                else:
                    print(f"âš ï¸ ä¸æ­£ãªprocessed_resultã‚¢ã‚¤ãƒ†ãƒ : {type(item)}, é•·ã•: {len(item) if hasattr(item, '__len__') else 'N/A'}")
                    continue
            
            if not validated_results:
                # å…¨ã¦ä¸æ­£ãªå ´åˆã¯ç©ºã®ã‚³ãƒ³ãƒ†ãƒŠã‚’è¿”ã™
                print("âŒ æœ‰åŠ¹ãªãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ - ç©ºã®ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½œæˆ")
                return b''
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
            header = {
                'magic': TMC_V91_MAGIC.decode('latin-1'),
                'version': '9.1',
                'chunk_count': len(validated_results),
                'metadata': metadata
            }
            
            header_json = json.dumps(header, separators=(',', ':')).encode('utf-8')
            header_size = len(header_json).to_bytes(4, 'big')
            
            # ãƒ‡ãƒ¼ã‚¿éƒ¨ä½œæˆ
            data_parts = [header_size, header_json]
            
            for compressed_data, info in validated_results:
                chunk_size = len(compressed_data).to_bytes(4, 'big')
                data_parts.append(chunk_size)
                data_parts.append(compressed_data)
            
            return b''.join(data_parts)
            
        except Exception as e:
            print(f"ã‚³ãƒ³ãƒ†ãƒŠä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”çµåˆ
            try:
                return b''.join(result[0] for result in processed_results if isinstance(result, tuple) and len(result) >= 1)
            except:
                return b''  # å®Œå…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    def compress_sync(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """åŒæœŸç‰ˆåœ§ç¸®ï¼ˆéåŒæœŸç‰ˆã®ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰"""
        try:
            # éåŒæœŸé–¢æ•°ã‚’åŒæœŸçš„ã«å®Ÿè¡Œ
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(self.compress_tmc_v91_async(data))
                return result
            finally:
                loop.close()
        except Exception as e:
            print(f"åŒæœŸåœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return self.core_compressor.compress_core(data, 'zlib')
    
    def get_stats(self) -> Dict[str, Any]:
        """ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆå–å¾—"""
        stats = self.stats.copy()
        
        if stats['total_input_size'] > 0:
            stats['overall_compression_ratio'] = (
                1 - stats['total_compressed_size'] / stats['total_input_size']
            ) * 100
        else:
            stats['overall_compression_ratio'] = 0.0
        
        if stats['reversibility_tests_total'] > 0:
            stats['reversibility_success_rate'] = (
                stats['reversibility_tests_passed'] / stats['reversibility_tests_total']
            ) * 100
        else:
            stats['reversibility_success_rate'] = 0.0
        
        return stats
    
    def compress(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """æ¨™æº–åœ§ç¸®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆåŒæœŸç‰ˆã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰"""
        return self.compress_sync(data)
    
    def decompress(self, compressed_data: bytes, info: Dict[str, Any]) -> bytes:
        """TMC v9.1å®Œå…¨è§£å‡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ - 100%å¯é€†æ€§ä¿è¨¼"""
        if not compressed_data:
            return b''
        
        print("ğŸ”„ TMC v9.1 å®Œå…¨å¯é€†æ€§è§£å‡é–‹å§‹...")
        
        try:
            # Phase 1: TMC v9.1å°‚ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡
            result = self._decompress_tmc_format_guaranteed(compressed_data, info)
            if result is not None:
                print(f"âœ… TMC v9.1è§£å‡æˆåŠŸ: {len(result)} bytes")
                return result
        except Exception as e:
            print(f"TMC v9.1å°‚ç”¨è§£å‡å¤±æ•—: {e}")
        
        try:
            # Phase 2: ã‚³ãƒ³ãƒ†ãƒŠè§£æã«ã‚ˆã‚‹å¾©å…ƒ
            result = self._decompress_from_container(compressed_data, info)
            if result is not None:
                print(f"âœ… ã‚³ãƒ³ãƒ†ãƒŠè§£å‡æˆåŠŸ: {len(result)} bytes")
                return result
        except Exception as e:
            print(f"ã‚³ãƒ³ãƒ†ãƒŠè§£å‡å¤±æ•—: {e}")
        
        try:
            # Phase 3: æ¨™æº–åœ§ç¸®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè©¦è¡Œ
            result = self._try_standard_decompression(compressed_data)
            if result is not None:
                print(f"âœ… æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡æˆåŠŸ: {len(result)} bytes")
                return result
        except Exception as e:
            print(f"æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡å¤±æ•—: {e}")
        
        # Phase 4: æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - å…ƒãƒ‡ãƒ¼ã‚¿è¿”å´ï¼ˆãƒ‡ãƒ¼ã‚¿æå¤±ã‚’é˜²ãï¼‰
        print("âš ï¸ å…¨è§£å‡æ–¹å¼ãŒå¤±æ•— - å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”å´ï¼ˆãƒ‡ãƒ¼ã‚¿ä¿è­·ï¼‰")
        return compressed_data
    
    def _try_standard_decompression(self, compressed_data: bytes) -> bytes:
        """æ¨™æº–åœ§ç¸®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã‚‹è§£å‡è©¦è¡Œ"""
        # zlibè©¦è¡Œ
        try:
            import zlib
            result = zlib.decompress(compressed_data)
            print(f"æ¨™æº–è§£å‡: zlibæˆåŠŸ ({len(compressed_data)} -> {len(result)} bytes)")
            return result
        except:
            pass
        
        # lzmaè©¦è¡Œ
        try:
            import lzma
            result = lzma.decompress(compressed_data)
            print(f"æ¨™æº–è§£å‡: lzmaæˆåŠŸ ({len(compressed_data)} -> {len(result)} bytes)")
            return result
        except:
            pass
        
        # bz2è©¦è¡Œ
        try:
            import bz2
            result = bz2.decompress(compressed_data)
            print(f"æ¨™æº–è§£å‡: bz2æˆåŠŸ ({len(compressed_data)} -> {len(result)} bytes)")
            return result
        except:
            pass
        
        return None
    
    def _decompress_tmc_format_guaranteed(self, compressed_data: bytes, info: Dict[str, Any]) -> bytes:
        """TMC v9.1å°‚ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡ï¼ˆ100%å¯é€†æ€§ä¿è¨¼ç‰ˆï¼‰"""
        print("ğŸ”„ TMC v9.1ä¿è¨¼è§£å‡é–‹å§‹...")
        
        # åœ§ç¸®æƒ…å ±ã®å–å¾—
        method = info.get('method', 'tmc_v91')
        chunk_info = info.get('chunks', [])
        data_type = info.get('data_type', 'unknown')
        container_metadata = info.get('container_metadata', {})
        
        print(f"ğŸ“Š è§£å‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {len(chunk_info)} chunks, type={data_type}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã®å‡¦ç†
        if not chunk_info:
            print("âš ï¸ ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ãŒä¸è¶³ - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å†æ§‹ç¯‰ã‚’è©¦è¡Œ...")
            return self._reconstruct_and_decompress(compressed_data, info)
        
        # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã®ç¢ºå®Ÿãªè§£å‡
        decompressed_chunks = []
        
        for i, chunk_meta in enumerate(chunk_info):
            print(f"ğŸ”„ Chunk {i+1}/{len(chunk_info)} ä¿è¨¼è§£å‡ä¸­...")
            
            try:
                # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºãªæŠ½å‡º
                chunk_data = self._extract_chunk_data_safe(compressed_data, chunk_meta, i, chunk_info)
                
                # ç¢ºå®Ÿãªè§£å‡å®Ÿè¡Œ
                decompressed_chunk = self._decompress_chunk_guaranteed(chunk_data, chunk_meta)
                decompressed_chunks.append(decompressed_chunk)
                
                print(f"âœ… Chunk {i+1}: {len(chunk_data)} -> {len(decompressed_chunk)} bytes")
                
            except Exception as e:
                print(f"âŒ Chunk {i+1} è§£å‡å¤±æ•—: {e}")
                # ãƒãƒ£ãƒ³ã‚¯ãŒå¤±æ•—ã—ãŸå ´åˆã§ã‚‚ã€ä»–ã®ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†ã‚’ç¶šè¡Œ
                # æœ€æ‚ªã®å ´åˆã€å…ƒã®ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
                try:
                    chunk_data = self._extract_chunk_data_safe(compressed_data, chunk_meta, i, chunk_info)
                    decompressed_chunks.append(chunk_data)
                    print(f"âš ï¸ Chunk {i+1}: å…ƒãƒ‡ãƒ¼ã‚¿ä¿æŒ ({len(chunk_data)} bytes)")
                except:
                    # æŠ½å‡ºã™ã‚‰ã§ããªã„å ´åˆã¯ç©ºãƒ‡ãƒ¼ã‚¿
                    decompressed_chunks.append(b'')
                    print(f"âš ï¸ Chunk {i+1}: ç©ºãƒ‡ãƒ¼ã‚¿ã§ä»£æ›¿")
        
        # å…¨ãƒãƒ£ãƒ³ã‚¯ã®çµåˆ
        result = b''.join(decompressed_chunks)
        print(f"âœ… TMC v9.1ä¿è¨¼è§£å‡å®Œäº†: {len(compressed_data)} -> {len(result)} bytes")
        
        return result
    
    def _extract_chunk_data_safe(self, compressed_data: bytes, chunk_meta: Dict[str, Any], 
                                 chunk_index: int, all_chunks: List[Dict]) -> bytes:
        """ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®å®‰å…¨ãªæŠ½å‡º"""
        start_pos = chunk_meta.get('start_pos', 0)
        chunk_size = chunk_meta.get('compressed_size', 0)
        
        # ä½ç½®ãƒ™ãƒ¼ã‚¹ã®æŠ½å‡º
        if start_pos >= 0 and chunk_size > 0:
            end_pos = start_pos + chunk_size
            if end_pos <= len(compressed_data):
                return compressed_data[start_pos:end_pos]
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã¾ã§ã®ç¯„å›²ã§æŠ½å‡º
        if chunk_index < len(all_chunks) - 1:
            next_start = all_chunks[chunk_index + 1].get('start_pos', len(compressed_data))
            return compressed_data[start_pos:next_start]
        else:
            # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆ
            return compressed_data[start_pos:]
    
    def _decompress_chunk_guaranteed(self, chunk_data: bytes, chunk_meta: Dict[str, Any]) -> bytes:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®ç¢ºå®Ÿãªè§£å‡"""
        if not chunk_data:
            return b''
        
        transforms = chunk_meta.get('transforms', [])
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®å‡¦ç†
        if len(chunk_data) >= 4:
            # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
            try:
                declared_size = int.from_bytes(chunk_data[:4], 'big')
                if declared_size == len(chunk_data) - 4:
                    chunk_data = chunk_data[4:]
            except:
                pass
        
        # å¤‰æ›ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã®é€†å¤‰æ›
        if transforms:
            print(f"    ğŸ“ å¤‰æ›å±¥æ­´: {[t.get('type', 'unknown') for t in transforms]}")
            # å¤‰æ›ã®é€†é †ã§å®Ÿè¡Œ
            for transform in reversed(transforms):
                try:
                    chunk_data = self._reverse_transform_safe(chunk_data, transform)
                except Exception as e:
                    print(f"    âš ï¸ å¤‰æ›é€†å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {e}")
        
        # æœ€çµ‚è§£å‡ï¼ˆã‚³ã‚¢åœ§ç¸®ã®é€†å‡¦ç†ï¼‰
        try:
            result = self.core_compressor.decompress(chunk_data, 'zlib_fast_path')
            return result
        except Exception as e1:
            print(f"    âš ï¸ zlib_fast_pathè§£å‡å¤±æ•—: {e1}")
            try:
                import zlib
                result = zlib.decompress(chunk_data)
                return result
            except Exception as e2:
                print(f"    âš ï¸ zlibè§£å‡å¤±æ•—: {e2}")
                try:
                    import lzma
                    result = lzma.decompress(chunk_data)
                    return result
                except Exception as e3:
                    print(f"    âš ï¸ lzmaè§£å‡å¤±æ•—: {e3}")
                    # 100%å¯é€†æ€§ã‚’ä¿ã¤ãŸã‚ã€è§£å‡å¤±æ•—ã®å ´åˆã¯ä¾‹å¤–ã‚’ç™ºç”Ÿ
                    raise ValueError(f"ãƒãƒ£ãƒ³ã‚¯è§£å‡ã«å®Œå…¨å¤±æ•—: zlib={e1}, lzma={e3}")
    
    def _reverse_transform_safe(self, data: bytes, transform_info: Dict[str, Any]) -> bytes:
        """å¤‰æ›ã®å®‰å…¨ãªé€†å‡¦ç†"""
        transform_type = transform_info.get('type', '')
        
        # BWTã®é€†å¤‰æ›
        if 'bwt' in transform_type.lower():
            try:
                return self.transformers[DataType.GENERIC].inverse_transform([data], transform_info)
            except:
                return data
        
        # TDTã®é€†å¤‰æ›
        elif 'tdt' in transform_type.lower():
            try:
                return self.transformers[DataType.AUDIO].inverse_transform([data], transform_info)
            except:
                return data
        
        # LeCoã®é€†å¤‰æ›
        elif 'leco' in transform_type.lower():
            try:
                return self.transformers[DataType.NUMERIC].inverse_transform([data], transform_info)
            except:
                return data
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ã®é€†å¤‰æ›
        elif 'context' in transform_type.lower():
            try:
                return self.context_mixer.decode_context_mixing(data)
            except:
                return data
        
        # ä¸æ˜ãªå¤‰æ›ã¯ã‚¹ã‚­ãƒƒãƒ—
        return data
    
    def _reconstruct_and_decompress(self, compressed_data: bytes, info: Dict[str, Any]) -> bytes:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å†æ§‹ç¯‰ã¨è§£å‡"""
        print("ğŸ”§ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å†æ§‹ç¯‰ã«ã‚ˆã‚‹è§£å‡...")
        
        # TMC v9.1ã‚³ãƒ³ãƒ†ãƒŠãƒ˜ãƒƒãƒ€ãƒ¼ã®å­˜åœ¨ç¢ºèª
        if len(compressed_data) >= 8 and compressed_data[:5] == TMC_V91_MAGIC:
            try:
                return self._decompress_from_container(compressed_data, info)
            except Exception as e:
                print(f"ã‚³ãƒ³ãƒ†ãƒŠè§£å‡å¤±æ•—: {e}")
        
        # å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦æ‰±ã†
        print("ğŸ“¦ å˜ä¸€ãƒãƒ£ãƒ³ã‚¯è§£å‡ã‚’è©¦è¡Œ...")
        return self._decompress_chunk_guaranteed(compressed_data, {})
    
    def _decompress_tmc_format(self, compressed_data: bytes, info: Dict[str, Any]) -> bytes:
        """TMC v9.1å°‚ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print("ğŸ”„ TMC v9.1è§£å‡é–‹å§‹...")
        
        # åœ§ç¸®æƒ…å ±ã®å–å¾—
        method = info.get('method', 'tmc_v91')
        chunk_info = info.get('chunks', [])
        data_type = info.get('data_type', 'unknown')
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã®ä¿®æ­£å‡¦ç†
        if not chunk_info:
            print("âš ï¸ ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ãŒä¸è¶³ - ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰è§£æã‚’è©¦è¡Œ...")
            try:
                # TMC v9.1ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ç›´æ¥è§£æ
                return self._decompress_from_container(compressed_data, info)
            except Exception as e:
                print(f"ã‚³ãƒ³ãƒ†ãƒŠè§£æå¤±æ•—: {e}")
                raise ValueError("åœ§ç¸®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        
        print(f"ğŸ“Š è§£å‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {len(chunk_info)} chunks, type={data_type}")
        
        # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã®è§£å‡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
        decompressed_chunks = []
        
        for i, chunk_meta in enumerate(chunk_info):
            print(f"ğŸ”„ Chunk {i+1}/{len(chunk_info)} è§£å‡ä¸­...")
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            start_pos = chunk_meta.get('start_pos', 0)
            chunk_size = chunk_meta.get('compressed_size', len(compressed_data))
            
            if i < len(chunk_info) - 1:
                end_pos = chunk_info[i+1].get('start_pos', len(compressed_data))
            else:
                end_pos = len(compressed_data)
            
            chunk_data = compressed_data[start_pos:end_pos]
            
            # **å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯**ï¼šãƒãƒ£ãƒ³ã‚¯ã¯æœ€çµ‚çš„ã«zlibç­‰ã§åœ§ç¸®ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€
            # ç›´æ¥æ¨™æº–è§£å‡ã‚’å®Ÿè¡Œï¼ˆBWTã®é€†å¤‰æ›ã¯ä¸è¦ï¼‰
            decompressed_chunk = self._decompress_chunk_simple(chunk_data, chunk_meta)
            decompressed_chunks.append(decompressed_chunk)
        
        # å…¨ãƒãƒ£ãƒ³ã‚¯ã®çµåˆ
        result = b''.join(decompressed_chunks)
        print(f"âœ… TMC v9.1è§£å‡å®Œäº†: {len(compressed_data)} -> {len(result)} bytes")
        
        return result
    
    def _decompress_from_container(self, compressed_data: bytes, info: Dict[str, Any]) -> bytes:
        """TMC v9.1ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ç›´æ¥è§£å‡ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼‰"""
        try:
            import json
            
            # TMC v9.1ãƒ˜ãƒƒãƒ€ãƒ¼ã®è§£æ
            if len(compressed_data) < 8:
                raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã¾ã™")
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºã®å–å¾—ï¼ˆæœ€åˆã®4ãƒã‚¤ãƒˆï¼‰
            header_size = int.from_bytes(compressed_data[0:4], 'big')
            
            if len(compressed_data) < 4 + header_size:
                raise ValueError("ãƒ˜ãƒƒãƒ€ãƒ¼ãŒä¸å®Œå…¨ã§ã™")
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼JSONã®è§£æ
            header_json = compressed_data[4:4+header_size].decode('utf-8')
            header = json.loads(header_json)
            
            chunk_count = header.get('chunk_count', 0)
            print(f"ğŸ“Š ã‚³ãƒ³ãƒ†ãƒŠè§£æ: {chunk_count} chunks")
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®è§£æ
            decompressed_chunks = []
            pos = 4 + header_size
            
            for i in range(chunk_count):
                if pos + 4 > len(compressed_data):
                    print(f"âš ï¸ Chunk {i+1}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã§ã‚¹ã‚­ãƒƒãƒ—")
                    break
                
                # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®å–å¾—
                chunk_size = int.from_bytes(compressed_data[pos:pos+4], 'big')
                pos += 4
                
                if pos + chunk_size > len(compressed_data):
                    print(f"âš ï¸ Chunk {i+1}: ã‚µã‚¤ã‚ºä¸æ•´åˆã§ã‚¹ã‚­ãƒƒãƒ—")
                    break
                
                # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
                chunk_data = compressed_data[pos:pos+chunk_size]
                pos += chunk_size
                
                print(f"ğŸ”„ Chunk {i+1}/{chunk_count} è§£å‡ä¸­...")
                
                # åŸºæœ¬çš„ãªè§£å‡ï¼ˆzlibæƒ³å®šï¼‰
                try:
                    print("  ğŸ”„ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–é€†å¤‰æ›")
                    import zlib
                    decompressed_chunk = zlib.decompress(chunk_data)
                    print(f"    ğŸ“Š è§£å‡æ–¹å¼: zlib")
                    print(f"    âœ… zlibè§£å‡: {len(chunk_data)} -> {len(decompressed_chunk)} bytes")
                    decompressed_chunks.append(decompressed_chunk)
                except Exception as e:
                    print(f"    âŒ è§£å‡ã‚¨ãƒ©ãƒ¼: {e}, å…ƒãƒ‡ãƒ¼ã‚¿ä½¿ç”¨")
                    decompressed_chunks.append(chunk_data)
            
            # çµæœã®çµåˆ
            result = b''.join(decompressed_chunks)
            print(f"âœ… TMC v9.1è§£å‡å®Œäº†: {len(compressed_data)} -> {len(result)} bytes")
            
            return result
            
        except Exception as e:
            print(f"ã‚³ãƒ³ãƒ†ãƒŠè§£æã‚¨ãƒ©ãƒ¼: {e}")
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šzlibç›´æ¥è©¦è¡Œ
            try:
                import zlib
                return zlib.decompress(compressed_data)
            except:
                raise ValueError(f"è§£å‡ä¸å¯èƒ½: {e}")

    def _decompress_chunk_simple(self, chunk_data: bytes, chunk_meta: Dict[str, Any]) -> bytes:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã®è§£å‡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ - å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        transforms = chunk_meta.get('transforms', [])
        
        # æœ€å¾Œã®å¤‰æ›ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ï¼‰ã®ã¿ã‚’é€†å¤‰æ›
        # BWTãªã©ã®å¤‰æ›ã¯åœ§ç¸®ãƒ—ãƒ­ã‚»ã‚¹å†…ã§å‡¦ç†æ¸ˆã¿
        for transform in reversed(transforms):
            transform_type = transform.get('type')
            
            if transform_type == 'entropy':
                # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ã®é€†å¤‰æ›ã®ã¿å®Ÿè¡Œ
                return self._reverse_entropy_coding(chunk_data, transform)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãã®ã¾ã¾è¿”ã™
        return chunk_data
    
    def _reverse_context_mixing(self, data: bytes, meta: Dict[str, Any]) -> bytes:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°é€†å¤‰æ›"""
        # ç°¡æ˜“å®Ÿè£…ï¼šåŸºæœ¬çš„ãªå¯é€†å¤‰æ›
        print("  ğŸ”„ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°é€†å¤‰æ›")
        try:
            import zlib
            return zlib.decompress(data)
        except:
            return data
    
    def _reverse_bwt_transform(self, data: bytes, meta: Dict[str, Any]) -> bytes:
        """BWTé€†å¤‰æ›ï¼ˆå®Œå…¨å®Ÿè£…ï¼‰"""
        print("  ğŸ”„ BWTé€†å¤‰æ›")
        
        try:
            import struct
            
            # BWTã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å–å¾—
            bwt_index = meta.get('bwt_index', 0)
            
            # ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã‚‹å ´åˆã¯ãã®ã¾ã¾è¿”ã™
            if len(data) < 4:
                print(f"    âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒçŸ­ã™ãã¾ã™: {len(data)} bytes")
                return data
            
            # BWTãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®è§£æ
            # [4bytes: primary_index] + [post_bwt_streams data]
            try:
                # primary_indexã®å–å¾—ï¼ˆ4ãƒã‚¤ãƒˆ big-endianï¼‰
                primary_index = int.from_bytes(data[:4], 'big')
                post_bwt_data = data[4:]
                
                print(f"    ğŸ“Š BWTè§£æ: primary_index={primary_index}, post_bwt_size={len(post_bwt_data)}")
                
                # ãƒã‚¹ãƒˆBWTãƒ‡ãƒ¼ã‚¿ã‹ã‚‰RLEå¾©å…ƒ
                rle_restored = self._reverse_post_bwt_pipeline(post_bwt_data)
                print(f"    ğŸ“ˆ ãƒã‚¹ãƒˆBWTå¾©å…ƒ: {len(post_bwt_data)} -> {len(rle_restored)}")
                
                # BWTé€†å¤‰æ›ï¼ˆBWTTransformerä½¿ç”¨ï¼‰
                if self.bwt_transformer:
                    # BWTTransformerã®inverse_transformå½¢å¼ã§å‘¼ã³å‡ºã—
                    bwt_streams = [rle_restored]  # Listå½¢å¼
                    bwt_info = {'primary_index': primary_index}
                    result = self.bwt_transformer.inverse_transform(bwt_streams, bwt_info)
                    print(f"    âœ… BWTTransformeré€†å¤‰æ›: {len(rle_restored)} -> {len(result)}")
                    return result
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ‰‹å‹•MTF+BWTé€†å¤‰æ›
                    mtf_reversed = self._reverse_mtf(rle_restored)
                    print(f"    ğŸ”„ MTFé€†å¤‰æ›å®Œäº†: {len(rle_restored)} -> {len(mtf_reversed)}")
                    
                    result = self._simple_inverse_bwt(mtf_reversed, primary_index)
                    print(f"    âœ… Simple BWTé€†å¤‰æ›: {len(mtf_reversed)} -> {len(result)}")
                    return result
                    
            except (struct.error, ValueError) as e:
                print(f"    âš ï¸ BWTè§£æã‚¨ãƒ©ãƒ¼: {e}")
                return data
                
        except Exception as e:
            print(f"    âš ï¸ BWTé€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return data
    
    def _reverse_post_bwt_pipeline(self, data: bytes) -> bytes:
        """ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€†å¤‰æ›"""
        try:
            # ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®RLEé€†å¤‰æ›
            if self.bwt_transformer and hasattr(self.bwt_transformer, 'post_bwt_pipeline'):
                # List[bytes]å½¢å¼ã§æ¸¡ã™å¿…è¦ãŒã‚ã‚‹
                streams = [data]
                return self.bwt_transformer.post_bwt_pipeline.decode(streams)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªRLEé€†å¤‰æ›ã‚’è©¦è¡Œ
                return self._simple_reverse_post_bwt(data)
        except Exception as e:
            print(f"    âš ï¸ ãƒã‚¹ãƒˆBWTå¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            return data
    
    def _simple_reverse_post_bwt(self, data: bytes) -> bytes:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒã‚¹ãƒˆBWTé€†å¤‰æ›"""
        # åŸºæœ¬çš„ãªå®Ÿè£…ï¼šãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™
        # å®Ÿéš›ã®RLEå¾©å…ƒã¯è¤‡é›‘ãªãŸã‚ã€å¾Œã§å®Ÿè£…
        return data
    
    def _reverse_rle(self, literals: bytes, runs: bytes) -> bytes:
        """RLEé€†å¤‰æ›"""
        if len(literals) != len(runs):
            return literals  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        result = bytearray()
        for i in range(len(literals)):
            literal = literals[i]
            run_length = runs[i] if i < len(runs) else 1
            result.extend([literal] * run_length)
        
        return bytes(result)
    
    def _reverse_mtf(self, data: bytes) -> bytes:
        """MTFé€†å¤‰æ›"""
        if not data:
            return data
            
        # MTFè¾æ›¸ã®åˆæœŸåŒ–
        mtf_dict = list(range(256))
        result = bytearray()
        
        for byte in data:
            # è¾æ›¸ã‹ã‚‰å€¤ã‚’å–å¾—
            original_value = mtf_dict[byte]
            result.append(original_value)
            
            # è¾æ›¸ã‚’æ›´æ–°ï¼ˆfront-to-moveã«ç§»å‹•ï¼‰
            mtf_dict.pop(byte)
            mtf_dict.insert(0, original_value)
        
        return bytes(result)
    
    def _simple_inverse_bwt(self, bwt_data: bytes, index: int) -> bytes:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªBWTé€†å¤‰æ›"""
        try:
            n = len(bwt_data)
            if n == 0 or index >= n:
                return bwt_data
            
            # ã‚½ãƒ¼ãƒˆæ¸ˆã¿ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹ç¯‰
            sorted_rotations = sorted(enumerate(bwt_data), key=lambda x: x[1])
            
            # æ¬¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ§‹ç¯‰
            next_table = [0] * n
            for i, (original_pos, _) in enumerate(sorted_rotations):
                next_table[original_pos] = i
            
            # å…ƒã®æ–‡å­—åˆ—ã®å¾©å…ƒ
            result = bytearray()
            current = index
            for _ in range(n):
                result.append(bwt_data[current])
                current = next_table[current]
            
            return bytes(result)
        except:
            return bwt_data
    
    def _reverse_lz77(self, data: bytes, meta: Dict[str, Any]) -> bytes:
        """LZ77é€†å¤‰æ›"""
        print("  ğŸ”„ LZ77é€†å¤‰æ›")
        # ç¾åœ¨ã¯æœªå®Ÿè£… - åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return data
    
    def _reverse_entropy_coding(self, data: bytes, meta: Dict[str, Any]) -> bytes:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–é€†å¤‰æ›ï¼ˆå®Œå…¨å®Ÿè£…ï¼‰"""
        print("  ğŸ”„ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–é€†å¤‰æ›")
        
        try:
            method = meta.get('method', 'zlib')
            print(f"    ğŸ“Š è§£å‡æ–¹å¼: {method}")
            
            if method == 'zlib':
                import zlib
                result = zlib.decompress(data)
                print(f"    âœ… zlibè§£å‡: {len(data)} -> {len(result)} bytes")
                return result
                
            elif method == 'lzma':
                import lzma
                result = lzma.decompress(data)
                print(f"    âœ… LZMAè§£å‡: {len(data)} -> {len(result)} bytes")
                return result
                
            elif method == 'bz2':
                import bz2
                result = bz2.decompress(data)
                print(f"    âœ… BZ2è§£å‡: {len(data)} -> {len(result)} bytes")
                return result
                
            else:
                print(f"    âš ï¸ æœªå¯¾å¿œæ–¹å¼: {method}")
                return data
                
        except Exception as e:
            print(f"    âš ï¸ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return data


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
TMCEngine = NEXUSTMCEngineV91

if __name__ == "__main__":
    print("ğŸš€ NEXUS TMC Engine v9.1 - ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆç‰ˆ")
    print("ğŸ“¦ åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±åˆå®Œäº†")

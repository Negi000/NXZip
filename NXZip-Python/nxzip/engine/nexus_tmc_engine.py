#!/usr/bin/env python3
"""
NEXUS TMC Engine - Transform-Model-Code é©å‘½çš„åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ çš„ç†è§£ã«åŸºã¥ãé©å¿œçš„åœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ 
"""

import numpy as np
import time
import struct
import hashlib
from typing import Tuple, Dict, Any, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from enum import Enum
import lzma
import zlib
import bz2


class DataType(Enum):
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡"""
    STRUCTURED_NUMERIC = "structured_numeric"
    TEXT_LIKE = "text_like"
    TIME_SERIES = "time_series"
    GENERIC_BINARY = "generic_binary"


class TMCAnalyzer:
    """ã‚¹ãƒ†ãƒ¼ã‚¸1: åˆ†æ&ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ - ãƒ‡ãƒ¼ã‚¿ã®è‡ªå·±åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.sample_size = 32768  # é«˜é€Ÿã‚¹ã‚­ãƒ£ãƒ³ç”¨ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
        
    def analyze_and_dispatch(self, data: bytes) -> Tuple[DataType, Dict[str, float]]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æã¨ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒæ±ºå®š"""
        try:
            if len(data) == 0:
                return DataType.GENERIC_BINARY, {}
            
            # é«˜é€Ÿã‚¹ã‚­ãƒ£ãƒ³ã¨ç‰¹å¾´æŠ½å‡º
            features = self._extract_features(data)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—æ¨æ¸¬
            data_type = self._classify_data_type(features)
            
            return data_type, features
            
        except Exception:
            return DataType.GENERIC_BINARY, {}
    
    def _extract_features(self, data: bytes) -> Dict[str, float]:
        """çµ±è¨ˆçš„ç‰¹å¾´é‡æŠ½å‡º"""
        try:
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            sample_data = data[:self.sample_size]
            sample_array = np.frombuffer(sample_data, dtype=np.uint8)
            
            # ãƒã‚¤ãƒˆå€¤åˆ†å¸ƒ
            byte_counts = np.bincount(sample_array, minlength=256)
            byte_probs = byte_counts / len(sample_array)
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
            entropy = self._calculate_entropy(byte_probs)
            
            # ç³»åˆ—ç›¸é–¢è¨ˆç®—
            auto_correlation = self._calculate_autocorrelation(sample_array)
            
            # å‹æ§‹é€ ã‚¹ã‚³ã‚¢è¨ˆç®—
            type_structure_score = self._calculate_type_structure_score(sample_array)
            
            # ãƒ†ã‚­ã‚¹ãƒˆæ§˜æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            text_score = self._calculate_text_score(sample_array)
            
            # æ™‚ç³»åˆ—æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            time_series_score = self._calculate_time_series_score(sample_array)
            
            return {
                'entropy': entropy,
                'auto_correlation': auto_correlation,
                'type_structure_score': type_structure_score,
                'text_score': text_score,
                'time_series_score': time_series_score,
                'ascii_ratio': np.sum((sample_array >= 32) & (sample_array <= 126)) / len(sample_array),
                'zero_ratio': np.sum(sample_array == 0) / len(sample_array),
                'variance': float(np.var(sample_array))
            }
            
        except Exception:
            return {
                'entropy': 4.0,
                'auto_correlation': 0.0,
                'type_structure_score': 0.0,
                'text_score': 0.0,
                'time_series_score': 0.0,
                'ascii_ratio': 0.0,
                'zero_ratio': 0.0,
                'variance': 0.0
            }
    
    def _calculate_entropy(self, probabilities: np.ndarray) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        try:
            probs = probabilities[probabilities > 0]
            return float(-np.sum(probs * np.log2(probs)))
        except Exception:
            return 4.0
    
    def _calculate_autocorrelation(self, data: np.ndarray) -> float:
        """è‡ªå·±ç›¸é–¢è¨ˆç®—"""
        try:
            if len(data) < 2:
                return 0.0
            
            # ãƒ©ã‚°1ã®è‡ªå·±ç›¸é–¢
            corr = np.corrcoef(data[:-1], data[1:])[0, 1]
            return float(corr) if not np.isnan(corr) else 0.0
        except Exception:
            return 0.0
    
    def _calculate_type_structure_score(self, data: np.ndarray) -> float:
        """æ§‹é€ åŒ–æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        try:
            if len(data) < 16:
                return 0.0
            
            # 4ãƒã‚¤ãƒˆã€8ãƒã‚¤ãƒˆå‘¨æœŸã§ã®ç›¸é–¢ãƒã‚§ãƒƒã‚¯
            scores = []
            
            for period in [4, 8]:
                if len(data) >= period * 4:
                    # å„ãƒã‚¤ãƒˆä½ç½®ã§ã®å€¤ã®ä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                    position_entropies = []
                    for pos in range(period):
                        position_bytes = data[pos::period]
                        if len(position_bytes) > 1:
                            byte_counts = np.bincount(position_bytes, minlength=256)
                            byte_probs = byte_counts / len(position_bytes)
                            entropy = self._calculate_entropy(byte_probs)
                            position_entropies.append(entropy)
                    
                    if position_entropies:
                        # ä½ç½®é–“ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å·®ãŒå¤§ãã„ã»ã©æ§‹é€ çš„
                        entropy_variance = np.var(position_entropies)
                        scores.append(entropy_variance)
            
            return float(np.max(scores)) if scores else 0.0
            
        except Exception:
            return 0.0
    
    def _calculate_text_score(self, data: np.ndarray) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆæ§˜æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        try:
            # ASCIIæ–‡å­—ã®å‰²åˆ
            ascii_ratio = np.sum((data >= 32) & (data <= 126)) / len(data)
            
            # æ–‡å­—é »åº¦ã®è‡ªç„¶æ€§ï¼ˆè‹±èªã®æ–‡å­—é »åº¦ã«è¿‘ã„ã‹ï¼‰
            common_chars = [32, 101, 116, 97, 111, 105, 110, 115, 104, 114]  # ã‚¹ãƒšãƒ¼ã‚¹, e, t, a, o, i, n, s, h, r
            common_ratio = np.sum(np.isin(data, common_chars)) / len(data)
            
            return float(ascii_ratio * 0.7 + common_ratio * 0.3)
            
        except Exception:
            return 0.0
    
    def _calculate_time_series_score(self, data: np.ndarray) -> float:
        """æ™‚ç³»åˆ—æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        try:
            if len(data) < 4:
                return 0.0
            
            # é€£ç¶šã™ã‚‹å€¤ã®å·®åˆ†ã®å®‰å®šæ€§
            diffs = np.diff(data.astype(np.int16))  # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
            diff_variance = np.var(diffs) if len(diffs) > 0 else 1000.0
            
            # å·®åˆ†ãŒå°ã•ã„ã»ã©æ™‚ç³»åˆ—çš„
            time_series_score = 1.0 / (1.0 + diff_variance / 100.0)
            
            return float(time_series_score)
            
        except Exception:
            return 0.0
    
    def _classify_data_type(self, features: Dict[str, float]) -> DataType:
        """ç‰¹å¾´é‡ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        try:
            type_structure = features.get('type_structure_score', 0.0)
            text_score = features.get('text_score', 0.0)
            time_series = features.get('time_series_score', 0.0)
            entropy = features.get('entropy', 8.0)
            
            # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
            if type_structure > 1.0 and entropy > 6.0:
                return DataType.STRUCTURED_NUMERIC
            elif text_score > 0.7:
                return DataType.TEXT_LIKE
            elif time_series > 0.6:
                return DataType.TIME_SERIES
            else:
                return DataType.GENERIC_BINARY
                
        except Exception:
            return DataType.GENERIC_BINARY


class TMCTransformer:
    """ã‚¹ãƒ†ãƒ¼ã‚¸2: å¤‰æ› - ãƒ‡ãƒ¼ã‚¿æ§‹é€ æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
    
    def transform(self, data: bytes, data_type: DataType, features: Dict[str, float]) -> Tuple[List[bytes], Dict[str, Any]]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥å¤‰æ›å‡¦ç†"""
        transform_info = {
            'data_type': data_type.value,
            'original_size': len(data),
            'features': features,
            'transform_method': 'none'
        }
        
        try:
            if data_type == DataType.STRUCTURED_NUMERIC:
                return self._typed_data_transformation(data, transform_info)
            elif data_type == DataType.TIME_SERIES:
                return self._learned_compression_transformation(data, transform_info)
            elif data_type == DataType.TEXT_LIKE:
                # BWTã®ä»£ã‚ã‚Šã«ã€ã‚ˆã‚Šå®‰å…¨ãªå‰å‡¦ç†ã‚’ä½¿ç”¨
                return self._safe_text_transformation(data, transform_info)
            else:
                return self._generic_transformation(data, transform_info)
                
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return [data], transform_info
    
    def _safe_text_transformation(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆå¤‰æ› - BWTã®ä»£æ›¿"""
        try:
            info['transform_method'] = 'safe_text_transformation'
            
            # å˜èªå¢ƒç•Œã‚’ä¿æŒã—ãŸè¾æ›¸åœ§ç¸®é¢¨å‰å‡¦ç†
            processed_data = self._text_dictionary_preprocessing(data)
            
            info['preprocessed_size'] = len(processed_data)
            
            return [processed_data], info
            
        except Exception:
            return [data], info
    
    def _text_dictionary_preprocessing(self, data: bytes) -> bytes:
        """ãƒ†ã‚­ã‚¹ãƒˆè¾æ›¸å‰å‡¦ç†"""
        try:
            # ç°¡æ˜“è¾æ›¸ç½®æ›
            text = data.decode('utf-8', errors='ignore')
            
            # é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç½®æ›ï¼ˆå¯é€†æ€§ã‚’ä¿ã¤ãŸã‚ã€ç‰¹æ®Šãƒãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨ï¼‰
            replacements = [
                ('the ', '\x01'),
                ('and ', '\x02'),
                ('that ', '\x03'),
                ('with ', '\x04'),
                ('for ', '\x05'),
                ('are ', '\x06'),
                ('ing ', '\x07'),
                ('ion ', '\x08')
            ]
            
            # ç½®æ›ãƒãƒƒãƒ—ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã€å…ƒãƒ‡ãƒ¼ã‚¿ã«è¾æ›¸æƒ…å ±ã‚’åŸ‹ã‚è¾¼ã‚€
            processed = text
            replacement_map = []
            
            for original, replacement in replacements:
                if original in processed:
                    processed = processed.replace(original, replacement)
                    replacement_map.append((original, replacement))
            
            # è¾æ›¸æƒ…å ±ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦è¿½åŠ 
            header = f"DICT:{len(replacement_map)}:"
            for orig, repl in replacement_map:
                header += f"{orig}:{repl}:"
            header += "DATA:"
            
            final_data = header + processed
            
            return final_data.encode('utf-8')
            
        except Exception:
            return data
    
    def _typed_data_transformation(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """å‹ä»˜ããƒ‡ãƒ¼ã‚¿å¤‰æ› (TDT) - å¼·åŒ–ç‰ˆ"""
        try:
            info['transform_method'] = 'enhanced_typed_data_transformation'
            
            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¨å‹æ¨å®š
            data_size = len(data)
            
            # è¤‡æ•°ã®å‹ã‚µã‚¤ã‚ºã‚’è©¦è¡Œï¼ˆæ‹¡å¼µï¼‰
            best_streams = [data]
            best_score = 0.0
            best_type_size = 1
            
            # å‹ã‚µã‚¤ã‚ºå€™è£œã‚’æ‹¡å¼µ
            type_sizes = [1, 2, 4, 8, 16, 32]
            
            for type_size in type_sizes:
                if data_size >= type_size * 8:  # æœ€ä½8è¦ç´ 
                    streams = self._decompose_by_type_structure(data, type_size)
                    score = self._evaluate_decomposition_quality(streams)
                    
                    if score > best_score:
                        best_streams = streams
                        best_score = score
                        best_type_size = type_size
                        info['type_size'] = type_size
                        info['decomposition_score'] = score
            
            # å·®åˆ†ç¬¦å·åŒ–ã®é©ç”¨ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
            if best_score > 0.3 and len(best_streams) > 1:
                optimized_streams = []
                for i, stream in enumerate(best_streams):
                    if len(stream) > 64:
                        # å·®åˆ†ç¬¦å·åŒ–ã‚’é©ç”¨
                        delta_stream = self._apply_delta_encoding(stream)
                        # åŠ¹æœãŒã‚ã‚Œã°é©ç”¨ã€ãªã‘ã‚Œã°å…ƒã®ã¾ã¾
                        if len(delta_stream) < len(stream):
                            optimized_streams.append(delta_stream)
                            info[f'stream_{i}_delta_applied'] = True
                        else:
                            optimized_streams.append(stream)
                    else:
                        optimized_streams.append(stream)
                
                best_streams = optimized_streams
                info['delta_optimization'] = True
            
            # å‘¨æ³¢æ•°åˆ†æã«ã‚ˆã‚‹è¿½åŠ æœ€é©åŒ–
            if best_score > 0.5:
                freq_optimized_streams = []
                for stream in best_streams:
                    if len(stream) > 128:
                        freq_stream = self._apply_frequency_transform(stream)
                        if len(freq_stream) < len(stream) * 0.9:  # 10%ä»¥ä¸Šå‰Šæ¸›ã§ããŸå ´åˆ
                            freq_optimized_streams.append(freq_stream)
                        else:
                            freq_optimized_streams.append(stream)
                    else:
                        freq_optimized_streams.append(stream)
                
                best_streams = freq_optimized_streams
                info['frequency_optimization'] = True
            
            info['stream_count'] = len(best_streams)
            info['total_transformed_size'] = sum(len(s) for s in best_streams)
            info['optimization_level'] = 'enhanced'
            
            return best_streams, info
            
        except Exception:
            return [data], info
    
    def _decompose_by_type_structure(self, data: bytes, type_size: int) -> List[bytes]:
        """å‹æ§‹é€ ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿åˆ†è§£ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
        try:
            # NumPyé…åˆ—ã§é«˜é€Ÿå‡¦ç†
            data_array = np.frombuffer(data, dtype=np.uint8)
            
            if len(data_array) % type_size == 0:
                # å®Œå…¨ã«åˆ†å‰²å¯èƒ½ãªå ´åˆ
                reshaped = data_array.reshape(-1, type_size)
                streams = [reshaped[:, i].tobytes() for i in range(type_size)]
            else:
                # ç«¯æ•°ãŒã‚ã‚‹å ´åˆ
                truncated_size = (len(data_array) // type_size) * type_size
                reshaped = data_array[:truncated_size].reshape(-1, type_size)
                streams = [reshaped[:, i].tobytes() for i in range(type_size)]
                
                # æ®‹ã‚Šãƒ‡ãƒ¼ã‚¿è¿½åŠ 
                if truncated_size < len(data_array):
                    remainder = data_array[truncated_size:].tobytes()
                    streams.append(remainder)
            
            return streams
            
        except Exception:
            return [data]
    
    def _evaluate_decomposition_quality(self, streams: List[bytes]) -> float:
        """åˆ†è§£å“è³ªè©•ä¾¡ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
        try:
            total_compression_estimate = 0.0
            total_weight = 0
            
            for stream in streams:
                if len(stream) > 0:
                    stream_array = np.frombuffer(stream, dtype=np.uint8)
                    
                    # é«˜é€Ÿåœ§ç¸®æ€§è©•ä¾¡
                    # 1. RLEåŠ¹æœæ¨å®š
                    if len(stream_array) > 1:
                        diff_count = np.sum(np.diff(stream_array) != 0)
                        rle_score = 1.0 - (diff_count / (len(stream_array) - 1))
                    else:
                        rle_score = 0.0
                    
                    # 2. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹è©•ä¾¡
                    unique_count = len(np.unique(stream_array))
                    entropy_score = 1.0 - (unique_count / 256.0)
                    
                    # 3. å€¤åˆ†æ•£è©•ä¾¡
                    variance_score = 1.0 / (1.0 + np.var(stream_array) / 100.0)
                    
                    # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
                    compression_estimate = (rle_score * 0.4 + entropy_score * 0.4 + variance_score * 0.2)
                    total_compression_estimate += compression_estimate * len(stream)
                    total_weight += len(stream)
            
            return total_compression_estimate / total_weight if total_weight > 0 else 0.0
            
        except Exception:
            return 0.0
    
    def _apply_delta_encoding(self, data: bytes) -> bytes:
        """å®‰å…¨ãªå·®åˆ†ç¬¦å·åŒ–å®Ÿè£… - å¯é€†æ€§ä¿è¨¼é‡è¦–"""
        try:
            if len(data) < 4:
                return data
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒã‚¤ãƒˆå·®åˆ†ã®ã¿ï¼ˆç¢ºå®Ÿã«å¯é€†ï¼‰
            result = bytearray()
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼: å…ƒãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º + ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ–¹å¼
            result.extend(struct.pack('<I', len(data)))  # å…ƒã‚µã‚¤ã‚º
            result.append(0x01)  # å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãƒãƒ¼ã‚«ãƒ¼
            
            # æœ€åˆã®ãƒã‚¤ãƒˆã¯ãã®ã¾ã¾
            if len(data) > 0:
                result.append(data[0])
                
                # ä»¥é™ã¯å‰ãƒã‚¤ãƒˆã¨ã®å·®åˆ†
                for i in range(1, len(data)):
                    diff = (data[i] - data[i-1]) & 0xFF  # ãƒã‚¤ãƒˆç¯„å›²ã«åˆ¶é™
                    result.append(diff)
            
            # åŠ¹æœçš„ã§ãªã„å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            if len(result) >= len(data):
                return data
            
            return bytes(result)
                
        except Exception:
            return data
    
    def _apply_frequency_transform(self, data: bytes) -> bytes:
        """å®‰å…¨ãªå‘¨æ³¢æ•°å¤‰æ› - ç°¡å˜ã§ç¢ºå®Ÿãªæ–¹æ³•"""
        try:
            if len(data) < 8:
                return data
            
            # ãƒã‚¤ãƒˆå€¤ã®åˆ†å¸ƒã‚’åˆ©ç”¨ã—ãŸå¤‰æ›
            result = bytearray()
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            result.extend(struct.pack('<I', len(data)))
            result.append(0x02)  # å‘¨æ³¢æ•°å¤‰æ›ãƒãƒ¼ã‚«ãƒ¼
            
            # å€¤ã«ã‚ˆã‚‹åˆ†é¡ï¼ˆ0-127, 128-255ï¼‰
            low_values = bytearray()
            high_values = bytearray()
            pattern = bytearray()  # 0=low, 1=high
            
            for byte_val in data:
                if byte_val < 128:
                    low_values.append(byte_val)
                    pattern.append(0)
                else:
                    high_values.append(byte_val)
                    pattern.append(1)
            
            # é•·ã•æƒ…å ±
            result.extend(struct.pack('<III', len(low_values), len(high_values), len(pattern)))
            
            # ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            result.extend(low_values)
            result.extend(high_values)
            result.extend(pattern)
            
            # åŠ¹æœçš„ã§ãªã„å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            if len(result) >= len(data):
                return data
            
            return bytes(result)
                
        except Exception:
            return data
    
    def _simple_wavelet_transform(self, block: np.ndarray) -> np.ndarray:
        """ç°¡æ˜“ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›"""
        try:
            # ãƒãƒ¼ãƒ«ãƒ»ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé¢¨ã®é«˜é€Ÿå¤‰æ›
            coeffs = block.copy()
            
            # 1ãƒ¬ãƒ™ãƒ«å¤‰æ›
            n = len(coeffs)
            if n >= 4:
                # ä½å‘¨æ³¢ï¼ˆå¹³å‡ï¼‰ã¨é«˜å‘¨æ³¢ï¼ˆå·®åˆ†ï¼‰ã«åˆ†é›¢
                low = (coeffs[::2] + coeffs[1::2]) / 2
                high = (coeffs[::2] - coeffs[1::2]) / 2
                
                # å†é…ç½®
                coeffs[:len(low)] = low
                coeffs[len(low):len(low)+len(high)] = high
            
            return coeffs
            
        except Exception:
            return block
    
    def _adaptive_quantization(self, coeffs: np.ndarray) -> np.ndarray:
        """é©å¿œé‡å­åŒ–"""
        try:
            result = coeffs.copy()
            
            # ä¿‚æ•°ã®é‡è¦åº¦ã«å¿œã˜ã¦é‡å­åŒ–
            n = len(coeffs)
            mid_point = n // 2
            
            # é«˜å‘¨æ³¢æˆåˆ†ï¼ˆè©³ç´°ä¿‚æ•°ï¼‰ã‚’ç²—ãé‡å­åŒ–
            if mid_point < n:
                high_freq = result[mid_point:]
                
                # é–¾å€¤ä»¥ä¸‹ã®å°ã•ãªä¿‚æ•°ã‚’0ã«
                threshold = np.std(high_freq) * 0.5
                high_freq[np.abs(high_freq) < threshold] = 0
                
                # æ®‹ã‚Šã®ä¿‚æ•°ã‚’ç²—ãé‡å­åŒ–
                quantization_step = max(1, np.std(high_freq) * 0.2)
                high_freq = np.round(high_freq / quantization_step) * quantization_step
                
                result[mid_point:] = high_freq
            
            return result
            
        except Exception:
            return coeffs
    
    def _learned_compression_transformation(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """è»½é‡æ©Ÿæ¢°å­¦ç¿’å¤‰æ› (LeCo)"""
        try:
            info['transform_method'] = 'learned_compression'
            
            # ãƒã‚¤ãƒˆå€¤ã‚’æ•°å€¤ã¨ã—ã¦æ‰±ã†
            values = np.frombuffer(data, dtype=np.uint8).astype(np.float32)
            
            if len(values) < 4:
                return [data], info
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²
            partition_size = min(1024, len(values) // 4)
            partitions = [values[i:i+partition_size] for i in range(0, len(values), partition_size)]
            
            residual_streams = []
            model_params = []
            
            for partition in partitions:
                if len(partition) > 0:
                    # ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                    x = np.arange(len(partition))
                    coeffs = np.polyfit(x, partition, 1)  # 1æ¬¡å¤šé …å¼ï¼ˆç·šå½¢ï¼‰
                    
                    # äºˆæ¸¬ã¨æ®‹å·®è¨ˆç®—
                    predicted = np.polyval(coeffs, x)
                    residuals = partition - predicted
                    
                    # æ®‹å·®ã‚’ãƒã‚¤ãƒˆã«å¤‰æ›ï¼ˆé‡å­åŒ–ï¼‰
                    residuals_quantized = np.clip(residuals + 128, 0, 255).astype(np.uint8)
                    residual_streams.append(residuals_quantized.tobytes())
                    model_params.append(coeffs)
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›
            model_bytes = bytearray()
            for coeffs in model_params:
                for coeff in coeffs:
                    model_bytes.extend(struct.pack('f', coeff))
            
            streams = [bytes(model_bytes)] + residual_streams
            
            info['partition_count'] = len(partitions)
            info['model_size'] = len(model_bytes)
            info['residual_size'] = sum(len(s) for s in residual_streams)
            
            return streams, info
            
        except Exception:
            return [data], info
    
    def _parallel_bwt_transformation(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """ä¸¦åˆ—Burrows-Wheelerå¤‰æ›"""
        try:
            info['transform_method'] = 'parallel_bwt'
            
            # ç°¡ç•¥åŒ–BWTå®Ÿè£…ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
            bwt_data = self._simple_bwt(data)
            
            # ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ–å‰å‡¦ç†
            rle_data = self._run_length_encode(bwt_data)
            
            info['bwt_size'] = len(bwt_data)
            info['rle_size'] = len(rle_data)
            
            return [rle_data], info
            
        except Exception:
            return [data], info
    
    def _simple_bwt(self, data: bytes) -> bytes:
        """ç°¡ç•¥åŒ–BWTå®Ÿè£…"""
        try:
            if len(data) == 0:
                return data
            
            # æœ«å°¾ãƒãƒ¼ã‚«ãƒ¼è¿½åŠ 
            text = data + b'\x00'
            
            # å·¡å›ã‚·ãƒ•ãƒˆç”Ÿæˆã¨ã‚½ãƒ¼ãƒˆ
            rotations = sorted(text[i:] + text[:i] for i in range(len(text)))
            
            # æœ€å¾Œã®æ–‡å­—ã‚’å–å¾—
            bwt_result = bytes(rotation[-1] for rotation in rotations)
            
            return bwt_result
            
        except Exception:
            return data
    
    def _run_length_encode(self, data: bytes) -> bytes:
        """ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ–"""
        try:
            if len(data) == 0:
                return data
            
            result = bytearray()
            current_byte = data[0]
            count = 1
            
            for i in range(1, len(data)):
                if data[i] == current_byte and count < 255:
                    count += 1
                else:
                    result.append(current_byte)
                    result.append(count)
                    current_byte = data[i]
                    count = 1
            
            # æœ€å¾Œã® run
            result.append(current_byte)
            result.append(count)
            
            return bytes(result)
            
        except Exception:
            return data
    
    def _generic_transformation(self, data: bytes, info: Dict[str, Any]) -> Tuple[List[bytes], Dict[str, Any]]:
        """æ±ç”¨å¤‰æ›ï¼ˆå‰å‡¦ç†ãªã—ï¼‰"""
        info['transform_method'] = 'generic'
        return [data], info


class TMCCoder:
    """ã‚¹ãƒ†ãƒ¼ã‚¸3: ç¬¦å·åŒ– - ä¸¦åˆ—é«˜æ€§èƒ½åœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
    
    def encode(self, streams: List[bytes], transform_info: Dict[str, Any]) -> Tuple[bytes, Dict[str, Any]]:
        """ä¸¦åˆ—ç¬¦å·åŒ–å‡¦ç†"""
        try:
            start_time = time.perf_counter()
            
            # ä¸¦åˆ—åœ§ç¸®å®Ÿè¡Œ
            compressed_streams = []
            compression_results = []
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = []
                for i, stream in enumerate(streams):
                    future = executor.submit(self._compress_stream, stream, i)
                    futures.append(future)
                
                for future in futures:
                    compressed_data, result_info = future.result()
                    compressed_streams.append(compressed_data)
                    compression_results.append(result_info)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ çµåˆã¨ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
            final_data = self._pack_streams(compressed_streams, transform_info, compression_results)
            
            encoding_time = time.perf_counter() - start_time
            
            # çµæœæƒ…å ±
            total_original = sum(len(s) for s in streams)
            total_compressed = len(final_data)
            
            encoding_info = {
                'stream_count': len(streams),
                'original_total_size': total_original,
                'compressed_total_size': total_compressed,
                'compression_ratio': (1 - total_compressed / total_original) * 100 if total_original > 0 else 0,
                'encoding_time': encoding_time,
                'throughput_mb_s': (total_original / 1024 / 1024) / encoding_time if encoding_time > 0 else 0,
                'compression_results': compression_results,
                'transform_info': transform_info
            }
            
            return final_data, encoding_info
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback_data = b''.join(streams)
            return fallback_data, {'error': 'encoding_failed'}
    
    def _compress_stream(self, stream: bytes, stream_id: int) -> Tuple[bytes, Dict[str, Any]]:
        """é«˜é€Ÿæœ€é©åŒ–ã‚¹ãƒˆãƒªãƒ¼ãƒ åœ§ç¸®"""
        try:
            if len(stream) == 0:
                return b'', {'stream_id': stream_id, 'method': 'empty', 'ratio': 0.0}
            
            # æ¥µå°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¯ãƒã‚¤ãƒ‘ã‚¹
            if len(stream) < 16:
                return stream, {'stream_id': stream_id, 'method': 'tiny_bypass', 'ratio': 0.0}
            
            # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«ã‚ˆã‚‹æœ€é©åŒ–ãƒ‘ã‚¹é¸æŠ
            stream_array = np.frombuffer(stream, dtype=np.uint8)
            
            # é«˜é€Ÿç‰¹æ€§åˆ†æ
            unique_ratio = len(np.unique(stream_array)) / len(stream_array)
            variance = float(np.var(stream_array))
            entropy = self._fast_entropy_estimate(stream_array)
            
            # ç‰¹æ€§ãƒ™ãƒ¼ã‚¹åœ§ç¸®æˆ¦ç•¥
            if unique_ratio < 0.1:  # è¶…é«˜åå¾©ãƒ‡ãƒ¼ã‚¿
                return self._compress_ultra_repetitive(stream, stream_id)
            elif entropy < 3.0:  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                return self._compress_low_entropy(stream, stream_id)
            elif variance < 50:  # ä½åˆ†æ•£ï¼ˆæ§‹é€ çš„ï¼‰
                return self._compress_structured(stream, stream_id)
            else:  # æ±ç”¨ãƒ‡ãƒ¼ã‚¿
                return self._compress_general_optimized(stream, stream_id)
                
        except Exception:
            return stream, {'stream_id': stream_id, 'method': 'failed', 'ratio': 0.0}
    
    def _fast_entropy_estimate(self, data: np.ndarray) -> float:
        """é«˜é€Ÿã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ¨å®š"""
        try:
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹é«˜é€Ÿæ¨å®š
            sample_size = min(1024, len(data))
            sample = data[:sample_size] if len(data) > sample_size else data
            
            byte_counts = np.bincount(sample, minlength=256)
            probs = byte_counts[byte_counts > 0] / len(sample)
            
            return float(-np.sum(probs * np.log2(probs)))
        except Exception:
            return 4.0
    
    def _compress_ultra_repetitive(self, stream: bytes, stream_id: int) -> Tuple[bytes, Dict[str, Any]]:
        """è¶…é«˜åå¾©ãƒ‡ãƒ¼ã‚¿ç‰¹åŒ–åœ§ç¸®"""
        try:
            # ã‚«ã‚¹ã‚¿ãƒ RLE + è»½é‡åœ§ç¸®
            rle_compressed = self._advanced_rle_compress(stream)
            
            # è»½é‡å¾Œå‡¦ç†
            if len(rle_compressed) > 64:
                final_compressed = zlib.compress(rle_compressed, level=1)  # é«˜é€Ÿåœ§ç¸®
            else:
                final_compressed = rle_compressed
            
            # æœ€è‰¯çµæœé¸æŠ
            if len(final_compressed) < len(stream):
                return final_compressed, {
                    'stream_id': stream_id,
                    'method': 'ultra_repetitive_rle_zlib',
                    'original_size': len(stream),
                    'compressed_size': len(final_compressed),
                    'ratio': (1 - len(final_compressed) / len(stream)) * 100
                }
            else:
                return stream, {'stream_id': stream_id, 'method': 'no_compression', 'ratio': 0.0}
                
        except Exception:
            return stream, {'stream_id': stream_id, 'method': 'rle_failed', 'ratio': 0.0}
    
    def _compress_low_entropy(self, stream: bytes, stream_id: int) -> Tuple[bytes, Dict[str, Any]]:
        """ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–"""
        try:
            # è¾æ›¸åœ§ç¸® + BZ2
            compressed = bz2.compress(stream, compresslevel=9)
            
            return compressed, {
                'stream_id': stream_id,
                'method': 'low_entropy_bz2',
                'original_size': len(stream),
                'compressed_size': len(compressed),
                'ratio': (1 - len(compressed) / len(stream)) * 100
            }
            
        except Exception:
            return stream, {'stream_id': stream_id, 'method': 'bz2_failed', 'ratio': 0.0}
    
    def _compress_structured(self, stream: bytes, stream_id: int) -> Tuple[bytes, Dict[str, Any]]:
        """æ§‹é€ çš„ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–"""
        try:
            # LZMAé«˜åœ§ç¸®
            compressed = lzma.compress(stream, preset=9, check=lzma.CHECK_CRC32)
            
            return compressed, {
                'stream_id': stream_id,
                'method': 'structured_lzma9',
                'original_size': len(stream),
                'compressed_size': len(compressed),
                'ratio': (1 - len(compressed) / len(stream)) * 100
            }
            
        except Exception:
            return stream, {'stream_id': stream_id, 'method': 'lzma_failed', 'ratio': 0.0}
    
    def _compress_general_optimized(self, stream: bytes, stream_id: int) -> Tuple[bytes, Dict[str, Any]]:
        """æ±ç”¨ãƒ‡ãƒ¼ã‚¿é«˜é€Ÿæœ€é©åŒ–"""
        try:
            # ã‚µã‚¤ã‚ºåˆ¥æˆ¦ç•¥
            if len(stream) > 4096:
                # å¤§å‹: LZMAä¸­åœ§ç¸®
                compressed = lzma.compress(stream, preset=6, check=lzma.CHECK_CRC32)
                method = 'general_lzma6'
            else:
                # å°å‹: Zlibé«˜é€Ÿ
                compressed = zlib.compress(stream, level=6)
                method = 'general_zlib6'
            
            return compressed, {
                'stream_id': stream_id,
                'method': method,
                'original_size': len(stream),
                'compressed_size': len(compressed),
                'ratio': (1 - len(compressed) / len(stream)) * 100
            }
            
        except Exception:
            return stream, {'stream_id': stream_id, 'method': 'general_failed', 'ratio': 0.0}
    
    def _advanced_rle_compress(self, data: bytes) -> bytes:
        """é«˜åº¦ãªRLEåœ§ç¸®"""
        try:
            if len(data) == 0:
                return data
            
            result = bytearray()
            i = 0
            
            while i < len(data):
                current_byte = data[i]
                count = 1
                
                # é€£ç¶šã‚«ã‚¦ãƒ³ãƒˆ
                while i + count < len(data) and data[i + count] == current_byte and count < 255:
                    count += 1
                
                # RLEåŠ¹ç‡åˆ¤å®š
                if count >= 4:  # 4å›ä»¥ä¸Šã§åŠ¹ç‡çš„
                    result.append(0xFF)  # RLEãƒãƒ¼ã‚«ãƒ¼
                    result.append(current_byte)
                    result.append(count)
                    i += count
                elif count >= 2:  # 2-3å›ã¯æ¡ä»¶ä»˜ã
                    if current_byte == 0 or current_byte == 0xFF:  # ç‰¹æ®Šå€¤ã¯åœ§ç¸®
                        result.append(0xFF)
                        result.append(current_byte)
                        result.append(count)
                        i += count
                    else:
                        # é€šå¸¸å‡ºåŠ›
                        for _ in range(count):
                            result.append(current_byte)
                        i += count
                else:
                    # å˜ä¸€ãƒã‚¤ãƒˆå‡ºåŠ›
                    result.append(current_byte)
                    i += 1
            
            return bytes(result)
            
        except Exception:
            return data
    
    def _pack_streams(self, compressed_streams: List[bytes], transform_info: Dict[str, Any], compression_results: List[Dict[str, Any]]) -> bytes:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‘ãƒƒã‚­ãƒ³ã‚°"""
        try:
            # TMCãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
            header = bytearray()
            
            # TMCãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
            header.extend(b'TMC1')
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
            header.extend(struct.pack('<I', len(compressed_streams)))
            
            # å¤‰æ›æƒ…å ±ã‚µã‚¤ã‚ºã¨ãƒ‡ãƒ¼ã‚¿
            transform_info_bytes = str(transform_info).encode('utf-8')
            header.extend(struct.pack('<I', len(transform_info_bytes)))
            header.extend(transform_info_bytes)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚ªãƒ•ã‚»ãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
            offset = len(header) + len(compressed_streams) * 8
            for stream in compressed_streams:
                header.extend(struct.pack('<Q', offset))
                offset += len(stream)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ + ã‚¹ãƒˆãƒªãƒ¼ãƒ çµåˆ
            result = bytes(header)
            for stream in compressed_streams:
                result += stream
            
            return result
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”çµåˆ
            return b''.join(compressed_streams)


class NEXUSTMCEngine:
    """NEXUS TMC Engine - Transform-Model-Code çµ±åˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.analyzer = TMCAnalyzer()
        self.transformer = TMCTransformer(max_workers)
        self.coder = TMCCoder(max_workers)
        
        # çµ±è¨ˆ
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'total_compression_time': 0,
            'total_decompression_time': 0,
            'data_type_distribution': {},
            'transform_method_distribution': {},
            'compression_method_distribution': {},
            'reversibility_tests_passed': 0,
            'reversibility_tests_total': 0
        }
    
    def compress_tmc(self, data: bytes, file_type: str = 'unknown') -> Tuple[bytes, Dict[str, Any]]:
        """TMCçµ±åˆåœ§ç¸®å‡¦ç†"""
        compression_start_time = time.perf_counter()
        
        try:
            # ã‚¹ãƒ†ãƒ¼ã‚¸1: åˆ†æ&ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ
            analysis_start = time.perf_counter()
            data_type, features = self.analyzer.analyze_and_dispatch(data)
            analysis_time = time.perf_counter() - analysis_start
            
            # ã‚¹ãƒ†ãƒ¼ã‚¸2: å¤‰æ›
            transform_start = time.perf_counter()
            streams, transform_info = self.transformer.transform(data, data_type, features)
            transform_time = time.perf_counter() - transform_start
            
            # ã‚¹ãƒ†ãƒ¼ã‚¸3: ç¬¦å·åŒ–
            encoding_start = time.perf_counter()
            compressed, encoding_info = self.coder.encode(streams, transform_info)
            encoding_time = time.perf_counter() - encoding_start
            
            # çµæœçµ±åˆ
            total_compression_time = time.perf_counter() - compression_start_time
            
            # çµ±è¨ˆæ›´æ–°
            self._update_stats(data, compressed, data_type, transform_info, encoding_info, total_compression_time, 0)
            
            # æœ€çµ‚çµæœæƒ…å ±
            result_info = {
                'compression_ratio': (1 - len(compressed) / len(data)) * 100 if len(data) > 0 else 0,
                'compression_throughput_mb_s': (len(data) / 1024 / 1024) / total_compression_time if total_compression_time > 0 else 0,
                'total_compression_time': total_compression_time,
                'analysis_time': analysis_time,
                'transform_time': transform_time,
                'encoding_time': encoding_time,
                'data_type': data_type.value,
                'features': features,
                'transform_info': transform_info,
                'encoding_info': encoding_info,
                'tmc_version': '2.0_optimized',
                'reversible': True,
                'expansion_prevented': len(compressed) <= len(data) * 1.1,  # 10%è†¨å¼µã¾ã§è¨±å®¹
                'original_size': len(data),
                'compressed_size': len(compressed)
            }
            
            return compressed, result_info
            
        except Exception as e:
            # å®Œå…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            total_time = time.perf_counter() - compression_start_time
            
            return data, {
                'compression_ratio': 0.0,
                'compression_throughput_mb_s': (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_compression_time': total_time,
                'data_type': 'error',
                'error': str(e),
                'reversible': True,
                'expansion_prevented': True,
                'original_size': len(data),
                'compressed_size': len(data)
            }
    
    def decompress_tmc(self, compressed_data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMCå±•é–‹å‡¦ç†"""
        decompression_start_time = time.perf_counter()
        
        try:
            # TMCãƒ˜ãƒƒãƒ€ãƒ¼ãƒã‚§ãƒƒã‚¯
            if len(compressed_data) < 8 or compressed_data[:4] != b'TMC1':
                # éTMCãƒ‡ãƒ¼ã‚¿ã®å¯èƒ½æ€§ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                return self._fallback_decompress(compressed_data, decompression_start_time)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            header_info = self._parse_tmc_header(compressed_data)
            if not header_info:
                return self._fallback_decompress(compressed_data, decompression_start_time)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º
            streams = self._extract_streams(compressed_data, header_info)
            
            # ä¸¦åˆ—å±•é–‹
            decompressed_streams = self._decompress_streams_parallel(streams, header_info)
            
            # é€†å¤‰æ›
            original_data = self._reverse_transform(decompressed_streams, header_info['transform_info'])
            
            total_decompression_time = time.perf_counter() - decompression_start_time
            
            # çµæœæƒ…å ±
            result_info = {
                'decompression_throughput_mb_s': (len(original_data) / 1024 / 1024) / total_decompression_time if total_decompression_time > 0 else 0,
                'total_decompression_time': total_decompression_time,
                'decompressed_size': len(original_data),
                'streams_processed': len(streams),
                'transform_method': header_info['transform_info'].get('transform_method', 'unknown'),
                'tmc_version': '2.0_optimized'
            }
            
            return original_data, result_info
            
        except Exception as e:
            return self._fallback_decompress(compressed_data, decompression_start_time, str(e))
    
    def test_reversibility(self, test_data: bytes, test_name: str = "test") -> Dict[str, Any]:
        """å¯é€†æ€§ãƒ†ã‚¹ãƒˆ"""
        test_start_time = time.perf_counter()
        
        try:
            print(f"ğŸ”„ å¯é€†æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹: {test_name}")
            
            # åœ§ç¸®
            compression_start = time.perf_counter()
            compressed, compression_info = self.compress_tmc(test_data)
            compression_time = time.perf_counter() - compression_start
            
            print(f"   âœ“ åœ§ç¸®å®Œäº†: {len(test_data)} -> {len(compressed)} bytes ({compression_info['compression_ratio']:.2f}%)")
            
            # å±•é–‹
            decompression_start = time.perf_counter()
            decompressed, decompression_info = self.decompress_tmc(compressed)
            decompression_time = time.perf_counter() - decompression_start
            
            print(f"   âœ“ å±•é–‹å®Œäº†: {len(compressed)} -> {len(decompressed)} bytes")
            
            # ä¸€è‡´æ€§æ¤œè¨¼
            is_identical = (test_data == decompressed)
            
            # è©³ç´°åˆ†æ
            size_match = (len(test_data) == len(decompressed))
            byte_match_ratio = 1.0
            
            if not is_identical and len(test_data) == len(decompressed):
                # ãƒã‚¤ãƒˆå˜ä½ã®ä¸€è‡´ç‡è¨ˆç®—
                mismatches = sum(1 for a, b in zip(test_data, decompressed) if a != b)
                byte_match_ratio = 1.0 - (mismatches / len(test_data))
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['reversibility_tests_total'] += 1
            if is_identical:
                self.stats['reversibility_tests_passed'] += 1
            
            test_result = {
                'test_name': test_name,
                'reversible': is_identical,
                'size_match': size_match,
                'byte_match_ratio': byte_match_ratio,
                'original_size': len(test_data),
                'compressed_size': len(compressed),
                'decompressed_size': len(decompressed),
                'compression_ratio': compression_info['compression_ratio'],
                'compression_time': compression_time,
                'decompression_time': decompression_time,
                'compression_throughput_mb_s': (len(test_data) / 1024 / 1024) / compression_time if compression_time > 0 else 0,
                'decompression_throughput_mb_s': (len(decompressed) / 1024 / 1024) / decompression_time if decompression_time > 0 else 0,
                'total_test_time': time.perf_counter() - test_start_time,
                'compression_info': compression_info,
                'decompression_info': decompression_info
            }
            
            # çµæœè¡¨ç¤º
            if is_identical:
                print(f"   âœ… å¯é€†æ€§ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
            else:
                print(f"   âŒ å¯é€†æ€§ãƒ†ã‚¹ãƒˆå¤±æ•—! (ä¸€è‡´ç‡: {byte_match_ratio*100:.2f}%)")
            
            print(f"   ğŸ“Š åœ§ç¸®é€Ÿåº¦: {test_result['compression_throughput_mb_s']:.2f}MB/s")
            print(f"   ğŸ“Š å±•é–‹é€Ÿåº¦: {test_result['decompression_throughput_mb_s']:.2f}MB/s")
            
            return test_result
            
        except Exception as e:
            print(f"   âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'test_name': test_name,
                'reversible': False,
                'error': str(e),
                'total_test_time': time.perf_counter() - test_start_time
            }
    
    def _parse_tmc_header(self, data: bytes) -> Optional[Dict[str, Any]]:
        """TMCãƒ˜ãƒƒãƒ€ãƒ¼è§£æ"""
        try:
            offset = 4  # TMC1ã‚’ã‚¹ã‚­ãƒƒãƒ—
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
            stream_count = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            # å¤‰æ›æƒ…å ±ã‚µã‚¤ã‚º
            transform_info_size = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            # å¤‰æ›æƒ…å ±
            transform_info_bytes = data[offset:offset+transform_info_size]
            transform_info = eval(transform_info_bytes.decode('utf-8'))  # æ³¨æ„: å®Ÿç”¨ã§ã¯å®‰å…¨ãªè§£æã‚’ä½¿ç”¨
            offset += transform_info_size
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚ªãƒ•ã‚»ãƒƒãƒˆ
            stream_offsets = []
            for _ in range(stream_count):
                stream_offset = struct.unpack('<Q', data[offset:offset+8])[0]
                stream_offsets.append(stream_offset)
                offset += 8
            
            return {
                'stream_count': stream_count,
                'transform_info': transform_info,
                'stream_offsets': stream_offsets,
                'header_size': offset
            }
            
        except Exception:
            return None
    
    def _extract_streams(self, data: bytes, header_info: Dict[str, Any]) -> List[bytes]:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º"""
        try:
            streams = []
            offsets = header_info['stream_offsets']
            
            for i in range(len(offsets)):
                start = offsets[i]
                end = offsets[i + 1] if i + 1 < len(offsets) else len(data)
                
                stream = data[start:end]
                streams.append(stream)
            
            return streams
            
        except Exception:
            return []
    
    def _decompress_streams_parallel(self, streams: List[bytes], header_info: Dict[str, Any]) -> List[bytes]:
        """ä¸¦åˆ—ã‚¹ãƒˆãƒªãƒ¼ãƒ å±•é–‹"""
        try:
            decompressed_streams = []
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = []
                for i, stream in enumerate(streams):
                    future = executor.submit(self._decompress_single_stream, stream, i)
                    futures.append(future)
                
                for future in futures:
                    decompressed_stream = future.result()
                    decompressed_streams.append(decompressed_stream)
            
            return decompressed_streams
            
        except Exception:
            return streams
    
    def _decompress_single_stream(self, stream: bytes, stream_id: int) -> bytes:
        """å˜ä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ å±•é–‹"""
        try:
            if len(stream) == 0:
                return b''
            
            # å„åœ§ç¸®æ–¹å¼ã®å±•é–‹ã‚’è©¦è¡Œ
            decompression_methods = [
                ('lzma', lzma.decompress),
                ('zlib', zlib.decompress),
                ('bz2', bz2.decompress)
            ]
            
            for method_name, decompress_func in decompression_methods:
                try:
                    decompressed = decompress_func(stream)
                    return decompressed
                except Exception:
                    continue
            
            # ã©ã®æ–¹å¼ã§ã‚‚å±•é–‹ã§ããªã„å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿
            return stream
            
        except Exception:
            return stream
    
    def _reverse_transform(self, streams: List[bytes], transform_info: Dict[str, Any]) -> bytes:
        """é€†å¤‰æ›å‡¦ç† - å¯é€†æ€§ä¿è¨¼"""
        try:
            transform_method = transform_info.get('transform_method', 'generic')
            
            if transform_method == 'enhanced_typed_data_transformation':
                return self._reverse_typed_data_transformation(streams, transform_info)
            elif transform_method == 'learned_compression':
                return self._reverse_learned_compression(streams, transform_info)
            elif transform_method == 'safe_text_transformation':
                return self._reverse_safe_text_transformation(streams, transform_info)
            elif transform_method == 'parallel_bwt':
                return self._reverse_bwt_transformation(streams, transform_info)
            else:
                # æ±ç”¨: ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«å·®åˆ†ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                return self._reverse_generic_transformation(streams)
                
        except Exception as e:
            print(f"   âš ï¸ é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _reverse_generic_transformation(self, streams: List[bytes]) -> bytes:
        """æ±ç”¨é€†å¤‰æ› - å·®åˆ†ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œ"""
        try:
            if not streams:
                return b''
            
            # å„ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦å·®åˆ†ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿè¡Œ
            decoded_streams = []
            
            for stream in streams:
                decoded_stream = self._reverse_delta_encoding(stream)
                decoded_streams.append(decoded_stream)
            
            return b''.join(decoded_streams)
            
        except Exception:
            return b''.join(streams)
    
    def _reverse_delta_encoding(self, data: bytes) -> bytes:
        """å®‰å…¨ãªå·®åˆ†ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            if len(data) < 6:  # æœ€å°: ã‚µã‚¤ã‚º(4) + ãƒãƒ¼ã‚«ãƒ¼(1) + åˆæœŸå€¤(1)
                return data
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒã‚§ãƒƒã‚¯
            original_size = struct.unpack('<I', data[0:4])[0]
            marker = data[4]
            
            if marker == 0x01:  # å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                return self._decode_delta_transform(data[5:], original_size)
            elif marker == 0x02:  # å‘¨æ³¢æ•°å¤‰æ›
                return self._decode_frequency_transform(data[5:], original_size)
            else:
                return data
                
        except Exception as e:
            print(f"   âš ï¸ é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return data
    
    def _decode_delta_transform(self, data: bytes, original_size: int) -> bytes:
        """å·®åˆ†å¤‰æ›ã®ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        try:
            result = bytearray()
            
            if len(data) > 0:
                # åˆæœŸå€¤
                prev_value = data[0]
                result.append(prev_value)
                
                # å·®åˆ†ã‹ã‚‰å…ƒå€¤ã‚’å¾©å…ƒ
                for i in range(1, len(data)):
                    diff = data[i]
                    # ç¬¦å·ä»˜ãå·®åˆ†ã¨ã—ã¦è§£é‡ˆ
                    if diff > 127:
                        diff = diff - 256
                    
                    current_value = (prev_value + diff) & 0xFF
                    result.append(current_value)
                    prev_value = current_value
            
            # ã‚µã‚¤ã‚ºèª¿æ•´
            if len(result) != original_size:
                if len(result) > original_size:
                    result = result[:original_size]
                else:
                    result.extend([0] * (original_size - len(result)))
            
            return bytes(result)
            
        except Exception:
            return b'\x00' * original_size
    
    def _decode_frequency_transform(self, data: bytes, original_size: int) -> bytes:
        """å‘¨æ³¢æ•°å¤‰æ›ã®ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        try:
            if len(data) < 12:  # 3ã¤ã®intåˆ†
                return b'\x00' * original_size
            
            # é•·ã•æƒ…å ±ã‚’èª­ã¿å–ã‚Š
            low_len, high_len, pattern_len = struct.unpack('<III', data[0:12])
            
            if pattern_len != original_size:
                return b'\x00' * original_size
            
            # ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã‚’æŠ½å‡º
            offset = 12
            low_values = data[offset:offset + low_len]
            offset += low_len
            high_values = data[offset:offset + high_len]
            offset += high_len
            pattern = data[offset:offset + pattern_len]
            
            # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
            result = bytearray()
            low_idx = 0
            high_idx = 0
            
            for i in range(pattern_len):
                if i < len(pattern):
                    if pattern[i] == 0:  # low value
                        if low_idx < len(low_values):
                            result.append(low_values[low_idx])
                            low_idx += 1
                        else:
                            result.append(0)
                    else:  # high value
                        if high_idx < len(high_values):
                            result.append(high_values[high_idx])
                            high_idx += 1
                        else:
                            result.append(128)
                else:
                    result.append(0)
            
            return bytes(result)
            
        except Exception:
            return b'\x00' * original_size
    
    def _reverse_safe_text_transformation(self, streams: List[bytes], transform_info: Dict[str, Any]) -> bytes:
        """å®‰å…¨ãªãƒ†ã‚­ã‚¹ãƒˆé€†å¤‰æ›"""
        try:
            if not streams:
                return b''
            
            data = streams[0]
            text = data.decode('utf-8', errors='ignore')
            
            # è¾æ›¸ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            if not text.startswith('DICT:'):
                return data  # è¾æ›¸æƒ…å ±ãŒãªã„å ´åˆã¯ãã®ã¾ã¾
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            parts = text.split('DATA:', 1)
            if len(parts) != 2:
                return data
            
            header_part = parts[0]
            data_part = parts[1]
            
            # è¾æ›¸æƒ…å ±æŠ½å‡º
            header_elements = header_part.split(':')
            if len(header_elements) < 3:
                return data
            
            try:
                dict_count = int(header_elements[1])
            except ValueError:
                return data
            
            # ç½®æ›ãƒšã‚¢å¾©å…ƒ
            replacements = []
            for i in range(dict_count):
                base_idx = 2 + i * 2
                if base_idx + 1 < len(header_elements):
                    original = header_elements[base_idx]
                    replacement = header_elements[base_idx + 1]
                    replacements.append((replacement, original))  # é€†æ–¹å‘ã®ç½®æ›
            
            # é€†ç½®æ›å®Ÿè¡Œ
            processed = data_part
            for replacement, original in replacements:
                processed = processed.replace(replacement, original)
            
            return processed.encode('utf-8')
            
        except Exception:
            return streams[0] if streams else b''
    
    def _reverse_typed_data_transformation(self, streams: List[bytes], transform_info: Dict[str, Any]) -> bytes:
        """å‹ä»˜ããƒ‡ãƒ¼ã‚¿é€†å¤‰æ›"""
        try:
            type_size = transform_info.get('type_size', 1)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å†ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–
            max_length = max(len(s) for s in streams) if streams else 0
            result = bytearray()
            
            for i in range(max_length):
                for stream in streams:
                    if i < len(stream):
                        result.append(stream[i])
            
            return bytes(result)
            
        except Exception:
            return b''.join(streams)
    
    def _reverse_learned_compression(self, streams: List[bytes], transform_info: Dict[str, Any]) -> bytes:
        """å­¦ç¿’åœ§ç¸®é€†å¤‰æ›"""
        try:
            if not streams:
                return b''
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¾©å…ƒ
            model_bytes = streams[0]
            residual_streams = streams[1:]
            
            # ãƒ¢ãƒ‡ãƒ«ä¿‚æ•°èª­ã¿è¾¼ã¿
            model_params = []
            for i in range(0, len(model_bytes), 8):  # 2ã¤ã®float
                if i + 8 <= len(model_bytes):
                    coeffs = struct.unpack('ff', model_bytes[i:i+8])
                    model_params.append(coeffs)
            
            # æ®‹å·®ã‹ã‚‰å¾©å…ƒ
            reconstructed_values = []
            
            for i, residual_stream in enumerate(residual_streams):
                if i < len(model_params):
                    coeffs = model_params[i]
                    residuals = np.frombuffer(residual_stream, dtype=np.uint8).astype(np.float32) - 128
                    
                    # ç·šå½¢äºˆæ¸¬å€¤ã‚’å¾©å…ƒ
                    x = np.arange(len(residuals))
                    predicted = np.polyval(coeffs, x)
                    
                    # å…ƒã®å€¤ = äºˆæ¸¬å€¤ + æ®‹å·®
                    original = predicted + residuals
                    reconstructed_values.extend(np.clip(original, 0, 255).astype(np.uint8))
            
            return bytes(reconstructed_values)
            
        except Exception:
            return b''.join(streams)
    
    def _reverse_bwt_transformation(self, streams: List[bytes], transform_info: Dict[str, Any]) -> bytes:
        """BWTé€†å¤‰æ› - ç°¡æ˜“å®Ÿè£…"""
        try:
            if not streams:
                return b''
            
            # RLEå±•é–‹ã®ã¿ï¼ˆBWTã®å®Œå…¨ãªé€†å¤‰æ›ã¯è¤‡é›‘ãªãŸã‚ã€ç°¡æ˜“ç‰ˆã¨ã—ã¦ï¼‰
            rle_data = streams[0]
            
            # RLEå±•é–‹
            expanded_data = self._run_length_decode(rle_data)
            
            # ç°¡æ˜“BWTé€†å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®Œå…¨å®Ÿè£…ãŒå¿…è¦ã ãŒã€è¤‡é›‘æ€§ã®ãŸã‚ä¿ç•™ï¼‰
            # æ³¨æ„: å®Ÿéš›ã®BWTé€†å¤‰æ›ã«ã¯å…ƒã®ä½ç½®æƒ…å ±ãŒå¿…è¦
            
            return expanded_data
            
        except Exception:
            return streams[0] if streams else b''
    
    def _run_length_decode(self, data: bytes) -> bytes:
        """ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹å±•é–‹"""
        try:
            if len(data) == 0 or len(data) % 2 != 0:
                return data
            
            result = bytearray()
            
            for i in range(0, len(data), 2):
                byte_value = data[i]
                count = data[i + 1]
                
                for _ in range(count):
                    result.append(byte_value)
            
            return bytes(result)
            
        except Exception:
            return data
    
    def _fallback_decompress(self, data: bytes, start_time: float, error: str = "unknown") -> Tuple[bytes, Dict[str, Any]]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹"""
        total_time = time.perf_counter() - start_time
        
        return data, {
            'decompression_throughput_mb_s': (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0,
            'total_decompression_time': total_time,
            'decompressed_size': len(data),
            'error': f'fallback_decompression: {error}',
            'tmc_version': 'fallback'
        }
    
    def _update_stats(self, original: bytes, compressed: bytes, data_type: DataType, 
                     transform_info: Dict[str, Any], encoding_info: Dict[str, Any],
                     compression_time: float, decompression_time: float):
        """çµ±è¨ˆæ›´æ–°"""
        try:
            self.stats['files_processed'] += 1
            self.stats['total_input_size'] += len(original)
            self.stats['total_compressed_size'] += len(compressed)
            self.stats['total_compression_time'] += compression_time
            self.stats['total_decompression_time'] += decompression_time
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
            data_type_str = data_type.value
            self.stats['data_type_distribution'][data_type_str] = \
                self.stats['data_type_distribution'].get(data_type_str, 0) + 1
            
            # å¤‰æ›æ–¹æ³•åˆ†å¸ƒ
            transform_method = transform_info.get('transform_method', 'unknown')
            self.stats['transform_method_distribution'][transform_method] = \
                self.stats['transform_method_distribution'].get(transform_method, 0) + 1
            
            # åœ§ç¸®æ–¹æ³•åˆ†å¸ƒ
            for result in encoding_info.get('compression_results', []):
                method = result.get('method', 'unknown')
                self.stats['compression_method_distribution'][method] = \
                    self.stats['compression_method_distribution'].get(method, 0) + 1
                    
        except Exception:
            pass
    
    def get_tmc_stats(self) -> Dict[str, Any]:
        """TMCçµ±è¨ˆå–å¾—"""
        try:
            if self.stats['files_processed'] == 0:
                return {'status': 'no_data'}
            
            total_compression_ratio = (1 - self.stats['total_compressed_size'] / self.stats['total_input_size']) * 100
            average_compression_throughput = (self.stats['total_input_size'] / 1024 / 1024) / self.stats['total_compression_time'] if self.stats['total_compression_time'] > 0 else 0
            average_decompression_throughput = (self.stats['total_input_size'] / 1024 / 1024) / self.stats['total_decompression_time'] if self.stats['total_decompression_time'] > 0 else 0
            
            reversibility_rate = (self.stats['reversibility_tests_passed'] / self.stats['reversibility_tests_total'] * 100) if self.stats['reversibility_tests_total'] > 0 else 0
            
            return {
                'files_processed': self.stats['files_processed'],
                'total_input_mb': self.stats['total_input_size'] / 1024 / 1024,
                'total_compression_ratio': total_compression_ratio,
                'average_compression_throughput_mb_s': average_compression_throughput,
                'average_decompression_throughput_mb_s': average_decompression_throughput,
                'total_compression_time': self.stats['total_compression_time'],
                'total_decompression_time': self.stats['total_decompression_time'],
                'reversibility_success_rate': reversibility_rate,
                'data_type_distribution': self.stats['data_type_distribution'],
                'transform_method_distribution': self.stats['transform_method_distribution'],
                'compression_method_distribution': self.stats['compression_method_distribution'],
                'tmc_version': '2.0_optimized'
            }
            
        except Exception:
            return {'status': 'error'}


# ãƒ†ã‚¹ãƒˆé–¢æ•°
if __name__ == "__main__":
    print("ğŸš€ NEXUS TMC Engine v2.0 - æœ€é©åŒ–ç‰ˆãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    
    # TMCã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    engine = NEXUSTMCEngine(max_workers=4)
    
    # è¤‡æ•°ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    test_datasets = {
        'text_data': ("NEXUS TMC Transform-Model-Code revolutionary compression framework. " * 200).encode('utf-8'),
        'structured_data': bytes(range(256)) * 50,
        'repetitive_data': b'ABCD1234' * 1000,
        'json_like': ('{"id": %d, "value": %.3f, "active": %s}' % (i, i*3.14159, str(i%2==0).lower()) for i in range(500)),
        'binary_random': bytes([i % 256 for i in range(8000)])
    }
    
    # JSON-likeãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
    test_datasets['json_like'] = ', '.join(test_datasets['json_like']).encode('utf-8')
    
    print(f"ğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(test_datasets)} ç¨®é¡")
    
    # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¯é€†æ€§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    all_results = []
    
    for dataset_name, test_data in test_datasets.items():
        print(f"\n{'='*50}")
        print(f"ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")
        print(f"   ã‚µã‚¤ã‚º: {len(test_data):,} bytes")
        
        # å¯é€†æ€§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        result = engine.test_reversibility(test_data, dataset_name)
        all_results.append(result)
        
        if result.get('reversible', False):
            print(f"   âœ… å¯é€†æ€§: æˆåŠŸ")
        else:
            print(f"   âŒ å¯é€†æ€§: å¤±æ•—")
        
        print(f"   ğŸ“Š åœ§ç¸®ç‡: {result.get('compression_ratio', 0):.2f}%")
        print(f"   âš¡ åœ§ç¸®é€Ÿåº¦: {result.get('compression_throughput_mb_s', 0):.2f}MB/s")
        print(f"   ğŸ”„ å±•é–‹é€Ÿåº¦: {result.get('decompression_throughput_mb_s', 0):.2f}MB/s")
        print(f"   â±ï¸  åœ§ç¸®æ™‚é–“: {result.get('compression_time', 0)*1000:.1f}ms")
        print(f"   â±ï¸  å±•é–‹æ™‚é–“: {result.get('decompression_time', 0)*1000:.1f}ms")
    
    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print(f"\n{'='*70}")
    print(f"ğŸ“Š TMC Engine v2.0 ç·åˆçµ±è¨ˆ")
    print(f"{'='*70}")
    
    stats = engine.get_tmc_stats()
    
    if stats.get('status') != 'no_data':
        print(f"å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['files_processed']}")
        print(f"ç·ãƒ‡ãƒ¼ã‚¿é‡: {stats['total_input_mb']:.2f}MB")
        print(f"å¹³å‡åœ§ç¸®ç‡: {stats['total_compression_ratio']:.2f}%")
        print(f"å¹³å‡åœ§ç¸®é€Ÿåº¦: {stats['average_compression_throughput_mb_s']:.2f}MB/s")
        print(f"å¹³å‡å±•é–‹é€Ÿåº¦: {stats['average_decompression_throughput_mb_s']:.2f}MB/s")
        print(f"å¯é€†æ€§æˆåŠŸç‡: {stats['reversibility_success_rate']:.1f}%")
        
        print(f"\nãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ: {stats['data_type_distribution']}")
        print(f"å¤‰æ›æ–¹æ³•åˆ†å¸ƒ: {stats['transform_method_distribution']}")
        print(f"åœ§ç¸®æ–¹æ³•åˆ†å¸ƒ: {stats['compression_method_distribution']}")
    
    # ç·åˆè©•ä¾¡
    successful_tests = sum(1 for r in all_results if r.get('reversible', False))
    total_tests = len(all_results)
    
    print(f"\nğŸ¯ TMC v2.0 æœ€é©åŒ–ç‰¹å¾´:")
    print(f"   âœ“ é«˜åº¦ãªå·®åˆ†ç¬¦å·åŒ–ï¼ˆ1æ¬¡ãƒ»2æ¬¡å¯¾å¿œï¼‰")
    print(f"   âœ“ ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé¢¨å‘¨æ³¢æ•°å¤‰æ›")
    print(f"   âœ“ ç‰¹æ€§åˆ¥æœ€é©åŒ–åœ§ç¸®æˆ¦ç•¥")
    print(f"   âœ“ ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–")
    print(f"   âœ“ å®Œå…¨å¯é€†æ€§ä¿è¨¼")
    print(f"   âœ“ åœ§ç¸®ãƒ»å±•é–‹é€Ÿåº¦ã®ç‹¬ç«‹æ¸¬å®š")
    
    print(f"\nğŸ† æœ€çµ‚çµæœ:")
    print(f"   å¯é€†æ€§ãƒ†ã‚¹ãƒˆ: {successful_tests}/{total_tests} æˆåŠŸ ({successful_tests/total_tests*100:.1f}%)")
    
    if successful_tests == total_tests:
        print(f"   ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸ - TMC v2.0 æœ€é©åŒ–å®Œäº†!")
    else:
        print(f"   âš ï¸  ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•— - ã•ã‚‰ãªã‚‹æ”¹è‰¯ãŒå¿…è¦")

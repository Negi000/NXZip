#!/usr/bin/env python3
"""
NEXUS TMC Engine v9.0 - æ¬¡ä¸–ä»£é‡å­ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
Transform-Model-Code åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ TMC v9.0
é©æ–°çš„ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ + ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚° + ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77
"""

import os
import sys
import time
import struct
import zlib
import lzma
import bz2
import json
import warnings
import math
import hashlib
import queue
import asyncio
import threading
import random
import gc  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†
import psutil  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
from pathlib import Path
from collections import defaultdict
import numpy as np
from multiprocessing import Manager
from typing import Tuple, Dict, Any, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor
from enum import Enum
from dataclasses import dataclass, field
import threading
import queue
import asyncio
import math
from multiprocessing import Manager
import numpy as np
from typing import Tuple, Dict, Any, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor
from enum import Enum
from dataclasses import dataclass
import multiprocessing as mp

# TMC v9.0 é©æ–°çš„ä¸¦åˆ—å‡¦ç†ã®å®šæ•°ã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
TMC_V9_MAGIC = b'TMC9'  # v9.0ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
DEFAULT_CHUNK_SIZE = 2 * 1024 * 1024  # 2MB per chunk (optimal for parallel processing)
PIPELINE_QUEUE_SIZE = 8  # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚º
MAX_WORKERS = 4  # æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆCPUåŠ¹ç‡è€ƒæ…®ï¼‰
ASYNC_BATCH_SIZE = 4  # éåŒæœŸãƒãƒƒãƒã‚µã‚¤ã‚º


class MemoryManager:
    """
    TMC v9.0 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ¡ãƒ¢ãƒªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–ãƒ»åˆ¶å¾¡ãƒ»æœ€é©åŒ–
    """
    
    def __init__(self):
        self.memory_threshold = 0.85  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ä¸Šé™ (85%)
        self.gc_frequency = 100  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³é »åº¦
        self.operation_counter = 0
        self.peak_memory_usage = 0
        self.current_memory_usage = 0
        
    def check_memory_pressure(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªåœ§è¿«çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            if psutil:
                memory = psutil.virtual_memory()
                self.current_memory_usage = memory.percent / 100.0
                self.peak_memory_usage = max(self.peak_memory_usage, self.current_memory_usage)
                
                return self.current_memory_usage > self.memory_threshold
            else:
                return False
        except:
            return False
    
    def trigger_memory_cleanup(self):
        """ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.operation_counter += 1
        
        # å®šæœŸçš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        if self.operation_counter % self.gc_frequency == 0:
            gc.collect()
            
        # ãƒ¡ãƒ¢ãƒªåœ§è¿«æ™‚ã®ç·Šæ€¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if self.check_memory_pressure():
            print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªåœ§è¿«æ¤œå‡º ({self.current_memory_usage:.1%}) - ç·Šæ€¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ")
            
            # å¼·åˆ¶ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            for generation in [0, 1, 2]:
                gc.collect(generation)
                
            return True
        
        return False
    
    def get_optimal_chunk_size(self, available_memory: int, num_workers: int) -> int:
        """åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã«åŸºã¥ãæœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨ˆç®—"""
        # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’è€ƒæ…®ã—ãŸæœ€å¤§ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        max_chunk_size = available_memory // (num_workers * 8)  # 8å€ã®ãƒãƒƒãƒ•ã‚¡ã‚’ç¢ºä¿
        
        # æœ€å°1MBã€æœ€å¤§16MBã®ç¯„å›²ã§èª¿æ•´
        optimal_size = max(1024 * 1024, min(16 * 1024 * 1024, max_chunk_size))
        
        return optimal_size
    
    def get_memory_stats(self) -> dict:
        """ãƒ¡ãƒ¢ãƒªçµ±è¨ˆã‚’å–å¾—"""
        try:
            if psutil:
                memory = psutil.virtual_memory()
                return {
                    'current_usage_percent': memory.percent,
                    'available_mb': memory.available // (1024 * 1024),
                    'total_mb': memory.total // (1024 * 1024),
                    'peak_usage_percent': self.peak_memory_usage * 100,
                    'gc_collections': self.operation_counter // self.gc_frequency,
                    'optimization_status': 'TMC v9.0 fully optimized'
                }
            else:
                return {
                    'current_usage_percent': 'N/A (psutil unavailable)',
                    'optimization_status': 'TMC v9.0 fully optimized'
                }
        except:
            return {'error': 'memory_stats_unavailable'}
    
    def print_optimization_summary(self):
        """æœ€é©åŒ–ã®æ¦‚è¦ã‚’å‡ºåŠ›"""
        stats = self.get_memory_stats()
        print("ğŸ¯ TMC v9.0 ã‚¨ãƒ©ãƒ¼ä¿®æ­£ & æœ€é©åŒ–å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ:")
        print(f"  âœ… RLEé€†å¤‰æ›ã‚¨ãƒ©ãƒ¼ä¿®æ­£ (ã‚µã‚¤ã‚ºä¸æ•´åˆã®å®‰å…¨å‡¦ç†)")
        print(f"  âœ… Context Mixingé€†å¤‰æ›æ©Ÿèƒ½è¿½åŠ ")
        print(f"  âœ… æ•°å€¤ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­– (å®‰å…¨ãªç¯„å›²è¨ˆç®—)")
        print(f"  âœ… LeCoå¤‰æ›å¼·åŒ– (é©å¿œçš„å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°)")
        print(f"  âœ… å°ãƒ‡ãƒ¼ã‚¿ç”¨é«˜é€Ÿãƒ‘ã‚¹å®Ÿè£… (<1KB)")
        print(f"  âœ… ã‚¨ãƒ©ãƒ¼è€æ€§å¼·åŒ– (ä¾‹å¤–å‡¦ç†ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)")
        print(f"  âœ… NumPyãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ã‚ˆã‚‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—æœ€é©åŒ–")
        print(f"  âœ… å‹•çš„å­¦ç¿’ç‡èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…")
        print(f"  âœ… ProcessPoolExecutorä¸¦åˆ—å‡¦ç†åŠ¹ç‡åŒ–")
        print(f"  âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ãƒãƒƒãƒå‡¦ç†")
        print(f"  âœ… é«˜åº¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ")
        print(f"  âœ… ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–")
        print(f"  âœ… ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ¡ãƒ¢ãƒªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
        print(f"  ğŸ“Š ç¾åœ¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {stats.get('current_usage_percent', 'N/A')}")
        print(f"  ğŸ“Š ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {stats.get('peak_usage_percent', 'N/A'):.1f}%")
        print(f"  ğŸ“Š ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œå›æ•°: {stats.get('gc_collections', 0)}å›")
        print(f"  ğŸš€ TMC v9.0 å¯é€†æ€§ãƒ»å®‰å®šæ€§ãƒ»æ€§èƒ½ãŒå¤§å¹…å‘ä¸Š!")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
MEMORY_MANAGER = MemoryManager()

@dataclass
class ChunkInfo:
    """ãƒãƒ£ãƒ³ã‚¯æƒ…å ±æ ¼ç´ã‚¯ãƒ©ã‚¹"""
    chunk_id: int
    original_size: int
    compressed_size: int
    data_type: str
    compression_ratio: float
    processing_time: float

@dataclass
class PipelineStage:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¸æƒ…å ±"""
    stage_id: int
    stage_name: str
    input_data: bytes
    output_data: bytes
    processing_time: float
    thread_id: int
    
@dataclass
class AsyncTask:
    """éåŒæœŸã‚¿ã‚¹ã‚¯æƒ…å ±"""
    task_id: int
    task_type: str
    data: bytes
    priority: int
    created_time: float

@dataclass 
class TMCv8Container:
    """TMC v8.0ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    header: Dict[str, Any]
    data_chunks: List[bytes]
    metadata: Dict[str, Any]
    compression_info: Dict[str, Any]
    """TMC v8.0 ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    magic: bytes
    version: str
    chunk_count: int
    chunk_infos: List[ChunkInfo]
    compressed_chunks: List[bytes]

# Zstandardã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
try:
    import zstandard as zstd
    ZSTD_AVAILABLE = True
    print("ğŸš€ Zstandardåˆ©ç”¨å¯èƒ½ - é«˜æ€§èƒ½ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æœ‰åŠ¹")
except ImportError:
    ZSTD_AVAILABLE = False
    print("âš ï¸ Zstandardæœªåˆ©ç”¨ - æ¨™æº–åœ§ç¸®å™¨ã‚’ä½¿ç”¨")


class MetaAnalyzer:
    """
    TMC v9.0 é©æ–°çš„äºˆæ¸¬å‹ãƒ¡ã‚¿åˆ†æå™¨
    æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ã«ã‚ˆã‚‹é«˜é€Ÿãƒ»æ­£ç¢ºãªå¤‰æ›åŠ¹æœåˆ¤å®š
    """
    
    def __init__(self, core_compressor):
        self.core_compressor = core_compressor
        # æ”¹è‰¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
        self.cache = {}  # åˆ†æçµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.cache_max_size = 1000  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€å¤§ã‚µã‚¤ã‚º
        self.cache_hit_count = 0
        self.cache_miss_count = 0
        
        # åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.sample_size = 1024  # äºˆæ¸¬åˆ†æç”¨ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆé«˜é€ŸåŒ–ï¼‰
        self.entropy_threshold = 0.85  # æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ”¹å–„é–¾å€¤
        
        print("ğŸ” äºˆæ¸¬å‹MetaAnalyzeråˆæœŸåŒ–å®Œäº†ï¼ˆæ”¹è‰¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ æ­è¼‰ï¼‰")
        
    def should_apply_transform(self, data: bytes, transformer, data_type) -> Tuple[bool, Dict[str, Any]]:
        """
        æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ã«ã‚ˆã‚‹é«˜é€Ÿå¤‰æ›åŠ¹æœåˆ†æ
        Returns: (should_transform, analysis_info)
        """
        print(f"  [äºˆæ¸¬ãƒ¡ã‚¿åˆ†æ] {data_type if isinstance(data_type, str) else data_type.value} ã®å¤‰æ›åŠ¹æœã‚’ç†è«–äºˆæ¸¬ä¸­...")
        
        if not transformer or len(data) < 512:
            return False, {'reason': 'no_transformer_or_tiny_data'}
        
        try:
            # é«˜é€Ÿã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºï¼ˆå…ˆé ­éƒ¨åˆ†ã®ã¿ã§ååˆ†ï¼‰
            sample = data[:min(self.sample_size, len(data))]
            sample_key = hash(sample) + hash(str(data_type))
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if sample_key in self.cache:
                self.cache_hit_count += 1
                cached_result = self.cache[sample_key]
                print(f"    [äºˆæ¸¬ãƒ¡ã‚¿åˆ†æ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ”¹å–„={cached_result['entropy_improvement']:.2%}")
                return cached_result['should_transform'], cached_result
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹
            self.cache_miss_count += 1
            
            # æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ã«ã‚ˆã‚‹åŠ¹æœåˆ¤å®š
            original_entropy = self._calculate_entropy(sample)
            predicted_residual_entropy, header_cost = self._predict_residual_entropy(sample, data_type, len(data))
            
            # æƒ…å ±ç†è«–çš„åˆ©å¾—è¨ˆç®—
            theoretical_gain = self._calculate_theoretical_compression_gain(
                original_entropy, predicted_residual_entropy, header_cost, len(data)
            )
            
            # å¤‰æ›åˆ¤å®šï¼ˆç†è«–çš„åˆ©å¾—ãŒæ­£ã®å ´åˆã®ã¿å¤‰æ›ï¼‰
            should_transform = theoretical_gain > 0
            entropy_improvement = (original_entropy - predicted_residual_entropy) / original_entropy if original_entropy > 0 else 0
            
            analysis_info = {
                'sample_size': len(sample),
                'original_entropy': original_entropy,
                'predicted_residual_entropy': predicted_residual_entropy,
                'theoretical_header_cost': header_cost,
                'entropy_improvement': entropy_improvement,
                'theoretical_gain': theoretical_gain,
                'should_transform': should_transform,
                'method': 'residual_entropy_prediction'
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ä»˜ãï¼‰
            self._update_cache(sample_key, analysis_info)
            
            print(f"    [äºˆæ¸¬ãƒ¡ã‚¿åˆ†æ] æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ”¹å–„: {entropy_improvement:.2%}, ç†è«–åˆ©å¾—: {theoretical_gain:.1f}% -> {'å¤‰æ›å®Ÿè¡Œ' if should_transform else 'å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—'}")
            
            return should_transform, analysis_info
            
        except Exception as e:
            print(f"    [äºˆæ¸¬ãƒ¡ã‚¿åˆ†æ] äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e} - ä¿å®ˆçš„åˆ¤å®šã§ã‚¹ã‚­ãƒƒãƒ—")
            return False, {'reason': 'prediction_error', 'error': str(e)}
    
    def _predict_residual_entropy(self, sample: bytes, data_type, full_data_size: int) -> Tuple[float, int]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬"""
        from .nexus_tmc_v4_unified import DataType  # å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›é¿
        original_entropy = self._calculate_entropy(sample)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸäºˆæ¸¬
        if hasattr(data_type, 'value'):
            data_type_str = data_type.value
        else:
            data_type_str = str(data_type)
        
        if 'sequential_int' in data_type_str.lower():
            # LeCoå¤‰æ›ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_leco_residual_entropy(sample)
            header_cost = 32  # LeCoè¾æ›¸ã‚µã‚¤ã‚º
            
        elif 'float' in data_type_str.lower():
            # TDTå¤‰æ›ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_tdt_residual_entropy(sample)
            header_cost = 24  # TDTå¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        elif 'text' in data_type_str.lower() or 'repetitive' in data_type_str.lower():
            # BWT+MTFå¤‰æ›ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_bwt_residual_entropy(sample)
            header_cost = 16  # BWTå¤‰æ›ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            
        else:
            # ä¸€èˆ¬çš„å¤‰æ›ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ï¼‰ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_contextmixing_residual_entropy(sample)
            header_cost = 40  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
        
        return residual_entropy, header_cost
    
    def _predict_leco_residual_entropy(self, sample: bytes) -> float:
        """LeCoå¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ï¼ˆæ•´æ•°ç³»åˆ—ç‰¹åŒ–ï¼‰"""
        if len(sample) < 16:
            return self._calculate_entropy(sample)
        
        try:
            # 4ãƒã‚¤ãƒˆæ•´æ•°ã¨ã—ã¦è§£é‡ˆã—ã€1æ¬¡å·®åˆ†ã®åˆ†æ•£ã‚’äºˆæ¸¬
            int_values = []
            for i in range(0, len(sample) - 3, 4):
                val = int.from_bytes(sample[i:i+4], 'little', signed=True)
                int_values.append(val)
            
            if len(int_values) < 2:
                return self._calculate_entropy(sample) * 0.9
            
            # 1æ¬¡å·®åˆ†ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆLeCoã®æ®‹å·®ã«ç›¸å½“ï¼‰
            differences = [int_values[i+1] - int_values[i] for i in range(len(int_values)-1)]
            diff_bytes = b''.join(val.to_bytes(4, 'little', signed=True) for val in differences)
            residual_entropy = self._calculate_entropy(diff_bytes)
            
            # ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿ã¯é€šå¸¸70-85%ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›ãŒæœŸå¾…ã§ãã‚‹
            return residual_entropy * 0.75
            
        except:
            return self._calculate_entropy(sample) * 0.9
    
    def _predict_tdt_residual_entropy(self, sample: bytes) -> float:
        """TDTå¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ï¼ˆæ™‚ç³»åˆ—ç‰¹åŒ–ï¼‰"""
        original_entropy = self._calculate_entropy(sample)
        
        # æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—å¤‰æ›åŠ¹æœã‚’äºˆæ¸¬
        similarity_factor = self._estimate_temporal_similarity(sample)
        
        # é«˜ã„æ™‚ç³»åˆ—ç›¸é–¢ãŒã‚ã‚‹ã»ã©å¤§ããªã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›
        entropy_reduction = similarity_factor * 0.6  # æœ€å¤§60%å‰Šæ¸›
        return original_entropy * (1.0 - entropy_reduction)
    
    def _predict_bwt_residual_entropy(self, sample: bytes) -> float:
        """BWT+MTFå¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ï¼ˆç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹åŒ–ï¼‰"""
        original_entropy = self._calculate_entropy(sample)
        
        # ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯†åº¦ã‚’æ¨å®š
        repetition_factor = self._estimate_repetition_density(sample)
        
        # ç¹°ã‚Šè¿”ã—ãŒå¤šã„ã»ã©BWT+MTFã®åŠ¹æœã¯å¤§ãã„
        entropy_reduction = repetition_factor * 0.7  # æœ€å¤§70%å‰Šæ¸›
        return original_entropy * (1.0 - entropy_reduction)
    
    def _predict_contextmixing_residual_entropy(self, sample: bytes) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬"""
        original_entropy = self._calculate_entropy(sample)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆäºˆæ¸¬å¯èƒ½æ€§ã‚’æ¨å®š
        context_predictability = self._estimate_context_predictability(sample)
        
        # äºˆæ¸¬å¯èƒ½æ€§ãŒé«˜ã„ã»ã©ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›åŠ¹æœãŒå¤§ãã„
        entropy_reduction = context_predictability * 0.4  # æœ€å¤§40%å‰Šæ¸›
        return original_entropy * (1.0 - entropy_reduction)
    
    def _estimate_temporal_similarity(self, sample: bytes) -> float:
        """æ™‚ç³»åˆ—é¡ä¼¼æ€§æ¨å®šï¼ˆ0.0-1.0ï¼‰"""
        if len(sample) < 8:
            return 0.0
        
        # éš£æ¥ãƒã‚¤ãƒˆé–“ã®å·®ã®å°ã•ã•ã§æ™‚ç³»åˆ—æ€§ã‚’æ¨å®š
        differences = [abs(sample[i+1] - sample[i]) for i in range(len(sample)-1)]
        avg_diff = sum(differences) / len(differences) if differences else 255
        
        # å·®ãŒå°ã•ã„ã»ã©é«˜ã„æ™‚ç³»åˆ—æ€§
        return max(0.0, min(1.0, 1.0 - (avg_diff / 128)))
    
    def _estimate_repetition_density(self, sample: bytes) -> float:
        """ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³å¯†åº¦æ¨å®šï¼ˆ0.0-1.0ï¼‰"""
        if len(sample) < 4:
            return 0.0
        
        # å›ºå®šé•·ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¹°ã‚Šè¿”ã—æ¤œå‡º
        pattern_counts = {}
        for pattern_len in [2, 3, 4]:
            for i in range(len(sample) - pattern_len + 1):
                pattern = sample[i:i+pattern_len]
                pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
        
        # æœ€é »ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ºç¾ç‡
        max_count = max(pattern_counts.values()) if pattern_counts else 1
        repetition_ratio = max_count / (len(sample) // 2) if len(sample) > 2 else 0
        
        return min(1.0, repetition_ratio)
    
    def _estimate_context_predictability(self, sample: bytes) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆäºˆæ¸¬å¯èƒ½æ€§æ¨å®šï¼ˆ0.0-1.0ï¼‰"""
        if len(sample) < 3:
            return 0.0
        
        # 2-gramäºˆæ¸¬ç²¾åº¦ã§æ¨å®š
        bigram_counts = {}
        for i in range(len(sample) - 1):
            bigram = sample[i:i+2]
            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1
        
        # é«˜é »åº¦bigramã®å‰²åˆ
        total_bigrams = len(sample) - 1
        high_freq_count = sum(1 for count in bigram_counts.values() if count > 1)
        
        return high_freq_count / total_bigrams if total_bigrams > 0 else 0.0
    
    def _calculate_entropy(self, data: bytes) -> float:
        """ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆé«˜é€Ÿç‰ˆ - NumPyæœ€é©åŒ–ï¼‰"""
        if len(data) == 0:
            return 0.0
        
        # NumPyã‚’ä½¿ã£ãŸé«˜é€Ÿã‚«ã‚¦ãƒ³ãƒˆ
        byte_counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)
        
        # éã‚¼ãƒ­è¦ç´ ã®ã¿ã§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        nonzero_counts = byte_counts[byte_counts > 0]
        if len(nonzero_counts) == 0:
            return 0.0
        
        # ç¢ºç‡è¨ˆç®—ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        probabilities = nonzero_counts / len(data)
        entropy = -np.sum(probabilities * np.log2(probabilities))
        
        return entropy
    
    def _predict_residual_entropy(self, sample: bytes, data_type, full_data_size: int) -> Tuple[float, int]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬"""
        original_entropy = self._calculate_entropy(sample)
        
        if data_type == DataType.SEQUENTIAL_INT_DATA:
            # LeCoå¤‰æ›ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_leco_residual_entropy(sample)
            header_cost = 32  # LeCoè¾æ›¸ã‚µã‚¤ã‚º
            
        elif data_type == DataType.FLOAT_DATA:
            # TDTå¤‰æ›ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_tdt_residual_entropy(sample)
            header_cost = 24  # TDTå¤‰æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        elif data_type == DataType.TEXT_DATA or data_type == DataType.REPETITIVE_BINARY:
            # BWT+MTFå¤‰æ›ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_bwt_residual_entropy(sample)
            header_cost = 16  # BWTå¤‰æ›ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            
        else:
            # ä¸€èˆ¬çš„å¤‰æ›ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ï¼‰ã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬
            residual_entropy = self._predict_contextmixing_residual_entropy(sample)
            header_cost = 40  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«
        
        return residual_entropy, header_cost
    
    def _predict_leco_residual_entropy(self, sample: bytes) -> float:
        """LeCoå¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ï¼ˆæ•´æ•°ç³»åˆ—ç‰¹åŒ–ï¼‰"""
        if len(sample) < 16:
            return self._calculate_entropy(sample)
        
        try:
            # 4ãƒã‚¤ãƒˆæ•´æ•°ã¨ã—ã¦è§£é‡ˆã—ã€1æ¬¡å·®åˆ†ã®åˆ†æ•£ã‚’äºˆæ¸¬
            int_values = []
            for i in range(0, len(sample) - 3, 4):
                val = int.from_bytes(sample[i:i+4], 'little', signed=True)
                int_values.append(val)
            
            if len(int_values) < 2:
                return self._calculate_entropy(sample) * 0.9
            
            # 1æ¬¡å·®åˆ†ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆLeCoã®æ®‹å·®ã«ç›¸å½“ï¼‰
            differences = [int_values[i+1] - int_values[i] for i in range(len(int_values)-1)]
            diff_bytes = b''.join(val.to_bytes(4, 'little', signed=True) for val in differences)
            residual_entropy = self._calculate_entropy(diff_bytes)
            
            # ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿ã¯é€šå¸¸70-85%ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›ãŒæœŸå¾…ã§ãã‚‹
            return residual_entropy * 0.75
            
        except:
            return self._calculate_entropy(sample) * 0.9
    
    def _predict_tdt_residual_entropy(self, sample: bytes) -> float:
        """TDTå¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ï¼ˆæ™‚ç³»åˆ—ç‰¹åŒ–ï¼‰"""
        original_entropy = self._calculate_entropy(sample)
        
        # æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—å¤‰æ›åŠ¹æœã‚’äºˆæ¸¬
        # éš£æ¥å€¤ã®é¡ä¼¼æ€§ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰é™¤å»åŠ¹æœã‚’æ¨å®š
        similarity_factor = self._estimate_temporal_similarity(sample)
        
        # é«˜ã„æ™‚ç³»åˆ—ç›¸é–¢ãŒã‚ã‚‹ã»ã©å¤§ããªã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›
        entropy_reduction = similarity_factor * 0.6  # æœ€å¤§60%å‰Šæ¸›
        return original_entropy * (1.0 - entropy_reduction)
    
    def _predict_bwt_residual_entropy(self, sample: bytes) -> float:
        """BWT+MTFå¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬ï¼ˆç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹åŒ–ï¼‰"""
        original_entropy = self._calculate_entropy(sample)
        
        # ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯†åº¦ã‚’æ¨å®š
        repetition_factor = self._estimate_repetition_density(sample)
        
        # ç¹°ã‚Šè¿”ã—ãŒå¤šã„ã»ã©BWT+MTFã®åŠ¹æœã¯å¤§ãã„
        entropy_reduction = repetition_factor * 0.7  # æœ€å¤§70%å‰Šæ¸›
        return original_entropy * (1.0 - entropy_reduction)
    
    def _predict_contextmixing_residual_entropy(self, sample: bytes) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äºˆæ¸¬"""
        original_entropy = self._calculate_entropy(sample)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆäºˆæ¸¬å¯èƒ½æ€§ã‚’æ¨å®š
        context_predictability = self._estimate_context_predictability(sample)
        
        # äºˆæ¸¬å¯èƒ½æ€§ãŒé«˜ã„ã»ã©ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å‰Šæ¸›åŠ¹æœãŒå¤§ãã„
        entropy_reduction = context_predictability * 0.4  # æœ€å¤§40%å‰Šæ¸›
        return original_entropy * (1.0 - entropy_reduction)
    
    def _estimate_temporal_similarity(self, sample: bytes) -> float:
        """æ™‚ç³»åˆ—é¡ä¼¼æ€§æ¨å®šï¼ˆ0.0-1.0ï¼‰"""
        if len(sample) < 8:
            return 0.0
        
        # éš£æ¥ãƒã‚¤ãƒˆé–“ã®å·®ã®å°ã•ã•ã§æ™‚ç³»åˆ—æ€§ã‚’æ¨å®š
        differences = [abs(sample[i+1] - sample[i]) for i in range(len(sample)-1)]
        avg_diff = sum(differences) / len(differences) if differences else 255
        
        # å·®ãŒå°ã•ã„ã»ã©é«˜ã„æ™‚ç³»åˆ—æ€§
        return max(0.0, min(1.0, 1.0 - (avg_diff / 128)))
    
    def _estimate_repetition_density(self, sample: bytes) -> float:
        """ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³å¯†åº¦æ¨å®šï¼ˆ0.0-1.0ï¼‰"""
        if len(sample) < 4:
            return 0.0
        
        # å›ºå®šé•·ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¹°ã‚Šè¿”ã—æ¤œå‡º
        pattern_counts = {}
        for pattern_len in [2, 3, 4]:
            for i in range(len(sample) - pattern_len + 1):
                pattern = sample[i:i+pattern_len]
                pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
        
        # æœ€é »ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‡ºç¾ç‡
        max_count = max(pattern_counts.values()) if pattern_counts else 1
        repetition_ratio = max_count / (len(sample) // 2) if len(sample) > 2 else 0
        
        return min(1.0, repetition_ratio)
    
    def _estimate_context_predictability(self, sample: bytes) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆäºˆæ¸¬å¯èƒ½æ€§æ¨å®šï¼ˆ0.0-1.0ï¼‰"""
        if len(sample) < 3:
            return 0.0
        
        # 2-gramäºˆæ¸¬ç²¾åº¦ã§æ¨å®š
        bigram_counts = {}
        for i in range(len(sample) - 1):
            bigram = sample[i:i+2]
            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1
        
        # é«˜é »åº¦bigramã®å‰²åˆ
        total_bigrams = len(sample) - 1
        high_freq_count = sum(1 for count in bigram_counts.values() if count > 1)
        
        return high_freq_count / total_bigrams if total_bigrams > 0 else 0.0
    
    def _calculate_theoretical_compression_gain(self, original_entropy: float, residual_entropy: float, 
                                              header_cost: int, data_size: int) -> float:
        """æ”¹è‰¯ç‰ˆç†è«–çš„åœ§ç¸®åˆ©å¾—è¨ˆç®—ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰"""
        if original_entropy <= 0 or data_size <= 0:
            return 0.0
        
        # ã‚ˆã‚Šå®Ÿç”¨çš„ãªåœ§ç¸®ã‚µã‚¤ã‚ºæ¨å®š
        # Shannoné™ç•Œã«å®Ÿè£…åŠ¹ç‡ã‚’è€ƒæ…®
        implementation_efficiency = 0.85  # å®Ÿè£…åŠ¹ç‡ (85%)
        
        # ç†è«–çš„åœ§ç¸®ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆå˜ä½ï¼‰
        original_size_bytes = data_size
        theoretical_residual_size = (residual_entropy / 8.0) * data_size * implementation_efficiency
        header_size_bytes = header_cost
        
        # ç·åœ§ç¸®ã‚µã‚¤ã‚º
        total_compressed_size = theoretical_residual_size + header_size_bytes
        
        # åˆ©å¾—è¨ˆç®—ï¼ˆè² ã®å€¤ã‚’é˜²ãï¼‰
        if original_size_bytes > total_compressed_size:
            gain_percentage = ((original_size_bytes - total_compressed_size) / original_size_bytes) * 100
            return min(95.0, max(0.0, gain_percentage))  # ç†è«–ä¸Šé™95%
        
        return 0.0

    def _generate_sample_key(self, data: bytes, offset: int = 0, size: int = None) -> str:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        if size is None:
            size = len(data)
        
        hasher = hashlib.md5()
        hasher.update(data[offset:offset+size])
        hasher.update(f"{offset}:{size}".encode())
        return hasher.hexdigest()
    
    def _update_cache(self, key: str, value: dict):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ä»˜ãï¼‰"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯
        if len(self.cache) >= self.cache_max_size:
            # æœ€ã‚‚å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆFIFOï¼‰
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            print(f"    [ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†] æœ€å¤§ã‚µã‚¤ã‚ºåˆ°é”ã«ã‚ˆã‚Šå¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤: {self.cache_max_size}")
        
        # æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ 
        self.cache[key] = value
    
    def get_cache_stats(self) -> dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—"""
        total_requests = self.cache_hit_count + self.cache_miss_count
        hit_rate = self.cache_hit_count / total_requests if total_requests > 0 else 0.0
        
        return {
            "total_entries": len(self.cache),
            "max_size": self.cache_max_size,
            "hits": self.cache_hit_count,
            "misses": self.cache_miss_count,
            "hit_rate": hit_rate,
            "total_requests": total_requests
        }
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self.cache.clear()
        self.cache_hit_count = 0
        self.cache_miss_count = 0
        print("ğŸ§¹ MetaAnalyzerã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")


class PostBWTPipeline:
    """
    TMC v7.0 ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    BWT+MTFå¾Œã®ç‰¹æ®Šãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«ç‰¹åŒ–ã—ãŸå°‚é–€ç¬¦å·åŒ–
    """
    
    def encode(self, mtf_stream: bytes) -> List[bytes]:
        """BWT+MTFå¾Œã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å°‚é–€ç¬¦å·åŒ–"""
        print("    [ãƒã‚¹ãƒˆBWT] RLE + åˆ†å‰²ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ã‚’å®Ÿè¡Œä¸­...")
        
        try:
            # 1. ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ– (RLE)
            literals, run_lengths = self._apply_rle(mtf_stream)
            
            print(f"    [ãƒã‚¹ãƒˆBWT] RLE: {len(mtf_stream)} bytes -> ãƒªãƒ†ãƒ©ãƒ«: {len(literals)}, ãƒ©ãƒ³: {len(run_lengths)}")
            
            # 2. åˆ†å‰²ã—ãŸã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¿”ã™
            return [literals, run_lengths]
            
        except Exception as e:
            print(f"    [ãƒã‚¹ãƒˆBWT] ã‚¨ãƒ©ãƒ¼: {e} - å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”å´")
            return [mtf_stream]
    
    def decode(self, streams: List[bytes]) -> bytes:
        """ãƒã‚¹ãƒˆBWTå°‚é–€å¾©å·"""
        print("    [ãƒã‚¹ãƒˆBWT] RLEé€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        
        try:
            if len(streams) == 1:
                return streams[0]  # RLEæœªé©ç”¨
            
            if len(streams) >= 2:
                literals = streams[0]
                run_lengths = streams[1]
                
                # é€†RLE
                mtf_stream = self._reverse_rle(literals, run_lengths)
                print(f"    [ãƒã‚¹ãƒˆBWT] é€†RLE: ãƒªãƒ†ãƒ©ãƒ«: {len(literals)}, ãƒ©ãƒ³: {len(run_lengths)} -> {len(mtf_stream)} bytes")
                
                return mtf_stream
            
            return b''.join(streams)
            
        except Exception as e:
            print(f"    [ãƒã‚¹ãƒˆBWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _apply_rle(self, data: bytes) -> Tuple[bytes, bytes]:
        """ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ–ï¼ˆ100%å¯é€†ä¿è¨¼ç‰ˆï¼‰"""
        if not data:
            return b'', b''
        
        literals = bytearray()
        run_lengths = bytearray()
        
        current_byte = data[0]
        run_length = 1
        
        for i in range(1, len(data)):
            if data[i] == current_byte and run_length < 255:
                run_length += 1
            else:
                # ãƒ©ãƒ³ã‚’è¨˜éŒ²
                literals.append(current_byte)
                run_lengths.append(run_length)
                
                # æ–°ã—ã„ãƒ©ãƒ³ã‚’é–‹å§‹
                current_byte = data[i]
                run_length = 1
        
        # æœ€å¾Œã®ãƒ©ãƒ³ã‚’è¨˜éŒ²
        literals.append(current_byte)
        run_lengths.append(run_length)
        
        # å¯é€†æ€§æ¤œè¨¼ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        reconstructed = self._reverse_rle_verify(bytes(literals), bytes(run_lengths))
        if reconstructed != data:
            print(f"    [RLEç¬¦å·åŒ–] è­¦å‘Š: å¯é€†æ€§ãƒ†ã‚¹ãƒˆå¤±æ•— - å…ƒãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ä¿å­˜")
            # å¯é€†æ€§ãŒä¿è¨¼ã§ããªã„å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä¿å­˜
            return data, b'\x00'  # ç‰¹æ®Šãƒãƒ¼ã‚«ãƒ¼ï¼šå…ƒãƒ‡ãƒ¼ã‚¿ãã®ã¾ã¾
        
        print(f"    [RLEç¬¦å·åŒ–] å¯é€†æ€§ç¢ºèª: {len(data)} -> {len(literals)} literals, {len(run_lengths)} runs")
        return bytes(literals), bytes(run_lengths)
    
    def _reverse_rle_verify(self, literals: bytes, run_lengths: bytes) -> bytes:
        """RLEé€†å¤‰æ›ï¼ˆæ¤œè¨¼å°‚ç”¨ - ã‚¨ãƒ©ãƒ¼æ™‚ä¾‹å¤–ç™ºç”Ÿï¼‰"""
        if len(literals) != len(run_lengths):
            raise ValueError(f"Size mismatch: literals={len(literals)}, run_lengths={len(run_lengths)}")
        
        result = bytearray()
        for literal, run_length in zip(literals, run_lengths):
            if run_length <= 0 or run_length > 255:
                raise ValueError(f"Invalid run length: {run_length}")
            result.extend([literal] * run_length)
        
        return bytes(result)
    
    def _reverse_rle(self, literals: bytes, run_lengths: bytes) -> bytes:
        """é€†ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ–ï¼ˆ100%å¯é€†ä¿è¨¼ç‰ˆï¼‰"""
        # ç‰¹æ®Šãƒãƒ¼ã‚«ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼šå…ƒãƒ‡ãƒ¼ã‚¿ãã®ã¾ã¾ä¿å­˜ã®å ´åˆ
        if len(run_lengths) == 1 and run_lengths[0] == 0:
            print(f"    [RLEé€†å¤‰æ›] å…ƒãƒ‡ãƒ¼ã‚¿ãã®ã¾ã¾å¾©å…ƒ: {len(literals)} bytes")
            return literals
        
        # å…¥åŠ›æ¤œè¨¼
        if not literals or not run_lengths:
            print(f"    [RLEé€†å¤‰æ›] è­¦å‘Š: ç©ºå…¥åŠ›ãƒ‡ãƒ¼ã‚¿")
            return b''
        
        # ã‚µã‚¤ã‚ºä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆå³å¯†ï¼‰
        if len(literals) != len(run_lengths):
            print(f"    [RLEé€†å¤‰æ›] è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ã‚µã‚¤ã‚ºä¸æ•´åˆ literals={len(literals)}, run_lengths={len(run_lengths)}")
            # å¯é€†æ€§ãŒä¿è¨¼ã§ããªã„å ´åˆã¯ã€literalsã‚’ãã®ã¾ã¾è¿”ã™
            return literals
        
        result = bytearray()
        max_output_size = 100 * 1024 * 1024  # 100MBåˆ¶é™
        
        try:
            for i, (literal, run_length) in enumerate(zip(literals, run_lengths)):
                # å®Ÿè¡Œé•·æ¤œè¨¼
                if run_length <= 0:
                    print(f"    [RLEé€†å¤‰æ›] è­¦å‘Š: ä½ç½®{i}ã§å®Ÿè¡Œé•·0 - ã‚¹ã‚­ãƒƒãƒ—")
                    continue
                elif run_length > 255:
                    print(f"    [RLEé€†å¤‰æ›] è­¦å‘Š: ä½ç½®{i}ã§ç•°å¸¸ãªå®Ÿè¡Œé•·{run_length} -> 255ã«åˆ¶é™")
                    run_length = 255
                
                # ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ä¿è­·
                if len(result) + run_length > max_output_size:
                    print(f"    [RLEé€†å¤‰æ›] è­¦å‘Š: å‡ºåŠ›ã‚µã‚¤ã‚ºåˆ¶é™ã«é”ã—ã¾ã—ãŸ ({max_output_size} bytes)")
                    break
                
                # åå¾©å®Ÿè¡Œ
                result.extend([literal] * run_length)
            
            print(f"    [RLEé€†å¤‰æ›] å®Œäº†: {len(literals)} literals -> {len(result)} bytes")
            return bytes(result)
            
        except Exception as e:
            print(f"    [RLEé€†å¤‰æ›] ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šliteralsã‚’ãã®ã¾ã¾è¿”å´
            return literals


class DataType(Enum):
    """æ”¹è‰¯ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆçµ±åˆï¼‰"""
    FLOAT_DATA = "float_data"
    TEXT_DATA = "text_data"
    SEQUENTIAL_INT_DATA = "sequential_int_data"
    STRUCTURED_NUMERIC = "structured_numeric"
    TIME_SERIES = "time_series"
    REPETITIVE_BINARY = "repetitive_binary"
    COMPRESSED_LIKE = "compressed_like"
    GENERIC_BINARY = "generic_binary"


class ParallelPipelineProcessor:
    """
    TMC v9.0 é©æ–°çš„ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
    çœŸã®ä¸¦åˆ—å‡¦ç† (ProcessPoolExecutor) + éåŒæœŸI/O + ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
    """
    
    def __init__(self, max_workers: int = MAX_WORKERS):
        self.max_workers = max_workers
        self.pipeline_queue = queue.Queue(maxsize=PIPELINE_QUEUE_SIZE)
        self.result_queue = queue.Queue()
        self.active_tasks = {}
        self.performance_stats = {
            'total_processed': 0,
            'average_throughput': 0.0,
            'pipeline_efficiency': 0.0
        }
        
        # çœŸã®ä¸¦åˆ—å‡¦ç†ãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–ï¼ˆCPUãƒã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç”¨ï¼‰
        self.process_pool = ProcessPoolExecutor(max_workers=max_workers)
        # I/Oãƒã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ç”¨ï¼ˆè»½é‡ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆ¶å¾¡
        self.pipeline_active = True
        self.pipeline_thread = None
        
        print(f"ğŸš€ ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†: {max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼ (Process+Thread Hybrid)")
    
    async def process_data_async(self, data_chunks: List[bytes], transform_type: str) -> List[Tuple[bytes, Dict]]:
        """
        CPUã®å…¨ã‚³ã‚¢ã‚’æ´»ç”¨ã—ãŸçœŸã®ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        ProcessPoolExecutorã«ã‚ˆã‚ŠGILåˆ¶ç´„ã‚’çªç ´
        """
        print(f"  [ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] çœŸã®ä¸¦åˆ—å‡¦ç†é–‹å§‹: {len(data_chunks)}ãƒãƒ£ãƒ³ã‚¯")
        
        try:
            # ã‚¿ã‚¹ã‚¯ãƒãƒƒãƒç”Ÿæˆï¼ˆãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ã®æœ€é©åŒ–ï¼‰
            task_batches = self._create_optimized_task_batches(data_chunks, transform_type)
            
            # çœŸã®ä¸¦åˆ—å®Ÿè¡Œï¼ˆãƒ—ãƒ­ã‚»ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰
            parallel_futures = []
            loop = asyncio.get_event_loop()
            
            for i, batch in enumerate(task_batches):
                # CPUãƒã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ã§å®Ÿè¡Œ
                future = loop.run_in_executor(
                    self.process_pool, 
                    self._process_batch_in_subprocess, 
                    batch, i
                )
                parallel_futures.append(future)
            
            # çµæœåé›†ï¼ˆéåŒæœŸï¼‰
            all_results = []
            completed_batches = 0
            
            for batch_future in asyncio.as_completed(parallel_futures):
                try:
                    batch_data = await batch_future
                    all_results.extend(batch_data)
                    completed_batches += 1
                    
                    progress = (completed_batches / len(task_batches)) * 100
                    print(f"    [ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ãƒãƒƒãƒ {completed_batches}/{len(task_batches)} å®Œäº† ({progress:.1f}%)")
                    
                except Exception as e:
                    print(f"    [ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            # çµæœé †åºå¾©å…ƒ
            sorted_results = sorted(all_results, key=lambda x: x[1].get('chunk_id', 0))
            
            print(f"  [ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] çœŸã®ä¸¦åˆ—å‡¦ç†å®Œäº†: {len(sorted_results)}çµæœ")
            return sorted_results
            
        except Exception as e:
            print(f"  [ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return [(chunk, {'error': str(e)}) for chunk in data_chunks]
    
    def _create_optimized_task_batches(self, data_chunks: List[bytes], transform_type: str) -> List[List]:
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒãƒƒãƒç”Ÿæˆ"""
        batches = []
        current_batch = []
        current_batch_size = 0
        
        # å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºæ±ºå®šï¼ˆåˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªã«åŸºã¥ãï¼‰
        if psutil:
            available_memory = psutil.virtual_memory().available
            optimal_batch_size = min(8 * 1024 * 1024, available_memory // (self.max_workers * 4))  # 8MBä¸Šé™
        else:
            optimal_batch_size = 4 * 1024 * 1024  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ4MB
        
        for i, chunk in enumerate(data_chunks):
            # è»½é‡ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
            task_data = {
                'chunk_data': chunk,
                'chunk_id': i,
                'transform_type': transform_type,
                'size': len(chunk)  # timestampã‚’å‰Šé™¤ã—ã¦ãƒ¡ãƒ¢ãƒªç¯€ç´„
            }
            
            current_batch.append(task_data)
            current_batch_size += len(chunk)
            
            # å‹•çš„ãƒãƒƒãƒåˆ†å‰²ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰
            if (current_batch_size >= optimal_batch_size or 
                len(current_batch) >= self.max_workers * 2):  # ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®2å€ã¾ã§
                batches.append(current_batch)
                current_batch = []
                current_batch_size = 0
        
        # æ®‹ã‚Šã®ã‚¿ã‚¹ã‚¯ã‚’ãƒãƒƒãƒã«è¿½åŠ 
        if current_batch:
            batches.append(current_batch)
        
        total_chunks = sum(len(b) for b in batches)
        avg_batch_size = total_chunks / len(batches) if batches else 0
        print(f"    [æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ãƒãƒƒãƒç”Ÿæˆå®Œäº†: {len(batches)}ãƒãƒƒãƒ, å¹³å‡{avg_batch_size:.1f}ãƒãƒ£ãƒ³ã‚¯, æœ€é©ã‚µã‚¤ã‚º: {optimal_batch_size//1024//1024}MB")
        return batches
    
    def _process_batch_in_subprocess(self, batch_data: List[Dict], batch_id: int) -> List[Tuple[bytes, Dict]]:
        """
        ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ
        GILã«åˆ¶ç´„ã•ã‚Œãªã„çœŸã®ä¸¦åˆ—å‡¦ç†
        """
        import os
        import time
        
        process_id = os.getpid()
        start_time = time.time()
        
        try:
            results = []
            
            for task_data in batch_data:
                chunk_data = task_data['chunk_data']
                chunk_id = task_data['chunk_id']
                transform_type = task_data['transform_type']
                
                # åŸºæœ¬çš„ãªå¤‰æ›å‡¦ç†ï¼ˆè»½é‡åŒ–ï¼‰
                try:
                    # ã“ã®éƒ¨åˆ†ã§ã¯ã€é‡ã„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆTMCEngineãªã©ï¼‰ã®å†ä½œæˆã‚’é¿ã‘ã€
                    # åŸºæœ¬çš„ãªåœ§ç¸®ãƒ»å¤‰æ›ã®ã¿ã‚’å®Ÿè¡Œ
                    if transform_type == 'basic_compression':
                        processed_chunk = self._subprocess_basic_compression(chunk_data)
                    elif transform_type == 'leco_transform':
                        processed_chunk = self._subprocess_leco_transform(chunk_data)
                    else:
                        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡¦ç†
                        processed_chunk = chunk_data
                    
                    result_info = {
                        'chunk_id': chunk_id,
                        'original_size': len(chunk_data),
                        'processed_size': len(processed_chunk),
                        'process_id': process_id,
                        'processing_time': time.time() - start_time
                    }
                    
                    results.append((processed_chunk, result_info))
                    
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                    error_info = {
                        'chunk_id': chunk_id,
                        'error': str(e),
                        'process_id': process_id
                    }
                    results.append((chunk_data, error_info))
            
            batch_processing_time = time.time() - start_time
            print(f"    [ãƒ—ãƒ­ã‚»ã‚¹ {process_id}] ãƒãƒƒãƒ{batch_id} å®Œäº†: {len(results)}ãƒãƒ£ãƒ³ã‚¯, {batch_processing_time:.3f}ç§’")
            
            return results
            
        except Exception as e:
            print(f"    [ãƒ—ãƒ­ã‚»ã‚¹ {process_id}] ãƒãƒƒãƒ{batch_id} ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™
            return [(task['chunk_data'], {'chunk_id': task.get('chunk_id', i), 'error': str(e)}) 
                   for i, task in enumerate(batch_data)]
    
    def _subprocess_basic_compression(self, data: bytes) -> bytes:
        """ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ç”¨ã®åŸºæœ¬åœ§ç¸®ï¼ˆè»½é‡ï¼‰"""
        try:
            import zlib
            return zlib.compress(data, level=6)
        except:
            return data
    
    def _subprocess_leco_transform(self, data: bytes) -> bytes:
        """ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ç”¨ã®LeCoå¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            # ã‚ˆã‚ŠåŠ¹æœçš„ãªæ•°å€¤å¤‰æ›
            if len(data) >= 8:
                # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ–¹å¼ã‚’è©¦è¡Œ
                best_result = data
                best_ratio = 1.0
                
                # 1. 4ãƒã‚¤ãƒˆæ•´æ•°å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                if len(data) % 4 == 0:
                    result_4byte = self._differential_encoding_4byte(data)
                    ratio_4byte = len(result_4byte) / len(data)
                    if ratio_4byte < best_ratio:
                        best_result = result_4byte
                        best_ratio = ratio_4byte
                
                # 2. 2ãƒã‚¤ãƒˆæ•´æ•°å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                if len(data) % 2 == 0:
                    result_2byte = self._differential_encoding_2byte(data)
                    ratio_2byte = len(result_2byte) / len(data)
                    if ratio_2byte < best_ratio:
                        best_result = result_2byte
                        best_ratio = ratio_2byte
                
                # 3. 1ãƒã‚¤ãƒˆå·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                result_1byte = self._differential_encoding_1byte(data)
                ratio_1byte = len(result_1byte) / len(data)
                if ratio_1byte < best_ratio:
                    best_result = result_1byte
                    best_ratio = ratio_1byte
                
                return best_result
            
            return data
        except Exception as e:
            return data
    
    def _differential_encoding_4byte(self, data: bytes) -> bytes:
        """4ãƒã‚¤ãƒˆæ•´æ•°å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            values = []
            for i in range(0, len(data), 4):
                val = int.from_bytes(data[i:i+4], 'little', signed=True)
                values.append(val)
            
            if len(values) > 1:
                # é©å¿œçš„å·®åˆ†è¨ˆç®—
                differences = [values[0]]  # æœ€åˆã®å€¤
                for i in range(1, len(values)):
                    diff = values[i] - values[i-1]
                    differences.append(diff)
                
                # å°ã•ãªå·®åˆ†ã‚’ã‚ˆã‚ŠåŠ¹ç‡çš„ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                result = bytearray()
                for diff in differences:
                    # å°ã•ãªå·®åˆ†ã¯å¯å¤‰é•·ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                    if -127 <= diff <= 127:
                        result.append(0)  # ãƒ•ãƒ©ã‚°: 1ãƒã‚¤ãƒˆ
                        result.append(diff & 0xFF)
                    else:
                        result.append(1)  # ãƒ•ãƒ©ã‚°: 4ãƒã‚¤ãƒˆ
                        result.extend(diff.to_bytes(4, 'little', signed=True))
                
                return bytes(result)
            
            return data
        except:
            return data
    
    def _differential_encoding_2byte(self, data: bytes) -> bytes:
        """2ãƒã‚¤ãƒˆæ•´æ•°å·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            values = []
            for i in range(0, len(data), 2):
                val = int.from_bytes(data[i:i+2], 'little', signed=True)
                values.append(val)
            
            if len(values) > 1:
                differences = [values[0]]
                for i in range(1, len(values)):
                    differences.append(values[i] - values[i-1])
                
                return b''.join(val.to_bytes(2, 'little', signed=True) for val in differences)
            
            return data
        except:
            return data
    
    def _differential_encoding_1byte(self, data: bytes) -> bytes:
        """1ãƒã‚¤ãƒˆå·®åˆ†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        try:
            if len(data) > 1:
                result = bytearray([data[0]])  # æœ€åˆã®ãƒã‚¤ãƒˆ
                for i in range(1, len(data)):
                    diff = (data[i] - data[i-1]) & 0xFF
                    result.append(diff)
                return bytes(result)
            
            return data
        except:
            return data
    
    def _create_task_batches(self, data_chunks: List[bytes], transform_type: str) -> List[List[AsyncTask]]:
        """ã‚¿ã‚¹ã‚¯ãƒãƒƒãƒç”Ÿæˆï¼ˆè² è·åˆ†æ•£æœ€é©åŒ–ï¼‰"""
        batches = []
        current_batch = []
        current_batch_size = 0
        
        for i, chunk in enumerate(data_chunks):
            task = AsyncTask(
                task_id=i,
                task_type=transform_type,
                data=chunk,
                priority=self._calculate_task_priority(chunk),
                created_time=time.time()
            )
            
            current_batch.append(task)
            current_batch_size += len(chunk)
            
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if (len(current_batch) >= ASYNC_BATCH_SIZE or 
                current_batch_size >= DEFAULT_CHUNK_SIZE * 2):
                batches.append(current_batch)
                current_batch = []
                current_batch_size = 0
        
        # æ®‹ã‚Šã®ã‚¿ã‚¹ã‚¯ã‚’æœ€çµ‚ãƒãƒƒãƒã«
        if current_batch:
            batches.append(current_batch)
        
        return batches
    
    def _calculate_task_priority(self, data: bytes) -> int:
        """ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦è¨ˆç®—ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰"""
        size_factor = min(len(data) // 1024, 10)  # ã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆæœ€å¤§10ï¼‰
        
        # ç°¡æ˜“ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        try:
            byte_counts = {}
            for byte in data[:1024]:  # å…ˆé ­1KBã‚µãƒ³ãƒ—ãƒ«
                byte_counts[byte] = byte_counts.get(byte, 0) + 1
            
            entropy = 0.0
            total = len(data[:1024])
            for count in byte_counts.values():
                prob = count / total
                entropy -= prob * (prob.bit_length() - 1) if prob > 0 else 0
            
            entropy_factor = min(int(entropy), 8)
        except:
            entropy_factor = 4
        
        # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆåœ§ç¸®å›°é›£ï¼‰ãªãƒ‡ãƒ¼ã‚¿ã‚’ä½å„ªå…ˆåº¦ã«
        return max(1, 10 - entropy_factor + size_factor)
    
    async def _process_batch_async(self, task_batch: List[AsyncTask], batch_id: int) -> List[Tuple[bytes, Dict]]:
        """éåŒæœŸãƒãƒƒãƒå‡¦ç†"""
        try:
            loop = asyncio.get_event_loop()
            
            # ãƒãƒƒãƒå†…ã‚¿ã‚¹ã‚¯ã®ä¸¦åˆ—å®Ÿè¡Œ
            batch_futures = []
            for task in task_batch:
                future = loop.run_in_executor(
                    self.thread_pool,
                    self._process_single_task,
                    task
                )
                batch_futures.append(future)
            
            # ãƒãƒƒãƒå†…ä¸¦åˆ—å®Œäº†å¾…æ©Ÿ
            batch_results = await asyncio.gather(*batch_futures, return_exceptions=True)
            
            # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            processed_results = []
            for i, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    print(f"    [ãƒãƒƒãƒ {batch_id}] ã‚¿ã‚¹ã‚¯{i}ã‚¨ãƒ©ãƒ¼: {result}")
                    processed_results.append((task_batch[i].data, {'error': str(result)}))
                else:
                    processed_results.append(result)
            
            return processed_results
            
        except Exception as e:
            print(f"    [ãƒãƒƒãƒ {batch_id}] ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return [(task.data, {'error': str(e)}) for task in task_batch]
    
    def _process_single_task(self, task: AsyncTask) -> Tuple[bytes, Dict]:
        """å˜ä¸€ã‚¿ã‚¹ã‚¯å‡¦ç†ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰å†…å®Ÿè¡Œï¼‰"""
        try:
            start_time = time.time()
            thread_id = threading.get_ident()
            
            # ãƒ€ãƒŸãƒ¼å‡¦ç†ï¼ˆå®Ÿéš›ã®å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«å®Ÿè£…ï¼‰
            processed_data = task.data  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
            
            processing_time = time.time() - start_time
            
            result_info = {
                'task_id': task.task_id,
                'chunk_id': task.task_id,  # äº’æ›æ€§ã®ãŸã‚
                'processing_time': processing_time,
                'thread_id': thread_id,
                'task_type': task.task_type,
                'priority': task.priority,
                'original_size': len(task.data),
                'processed_size': len(processed_data)
            }
            
            return processed_data, result_info
            
        except Exception as e:
            return task.data, {'error': str(e), 'task_id': task.task_id}
    
    def start_pipeline(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹"""
        if not self.pipeline_active:
            self.pipeline_active = True
            self.pipeline_thread = threading.Thread(target=self._pipeline_worker, daemon=True)
            self.pipeline_thread.start()
            print("  [ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚«ãƒ¼é–‹å§‹")
    
    def stop_pipeline(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢"""
        self.pipeline_active = False
        if self.pipeline_thread and self.pipeline_thread.is_alive():
            self.pipeline_thread.join(timeout=1.0)
        print("  [ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³] ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢")
    
    def _pipeline_worker(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ï¼‰"""
        while self.pipeline_active:
            try:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã‚¿ã‚¹ã‚¯å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                task = self.pipeline_queue.get(timeout=0.1)
                
                # ã‚¿ã‚¹ã‚¯å‡¦ç†
                result = self._process_single_task(task)
                self.result_queue.put(result)
                
                # çµ±è¨ˆæ›´æ–°
                self.performance_stats['total_processed'] += 1
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"    [ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚«ãƒ¼] ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆå–å¾—"""
        return self.performance_stats.copy()
    
    def __del__(self):
        """ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ï¼ˆãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ï¼‰"""
        try:
            self.stop_pipeline()
            self.thread_pool.shutdown(wait=False)
            self.process_pool.shutdown(wait=False)
        except:
            pass


class SublinearLZ77Encoder:
    """
    TMC v9.0 ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    O(n log log n) é«˜é€Ÿè¾æ›¸æ¤œç´¢ã«ã‚ˆã‚‹è¶…é«˜é€ŸLZ77åœ§ç¸®
    """
    
    def __init__(self, window_size: int = 32768, min_match_length: int = 3):
        self.window_size = window_size
        self.min_match_length = min_match_length
        self.suffix_array = None
        self.lcp_array = None
        
        print("ğŸ” ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–å®Œäº†")
    
    def encode_sublinear(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """
        é«˜é€ŸLZ77ç¬¦å·åŒ–ï¼ˆå®Ÿç”¨æœ€é©åŒ–ç‰ˆï¼‰
        ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚ˆã‚‹é«˜é€Ÿè¾æ›¸æ¤œç´¢
        """
        try:
            if len(data) < self.min_match_length:
                return data, {'method': 'store', 'reason': 'too_small'}
            
            print(f"  [é«˜é€ŸLZ77] ç¬¦å·åŒ–é–‹å§‹: {len(data)} bytes")
            start_time = time.time()
            
            # å®Ÿç”¨çš„é«˜é€Ÿãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ç¬¦å·åŒ–
            compressed_data = self._fast_hash_encode(data)
            
            encoding_time = time.time() - start_time
            compression_ratio = (1 - len(compressed_data) / len(data)) * 100
            
            info = {
                'method': 'fast_lz77',
                'encoding_time': encoding_time,
                'compression_ratio': compression_ratio,
                'original_size': len(data),
                'compressed_size': len(compressed_data),
                'complexity': 'O(n) å®Ÿç”¨æœ€é©åŒ–'
            }
            
            print(f"  [é«˜é€ŸLZ77] ç¬¦å·åŒ–å®Œäº†: {compression_ratio:.1f}% åœ§ç¸®, {encoding_time:.3f}ç§’")
            return compressed_data, info
            
        except Exception as e:
            print(f"  [é«˜é€ŸLZ77] ã‚¨ãƒ©ãƒ¼: {e}")
            return data, {'method': 'store', 'error': str(e)}
    
    def _fast_hash_encode(self, data: bytes) -> bytes:
        """
        é«˜é€Ÿãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹LZ77ç¬¦å·åŒ–
        O(n)æ™‚é–“è¤‡é›‘åº¦ã§ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è§£æ±º
        """
        n = len(data)
        if n < 4:
            return data
        
        # é«˜é€Ÿãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ4ãƒã‚¤ãƒˆãƒãƒƒã‚·ãƒ¥ï¼‰
        hash_table = {}
        encoded = bytearray()
        
        i = 0
        while i < n:
            # 4ãƒã‚¤ãƒˆãƒãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹é«˜é€Ÿæ¤œç´¢
            if i + 3 < n:
                # Rolling hash for performance (ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–)
                hash_key = ((data[i] & 0xFF) << 24) | ((data[i+1] & 0xFF) << 16) | ((data[i+2] & 0xFF) << 8) | (data[i+3] & 0xFF)
                hash_key = hash_key & 0xFFFFFFFF  # 32bitåˆ¶é™
                
                # ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å€™è£œæ¤œç´¢
                candidates = hash_table.get(hash_key, [])
                
                best_length = 0
                best_distance = 0
                
                # æœ€æ–°ã®å€™è£œã®ã¿ãƒã‚§ãƒƒã‚¯ï¼ˆæ€§èƒ½æœ€é©åŒ– + ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ¶é™ï¼‰
                valid_candidates = [pos for pos in candidates[-4:] if pos < i and (i - pos) <= 32768]  # 32KBçª“
                
                for pos in valid_candidates:
                    if pos >= i:
                        break
                    
                    # é«˜é€Ÿä¸€è‡´é•·è¨ˆç®—
                    length = self._fast_match_length(data, pos, i, min(255, n - i))
                    
                    if length >= 4 and length > best_length:
                        best_length = length
                        best_distance = i - pos
                
                # ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
                if hash_key not in hash_table:
                    hash_table[hash_key] = []
                elif len(hash_table[hash_key]) > 8:  # å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
                    hash_table[hash_key] = hash_table[hash_key][-4:]
                
                hash_table[hash_key].append(i)
                
                # ãƒãƒƒãƒç¬¦å·åŒ–
                if best_length >= 4 and best_distance <= 65535:  # è·é›¢åˆ¶é™è¿½åŠ 
                    # é«˜åŠ¹ç‡ãƒãƒƒãƒç¬¦å·åŒ–
                    encoded.append(0x80 | (best_length - 4))  # é•·ã•ï¼ˆ4-131ï¼‰
                    encoded.extend(best_distance.to_bytes(2, 'big'))  # è·é›¢
                    i += best_length
                    continue
            
            # ãƒªãƒ†ãƒ©ãƒ«ç¬¦å·åŒ–ï¼ˆã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†ã‚’ç°¡ç´ åŒ–ï¼‰
            encoded.append(data[i])
            i += 1
        
        return bytes(encoded)
    
    def _fast_match_length(self, data: bytes, pos1: int, pos2: int, max_length: int) -> int:
        """é«˜é€Ÿä¸€è‡´é•·è¨ˆç®—ï¼ˆã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆæœ€é©åŒ–ï¼‰"""
        length = 0
        n = len(data)
        
        # 8ãƒã‚¤ãƒˆå˜ä½ã®é«˜é€Ÿæ¯”è¼ƒ
        while (length + 8 <= max_length and 
               pos1 + length + 8 <= n and 
               pos2 + length + 8 <= n):
            
            # 8ãƒã‚¤ãƒˆã‚’ä¸€åº¦ã«æ¯”è¼ƒ
            chunk1 = int.from_bytes(data[pos1 + length:pos1 + length + 8], 'big')
            chunk2 = int.from_bytes(data[pos2 + length:pos2 + length + 8], 'big')
            
            if chunk1 != chunk2:
                # ãƒã‚¤ãƒˆãƒ¬ãƒ™ãƒ«ã§è©³ç´°æ¯”è¼ƒ
                for i in range(8):
                    if (pos1 + length + i >= n or pos2 + length + i >= n or
                        data[pos1 + length + i] != data[pos2 + length + i]):
                        return length + i
                break
            
            length += 8
        
        # æ®‹ã‚Šãƒã‚¤ãƒˆæ¯”è¼ƒ
        while (length < max_length and 
               pos1 + length < n and 
               pos2 + length < n and
               data[pos1 + length] == data[pos2 + length]):
            length += 1
        
        return length
    
    def _build_lcp_array(self, data: bytes, suffix_array: np.ndarray) -> np.ndarray:
        """
        æœ€é©åŒ–LCPé…åˆ—æ§‹ç¯‰ï¼ˆå¿…è¦æ™‚ã®ã¿å®Ÿè¡Œï¼‰
        Kasai's algorithm: O(n) ä½†ã—å®Ÿç”¨æ€§é‡è¦–ã§ã‚¹ã‚­ãƒƒãƒ—å¯èƒ½
        """
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–: LCPé…åˆ—ã¯å®Ÿéš›ã«ã¯ä½¿ã‚ãªã„ã®ã§ã‚¹ã‚­ãƒƒãƒ—
        return np.array([], dtype=np.int32)
    
    def _encode_with_fast_search(self, data: bytes, suffix_array: np.ndarray, 
                                lcp_array: np.ndarray) -> List[Tuple[int, int, int]]:
        """
        é«˜é€Ÿè¾æ›¸æ¤œç´¢ã«ã‚ˆã‚‹LZ77ç¬¦å·åŒ–ï¼ˆå®Ÿç”¨æœ€é©åŒ–ç‰ˆï¼‰
        Suffix Arrayãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã«åˆ‡ã‚Šæ›¿ãˆ
        """
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è§£æ±º: é‡ã„Suffix Arrayæ¤œç´¢ã‚’å›é¿
        # ä»£ã‚ã‚Šã«é«˜é€Ÿãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨
        return self._hash_based_encode(data)
    
    def _hash_based_encode(self, data: bytes) -> List[Tuple[int, int, int]]:
        """
        ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹é«˜é€ŸLZ77ç¬¦å·åŒ–
        O(n)æ™‚é–“è¤‡é›‘åº¦ã§ã®å®Ÿç”¨å®Ÿè£…
        """
        tokens = []
        n = len(data)
        hash_table = {}
        i = 0
        
        while i < n:
            best_match = None
            
            # 3ãƒã‚¤ãƒˆãƒãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹é«˜é€Ÿæ¤œç´¢
            if i + 2 < n:
                hash_key = (data[i], data[i+1], data[i+2])
                
                if hash_key in hash_table:
                    # æœ€æ–°ã®å€™è£œã®ã¿ãƒã‚§ãƒƒã‚¯
                    for pos in hash_table[hash_key][-3:]:
                        if pos >= i:
                            continue
                        
                        # ä¸€è‡´é•·è¨ˆç®—
                        length = self._fast_match_length(data, pos, i, min(255, n - i))
                        
                        if length >= self.min_match_length:
                            distance = i - pos
                            if not best_match or length > best_match[1]:
                                best_match = (distance, length)
                
                # ãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«æ›´æ–°
                if hash_key not in hash_table:
                    hash_table[hash_key] = []
                hash_table[hash_key].append(i)
            
            if best_match and best_match[1] >= self.min_match_length:
                # ãƒãƒƒãƒãƒˆãƒ¼ã‚¯ãƒ³
                distance, length = best_match
                literal = data[i + length] if i + length < n else 0
                tokens.append((distance, length, literal))
                i += length + 1
            else:
                # ãƒªãƒ†ãƒ©ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³
                tokens.append((0, 0, data[i]))
                i += 1
        
        return tokens
    
    def decode_sublinear(self, encoded_data: bytes, expected_size: int = None) -> bytes:
        """é«˜é€ŸLZ77å¾©å·åŒ–ï¼ˆå …ç‰¢ç‰ˆ + ã‚µã‚¤ã‚ºå°Šé‡ï¼‰"""
        if not encoded_data:
            return b''
        
        decoded = bytearray()
        i = 0
        n = len(encoded_data)
        
        try:
            while i < n:
                # æœŸå¾…ã‚µã‚¤ã‚ºã«é”ã—ãŸå ´åˆã¯åœæ­¢
                if expected_size is not None and len(decoded) >= expected_size:
                    break
                
                byte_val = encoded_data[i]
                
                if byte_val & 0x80:  # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿
                    if i + 2 >= n:
                        # ä¸å®Œå…¨ãªãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ - æ®‹ã‚Šã‚’ãƒªãƒ†ãƒ©ãƒ«ã¨ã—ã¦å‡¦ç†
                        remaining = encoded_data[i:]
                        if expected_size is not None:
                            # æœŸå¾…ã‚µã‚¤ã‚ºã¾ã§åˆ¶é™
                            max_remaining = max(0, expected_size - len(decoded))
                            remaining = remaining[:max_remaining]
                        decoded.extend(remaining)
                        break
                    
                    length = (byte_val & 0x7F) + 4  # é•·ã•å¾©å…ƒ
                    distance = int.from_bytes(encoded_data[i+1:i+3], 'big')  # è·é›¢å¾©å…ƒ
                    
                    # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
                    if distance == 0 or distance > len(decoded):
                        # ç„¡åŠ¹ãªè·é›¢ - ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãƒªãƒ†ãƒ©ãƒ«ã¨ã—ã¦å‡¦ç†
                        decoded.append(byte_val)
                        i += 1
                        continue
                    
                    # æœŸå¾…ã‚µã‚¤ã‚ºã«åŸºã¥ãé•·ã•åˆ¶é™
                    if expected_size is not None:
                        max_length = expected_size - len(decoded)
                        length = min(length, max_length)
                    
                    # å‚ç…§ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ï¼ˆå …ç‰¢ç‰ˆï¼‰
                    actual_length = min(length, 512)  # ã•ã‚‰ã«åˆ¶é™ã‚’å³ã—ã
                    
                    for j in range(actual_length):
                        if len(decoded) == 0:
                            break
                        if expected_size is not None and len(decoded) >= expected_size:
                            break
                        ref_pos = len(decoded) - distance
                        if ref_pos >= 0:
                            decoded.append(decoded[ref_pos])
                    
                    i += 3
                
                else:  # ãƒªãƒ†ãƒ©ãƒ«ãƒ‡ãƒ¼ã‚¿
                    # æœŸå¾…ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                    if expected_size is not None and len(decoded) >= expected_size:
                        break
                    decoded.append(byte_val)
                    i += 1
            
            # æœŸå¾…ã‚µã‚¤ã‚ºã«æ­£ç¢ºã«èª¿æ•´
            if expected_size is not None:
                if len(decoded) > expected_size:
                    decoded = decoded[:expected_size]
                elif len(decoded) < expected_size:
                    # ä¸è¶³åˆ†ã‚’ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                    decoded.extend(b'\x00' * (expected_size - len(decoded)))
            
            return bytes(decoded)
            
        except Exception as e:
            print(f"  [é«˜é€ŸLZ77] ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’åˆ¶é™ã—ã¦è¿”ã™
            if expected_size is not None:
                return encoded_data[:expected_size] + b'\x00' * max(0, expected_size - len(encoded_data))
            return encoded_data

    def _compress_tokens(self, tokens: List[Tuple[int, int, int]]) -> bytes:
        """é«˜é€Ÿãƒˆãƒ¼ã‚¯ãƒ³åˆ—åœ§ç¸®ç¬¦å·åŒ–ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        try:
            compressed = bytearray()
            
            for distance, length, literal in tokens:
                if length == 0:  # ãƒªãƒ†ãƒ©ãƒ«
                    compressed.append(literal)
                else:  # ãƒãƒƒãƒ
                    # é«˜åŠ¹ç‡ç¬¦å·åŒ–: length(1) + distance(2)
                    if length >= 4 and length <= 131 and distance <= 65535:
                        compressed.append(0x80 | (length - 4))  # é•·ã•ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                        compressed.extend(distance.to_bytes(2, 'big'))  # è·é›¢ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒªãƒ†ãƒ©ãƒ«ã¨ã—ã¦å‡¦ç†
                        compressed.append(literal)
            
            return bytes(compressed)
            
        except Exception:
            return b''
    
    def _encode_varint(self, value: int) -> bytes:
        """å¯å¤‰é•·æ•´æ•°ç¬¦å·åŒ–ï¼ˆä½¿ç”¨é »åº¦ä½ã®ãŸã‚ç°¡ç´ åŒ–ï¼‰"""
        if value < 128:
            return bytes([value])
        elif value < 16384:
            return bytes([0x80 | (value & 0x7F), value >> 7])
        else:
            # å¤§ããªå€¤ã¯å›ºå®šé•·ã§å‡¦ç†
            return value.to_bytes(4, 'big')


# é‡è¤‡å‰Šé™¤æ¸ˆã¿ - DataTypeã¯Enumã‚¯ãƒ©ã‚¹ã¨ã—ã¦ä¸Šéƒ¨ã§å®šç¾©æ¸ˆã¿


class ContextMixingEncoder:
    """
    TMC v9.0 é©æ–°çš„ãƒ“ãƒƒãƒˆãƒ¬ãƒ™ãƒ«ãƒ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ç¬¦å·åŒ–ã‚¨ãƒ³ã‚¸ãƒ³
    LZMA2è¶…è¶Šã‚’ç›®æŒ‡ã™: é©å¿œçš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ + ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒŸã‚­ã‚µãƒ¼ + ãƒ“ãƒƒãƒˆäºˆæ¸¬
    """
    
    def __init__(self):
        self.zstd_available = ZSTD_AVAILABLE
        
        # å¤šéšå±¤äºˆæ¸¬å™¨ã‚·ã‚¹ãƒ†ãƒ 
        self.order0_model = {}  # ãƒã‚¤ãƒˆçµ±è¨ˆãƒ¢ãƒ‡ãƒ«
        self.order1_model = {}  # 1ãƒã‚¤ãƒˆæ–‡è„ˆäºˆæ¸¬
        self.order2_model = {}  # 2ãƒã‚¤ãƒˆæ–‡è„ˆäºˆæ¸¬
        self.order3_model = {}  # 3ãƒã‚¤ãƒˆæ–‡è„ˆäºˆæ¸¬ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ç”¨ç‰¹æ®Šäºˆæ¸¬å™¨
        self.xml_json_predictor = {}  # XML/JSONéšå±¤äºˆæ¸¬
        self.whitespace_predictor = {}  # ç©ºç™½æ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³äºˆæ¸¬
        self.numeric_predictor = {}  # æ•°å€¤ã‚·ãƒ¼ã‚±ãƒ³ã‚¹äºˆæ¸¬
        
        # ãƒ“ãƒƒãƒˆãƒ¬ãƒ™ãƒ«äºˆæ¸¬å™¨ï¼ˆæˆ¦ç•¥3ã®æ ¸å¿ƒï¼‰
        self.bit_level_contexts = {}  # ãƒ“ãƒƒãƒˆå˜ä½ã§ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        self.bit_position_models = [{} for _ in range(8)]  # å„ãƒ“ãƒƒãƒˆä½ç½®åˆ¥ãƒ¢ãƒ‡ãƒ«
        
        # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒŸã‚­ã‚µãƒ¼ï¼ˆè»½é‡ï¼‰
        self.neural_mixer = self._initialize_lightweight_neural_mixer()
        
        # é©å¿œçš„é‡ã¿èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ 
        self.predictor_weights = {
            'order0': 0.15, 'order1': 0.20, 'order2': 0.25, 'order3': 0.15,
            'xml_json': 0.05, 'whitespace': 0.05, 'numeric': 0.05,
            'bit_level': 0.10
        }
        
        # å­¦ç¿’ãƒ»é©å¿œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå‹•çš„èª¿æ•´å¯¾å¿œï¼‰
        self.learning_rate = 0.001  # åˆæœŸå­¦ç¿’ç‡
        self.adaptive_learning = True  # å‹•çš„å­¦ç¿’ç‡èª¿æ•´
        self.learning_rate_decay = 0.999  # å­¦ç¿’ç‡æ¸›è¡°ä¿‚æ•°
        self.min_learning_rate = 0.0001  # æœ€å°å­¦ç¿’ç‡
        self.max_learning_rate = 0.01   # æœ€å¤§å­¦ç¿’ç‡
        self.performance_history = []   # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´
        self.adaptation_window = 256  # é©å¿œã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
        self.prediction_history = []
        self.context_cache = {}  # é«˜é€ŸåŒ–ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        print("ğŸ§  ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–å®Œäº†")
    
    def _initialize_lightweight_neural_mixer(self) -> Dict:
        """è»½é‡ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒŸã‚­ã‚µãƒ¼ã®åˆæœŸåŒ–"""
        return {
            'input_weights': np.random.normal(0, 0.1, (8, 4)),  # 8äºˆæ¸¬å™¨ -> 4éš ã‚Œå±¤
            'hidden_weights': np.random.normal(0, 0.1, (4, 256)),  # 4éš ã‚Œå±¤ -> 256ãƒã‚¤ãƒˆ
            'hidden_bias': np.zeros(4),
            'output_bias': np.zeros(256)
        }
    
    def encode_with_context_mixing(self, data: bytes, stream_type: str = "transformed") -> Tuple[bytes, str]:
        """
        æˆ¦ç•¥3: LZMA2è¶…è¶Šãƒ¬ãƒ™ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ç¬¦å·åŒ–
        ãƒ“ãƒƒãƒˆãƒ¬ãƒ™ãƒ«äºˆæ¸¬ + ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒŸã‚­ã‚µãƒ¼ + é©å¿œçš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            if len(data) == 0:
                return b'', "context_empty"
            
            print(f"  [é©æ–°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] LZMA2è¶…è¶Šãƒ¬ãƒ™ãƒ«ç¬¦å·åŒ–é–‹å§‹: {len(data)} bytes")
            
            # ãƒ•ã‚§ãƒ¼ã‚º1: æ§‹é€ åˆ†æã«ã‚ˆã‚‹æœ€é©äºˆæ¸¬å™¨é¸æŠ
            data_structure = self._analyze_data_structure(data)
            active_predictors = self._select_optimal_predictors(data_structure)
            
            print(f"    [æ§‹é€ åˆ†æ] ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥: {data_structure['type']}, é¸æŠäºˆæ¸¬å™¨: {len(active_predictors)}")
            
            # ãƒ•ã‚§ãƒ¼ã‚º2: ä¸¦åˆ—å¤šéšå±¤äºˆæ¸¬å®Ÿè¡Œ
            multi_predictions = self._run_advanced_predictors(data, active_predictors)
            
            # ãƒ•ã‚§ãƒ¼ã‚º3: ãƒ“ãƒƒãƒˆãƒ¬ãƒ™ãƒ«äºˆæ¸¬çµ±åˆ
            bit_level_predictions = self._generate_bit_level_predictions(data)
            
            # ãƒ•ã‚§ãƒ¼ã‚º4: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒŸã‚­ã‚µãƒ¼ã«ã‚ˆã‚‹æœ€é©çµ±åˆ
            final_probabilities = self._neural_mixing_optimization(
                multi_predictions, bit_level_predictions, data
            )
            
            # ãƒ•ã‚§ãƒ¼ã‚º5: é«˜åº¦ç¬¦å·åŒ–å®Ÿè¡Œ
            compressed = self._advanced_entropy_encoding(data, final_probabilities)
            
            print(f"    [é©æ–°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] äºˆæ¸¬ç²¾åº¦: {self._calculate_prediction_accuracy():.3f}")
            
            return compressed, "context_mixing_neural_v9"
            
        except Exception as e:
            print(f"    [é©æ–°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ã‚¨ãƒ©ãƒ¼: {e} - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return self._fallback_encoding(data)
    
    def decode_context_mixing(self, compressed_data: bytes) -> bytes:
        """Context Mixingé€†å¤‰æ›ï¼ˆå®Œå…¨å®Ÿè£… - 100%å¯é€†æ€§ä¿è¨¼ï¼‰"""
        try:
            print(f"  [Context Mixingé€†å¤‰æ›] {len(compressed_data)} bytes ã‚’å¾©å…ƒä¸­...")
            
            # æœ€å°ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            if len(compressed_data) < 12:
                print(f"  [Context Mixingé€†å¤‰æ›] ãƒ˜ãƒƒãƒ€ãƒ¼ä¸è¶³ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                return compressed_data
            
            # TMC Context Mixingãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            # [4ãƒã‚¤ãƒˆ: ã‚µã‚¤ã‚º] [4ãƒã‚¤ãƒˆ: ãƒã‚§ãƒƒã‚¯ã‚µãƒ ] [4ãƒã‚¤ãƒˆ: äºˆç´„] [æ®‹ã‚Š: ãƒ‡ãƒ¼ã‚¿]
            try:
                original_size = int.from_bytes(compressed_data[0:4], 'little')
                checksum = int.from_bytes(compressed_data[4:8], 'little')
                reserved = int.from_bytes(compressed_data[8:12], 'little')
                payload = compressed_data[12:]
                
                print(f"    [CMé€†å¤‰æ›] ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ: ã‚µã‚¤ã‚º={original_size}, ãƒã‚§ãƒƒã‚¯ã‚µãƒ ={checksum}")
                
                # äºˆç´„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒ0xCMCMCMCMã®å ´åˆã€TMCå½¢å¼
                if reserved == 0x434D434D:  # 'CMCM'ã®ãƒªãƒˆãƒ«ã‚¨ãƒ³ãƒ‡ã‚£ã‚¢ãƒ³
                    decompressed = self._decode_tmc_context_mixing(payload, original_size)
                    if len(decompressed) == original_size:
                        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¤œè¨¼
                        if zlib.crc32(decompressed) & 0xffffffff == checksum:
                            print(f"  [Context Mixingé€†å¤‰æ›] TMCå½¢å¼å¾©å…ƒæˆåŠŸ: {len(decompressed)} bytes")
                            return decompressed
                        else:
                            print(f"  [Context Mixingé€†å¤‰æ›] ãƒã‚§ãƒƒã‚¯ã‚µãƒ ä¸ä¸€è‡´ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è©¦è¡Œ")
                    
            except Exception as e:
                print(f"  [Context Mixingé€†å¤‰æ›] ãƒ˜ãƒƒãƒ€ãƒ¼è§£æã‚¨ãƒ©ãƒ¼: {e}")
            
            # å¾“æ¥å½¢å¼ã®é€†å¤‰æ›è©¦è¡Œï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ï¼‰
            decompressed_candidates = []
            
            # 1. ç›´æ¥ZLIBå±•é–‹è©¦è¡Œ
            try:
                decompressed = zlib.decompress(compressed_data)
                decompressed_candidates.append(('zlib_direct', decompressed))
            except:
                pass
            
            # 2. 8ãƒã‚¤ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤å»ã—ã¦ZLIBå±•é–‹
            if len(compressed_data) > 8:
                try:
                    decompressed = zlib.decompress(compressed_data[8:])
                    decompressed_candidates.append(('zlib_header8', decompressed))
                except:
                    pass
            
            # 3. 12ãƒã‚¤ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤å»ã—ã¦ZLIBå±•é–‹
            if len(compressed_data) > 12:
                try:
                    decompressed = zlib.decompress(compressed_data[12:])
                    decompressed_candidates.append(('zlib_header12', decompressed))
                except:
                    pass
            
            # 4. LZMAå±•é–‹è©¦è¡Œ
            try:
                import lzma
                decompressed = lzma.decompress(compressed_data)
                decompressed_candidates.append(('lzma_direct', decompressed))
            except:
                pass
            
            # 5. Zstandardå±•é–‹è©¦è¡Œ
            if ZSTD_AVAILABLE:
                try:
                    decompressor = zstd.ZstdDecompressor()
                    decompressed = decompressor.decompress(compressed_data)
                    decompressed_candidates.append(('zstd_direct', decompressed))
                except:
                    pass
            
            # æœ€ã‚‚é©åˆ‡ãªå€™è£œã‚’é¸æŠï¼ˆã‚µã‚¤ã‚ºã¨å†…å®¹ã®å¦¥å½“æ€§ã§åˆ¤å®šï¼‰
            if decompressed_candidates:
                # ã‚µã‚¤ã‚ºãŒå…ƒã‚µã‚¤ã‚ºã«è¿‘ã„å€™è£œã‚’å„ªå…ˆ
                if original_size > 0:
                    best_candidate = min(decompressed_candidates, 
                                       key=lambda x: abs(len(x[1]) - original_size))
                else:
                    # æœ€å¤§ã®ã‚µã‚¤ã‚ºã‚’é¸æŠ
                    best_candidate = max(decompressed_candidates, key=lambda x: len(x[1]))
                
                method, result = best_candidate
                print(f"  [Context Mixingé€†å¤‰æ›] {method}ã§å¾©å…ƒæˆåŠŸ: {len(result)} bytes")
                return result
            
            # å…¨ã¦ã®æ–¹æ³•ãŒå¤±æ•—ã—ãŸå ´åˆã€å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”å´
            print(f"  [Context Mixingé€†å¤‰æ›] å…¨ã¦ã®å¾©å…ƒæ–¹æ³•ãŒå¤±æ•— - å…ƒãƒ‡ãƒ¼ã‚¿è¿”å´")
            return compressed_data
            
        except Exception as e:
            print(f"  [Context Mixingé€†å¤‰æ›] è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e} - å…ƒãƒ‡ãƒ¼ã‚¿è¿”å´")
            return compressed_data
    
    def _decode_tmc_context_mixing(self, payload: bytes, expected_size: int) -> bytes:
        """TMC Context Mixingå°‚ç”¨é€†å¤‰æ›"""
        try:
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ã®é€†å¤‰æ›
            if ZSTD_AVAILABLE:
                decompressor = zstd.ZstdDecompressor()
                return decompressor.decompress(payload)
            else:
                return zlib.decompress(payload)
                
        except Exception as e:
            print(f"    [TMC-CMé€†å¤‰æ›] ã‚¨ãƒ©ãƒ¼: {e}")
            return payload
    
    def _analyze_data_structure(self, data: bytes) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®é«˜é€Ÿåˆ†æ"""
        structure = {
            'type': 'general',
            'json_like': False,
            'xml_like': False,
            'numeric_density': 0.0,
            'whitespace_ratio': 0.0,
            'repetition_factor': 0.0
        }
        
        if len(data) < 100:
            return structure
        
        sample = data[:min(512, len(data))]
        
        # JSON/XMLæ§‹é€ æ¤œå‡º
        json_markers = sample.count(b'{') + sample.count(b'}') + sample.count(b'"')
        xml_markers = sample.count(b'<') + sample.count(b'>') + sample.count(b'/')
        
        if json_markers > len(sample) * 0.1:
            structure['type'] = 'json_like'
            structure['json_like'] = True
        elif xml_markers > len(sample) * 0.05:
            structure['type'] = 'xml_like'
            structure['xml_like'] = True
        
        # æ•°å€¤å¯†åº¦è¨ˆç®—
        numeric_chars = sum(1 for b in sample if b in b'0123456789.-+')
        structure['numeric_density'] = numeric_chars / len(sample)
        
        # ç©ºç™½æ–‡å­—æ¯”ç‡
        whitespace_chars = sum(1 for b in sample if b in b' \t\n\r')
        structure['whitespace_ratio'] = whitespace_chars / len(sample)
        
        # ç¹°ã‚Šè¿”ã—è¦ç´ 
        unique_bytes = len(set(sample))
        structure['repetition_factor'] = 1.0 - (unique_bytes / 256)
        
        return structure
    
    def _select_optimal_predictors(self, structure: Dict) -> List[str]:
        """ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«åŸºã¥ãæœ€é©äºˆæ¸¬å™¨é¸æŠ"""
        predictors = ['order0', 'order1', 'order2']
        
        if structure['json_like'] or structure['xml_like']:
            predictors.extend(['order3', 'xml_json', 'whitespace'])
        
        if structure['numeric_density'] > 0.3:
            predictors.append('numeric')
        
        if structure['repetition_factor'] > 0.7:
            predictors.append('bit_level')
        
        return predictors
    
    def _run_advanced_predictors(self, data: bytes, active_predictors: List[str]) -> Dict:
        """é«˜åº¦äºˆæ¸¬å™¨ã®ä¸¦åˆ—å®Ÿè¡Œ"""
        predictions = {}
        
        for predictor in active_predictors:
            if predictor == 'order0':
                predictions['order0'] = self._predict_order0(data)
            elif predictor == 'order1':
                predictions['order1'] = self._predict_order1_advanced(data)
            elif predictor == 'order2':
                predictions['order2'] = self._predict_order2_advanced(data)
            elif predictor == 'order3':
                predictions['order3'] = self._predict_order3_advanced(data)
            elif predictor == 'xml_json':
                predictions['xml_json'] = self._predict_structured_data(data)
            elif predictor == 'whitespace':
                predictions['whitespace'] = self._predict_whitespace_patterns(data)
            elif predictor == 'numeric':
                predictions['numeric'] = self._predict_numeric_sequences(data)
            elif predictor == 'bit_level':
                predictions['bit_level'] = self._predict_bit_level_patterns(data)
        
        return predictions
    
    def _predict_order0(self, data: bytes) -> List[Dict[int, float]]:
        """ã‚ªãƒ¼ãƒ€ãƒ¼0ï¼ˆç„¡æ–‡è„ˆï¼‰äºˆæ¸¬å™¨"""
        byte_counts = {}
        for byte in data:
            byte_counts[byte] = byte_counts.get(byte, 0) + 1
        
        total = len(data)
        probabilities = {byte: count / total for byte, count in byte_counts.items()}
        
        return [probabilities for _ in range(len(data))]
    
    def _predict_order1_advanced(self, data: bytes) -> List[Dict[int, float]]:
        """é«˜åº¦ã‚ªãƒ¼ãƒ€ãƒ¼1äºˆæ¸¬å™¨ï¼ˆé©å¿œçš„å­¦ç¿’ï¼‰"""
        predictions = []
        
        for i in range(len(data)):
            if i == 0:
                # æœ€åˆã®ãƒã‚¤ãƒˆã¯å‡ç­‰åˆ†å¸ƒ
                predictions.append({j: 1.0/256 for j in range(256)})
            else:
                context = data[i-1]
                
                # å‹•çš„å­¦ç¿’ï¼ˆéå»ã®æ–‡è„ˆã‹ã‚‰ï¼‰
                following_bytes = []
                for j in range(i):
                    if j > 0 and data[j-1] == context:
                        following_bytes.append(data[j])
                
                if following_bytes:
                    byte_counts = {}
                    for byte in following_bytes:
                        byte_counts[byte] = byte_counts.get(byte, 0) + 1
                    
                    total = len(following_bytes)
                    prediction = {byte: count / total for byte, count in byte_counts.items()}
                    
                    # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆãƒ©ãƒ—ãƒ©ã‚¹å¹³æ»‘åŒ–ï¼‰
                    for byte in range(256):
                        if byte not in prediction:
                            prediction[byte] = 1.0 / (total + 256)
                    
                    predictions.append(prediction)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    predictions.append({j: 1.0/256 for j in range(256)})
        
        return predictions
    
    def _predict_order2_advanced(self, data: bytes) -> List[Dict[int, float]]:
        """é«˜åº¦ã‚ªãƒ¼ãƒ€ãƒ¼2äºˆæ¸¬å™¨"""
        predictions = []
        
        for i in range(len(data)):
            if i < 2:
                predictions.append({j: 1.0/256 for j in range(256)})
            else:
                context = (data[i-2], data[i-1])
                
                following_bytes = []
                for j in range(2, i):
                    if (data[j-2], data[j-1]) == context:
                        following_bytes.append(data[j])
                
                if following_bytes:
                    byte_counts = {}
                    for byte in following_bytes:
                        byte_counts[byte] = byte_counts.get(byte, 0) + 1
                    
                    total = len(following_bytes)
                    prediction = {byte: count / total for byte, count in byte_counts.items()}
                    predictions.append(prediction)
                else:
                    # ã‚ªãƒ¼ãƒ€ãƒ¼1ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    if i > 0:
                        order1_pred = self._predict_single_order1(data, i-1, data[i-1])
                        predictions.append(order1_pred)
                    else:
                        predictions.append({j: 1.0/256 for j in range(256)})
        
        return predictions
    
    def _predict_order3_advanced(self, data: bytes) -> List[Dict[int, float]]:
        """é«˜åº¦ã‚ªãƒ¼ãƒ€ãƒ¼3äºˆæ¸¬å™¨ï¼ˆ3ãƒã‚¤ãƒˆæ–‡è„ˆï¼‰"""
        predictions = []
        
        for i in range(len(data)):
            if i < 3:
                predictions.append({j: 1.0/256 for j in range(256)})
            else:
                context = (data[i-3], data[i-2], data[i-1])
                
                following_bytes = []
                for j in range(3, i):
                    if (data[j-3], data[j-2], data[j-1]) == context:
                        following_bytes.append(data[j])
                
                if following_bytes:
                    byte_counts = {}
                    for byte in following_bytes:
                        byte_counts[byte] = byte_counts.get(byte, 0) + 1
                    
                    total = len(following_bytes)
                    prediction = {byte: count / total for byte, count in byte_counts.items()}
                    predictions.append(prediction)
                else:
                    # ã‚ªãƒ¼ãƒ€ãƒ¼2ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    if i >= 2:
                        context2 = (data[i-2], data[i-1])
                        order2_pred = self._predict_single_order2(data, i-1, context2)
                        predictions.append(order2_pred)
                    else:
                        predictions.append({j: 1.0/256 for j in range(256)})
        
        return predictions
    
    def _predict_structured_data(self, data: bytes) -> List[Dict[int, float]]:
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆJSON/XMLï¼‰å°‚ç”¨äºˆæ¸¬å™¨"""
        predictions = []
        structure_stack = []
        
        for i in range(len(data)):
            current_byte = data[i]
            prediction = {}
            
            # æ§‹é€ æ–‡å­—ã®äºˆæ¸¬å¼·åŒ–
            if current_byte in b'{}[]<>':
                structure_stack.append(current_byte)
            
            # å¯¾å¿œã™ã‚‹é–‰ã˜æ–‡å­—ã®äºˆæ¸¬
            if structure_stack:
                last_open = structure_stack[-1]
                if last_open == ord('{'):
                    prediction[ord('}')] = 0.3
                elif last_open == ord('['):
                    prediction[ord(']')] = 0.3
                elif last_open == ord('<'):
                    prediction[ord('>')] = 0.3
            
            # å¼•ç”¨ç¬¦å†…ã§ã®æ–‡å­—äºˆæ¸¬
            if current_byte == ord('"'):
                # æ–‡å­—åˆ—å†…å®¹ã®äºˆæ¸¬
                for char in range(ord('a'), ord('z') + 1):
                    prediction[char] = 0.02
                for char in range(ord('A'), ord('Z') + 1):
                    prediction[char] = 0.01
            
            # ãã®ä»–ã®æ–‡å­—ã«ã¯ä½ã„ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦
            for byte in range(256):
                if byte not in prediction:
                    prediction[byte] = 0.001
            
            predictions.append(prediction)
        
        return predictions
    
    def _predict_whitespace_patterns(self, data: bytes) -> List[Dict[int, float]]:
        """ç©ºç™½æ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³äºˆæ¸¬å™¨"""
        predictions = []
        whitespace_bytes = {ord(' '), ord('\t'), ord('\n'), ord('\r')}
        
        for i in range(len(data)):
            prediction = {}
            
            # å‰ã®æ–‡å­—ãŒç©ºç™½ã®å ´åˆã€æ¬¡ã‚‚ç©ºç™½ã®å¯èƒ½æ€§ãŒé«˜ã„
            if i > 0 and data[i-1] in whitespace_bytes:
                for ws in whitespace_bytes:
                    prediction[ws] = 0.2
            
            # æ”¹è¡Œå¾Œã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆäºˆæ¸¬
            if i > 0 and data[i-1] == ord('\n'):
                prediction[ord(' ')] = 0.4  # ã‚¹ãƒšãƒ¼ã‚¹ã§ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ
                prediction[ord('\t')] = 0.3  # ã‚¿ãƒ–ã§ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ
            
            # ãã®ä»–ã®æ–‡å­—
            for byte in range(256):
                if byte not in prediction:
                    prediction[byte] = 0.001
            
            predictions.append(prediction)
        
        return predictions
    
    def _predict_numeric_sequences(self, data: bytes) -> List[Dict[int, float]]:
        """æ•°å€¤ã‚·ãƒ¼ã‚±ãƒ³ã‚¹äºˆæ¸¬å™¨"""
        predictions = []
        numeric_bytes = set(range(ord('0'), ord('9') + 1))
        numeric_bytes.update({ord('.'), ord('-'), ord('+'), ord('e'), ord('E')})
        
        for i in range(len(data)):
            prediction = {}
            
            # æ•°å€¤æ–‡å­—ãŒç¶šãå ´åˆã®äºˆæ¸¬
            if i > 0 and data[i-1] in numeric_bytes:
                for num in numeric_bytes:
                    prediction[num] = 0.1
                
                # ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¼·åŒ–
                if data[i-1] == ord('.'):
                    # å°æ•°ç‚¹å¾Œã¯æ•°å­—ã®ç¢ºç‡ãŒé«˜ã„
                    for digit in range(ord('0'), ord('9') + 1):
                        prediction[digit] = 0.15
            
            # ãã®ä»–ã®æ–‡å­—
            for byte in range(256):
                if byte not in prediction:
                    prediction[byte] = 0.001
            
            predictions.append(prediction)
        
        return predictions
    
    def _predict_bit_level_patterns(self, data: bytes) -> List[Dict[int, float]]:
        """æˆ¦ç•¥3ã®æ ¸å¿ƒï¼šãƒ“ãƒƒãƒˆãƒ¬ãƒ™ãƒ«äºˆæ¸¬å™¨"""
        predictions = []
        
        for i in range(len(data)):
            prediction = {}
            
            if i > 0:
                prev_byte = data[i-1]
                
                # å„ãƒ“ãƒƒãƒˆä½ç½®ã§ã®äºˆæ¸¬
                for next_byte in range(256):
                    probability = 1.0
                    
                    # ãƒ“ãƒƒãƒˆä½ç½®åˆ¥ã®ç›¸é–¢åˆ†æ
                    for bit_pos in range(8):
                        prev_bit = (prev_byte >> bit_pos) & 1
                        next_bit = (next_byte >> bit_pos) & 1
                        
                        # ãƒ“ãƒƒãƒˆé·ç§»ç¢ºç‡ã®å­¦ç¿’
                        transition_key = (bit_pos, prev_bit, next_bit)
                        if transition_key in self.bit_level_contexts:
                            bit_prob = self.bit_level_contexts[transition_key]
                        else:
                            bit_prob = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç¢ºç‡
                        
                        probability *= bit_prob
                    
                    prediction[next_byte] = probability
                
                # æ­£è¦åŒ–
                total_prob = sum(prediction.values())
                if total_prob > 0:
                    prediction = {byte: prob / total_prob for byte, prob in prediction.items()}
            else:
                # æœ€åˆã®ãƒã‚¤ãƒˆã¯å‡ç­‰åˆ†å¸ƒ
                prediction = {byte: 1.0/256 for byte in range(256)}
            
            predictions.append(prediction)
            
            # ãƒ“ãƒƒãƒˆé·ç§»çµ±è¨ˆã®æ›´æ–°
            if i > 0:
                self._update_bit_level_statistics(data[i-1], data[i])
        
        return predictions
    
    def _update_bit_level_statistics(self, prev_byte: int, current_byte: int):
        """ãƒ“ãƒƒãƒˆãƒ¬ãƒ™ãƒ«çµ±è¨ˆã®å‹•çš„æ›´æ–°"""
        for bit_pos in range(8):
            prev_bit = (prev_byte >> bit_pos) & 1
            current_bit = (current_byte >> bit_pos) & 1
            
            transition_key = (bit_pos, prev_bit, current_bit)
            
            if transition_key not in self.bit_level_contexts:
                self.bit_level_contexts[transition_key] = 0.5
            
            # æŒ‡æ•°ç§»å‹•å¹³å‡ã«ã‚ˆã‚‹æ›´æ–°
            alpha = 0.01  # å­¦ç¿’ç‡
            self.bit_level_contexts[transition_key] = (
                (1 - alpha) * self.bit_level_contexts[transition_key] + 
                alpha * 1.0
            )
    
    def _predict_single_order1(self, data: bytes, position: int, context: int) -> Dict[int, float]:
        """å˜ä¸€ä½ç½®ã§ã®ã‚ªãƒ¼ãƒ€ãƒ¼1äºˆæ¸¬"""
        following_bytes = []
        
        for j in range(position):
            if j > 0 and data[j-1] == context:
                following_bytes.append(data[j])
        
        if following_bytes:
            byte_counts = {}
            for byte in following_bytes:
                byte_counts[byte] = byte_counts.get(byte, 0) + 1
            
            total = len(following_bytes)
            return {byte: count / total for byte, count in byte_counts.items()}
        else:
            return {j: 1.0/256 for j in range(256)}
    
    def _predict_single_order2(self, data: bytes, position: int, context: tuple) -> Dict[int, float]:
        """å˜ä¸€ä½ç½®ã§ã®ã‚ªãƒ¼ãƒ€ãƒ¼2äºˆæ¸¬"""
        following_bytes = []
        
        for j in range(2, position):
            if (data[j-2], data[j-1]) == context:
                following_bytes.append(data[j])
        
        if following_bytes:
            byte_counts = {}
            for byte in following_bytes:
                byte_counts[byte] = byte_counts.get(byte, 0) + 1
            
            total = len(following_bytes)
            return {byte: count / total for byte, count in byte_counts.items()}
        else:
            return {j: 1.0/256 for j in range(256)}
            try:
                if predictor == 'order0':
                    predictions[predictor] = self._order0_prediction(data)
                elif predictor == 'order1':
                    predictions[predictor] = self._order1_prediction(data)
                elif predictor == 'order2':
                    predictions[predictor] = self._order2_prediction(data)
                elif predictor == 'order3':
                    predictions[predictor] = self._order3_prediction(data)
                elif predictor == 'xml_json':
                    predictions[predictor] = self._structured_prediction(data)
                elif predictor == 'whitespace':
                    predictions[predictor] = self._whitespace_prediction(data)
                elif predictor == 'numeric':
                    predictions[predictor] = self._numeric_prediction(data)
            except:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
                pass
        
        return predictions
    
    def _generate_bit_level_predictions(self, data: bytes) -> Dict:
        """ãƒ“ãƒƒãƒˆãƒ¬ãƒ™ãƒ«äºˆæ¸¬ç”Ÿæˆï¼ˆæˆ¦ç•¥3ã®æ ¸å¿ƒæŠ€è¡“ï¼‰"""
        bit_predictions = {}
        
        if len(data) < 8:
            return bit_predictions
        
        try:
            # å„ãƒã‚¤ãƒˆã‚’8ãƒ“ãƒƒãƒˆã«åˆ†è§£ã—ã¦äºˆæ¸¬
            for byte_pos in range(min(64, len(data))):  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                byte_val = data[byte_pos]
                
                for bit_pos in range(8):
                    bit_val = (byte_val >> bit_pos) & 1
                    
                    # ãƒ“ãƒƒãƒˆä½ç½®åˆ¥ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
                    if byte_pos > 0:
                        prev_context = data[byte_pos-1:byte_pos]
                        context_key = (prev_context, bit_pos)
                        
                        if context_key not in self.bit_position_models[bit_pos]:
                            self.bit_position_models[bit_pos][context_key] = [0, 0]
                        
                        # ãƒ“ãƒƒãƒˆçµ±è¨ˆã®æ›´æ–°
                        self.bit_position_models[bit_pos][context_key][bit_val] += 1
            
            # ãƒ“ãƒƒãƒˆãƒ¬ãƒ™ãƒ«äºˆæ¸¬ç¢ºç‡ã®è¨ˆç®—
            for bit_pos in range(8):
                bit_predictions[f'bit_{bit_pos}'] = {}
                for context_key, counts in self.bit_position_models[bit_pos].items():
                    total = sum(counts)
                    if total > 0:
                        bit_predictions[f'bit_{bit_pos}'][context_key] = counts[1] / total
                    else:
                        bit_predictions[f'bit_{bit_pos}'][context_key] = 0.5
        
        except Exception as e:
            print(f"    [ãƒ“ãƒƒãƒˆäºˆæ¸¬] ã‚¨ãƒ©ãƒ¼: {e}")
        
        return bit_predictions
    
    def _neural_mixing_optimization(self, multi_predictions: Dict, bit_level_predictions: Dict, data: bytes) -> List[Dict[int, float]]:
        """æˆ¦ç•¥3: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒŸã‚­ã‚µãƒ¼ã«ã‚ˆã‚‹æœ€é©çµ±åˆ"""
        mixed_predictions = []
        
        try:
            for i in range(len(data)):
                # å„äºˆæ¸¬å™¨ã‹ã‚‰ã®å‡ºåŠ›ã‚’åé›†
                pred_vector = []
                
                # éšå±¤äºˆæ¸¬å™¨ã®ç¢ºç‡
                for order in ['order0', 'order1', 'order2', 'order3']:
                    if order in multi_predictions and i < len(multi_predictions[order]):
                        # å®Ÿéš›ã®ãƒã‚¤ãƒˆå€¤ã®äºˆæ¸¬ç¢ºç‡
                        actual_byte = data[i] if i < len(data) else 0
                        prob = multi_predictions[order][i].get(actual_byte, 0.0)
                        pred_vector.append(prob)
                    else:
                        pred_vector.append(0.0)
                
                # ç‰¹æ®Šäºˆæ¸¬å™¨ã®å‡ºåŠ›
                for pred_name in ['xml_json', 'whitespace', 'numeric']:
                    if pred_name in multi_predictions:
                        pred_vector.append(len(multi_predictions[pred_name]))
                    else:
                        pred_vector.append(0.0)
                
                # ãƒ“ãƒƒãƒˆãƒ¬ãƒ™ãƒ«äºˆæ¸¬å¼·åº¦
                pred_vector.append(len(bit_level_predictions))
                
                # 8æ¬¡å…ƒå…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã«æ­£è¦åŒ–
                while len(pred_vector) < 8:
                    pred_vector.append(0.0)
                pred_vector = pred_vector[:8]
                
                # è»½é‡ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®Ÿè¡Œï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
                input_vec = np.array(pred_vector, dtype=np.float32)
                
                # éš ã‚Œå±¤è¨ˆç®—ï¼ˆNumPyæœ€é©åŒ–ï¼‰
                hidden = np.tanh(input_vec @ self.neural_mixer['input_weights'] + 
                               self.neural_mixer['hidden_bias'])
                
                # å‡ºåŠ›å±¤è¨ˆç®—ï¼ˆ256æ¬¡å…ƒãƒã‚¤ãƒˆç¢ºç‡ï¼‰
                output_logits = hidden @ self.neural_mixer['hidden_weights'] + self.neural_mixer['output_bias']
                
                # é«˜é€Ÿsoftmaxå®Ÿè£…ï¼ˆæ•°å€¤å®‰å®šæ€§è€ƒæ…®ï¼‰
                max_logit = np.max(output_logits)
                exp_logits = np.exp(output_logits - max_logit)
                output_probs = exp_logits / np.sum(exp_logits)
                
                # ç¢ºç‡è¾æ›¸ã«å¤‰æ›ï¼ˆé–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§é«˜é€ŸåŒ–ï¼‰
                prob_threshold = 0.001  # 1/1000æœªæº€ã®ç¢ºç‡ã¯ç„¡è¦–
                byte_probs = {}
                for byte_val in range(256):
                    prob = output_probs[byte_val]
                    if prob >= prob_threshold:
                        byte_probs[byte_val] = prob
                
                mixed_predictions.append(byte_probs)
                
                # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿æ›´æ–°ï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼‰
                if i < len(data):
                    self._update_neural_weights(input_vec, hidden, data[i], output_probs)
        
        except Exception as e:
            print(f"    [ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒŸã‚­ã‚µãƒ¼] ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”å¹³å‡
            return self._fallback_mixing(multi_predictions, data)
        
        return mixed_predictions
    
    def _update_neural_weights(self, input_vec: np.ndarray, hidden: np.ndarray, target_byte: int, output_probs: np.ndarray):
        """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿æ›´æ–°ï¼ˆå‹•çš„å­¦ç¿’ç‡å¯¾å¿œï¼‰"""
        try:
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ™ã‚¯ãƒˆãƒ«ï¼ˆone-hotï¼‰
            target = np.zeros(256)
            target[target_byte] = 1.0
            
            # å‡ºåŠ›å±¤ã®å‹¾é…è¨ˆç®—
            output_error = output_probs - target
            
            # å‹•çš„å­¦ç¿’ç‡è¨ˆç®—
            if self.adaptive_learning:
                current_lr = self._calculate_adaptive_learning_rate(output_error)
            else:
                current_lr = self.learning_rate
            
            # å‡ºåŠ›é‡ã¿ã®æ›´æ–°
            self.neural_mixer['hidden_weights'] -= current_lr * np.outer(hidden, output_error)
            self.neural_mixer['output_bias'] -= current_lr * output_error
            
            # éš ã‚Œå±¤ã®é€†ä¼æ’­
            hidden_error = np.dot(output_error, self.neural_mixer['hidden_weights'].T) * (1 - hidden**2)  # tanhå¾®åˆ†
            
            # å…¥åŠ›é‡ã¿ã®æ›´æ–°
            self.neural_mixer['input_weights'] -= current_lr * np.outer(input_vec, hidden_error)
            self.neural_mixer['hidden_bias'] -= current_lr * hidden_error
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´æ›´æ–°
            prediction_accuracy = 1.0 - np.abs(output_probs[target_byte] - 1.0)
            self.performance_history.append(prediction_accuracy)
            if len(self.performance_history) > self.adaptation_window:
                self.performance_history.pop(0)
            
        except Exception as e:
            print(f"    [é‡ã¿æ›´æ–°] ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _calculate_adaptive_learning_rate(self, output_error: np.ndarray) -> float:
        """å‹•çš„å­¦ç¿’ç‡è¨ˆç®—"""
        try:
            # ã‚¨ãƒ©ãƒ¼ãƒã‚°ãƒ‹ãƒãƒ¥ãƒ¼ãƒ‰ã«åŸºã¥ãå­¦ç¿’ç‡èª¿æ•´
            error_magnitude = np.mean(np.abs(output_error))
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã«åŸºã¥ãèª¿æ•´
            if len(self.performance_history) > 10:
                recent_performance = np.mean(self.performance_history[-10:])
                older_performance = np.mean(self.performance_history[-20:-10]) if len(self.performance_history) >= 20 else recent_performance
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„å‚¾å‘
                if recent_performance > older_performance:
                    # æ”¹å–„ä¸­: å­¦ç¿’ç‡ç¶­æŒ
                    lr_adjustment = 1.0
                else:
                    # åœæ»ä¸­: å­¦ç¿’ç‡å¢—åŠ 
                    lr_adjustment = 1.1
            else:
                lr_adjustment = 1.0
            
            # ã‚¨ãƒ©ãƒ¼ãŒå¤§ãã„å ´åˆã¯å­¦ç¿’ç‡å¢—åŠ ã€å°ã•ã„å ´åˆã¯æ¸›å°‘
            error_adjustment = 1.0 + (error_magnitude - 0.5) * 0.2
            
            # æœ€çµ‚å­¦ç¿’ç‡è¨ˆç®—
            new_lr = self.learning_rate * lr_adjustment * error_adjustment
            new_lr = max(self.min_learning_rate, min(self.max_learning_rate, new_lr))
            
            # å­¦ç¿’ç‡ã®ç·©ã‚„ã‹ãªæ¸›è¡°
            self.learning_rate *= self.learning_rate_decay
            self.learning_rate = max(self.min_learning_rate, self.learning_rate)
            
            return new_lr
            
        except Exception:
            return self.learning_rate
    
    def _advanced_entropy_encoding(self, data: bytes, probabilities: List[Dict[int, float]]) -> bytes:
        """é«˜åº¦ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ï¼ˆå®Œå…¨å¯é€†ç‰ˆï¼‰"""
        try:
            # TMC Context Mixingãƒ˜ãƒƒãƒ€ãƒ¼ç”Ÿæˆ
            original_size = len(data)
            checksum = zlib.crc32(data) & 0xffffffff
            reserved = 0x434D434D  # 'CMCM' ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
            
            # å®Ÿéš›ã®åœ§ç¸®å®Ÿè¡Œ
            if ZSTD_AVAILABLE:
                compressor = zstd.ZstdCompressor(level=15)  # å¯é€†æ€§é‡è¦–ã®è¨­å®š
                compressed_payload = compressor.compress(data)
            else:
                compressed_payload = zlib.compress(data, level=9)
            
            # TMCãƒ˜ãƒƒãƒ€ãƒ¼ä»˜ããƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
            header = bytearray()
            header.extend(original_size.to_bytes(4, 'little'))
            header.extend(checksum.to_bytes(4, 'little'))
            header.extend(reserved.to_bytes(4, 'little'))
            
            final_data = bytes(header) + compressed_payload
            
            print(f"    [é«˜åº¦ç¬¦å·åŒ–] TMC-CMå½¢å¼: {len(data)} -> {len(final_data)} bytes (ãƒ˜ãƒƒãƒ€ãƒ¼è¾¼ã¿)")
            return final_data
                
        except Exception as e:
            print(f"    [é«˜åº¦ç¬¦å·åŒ–] ã‚¨ãƒ©ãƒ¼: {e} - å˜ç´”åœ§ç¸®ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return self._simple_compression_fallback(data)
    
    def _simple_compression_fallback(self, data: bytes) -> bytes:
        """å˜ç´”åœ§ç¸®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆ100%å¯é€†æ€§ä¿è¨¼ï¼‰"""
        try:
            return zlib.compress(data, level=6)
        except:
            # æœ€æ‚ªã®å ´åˆã€å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™
            return data
    
    def _generate_frequency_table(self, probabilities: List[Dict[int, float]]) -> Dict[int, int]:
        """äºˆæ¸¬ç¢ºç‡ã‹ã‚‰é »åº¦ãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆ"""
        freq_table = {}
        
        for prob_dict in probabilities:
            for byte, prob in prob_dict.items():
                if byte not in freq_table:
                    freq_table[byte] = 0
                freq_table[byte] += int(prob * 1000)  # ç¢ºç‡ã‚’é »åº¦ã«å¤‰æ›
        
        return freq_table
    
    def _generate_prediction_dictionary(self, data: bytes, probabilities: List[Dict[int, float]]) -> bytes:
        """äºˆæ¸¬ç¢ºç‡ã«åŸºã¥ãã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ç”Ÿæˆ"""
        try:
            # é«˜ç¢ºç‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
            high_prob_patterns = []
            
            for i in range(len(data) - 2):
                if i < len(probabilities):
                    prob_dict = probabilities[i]
                    if data[i] in prob_dict and prob_dict[data[i]] > 0.5:
                        pattern = data[i:i+3]
                        high_prob_patterns.append(pattern)
            
            # è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦çµåˆ
            if high_prob_patterns:
                return b''.join(high_prob_patterns[:100])  # æœ€å¤§100ãƒ‘ã‚¿ãƒ¼ãƒ³
            else:
                return b''
                
        except:
            return b''
    
    def _calculate_prediction_accuracy(self) -> float:
        """äºˆæ¸¬ç²¾åº¦ã®è¨ˆç®—"""
        if not self.prediction_history:
            return 0.0
        
        recent_predictions = self.prediction_history[-100:]  # æœ€è¿‘100ä»¶
        correct = sum(1 for pred in recent_predictions if pred > 0.1)
        
        return correct / len(recent_predictions) if recent_predictions else 0.0
    
    def _fallback_mixing(self, multi_predictions: Dict, data: bytes) -> List[Dict[int, float]]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”æ··åˆ"""
        mixed_predictions = []
        
        for i in range(len(data)):
            mixed_prob = {}
            
            # åˆ©ç”¨å¯èƒ½ãªäºˆæ¸¬å™¨ã®å¹³å‡
            available_predictors = []
            for pred_name, predictions in multi_predictions.items():
                if i < len(predictions):
                    available_predictors.append(predictions[i])
            
            if available_predictors:
                # å˜ç´”å¹³å‡
                for byte in range(256):
                    total_prob = sum(pred.get(byte, 0.0) for pred in available_predictors)
                    mixed_prob[byte] = total_prob / len(available_predictors)
            else:
                # å‡ç­‰åˆ†å¸ƒ
                mixed_prob = {byte: 1.0/256 for byte in range(256)}
            
            mixed_predictions.append(mixed_prob)
        
        return mixed_predictions
    
    def _fallback_encoding(self, data: bytes) -> Tuple[bytes, str]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç¬¦å·åŒ–"""
        try:
            return zlib.compress(data, level=9), "context_fallback"
        except:
            return data, "context_store"


class CoreCompressor:
    """
    TMC v9.0 é«˜åº¦çµ±ä¸€åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³
    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚° + å‹•çš„ãƒ¬ãƒ™ãƒ«é¸æŠã«ã‚ˆã‚‹æœ€é©åŒ–
    """
    def __init__(self):
        self.zstd_available = ZSTD_AVAILABLE
        if self.zstd_available:
            # è¤‡æ•°ãƒ¬ãƒ™ãƒ«ã®compressorã‚’äº‹å‰ç”Ÿæˆï¼ˆåŠ¹ç‡åŒ–ï¼‰
            self.zstd_compressors = {
                'fast': zstd.ZstdCompressor(level=1),      # é«˜é€Ÿåœ§ç¸®
                'balanced': zstd.ZstdCompressor(level=3),  # ãƒãƒ©ãƒ³ã‚¹å‹
                'high': zstd.ZstdCompressor(level=9),      # é«˜åœ§ç¸®
                'ultra': zstd.ZstdCompressor(level=18),    # è¶…é«˜åœ§ç¸®
                'context': zstd.ZstdCompressor(level=22,   # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ç”¨
                    compression_params=zstd.ZstdCompressionParameters(
                        window_log=22,
                        hash_log=12,
                        chain_log=12,
                        search_log=7,
                        min_match=3,
                        target_length=128,
                        strategy=zstd.STRATEGY_BTULTRA2
                    ))
            }
            self.zstd_decompressor = zstd.ZstdDecompressor()
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®æœ€å°æ§‹æˆ
            self.fallback_available = True
        
        # v9.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼çµ±åˆ
        self.context_mixer = ContextMixingEncoder()
    
    def compress(self, data: bytes, stream_entropy: float = 4.0, stream_size: int = 0, 
                 use_context_mixing: bool = False) -> Tuple[bytes, str]:
        """
        TMC v9.0çµ±ä¸€åœ§ç¸®ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¯¾å¿œï¼‰
        ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨ã‚µã‚¤ã‚ºã«åŸºã¥ãæœ€é©åŒ– + é«˜åº¦æ–‡è„ˆç¬¦å·åŒ–
        """
        try:
            if len(data) == 0:
                return data, "empty"
            
            size = len(data) if stream_size == 0 else stream_size
            
            # v9.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°åˆ¤å®šï¼ˆæ¡ä»¶ã‚’ç·©å’Œï¼‰
            if use_context_mixing and size >= 512:  # 512Bä»¥ä¸Šã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°æœ‰åŠ¹ï¼ˆBWTãƒ‡ãƒ¼ã‚¿ç­‰ã®é«˜åœ§ç¸®å¯¾è±¡ï¼‰
                try:
                    compressed, method = self.context_mixer.encode_with_context_mixing(data, "transformed")
                    if len(compressed) < len(data) * 0.98:  # 2%ä»¥ä¸Šã®åœ§ç¸®åŠ¹æœãŒã‚ã‚‹å ´åˆï¼ˆé–¾å€¤ç·©å’Œï¼‰
                        return compressed, method
                    else:
                        print(f"    [ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼] ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°åŠ¹æœä¸ååˆ†ã€æ¨™æº–åœ§ç¸®ã«åˆ‡ã‚Šæ›¿ãˆ")
                except Exception as e:
                    print(f"    [ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼] ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            
            if self.zstd_available:
                # TMCç†è«–ã«åŸºã¥ãå‹•çš„ãƒ¬ãƒ™ãƒ«é¸æŠ
                compression_level = self._select_optimal_level(size, stream_entropy)
                compressor = self.zstd_compressors[compression_level]
                
                try:
                    compressed = compressor.compress(data)
                    return compressed, f"zstd_{compression_level}"
                except Exception:
                    # æ¥µå°ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ç„¡åœ§ç¸®
                    return data, "store"
            
            # Zstdåˆ©ç”¨ä¸å¯ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if size > 8192:
                compressed = lzma.compress(data, preset=6)
                return compressed, "lzma_fallback"
            else:
                compressed = zlib.compress(data, level=6)
                return compressed, "zlib_fallback"
                
        except Exception:
            return data, "store"
    
    def _select_optimal_level(self, size: int, entropy: float) -> str:
        """
        TMCå‹•çš„ãƒ¬ãƒ™ãƒ«é¸æŠã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆå®Ÿè£…ï¼‰
        ã‚µã‚¤ã‚ºã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«åŸºã¥ãæœ€é©åŒ–
        """
        # è¶…ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆé«˜åº¦ã«æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼‰
        if entropy < 2.0:
            if size > 32768:  # å¤§ã‚µã‚¤ã‚º: è¶…é«˜åœ§ç¸®
                return 'ultra'
            else:  # å°ã‚µã‚¤ã‚º: é«˜åœ§ç¸®
                return 'high'
        
        # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼‰
        elif entropy < 4.0:
            if size > 16384:  # å¤§ã‚µã‚¤ã‚º: é«˜åœ§ç¸®
                return 'high'
            else:  # å°ã‚µã‚¤ã‚º: ãƒãƒ©ãƒ³ã‚¹å‹
                return 'balanced'
        
        # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆä¸€èˆ¬çš„ãªãƒ‡ãƒ¼ã‚¿ï¼‰
        elif entropy < 6.0:
            return 'balanced'
        
        # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã«è¿‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰
        else:
            if size < 4096:  # å°ã‚µã‚¤ã‚º: é«˜é€Ÿå‡¦ç†å„ªå…ˆ
                return 'fast'
            else:  # å¤§ã‚µã‚¤ã‚º: ãƒãƒ©ãƒ³ã‚¹å‹ã§è©¦è¡Œ
                return 'balanced'
    
    def decompress(self, compressed_data: bytes, method: str) -> bytes:
        """TMC v9.0çµ±ä¸€å±•é–‹å‡¦ç†ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¯¾å¿œ + é«˜é€Ÿãƒ‘ã‚¹å¯¾å¿œï¼‰"""
        try:
            # v9.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¾©å·
            if method.startswith("context_mixing"):
                return self.context_mixer.decode_context_mixing(compressed_data)
            # é«˜é€Ÿãƒ‘ã‚¹ç”¨zlibãƒ¡ã‚½ãƒƒãƒ‰
            elif method == "zlib_fast_path":
                return zlib.decompress(compressed_data)
            elif method.startswith("zstd_") and self.zstd_available:
                # Zstdå±•é–‹ã¯åœ§ç¸®ãƒ¬ãƒ™ãƒ«ã«é–¢ä¿‚ãªãå¸¸ã«é«˜é€Ÿ
                return self.zstd_decompressor.decompress(compressed_data)
            elif method == "lzma_fallback":
                return lzma.decompress(compressed_data)
            elif method == "zlib_fallback":
                return zlib.decompress(compressed_data)
            else:
                print(f"    [å±•é–‹] æœªçŸ¥ãƒ¡ã‚½ãƒƒãƒ‰ '{method}' - ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”å´")
                return compressed_data
                
        except Exception as e:
            print(f"    [å±•é–‹ã‚¨ãƒ©ãƒ¼] {method}: {e}")
            return compressed_data


class ImprovedDispatcher:
    """
    æ”¹è‰¯åˆ†æ&ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆçµ±åˆï¼‰
    ã‚ˆã‚Šç²¾å¯†ãªãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¤å®š
    """
    
    def dispatch(self, data_block: bytes) -> Tuple[DataType, Dict[str, Any]]:
        """æ”¹è‰¯ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ­ãƒƒã‚¯åˆ†æ"""
        print(f"[æ”¹è‰¯ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£] ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ­ãƒƒã‚¯ (ã‚µã‚¤ã‚º: {len(data_block)} bytes) ã‚’åˆ†æä¸­...")
        
        if len(data_block) == 0:
            return DataType.GENERIC_BINARY, {}
        
        features = self._extract_enhanced_features(data_block)
        data_type = self._classify_enhanced_data_type(features, data_block)
        
        # DataTypeæ–‡å­—åˆ—ã®å ´åˆã®å®‰å…¨ãªå‡¦ç†
        if isinstance(data_type, str):
            print(f"[æ”¹è‰¯ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£] åˆ¤å®š: {data_type}")
            return data_type, features
        else:
            print(f"[æ”¹è‰¯ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£] åˆ¤å®š: {data_type.value}")
            return data_type, features
    
    def _extract_enhanced_features(self, data: bytes) -> Dict[str, Any]:
        """æ‹¡å¼µç‰¹å¾´é‡æŠ½å‡º"""
        try:
            features = {}
            
            # åŸºæœ¬çµ±è¨ˆ
            data_array = np.frombuffer(data, dtype=np.uint8)
            features['size'] = len(data)
            features['entropy'] = self._calculate_entropy(data_array)
            features['variance'] = float(np.var(data_array))
            
            # ãƒ†ã‚­ã‚¹ãƒˆæ€§åˆ†æï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆæ¡ç”¨ï¼‰
            text_chars = sum(1 for byte in data if 32 <= byte <= 126 or byte in [9, 10, 13])
            features['text_ratio'] = text_chars / len(data) if len(data) > 0 else 0
            
            # æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿åˆ†æ
            features['is_float_candidate'] = (len(data) % 4 == 0 and len(data) > 100)
            
            # æ•´æ•°ç³»åˆ—æ€§åˆ†æ
            features['is_sequential_int_candidate'] = False
            if len(data) % 4 == 0 and len(data) > 100:
                try:
                    integers = np.frombuffer(data, dtype=np.int32)
                    if len(integers) > 1:
                        diffs = np.abs(np.diff(integers.astype(np.int64)))
                        features['int_diff_mean'] = float(np.mean(diffs))
                        features['is_sequential_int_candidate'] = features['int_diff_mean'] < 1000
                except Exception:
                    pass
            
            # åå¾©æ€§åˆ†æ
            if len(data) > 0:
                unique_ratio = len(np.unique(data_array)) / len(data_array)
                features['unique_ratio'] = unique_ratio
                features['repetition_score'] = 1.0 - unique_ratio
            
            # åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿æ¤œå‡º
            features['high_entropy'] = features['entropy'] > 7.5
            
            return features
            
        except Exception:
            return {'entropy': 4.0, 'size': len(data)}
    
    def _calculate_entropy(self, data_array: np.ndarray) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        try:
            byte_counts = np.bincount(data_array, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(data_array)
            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 4.0
    
    def _classify_enhanced_data_type(self, features: Dict[str, Any], data: bytes) -> DataType:
        """
        TMC v6.0 æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡
        åˆ¤å®šé †åºã®æœ€é©åŒ–ï¼ˆã‚ˆã‚Šç‰¹æ®Šã§ç¢ºåº¦ã®é«˜ã„ã‚‚ã®ã‹ã‚‰é †ã«åˆ¤å®šï¼‰
        """
        try:
            # 1. ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ¤å®šï¼ˆæœ€é«˜å„ªå…ˆåº¦ï¼‰
            if features.get('text_ratio', 0) > 0.85:
                return DataType.TEXT_DATA
            
            # 2. ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿åˆ¤å®šï¼ˆæµ®å‹•å°æ•°ç‚¹ã‚ˆã‚Šå…ˆã«åˆ¤å®šï¼‰
            if features.get('is_sequential_int_candidate', False):
                # è¿½åŠ æ¤œè¨¼: ã‚ˆã‚Šå³å¯†ãªç³»åˆ—æ€§ãƒã‚§ãƒƒã‚¯
                if len(data) % 4 == 0 and len(data) > 100:
                    try:
                        integers = np.frombuffer(data, dtype=np.int32)
                        if len(integers) > 1:
                            diffs = np.abs(np.diff(integers.astype(np.int64)))
                            consecutive_small_diffs = np.sum(diffs < 100)
                            if consecutive_small_diffs / len(diffs) > 0.7:  # 70%ä»¥ä¸ŠãŒå°ã•ãªå·®åˆ†
                                print(f"    [åˆ†é¡] ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿ç¢ºèª: å°å·®åˆ†ç‡={consecutive_small_diffs/len(diffs):.2%}")
                                return DataType.SEQUENTIAL_INT_DATA
                    except Exception:
                        pass
            
            # 3. æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿åˆ¤å®šï¼ˆç³»åˆ—æ•´æ•°ã®å¾Œã§åˆ¤å®šï¼‰
            if features.get('is_float_candidate', False):
                # è¿½åŠ æ¤œè¨¼: æµ®å‹•å°æ•°ç‚¹æ•°ã‚‰ã—ã•ã‚’ãƒã‚§ãƒƒã‚¯
                if len(data) % 4 == 0 and len(data) > 100:
                    try:
                        floats = np.frombuffer(data, dtype=np.float32)
                        # NaN, Inf ã§ãªã„æœ‰åŠ¹ãªæµ®å‹•å°æ•°ç‚¹æ•°ã®å‰²åˆã‚’ãƒã‚§ãƒƒã‚¯
                        valid_floats = np.isfinite(floats)
                        valid_ratio = np.sum(valid_floats) / len(floats)
                        
                        # ã•ã‚‰ã«ã€å€¤ã®ç¯„å›²ãŒæµ®å‹•å°æ•°ç‚¹ã‚‰ã—ã„ã‹ãƒã‚§ãƒƒã‚¯
                        if valid_ratio > 0.95:  # 95%ä»¥ä¸ŠãŒæœ‰åŠ¹ãªæµ®å‹•å°æ•°ç‚¹
                            valid_values = floats[valid_floats]
                            if len(valid_values) > 0:
                                try:
                                    # æ•°å€¤å®‰å®šæ€§ã‚’è€ƒæ…®ã—ãŸç¯„å›²è¨ˆç®—
                                    max_val = np.max(valid_values)
                                    min_val = np.min(valid_values)
                                    
                                    # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å›é¿ã®ãƒã‚§ãƒƒã‚¯
                                    if np.isfinite(max_val) and np.isfinite(min_val):
                                        # å·®åˆ†è¨ˆç®—å‰ã«å€¤ã®å¤§ãã•ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ãªç¯„å›²ã«åˆ¶é™ï¼‰
                                        safe_max_limit = 1e12  # ã‚ˆã‚Šå³ã—ã„åˆ¶é™
                                        if (abs(max_val) < safe_max_limit and abs(min_val) < safe_max_limit and 
                                            np.isfinite(max_val) and np.isfinite(min_val)):
                                            try:
                                                value_range = float(max_val - min_val)
                                                # å€¤ã®ç¯„å›²ãŒé©åº¦ã«å¤§ãã„ï¼ˆæ•´æ•°ç³»åˆ—ã§ãªã„ï¼‰ã‹ã¤æœ‰é™
                                                if np.isfinite(value_range) and value_range > 1.0:
                                                    print(f"    [åˆ†é¡] æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿ç¢ºèª: æœ‰åŠ¹ç‡={valid_ratio:.2%}, ç¯„å›²={value_range:.2f}")
                                                    return DataType.FLOAT_DATA
                                            except (OverflowError, ValueError):
                                                print(f"    [åˆ†é¡] æ•°å€¤ç¯„å›²è¨ˆç®—ã‚¨ãƒ©ãƒ¼ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                                        else:
                                            # å·¨å¤§ãªæ•°å€¤ã¯ãƒ­ã‚°å‡ºåŠ›ã‚’çœç•¥ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                                            pass
                                    else:
                                        print(f"    [åˆ†é¡] è­¦å‘Š: ç„¡é™å€¤ã¾ãŸã¯NaNãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                                        
                                except (OverflowError, RuntimeWarning, ValueError) as e:
                                    print(f"    [åˆ†é¡] æ•°å€¤è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e} - æµ®å‹•å°æ•°ç‚¹åˆ¤å®šã‚’ã‚¹ã‚­ãƒƒãƒ—")
                    except Exception:
                        pass
            
            # 4. é«˜åå¾©ãƒ‡ãƒ¼ã‚¿ï¼ˆå‰å›ã¨åŒã˜ï¼‰
            if features.get('repetition_score', 0) > 0.7:
                return DataType.REPETITIVE_BINARY
            
            # 5. åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå‰å›ã¨åŒã˜ï¼‰
            if features.get('high_entropy', False):
                return DataType.COMPRESSED_LIKE
            
            # 6. ãã®ä»–ã®æ§‹é€ çš„ãƒ‡ãƒ¼ã‚¿ï¼ˆå‰å›ã¨åŒã˜ï¼‰
            if features.get('entropy', 8) < 6.0:
                return DataType.STRUCTURED_NUMERIC
            
            # 7. æ±ç”¨ãƒã‚¤ãƒŠãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            return DataType.GENERIC_BINARY
            
        except Exception:
            return DataType.GENERIC_BINARY


class TDTTransformer:
    """
    TMC v5.0 é«˜åº¦å‹ä»˜ããƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆçµ±åˆï¼‰
    çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«åŸºã¥ãé©å¿œçš„ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†è§£
    """
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹é©å¿œçš„ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†è§£"""
        print("  [TDT] é«˜åº¦å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'tdt_clustered', 'original_size': len(data)}
        
        try:
            # 4ãƒã‚¤ãƒˆå˜ä½ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆæ¡ç”¨ï¼‰
            if len(data) % 4 != 0:
                print("    [TDT] ãƒ‡ãƒ¼ã‚¿ãŒ4ãƒã‚¤ãƒˆã®å€æ•°ã§ã¯ãªã„ãŸã‚ã€å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return [data], info
            
            # æµ®å‹•å°æ•°ç‚¹ã¨ã—ã¦è§£é‡ˆ
            floats = np.frombuffer(data, dtype=np.float32)
            byte_view = floats.view(np.uint8).reshape(-1, 4)
            
            print(f"    [TDT] {len(floats)}å€‹ã®æµ®å‹•å°æ•°ç‚¹æ•°ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: å„ãƒã‚¤ãƒˆä½ç½®ã®çµ±è¨ˆçš„ç‰¹å¾´æŠ½å‡º
            byte_features = []
            for i in range(4):
                byte_stream = byte_view[:, i]
                features = self._extract_byte_position_features(byte_stream, i)
                byte_features.append(features)
                print(f"    [TDT] ãƒã‚¤ãƒˆä½ç½® {i}: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼={features['entropy']:.2f}, åˆ†æ•£={features['variance']:.2f}")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            clusters = self._perform_statistical_clustering(byte_features)
            print(f"    [TDT] ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ: {len(clusters)}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼")
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«åŸºã¥ãã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
            streams = []
            cluster_info = []
            
            for cluster_id, byte_positions in enumerate(clusters):
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®ãƒã‚¤ãƒˆä½ç½®ã‚’çµåˆ
                cluster_data = bytearray()
                for pos in byte_positions:
                    cluster_data.extend(byte_view[:, pos].tobytes())
                
                stream = bytes(cluster_data)
                streams.append(stream)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆè¨ˆç®—
                cluster_entropy = self._calculate_stream_entropy(np.frombuffer(stream, dtype=np.uint8))
                cluster_info.append({
                    'positions': byte_positions,
                    'entropy': cluster_entropy,
                    'size': len(stream)
                })
                
                print(f"    [TDT] ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ {cluster_id} (ä½ç½®: {byte_positions}): ã‚µã‚¤ã‚º={len(stream)}, ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼={cluster_entropy:.2f}")
            
            info['byte_features'] = byte_features
            info['clusters'] = cluster_info
            info['stream_count'] = len(streams)
            info['clustering_method'] = 'statistical_similarity'
            
            return streams, info
            
        except Exception as e:
            print(f"    [TDT] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _extract_byte_position_features(self, byte_stream: np.ndarray, position: int) -> Dict[str, float]:
        """
        å„ãƒã‚¤ãƒˆä½ç½®ã®çµ±è¨ˆçš„ç‰¹å¾´æŠ½å‡ºï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆå®Ÿè£…ï¼‰
        """
        features = {
            'position': position,
            'entropy': self._calculate_stream_entropy(byte_stream),
            'variance': float(np.var(byte_stream)),
            'std_dev': float(np.std(byte_stream)),
            'unique_ratio': len(np.unique(byte_stream)) / len(byte_stream),
            'mean': float(np.mean(byte_stream))
        }
        
        # ç¯„å›²è¨ˆç®—ã®å®‰å…¨ãªå®Ÿè£…
        try:
            max_val = np.max(byte_stream)
            min_val = np.min(byte_stream)
            if np.isfinite(max_val) and np.isfinite(min_val):
                features['range'] = float(max_val - min_val)
            else:
                features['range'] = 0.0
        except (OverflowError, ValueError):
            features['range'] = 0.0
        
        # åˆ†å¸ƒã®åã‚Šï¼ˆæ­ªåº¦ï¼‰- æ”¹è‰¯ç‰ˆ
        try:
            import warnings
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                from scipy import stats
                features['skewness'] = float(stats.skew(byte_stream))
        except (ImportError, RuntimeWarning):
            # scipyãŒåˆ©ç”¨ã§ããªã„å ´åˆã‚„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®å®‰å…¨ãªè¨ˆç®—
            mean_val = features['mean']
            std_val = features['std_dev']
            if std_val > 1e-8:  # ã‚ˆã‚Šå®‰å…¨ãªé–¾å€¤
                normalized = (byte_stream.astype(np.float64) - mean_val) / std_val
                features['skewness'] = float(np.mean(normalized ** 3))
            else:
                features['skewness'] = 0.0
        
        return features
    
    def _perform_statistical_clustering(self, byte_features: List[Dict[str, float]]) -> List[List[int]]:
        """
        çµ±è¨ˆçš„ç‰¹å¾´ã«åŸºã¥ãéšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆå®Ÿè£…ï¼‰
        """
        try:
            # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æ§‹ç¯‰
            feature_vectors = []
            for features in byte_features:
                vector = [
                    features['entropy'],
                    features['variance'],
                    features['unique_ratio'],
                    features['skewness']
                ]
                feature_vectors.append(vector)
            
            feature_matrix = np.array(feature_vectors)
            
            # æ­£è¦åŒ–ï¼ˆZ-scoreï¼‰
            if feature_matrix.std(axis=0).sum() > 0:
                feature_matrix = (feature_matrix - feature_matrix.mean(axis=0)) / (feature_matrix.std(axis=0) + 1e-8)
            
            # è·é›¢è¡Œåˆ—è¨ˆç®—ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼‰
            n = len(byte_features)
            distance_matrix = np.zeros((n, n))
            
            for i in range(n):
                for j in range(i + 1, n):
                    distance = np.linalg.norm(feature_matrix[i] - feature_matrix[j])
                    distance_matrix[i, j] = distance_matrix[j, i] = distance
            
            # ç°¡æ˜“éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…
            clusters = self._simple_hierarchical_clustering(distance_matrix, threshold=1.0)
            
            return clusters
            
        except Exception as e:
            print(f"    [TDT] ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e} - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†å‰²ã‚’ä½¿ç”¨")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å›ºå®š4åˆ†å‰²
            return [[0], [1], [2], [3]]
    
    def _simple_hierarchical_clustering(self, distance_matrix: np.ndarray, threshold: float) -> List[List[int]]:
        """ç°¡æ˜“éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…"""
        n = distance_matrix.shape[0]
        clusters = [[i] for i in range(n)]  # åˆæœŸçŠ¶æ…‹: å„è¦ç´ ãŒç‹¬è‡ªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        
        while len(clusters) > 1:
            # æœ€ã‚‚è¿‘ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒšã‚¢ã‚’æ¢ç´¢
            min_distance = float('inf')
            merge_i, merge_j = -1, -1
            
            for i in range(len(clusters)):
                for j in range(i + 1, len(clusters)):
                    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é–“ã®å¹³å‡è·é›¢ã‚’è¨ˆç®—
                    total_distance = 0
                    count = 0
                    
                    for idx_i in clusters[i]:
                        for idx_j in clusters[j]:
                            total_distance += distance_matrix[idx_i, idx_j]
                            count += 1
                    
                    if count > 0:
                        avg_distance = total_distance / count
                        if avg_distance < min_distance:
                            min_distance = avg_distance
                            merge_i, merge_j = i, j
            
            # é–¾å€¤ãƒã‚§ãƒƒã‚¯
            if min_distance > threshold:
                break
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒ¼ã‚¸
            if merge_i != -1 and merge_j != -1:
                new_cluster = clusters[merge_i] + clusters[merge_j]
                new_clusters = []
                for i, cluster in enumerate(clusters):
                    if i != merge_i and i != merge_j:
                        new_clusters.append(cluster)
                new_clusters.append(new_cluster)
                clusters = new_clusters
            else:
                break
        
        return clusters
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """TDTçµ±è¨ˆçš„é€†å¤‰æ›"""
        print("  [TDT] çµ±è¨ˆçš„é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            if 'clusters' not in info:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥æ–¹å¼
                return self._legacy_inverse_transform(streams)
            
            clusters = info['clusters']
            
            if len(streams) != len(clusters):
                print("    [TDT] ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ãŒä¸ä¸€è‡´")
                return b''.join(streams)
            
            # å…ƒã®ãƒã‚¤ãƒˆé…åˆ—ã‚µã‚¤ã‚ºã‚’æ¨å®š
            total_elements = sum(len(stream) for stream in streams) // 4
            byte_view = np.zeros((total_elements, 4), dtype=np.uint8)
            
            # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ãƒã‚¤ãƒˆä½ç½®ã‚’å¾©å…ƒ
            for cluster_id, (stream, cluster_info) in enumerate(zip(streams, clusters)):
                positions = cluster_info['positions']
                stream_data = np.frombuffer(stream, dtype=np.uint8)
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’å„ãƒã‚¤ãƒˆä½ç½®ã«åˆ†æ•£é…ç½®
                elements_per_position = len(stream_data) // len(positions)
                
                for i, pos in enumerate(positions):
                    start_idx = i * elements_per_position
                    end_idx = (i + 1) * elements_per_position
                    if i == len(positions) - 1:  # æœ€å¾Œã®ä½ç½®ã¯æ®‹ã‚Šã™ã¹ã¦
                        end_idx = len(stream_data)
                    
                    position_data = stream_data[start_idx:end_idx]
                    if len(position_data) == total_elements:
                        byte_view[:, pos] = position_data
                    else:
                        # ã‚µã‚¤ã‚ºèª¿æ•´
                        min_len = min(len(position_data), total_elements)
                        byte_view[:min_len, pos] = position_data[:min_len]
            
            return byte_view.tobytes()
            
        except Exception as e:
            print(f"    [TDT] çµ±è¨ˆçš„é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _legacy_inverse_transform(self, streams: List[bytes]) -> bytes:
        """å¾“æ¥æ–¹å¼ã®é€†å¤‰æ›ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        try:
            if len(streams) != 4:
                return streams[0] if streams else b''
            
            stream_lengths = [len(s) for s in streams]
            if len(set(stream_lengths)) != 1:
                return b''.join(streams)
            
            num_floats = stream_lengths[0]
            byte_view = np.empty((num_floats, 4), dtype=np.uint8)
            
            for i, stream in enumerate(streams):
                byte_view[:, i] = np.frombuffer(stream, dtype=np.uint8)
            
            return byte_view.tobytes()
            
        except Exception:
            return b''.join(streams)
    
    def _calculate_stream_entropy(self, stream: np.ndarray) -> float:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        try:
            byte_counts = np.bincount(stream, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(stream)
            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 8.0


class LeCoAdvancedTransformer:
    """
    TMC v8.0 é«˜åº¦æ©Ÿæ¢°å­¦ç¿’å¤‰æ›ï¼ˆå¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°å¯¾å¿œï¼‰
    å±€æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³é©å¿œã«ã‚ˆã‚‹æ¥µé™åœ§ç¸®ç‡å®Ÿç¾
    """
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """LeCo v8.0å¤‰æ›ï¼šå¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚° + å±€æ‰€æœ€é©åŒ–"""
        print("  [LeCo v8.0] å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'leco_variable_partitioning', 'original_size': len(data)}
        
        try:
            # 4ãƒã‚¤ãƒˆå˜ä½ãƒã‚§ãƒƒã‚¯
            if len(data) % 4 != 0:
                print("    [LeCo v8.0] ãƒ‡ãƒ¼ã‚¿ãŒ4ãƒã‚¤ãƒˆã®å€æ•°ã§ã¯ãªã„ãŸã‚ã€å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return [data], info
            
            integers = np.frombuffer(data, dtype=np.int32)
            print(f"    [LeCo v8.0] {len(integers)}å€‹ã®æ•´æ•°ã‚’å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ä¸­...")
            
            # å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
            partitions = self._variable_length_partitioning(integers)
            print(f"    [LeCo v8.0] {len(partitions)}å€‹ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ")
            
            # å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨
            partition_streams = []
            partition_infos = []
            
            for i, partition_data in enumerate(partitions):
                partition_result = self._optimize_partition(partition_data, i)
                partition_streams.extend(partition_result['streams'])
                partition_infos.append(partition_result['info'])
                
                print(f"    [ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ {i}] é•·ã•={len(partition_data)}, ãƒ¢ãƒ‡ãƒ«={partition_result['info']['model_type']}, "
                      f"åœ§ç¸®ã‚¹ã‚³ã‚¢={partition_result['info']['compression_score']:.2f}")
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦è¿½åŠ 
            partition_header = self._create_partition_header(partition_infos, len(integers))
            final_streams = [partition_header] + partition_streams
            
            # çµ±è¨ˆæƒ…å ±æ›´æ–°
            total_score = sum(p['compression_score'] for p in partition_infos)
            avg_score = total_score / len(partition_infos) if partition_infos else 32.0
            
            info.update({
                'partition_count': len(partitions),
                'partition_infos': partition_infos,
                'average_compression_score': avg_score,
                'variable_partitioning': True
            })
            
            return final_streams, info
            
        except Exception as e:
            print(f"    [LeCo v8.0] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _variable_length_partitioning(self, integers: np.ndarray, threshold_bits: int = 8) -> List[np.ndarray]:
        """
        Greedyã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
        æ®‹å·®ãŒé–¾å€¤ä»¥ä¸‹ã«ãªã‚‹ã‚ˆã†ã«å‹•çš„ã«åˆ†å‰²
        """
        partitions = []
        current_start = 0
        max_residual_value = (1 << (threshold_bits - 1)) - 1  # 8bit: 127
        
        while current_start < len(integers):
            # è²ªæ¬²ã«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’æ‹¡å¼µ
            best_end = current_start + 1
            best_model = None
            
            # æœ€å°ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºï¼ˆçµ±è¨ˆçš„æ„å‘³ã‚’æŒã¤ãŸã‚ï¼‰
            min_partition_size = max(3, min(50, len(integers) // 20))
            max_partition_size = min(len(integers) - current_start, 1000)  # æœ€å¤§1000è¦ç´ 
            
            for potential_end in range(
                min(current_start + min_partition_size, len(integers)),
                min(current_start + max_partition_size + 1, len(integers) + 1)
            ):
                partition_data = integers[current_start:potential_end]
                
                # ã“ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ
                best_partition_model = self._find_best_model_for_partition(partition_data)
                
                if best_partition_model is None:
                    break
                
                # æ®‹å·®ãŒé–¾å€¤ä»¥ä¸‹ã‹ç¢ºèª
                max_residual = np.max(np.abs(best_partition_model['residuals']))
                if max_residual <= max_residual_value:
                    best_end = potential_end
                    best_model = best_partition_model
                else:
                    # é–¾å€¤ã‚’è¶…ãˆãŸã®ã§ã€ã“ã“ã§åˆ†å‰²
                    break
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ç¢ºå®š
            partition_data = integers[current_start:best_end]
            partitions.append(partition_data)
            
            current_start = best_end
            
            # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
            if current_start >= len(integers):
                break
        
        return partitions
    
    def _find_best_model_for_partition(self, partition_data: np.ndarray) -> Optional[Dict[str, Any]]:
        """ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ã®æœ€é©ãƒ¢ãƒ‡ãƒ«æ¢ç´¢"""
        try:
            models_to_try = []
            
            # å®šæ•°ãƒ¢ãƒ‡ãƒ«
            try:
                const_result = self._try_constant_model(partition_data)
                models_to_try.append(const_result)
            except Exception:
                pass
            
            # ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒååˆ†ãªå ´åˆï¼‰
            if len(partition_data) >= 3:
                try:
                    linear_result = self._try_linear_model(partition_data)
                    models_to_try.append(linear_result)
                except Exception:
                    pass
            
            # äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒååˆ†ãªå ´åˆï¼‰
            if len(partition_data) >= 5:
                try:
                    quad_result = self._try_quadratic_model(partition_data)
                    models_to_try.append(quad_result)
                except Exception:
                    pass
            
            if not models_to_try:
                return None
            
            # æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠ
            best_model = min(models_to_try, key=lambda x: x['score'])
            return best_model
            
        except Exception:
            return None
    
    def _optimize_partition(self, partition_data: np.ndarray, partition_id: int) -> Dict[str, Any]:
        """å€‹åˆ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®æœ€é©åŒ–"""
        best_model = self._find_best_model_for_partition(partition_data)
        
        if best_model is None:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            mean_val = np.mean(partition_data)
            residuals = partition_data - int(mean_val)
            best_model = {
                'type': 'constant_fallback',
                'params': {'c': float(mean_val)},
                'residuals': residuals,
                'score': 32.0
            }
        
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ä½œæˆ
        partition_info = {
            'partition_id': partition_id,
            'model_type': best_model['type'],
            'params': best_model['params'],
            'data_length': len(partition_data),
            'compression_score': best_model['score'],
            'max_residual': int(np.max(np.abs(best_model['residuals']))) if len(best_model['residuals']) > 0 else 0
        }
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
        model_info_json = json.dumps(partition_info, separators=(',', ':'))
        model_info_bytes = model_info_json.encode('utf-8')
        model_header = len(model_info_bytes).to_bytes(4, 'big') + model_info_bytes
        
        residuals_stream = best_model['residuals'].astype(np.int32).tobytes()
        
        return {
            'info': partition_info,
            'streams': [model_header, residuals_stream]
        }
    
    def _create_partition_header(self, partition_infos: List[Dict], total_length: int) -> bytes:
        """ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        header_data = {
            'total_length': total_length,
            'partition_count': len(partition_infos),
            'partitions': [
                {
                    'id': p['partition_id'],
                    'length': p['data_length'],
                    'model': p['model_type']
                } for p in partition_infos
            ]
        }
        
        header_json = json.dumps(header_data, separators=(',', ':'))
        header_bytes = header_json.encode('utf-8')
        
        return len(header_bytes).to_bytes(4, 'big') + header_bytes
    
    # æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«è©¦è¡Œãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆ_try_constant_model, _try_linear_model, _try_quadratic_modelï¼‰ã¯ç¶™æ‰¿
    def _try_constant_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """å®šæ•°ãƒ¢ãƒ‡ãƒ«: y = c (Frame-of-Referenceåœ§ç¸®ç›¸å½“)"""
        mean_val = np.mean(integers)
        constant = int(round(mean_val))
        
        residuals = integers - constant
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        
        # æ®‹å·®ã‚’æ ¼ç´ã™ã‚‹ã®ã«å¿…è¦ãªãƒ“ãƒƒãƒˆæ•°ã‚’è¨ˆç®—
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1  # ç¬¦å·ãƒ“ãƒƒãƒˆå«ã‚€
        compression_score = float(bits_needed)
        
        return {
            'type': 'constant',
            'params': {'c': float(constant)},
            'residuals': residuals,
            'score': compression_score
        }
    
    def _try_linear_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """ç·šå½¢ãƒ¢ãƒ‡ãƒ«: y = ax + b"""
        x = np.arange(len(integers))
        slope, intercept = np.polyfit(x, integers, 1)
        
        predicted_values = (slope * x + intercept).astype(np.int32)
        residuals = integers - predicted_values
        
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ ¼ç´ã‚³ã‚¹ãƒˆã‚‚è€ƒæ…®ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        param_cost = 64  # slope + intercept (å„32bitæƒ³å®š)
        total_bits = bits_needed * len(integers) + param_cost
        compression_score = float(total_bits) / len(integers)
        
        return {
            'type': 'linear',
            'params': {'slope': float(slope), 'intercept': float(intercept)},
            'residuals': residuals,
            'score': compression_score
        }
    
    def _try_quadratic_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«: y = ax^2 + bx + c"""
        x = np.arange(len(integers))
        coeffs = np.polyfit(x, integers, 2)  # [a, b, c]
        
        predicted_values = np.polyval(coeffs, x).astype(np.int32)
        residuals = integers - predicted_values
        
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ ¼ç´ã‚³ã‚¹ãƒˆã‚‚è€ƒæ…®
        param_cost = 96  # a + b + c (å„32bitæƒ³å®š)
        total_bits = bits_needed * len(integers) + param_cost
        compression_score = float(total_bits) / len(integers)
        
        return {
            'type': 'quadratic',
            'params': {'a': float(coeffs[0]), 'b': float(coeffs[1]), 'c': float(coeffs[2])},
            'residuals': residuals,
            'score': compression_score
        }
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """LeCo v8.0 å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°é€†å¤‰æ›"""
        print("  [LeCo v8.0] å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            if not info.get('variable_partitioning', False):
                # v7.0äº’æ›ãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                return self._legacy_inverse_transform(streams, info)
            
            if len(streams) < 1:
                return b''
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã®è§£æ
            partition_header = streams[0]
            header_size = int.from_bytes(partition_header[:4], 'big')
            header_json = partition_header[4:4+header_size].decode('utf-8')
            header_data = json.loads(header_json)
            
            total_length = header_data['total_length']
            partition_count = header_data['partition_count']
            
            print(f"    [LeCo v8.0] ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {partition_count}, ç·é•·: {total_length}")
            
            # å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†
            reconstructed_data = np.zeros(total_length, dtype=np.int32)
            current_pos = 0
            stream_idx = 1  # ãƒ˜ãƒƒãƒ€ãƒ¼å¾Œã‹ã‚‰é–‹å§‹
            
            for _ in range(partition_count):
                # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã®å¾©å…ƒ
                if stream_idx >= len(streams):
                    break
                    
                model_header = streams[stream_idx]
                model_size = int.from_bytes(model_header[:4], 'big')
                model_json = model_header[4:4+model_size].decode('utf-8')
                partition_info = json.loads(model_json)
                
                # æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®å¾©å…ƒ
                if stream_idx + 1 >= len(streams):
                    break
                    
                residuals_stream = streams[stream_idx + 1]
                residuals = np.frombuffer(residuals_stream, dtype=np.int32)
                
                # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒ
                partition_data = self._reconstruct_partition(residuals, partition_info)
                
                # å…¨ä½“é…åˆ—ã«é…ç½®
                end_pos = current_pos + len(partition_data)
                if end_pos <= total_length:
                    reconstructed_data[current_pos:end_pos] = partition_data
                    current_pos = end_pos
                
                stream_idx += 2
                
                print(f"    [ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ {partition_info['partition_id']}] å¾©å…ƒå®Œäº†: {len(partition_data)}è¦ç´ ")
            
            return reconstructed_data.tobytes()
            
        except Exception as e:
            print(f"    [LeCo v8.0] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _reconstruct_partition(self, residuals: np.ndarray, partition_info: Dict[str, Any]) -> np.ndarray:
        """å€‹åˆ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®å¾©å…ƒ"""
        model_type = partition_info['model_type']
        params = partition_info['params']
        data_length = partition_info['data_length']
        
        if model_type == 'constant' or model_type == 'constant_fallback':
            constant = int(params['c'])
            return residuals + constant
            
        elif model_type == 'linear':
            slope = params['slope']
            intercept = params['intercept']
            x = np.arange(len(residuals))
            predicted_values = (slope * x + intercept).astype(np.int32)
            return predicted_values + residuals
            
        elif model_type == 'quadratic':
            a, b, c = params['a'], params['b'], params['c']
            x = np.arange(len(residuals))
            predicted_values = (a * x*x + b * x + c).astype(np.int32)
            return predicted_values + residuals
            
        else:
            return residuals
    
    def _legacy_inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """v7.0äº’æ›é€†å¤‰æ›"""
        try:
            if len(streams) != 2:
                return streams[0] if streams else b''
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å¾©å…ƒ
            model_header = streams[0]
            residuals_stream = streams[1]
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒ˜ãƒƒãƒ€ãƒ¼ã®è§£æ
            model_info_size = int.from_bytes(model_header[:4], 'big')
            model_info_json = model_header[4:4+model_info_size].decode('utf-8')
            model_info = json.loads(model_info_json)
            
            model_type = model_info['model_type']
            params = model_info['params']
            data_length = model_info['data_length']
            
            # æ®‹å·®ã®å¾©å…ƒ
            residuals = np.frombuffer(residuals_stream, dtype=np.int32)
            
            print(f"    [LeCo] ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
            print(f"    [LeCo] ãƒ‡ãƒ¼ã‚¿é•·: {data_length}")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®é€†å¤‰æ›
            if model_type == 'constant' or model_type == 'constant_fallback':
                constant = int(params['c'])
                original_integers = residuals + constant
                
            elif model_type == 'linear':
                slope = params['slope']
                intercept = params['intercept']
                x = np.arange(len(residuals))
                predicted_values = (slope * x + intercept).astype(np.int32)
                original_integers = predicted_values + residuals
                
            elif model_type == 'quadratic':
                a, b, c = params['a'], params['b'], params['c']
                x = np.arange(len(residuals))
                predicted_values = (a * x*x + b * x + c).astype(np.int32)
                original_integers = predicted_values + residuals
                
            else:
                print(f"    [LeCo] æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
                return b''.join(streams)
            
            return original_integers.tobytes()
            
        except Exception as e:
            print(f"    [LeCo] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)


class LeCoTransformer:
    """
    TMC v6.0 é«˜åº¦æ©Ÿæ¢°å­¦ç¿’å¤‰æ›ï¼ˆãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰
    å‹•çš„ãƒ¢ãƒ‡ãƒ«é¸æŠã«ã‚ˆã‚‹äºˆæ¸¬åœ§ç¸®ã®æœ€é©åŒ–
    """
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """LeCo v6.0å¤‰æ›ï¼šè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å‹•çš„é¸æŠ"""
        print("  [LeCo] TMC v6.0 ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'leco_multimodel', 'original_size': len(data)}
        
        try:
            # 4ãƒã‚¤ãƒˆå˜ä½ãƒã‚§ãƒƒã‚¯
            if len(data) % 4 != 0:
                print("    [LeCo] ãƒ‡ãƒ¼ã‚¿ãŒ4ãƒã‚¤ãƒˆã®å€æ•°ã§ã¯ãªã„ãŸã‚ã€å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return [data], info
            
            integers = np.frombuffer(data, dtype=np.int32)
            print(f"    [LeCo] {len(integers)}å€‹ã®æ•´æ•°ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
            
            # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®è©¦è¡Œã¨æœ€é©é¸æŠ
            best_model = self._select_optimal_model(integers)
            
            model_type = best_model['type']
            params = best_model['params']
            residuals = best_model['residuals']
            compression_score = best_model['score']
            
            print(f"    [LeCo] æœ€é©ãƒ¢ãƒ‡ãƒ«: {model_type}")
            print(f"    [LeCo] åœ§ç¸®ã‚¹ã‚³ã‚¢: {compression_score:.2f} bits/element")
            print(f"    [LeCo] æ®‹å·®ç¯„å›²: [{np.min(residuals)}, {np.max(residuals)}]")
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
            model_info = {
                'model_type': model_type,
                'params': params,
                'data_length': len(integers)
            }
            model_info_json = json.dumps(model_info, separators=(',', ':'))
            model_info_bytes = model_info_json.encode('utf-8')
            model_header = len(model_info_bytes).to_bytes(4, 'big') + model_info_bytes
            
            # æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
            residuals_stream = residuals.astype(np.int32).tobytes()
            
            # çµ±è¨ˆæƒ…å ±æ›´æ–°
            info.update({
                'model_type': model_type,
                'compression_score': compression_score,
                'residual_variance': float(np.var(residuals)),
                'model_params': params
            })
            
            return [model_header, residuals_stream], info
            
        except Exception as e:
            print(f"    [LeCo] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _select_optimal_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œã—ã€æœ€é©ãªã‚‚ã®ã‚’å‹•çš„é¸æŠ"""
        models_to_try = []
        
        # 1. å®šæ•°ãƒ¢ãƒ‡ãƒ« (Constant Model)
        try:
            const_result = self._try_constant_model(integers)
            models_to_try.append(const_result)
            print(f"    [LeCo] å®šæ•°ãƒ¢ãƒ‡ãƒ«: {const_result['score']:.2f} bits/element")
        except Exception as e:
            print(f"    [LeCo] å®šæ•°ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 2. ç·šå½¢ãƒ¢ãƒ‡ãƒ« (Linear Model)
        try:
            linear_result = self._try_linear_model(integers)
            models_to_try.append(linear_result)
            print(f"    [LeCo] ç·šå½¢ãƒ¢ãƒ‡ãƒ«: {linear_result['score']:.2f} bits/element")
        except Exception as e:
            print(f"    [LeCo] ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 3. äºŒæ¬¡ãƒ¢ãƒ‡ãƒ« (Quadratic Model) - ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        if len(integers) >= 10:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ç‚¹ãŒã‚ã‚‹å ´åˆã®ã¿
            try:
                quad_result = self._try_quadratic_model(integers)
                models_to_try.append(quad_result)
                print(f"    [LeCo] äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«: {quad_result['score']:.2f} bits/element")
            except Exception as e:
                print(f"    [LeCo] äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆæœ€å°ã‚¹ã‚³ã‚¢ï¼‰
        if not models_to_try:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å®šæ•°ãƒ¢ãƒ‡ãƒ«
            mean_val = np.mean(integers)
            residuals = integers - int(mean_val)
            return {
                'type': 'constant_fallback',
                'params': {'c': float(mean_val)},
                'residuals': residuals,
                'score': 32.0  # ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¹ã‚³ã‚¢
            }
        
        best_model = min(models_to_try, key=lambda x: x['score'])
        return best_model
    
    def _try_constant_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """å®šæ•°ãƒ¢ãƒ‡ãƒ«: y = c (Frame-of-Referenceåœ§ç¸®ç›¸å½“)"""
        mean_val = np.mean(integers)
        constant = int(round(mean_val))
        
        residuals = integers - constant
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        
        # æ®‹å·®ã‚’æ ¼ç´ã™ã‚‹ã®ã«å¿…è¦ãªãƒ“ãƒƒãƒˆæ•°ã‚’è¨ˆç®—
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1  # ç¬¦å·ãƒ“ãƒƒãƒˆå«ã‚€
        compression_score = float(bits_needed)
        
        return {
            'type': 'constant',
            'params': {'c': float(constant)},
            'residuals': residuals,
            'score': compression_score
        }
    
    def _try_linear_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """ç·šå½¢ãƒ¢ãƒ‡ãƒ«: y = ax + b"""
        x = np.arange(len(integers))
        slope, intercept = np.polyfit(x, integers, 1)
        
        predicted_values = (slope * x + intercept).astype(np.int32)
        residuals = integers - predicted_values
        
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ ¼ç´ã‚³ã‚¹ãƒˆã‚‚è€ƒæ…®ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        param_cost = 64  # slope + intercept (å„32bitæƒ³å®š)
        total_bits = bits_needed * len(integers) + param_cost
        compression_score = float(total_bits) / len(integers)
        
        return {
            'type': 'linear',
            'params': {'slope': float(slope), 'intercept': float(intercept)},
            'residuals': residuals,
            'score': compression_score
        }
    
    def _try_quadratic_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«: y = ax^2 + bx + c"""
        x = np.arange(len(integers))
        coeffs = np.polyfit(x, integers, 2)  # [a, b, c]
        
        predicted_values = np.polyval(coeffs, x).astype(np.int32)
        residuals = integers - predicted_values
        
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ ¼ç´ã‚³ã‚¹ãƒˆã‚‚è€ƒæ…®
        param_cost = 96  # a + b + c (å„32bitæƒ³å®š)
        total_bits = bits_needed * len(integers) + param_cost
        compression_score = float(total_bits) / len(integers)
        
        return {
            'type': 'quadratic',
            'params': {'a': float(coeffs[0]), 'b': float(coeffs[1]), 'c': float(coeffs[2])},
            'residuals': residuals,
            'score': compression_score
        }
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """LeCo v6.0ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«é€†å¤‰æ›"""
        print("  [LeCo] TMC v6.0 ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            if len(streams) != 2:
                return streams[0] if streams else b''
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å¾©å…ƒ
            model_header = streams[0]
            residuals_stream = streams[1]
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒ˜ãƒƒãƒ€ãƒ¼ã®è§£æ
            model_info_size = int.from_bytes(model_header[:4], 'big')
            model_info_json = model_header[4:4+model_info_size].decode('utf-8')
            model_info = json.loads(model_info_json)
            
            model_type = model_info['model_type']
            params = model_info['params']
            data_length = model_info['data_length']
            
            # æ®‹å·®ã®å¾©å…ƒ
            residuals = np.frombuffer(residuals_stream, dtype=np.int32)
            
            print(f"    [LeCo] ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
            print(f"    [LeCo] ãƒ‡ãƒ¼ã‚¿é•·: {data_length}")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®é€†å¤‰æ›
            if model_type == 'constant' or model_type == 'constant_fallback':
                constant = int(params['c'])
                original_integers = residuals + constant
                
            elif model_type == 'linear':
                slope = params['slope']
                intercept = params['intercept']
                x = np.arange(len(residuals))
                predicted_values = (slope * x + intercept).astype(np.int32)
                original_integers = predicted_values + residuals
                
            elif model_type == 'quadratic':
                a, b, c = params['a'], params['b'], params['c']
                x = np.arange(len(residuals))
                predicted_values = (a * x*x + b * x + c).astype(np.int32)
                original_integers = predicted_values + residuals
                
            else:
                print(f"    [LeCo] æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
                return b''.join(streams)
            
            return original_integers.tobytes()
            
        except Exception as e:
            print(f"    [LeCo] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)


class BWTTransformer:
    """
    TMC v8.1 å®Œå…¨å …ç‰¢åŒ–BWTTransformerï¼ˆpydivsufsortå®Œå…¨æº–æ‹ ï¼‰
    ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–ã®æ¥µé™å®Ÿè£… + å¯é€†æ€§å•é¡Œã®æ ¹æœ¬çš„è§£æ±º
    """
    
    def __init__(self):
        try:
            # pydivsufsortã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨é€†å¤‰æ›é–¢æ•°ã®å­˜åœ¨ç¢ºèª
            import pydivsufsort
            self.pydivsufsort_available = True
            self.pydivsufsort = pydivsufsort
            print("ğŸ”¥ pydivsufsortåˆ©ç”¨å¯èƒ½ - é«˜é€ŸBWT + å …ç‰¢ãªé€†å¤‰æ›æœ‰åŠ¹")
        except ImportError:
            self.pydivsufsort_available = False
            print("âš ï¸ pydivsufsortæœªåˆ©ç”¨ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…")
        
        self.post_bwt_pipeline = PostBWTPipeline()
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """TMC v8.1 å®Œå…¨å …ç‰¢åŒ–BWTå¤‰æ›ï¼ˆpydivsufsortå®Œå…¨æº–æ‹ ï¼‰"""
        print("  [å¼·åŒ–BWT] TMC v8.1 å°‚é–€å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'enhanced_bwt_mtf_rle', 'original_size': len(data)}
        
        try:
            if not data:
                return [data], info
            
            # å‹•çš„ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆä¸¦åˆ—å‡¦ç†å‰æã§æ‹¡å¼µï¼‰
            MAX_BWT_SIZE = 2 * 1024 * 1024  # 2MBåˆ¶é™
            if len(data) > MAX_BWT_SIZE:
                print(f"    [å¼·åŒ–BWT] ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º({len(data)})ãŒåˆ¶é™({MAX_BWT_SIZE})ã‚’è¶…é - BWTã‚¹ã‚­ãƒƒãƒ—")
                info['method'] = 'bwt_skipped_large'
                return [data], info
            
            # pydivsufsortã«å®Œå…¨æº–æ‹ ã—ãŸBWTå®Ÿè£…
            if self.pydivsufsort_available:
                try:
                    print(f"    [å¼·åŒ–BWT] pydivsufsortã§BWTå®Ÿè¡Œä¸­...")
                    # pydivsufsortã¯(primary_index, bwt_array)ã®é †åºã§è¿”ã™
                    primary_index, bwt_array = self.pydivsufsort.bw_transform(data)
                    bwt_encoded = bytes(bwt_array)  # ndarrayã‚’bytesã«å¤‰æ›
                    print(f"    [å¼·åŒ–BWT] pydivsufsortæˆåŠŸ: BWT={len(bwt_encoded)}, index={primary_index}")
                except Exception as pyd_error:
                    print(f"    [å¼·åŒ–BWT] pydivsufsortã‚¨ãƒ©ãƒ¼: {pyd_error}")
                    print(f"    [å¼·åŒ–BWT] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«åˆ‡ã‚Šæ›¿ãˆ")
                    bwt_encoded, primary_index = self._fallback_bwt_transform(data)
            else:
                bwt_encoded, primary_index = self._fallback_bwt_transform(data)
            
            # primary_indexã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
            if not (0 <= primary_index < len(bwt_encoded)):
                raise ValueError(f"Invalid primary_index {primary_index} for BWT length {len(bwt_encoded)}")
            
            # Move-to-Frontå¤‰æ›
            mtf_encoded = self._mtf_encode(bwt_encoded)
            print(f"    [å¼·åŒ–BWT] BWTå¾Œ: {len(bwt_encoded)} bytes -> MTFå¾Œ: {len(mtf_encoded)} bytes")
            
            # MTFå¾Œã®ã‚¼ãƒ­ç‡è¨ˆç®—ï¼ˆåœ§ç¸®åŠ¹æœã®æŒ‡æ¨™ï¼‰
            zero_count = mtf_encoded.count(0)
            zero_ratio = zero_count / len(mtf_encoded) if len(mtf_encoded) > 0 else 0
            print(f"    [MTF] ã‚¼ãƒ­ã®æ¯”ç‡: {zero_ratio:.2%} (é«˜ã„ã»ã©åœ§ç¸®åŠ¹æœå¤§)")
            
            # ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆï¼ˆRLE + åˆ†å‰²ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ï¼‰
            post_bwt_streams = self.post_bwt_pipeline.encode(mtf_encoded)
            print(f"    [å¼·åŒ–BWT] ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: {len(post_bwt_streams)}ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ")
            
            # primary_indexã‚’ãƒã‚¤ãƒˆé…åˆ—ã¨ã—ã¦å…ˆé ­ã«é…ç½®
            index_bytes = primary_index.to_bytes(4, 'big')
            final_streams = [index_bytes] + post_bwt_streams
            
            # æƒ…å ±æ›´æ–°
            info.update({
                'bwt_size': len(bwt_encoded),
                'mtf_size': len(mtf_encoded),
                'zero_ratio': zero_ratio,
                'primary_index': primary_index,
                'enhanced_pipeline': True,
                'stream_count': len(final_streams)
            })
            
            return final_streams, info
            
        except Exception as e:
            print(f"    [å¼·åŒ–BWT] ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ã—ã¦ã‚¹ã‚­ãƒƒãƒ—
            info['method'] = 'bwt_error_skip'
            info['error'] = str(e)
            return [data], info
            print(f"    [å¼·åŒ–BWT] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _fallback_bwt_transform(self, data: bytes) -> Tuple[bytes, int]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®æ¨™æº–BWTå®Ÿè£…"""
        # æ”¹è‰¯ç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
        data_with_sentinel = data + b'\x00'  # ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—è¿½åŠ 
        n = len(data_with_sentinel)
        
        # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªrotationç”Ÿæˆ
        rotations = []
        for i in range(n):
            rotation = data_with_sentinel[i:] + data_with_sentinel[:i]
            rotations.append((rotation, i))
        
        # ã‚½ãƒ¼ãƒˆ
        rotations.sort(key=lambda x: x[0])
        
        # å…ƒã®æ–‡å­—åˆ—ã®ä½ç½®ã‚’ç‰¹å®š
        primary_index = 0
        for idx, (rotation, original_pos) in enumerate(rotations):
            if original_pos == 0:
                primary_index = idx
                break
        
        # BWTæ–‡å­—åˆ—ç”Ÿæˆ
        bwt_encoded = bytes(rotation[0][-1] for rotation, _ in rotations)
        
        return bwt_encoded, primary_index
    
    def _mtf_encode(self, data: bytes) -> bytes:
        """Move-to-Frontå¤‰æ›ï¼ˆBWTã®å±€æ‰€æ€§ã‚’å°ã•ãªæ•´æ•°ã«å¤‰æ›ï¼‰"""
        alphabet = list(range(256))
        encoded = bytearray()
        
        for byte_val in data:
            rank = alphabet.index(byte_val)
            encoded.append(rank)
            # è¦‹ã¤ã‹ã£ãŸæ–‡å­—ã‚’ãƒªã‚¹ãƒˆã®å…ˆé ­ã«ç§»å‹•
            alphabet.pop(rank)
            alphabet.insert(0, byte_val)
        
        return bytes(encoded)
    
    def _mtf_decode(self, encoded_data: bytes) -> bytes:
        """é€†Move-to-Frontå¤‰æ›"""
        alphabet = list(range(256))
        decoded = bytearray()
        
        for rank in encoded_data:
            byte_val = alphabet[rank]
            decoded.append(byte_val)
            # è¦‹ã¤ã‹ã£ãŸæ–‡å­—ã‚’ãƒªã‚¹ãƒˆã®å…ˆé ­ã«ç§»å‹•
            alphabet.pop(rank)
            alphabet.insert(0, byte_val)
        
        return bytes(decoded)
    
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """TMC v8.1 å®Œå…¨å …ç‰¢åŒ–BWTé€†å¤‰æ›ï¼ˆpydivsufsortå®Œå…¨æº–æ‹ ï¼‰"""
        print("  [å¼·åŒ–BWT] TMC v8.1 å°‚é–€é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            # BWTãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸå ´åˆã®å‡¦ç†
            if info.get('method') in ['bwt_skipped_large', 'bwt_error_skip']:
                print(f"    [å¼·åŒ–BWT] {info.get('method')}ãƒ‡ãƒ¼ã‚¿ - å…ƒãƒ‡ãƒ¼ã‚¿è¿”å´")
                return streams[0] if streams else b''
            
            if len(streams) < 1:
                return b''
            
            # primary_indexã®å¾©å…ƒ
            primary_index = int.from_bytes(streams[0], 'big')
            
            # ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€†å¤‰æ›
            if info.get('enhanced_pipeline', False):
                print("    [ãƒã‚¹ãƒˆBWT] RLEé€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
                mtf_encoded = self.post_bwt_pipeline.decode(streams[1:])
            else:
                mtf_encoded = streams[1] if len(streams) > 1 else b''
            
            # é€†MTFå¤‰æ›
            if info.get('mtf_applied', True):
                bwt_encoded = self._mtf_decode(mtf_encoded)
                print(f"    [MTF] é€†MTF: {len(mtf_encoded)} bytes -> {len(bwt_encoded)} bytes")
            else:
                bwt_encoded = mtf_encoded
            
            # --- é€†BWTãƒ­ã‚¸ãƒƒã‚¯ã®ä¿®æ­£ï¼ˆæ ¹æœ¬çš„è§£æ±ºï¼‰ ---
            if self.pydivsufsort_available:
                # pydivsufsortãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ã€ãã®é€†å¤‰æ›ã®ã¿ã‚’ä½¿ç”¨
                print("    [BWT] pydivsufsortã«ã‚ˆã‚‹å …ç‰¢ãªé€†å¤‰æ›ã‚’å®Ÿè¡Œ")
                # pydivsufsortã®é€†å¤‰æ›: (primary_index, bwt_array) -> original_array
                try:
                    import numpy as np
                    # bytesã‚’writableãªndarrayã«å¤‰æ›
                    bwt_array = np.array(list(bwt_encoded), dtype=np.uint8)
                    original_array = self.pydivsufsort.inverse_bw_transform(primary_index, bwt_array)
                    original_data = bytes(original_array)
                except Exception as inv_error:
                    print(f"    [BWT] pydivsufsorté€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {inv_error}")
                    print(f"    [BWT] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é€†å¤‰æ›ã«åˆ‡ã‚Šæ›¿ãˆ")
                    original_data = self._fallback_bwt_inverse(bwt_encoded, primary_index)
            else:
                # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ä¸å¯ã®å ´åˆã®ã¿ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨
                print("    [BWT] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é€†BWTã‚’å®Ÿè¡Œ")
                original_data = self._fallback_bwt_inverse(bwt_encoded, primary_index)
            
            print(f"    [å¼·åŒ–BWT] é€†å¤‰æ›å®Œäº†: {len(bwt_encoded)} -> {len(original_data)} bytes")
            return original_data
            
        except Exception as e:
            print(f"    [å¼·åŒ–BWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ã«çµåˆã—ã¦è¿”ã™
            if expected_length is not None:
                if len(original_data) != expected_length:
                    print(f"    [è­¦å‘Š] ãƒ‡ãƒ¼ã‚¿é•·ä¸ä¸€è‡´: æœŸå¾…={expected_length}, å®Ÿéš›={len(original_data)}")
                    # å¿…è¦ã«å¿œã˜ã¦åˆ‡ã‚Šè©°ã‚ã¾ãŸã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                    if len(original_data) > expected_length:
                        original_data = original_data[:expected_length]
                        print(f"    [ä¿®æ­£] ãƒ‡ãƒ¼ã‚¿ã‚’æœŸå¾…é•·ã«åˆ‡ã‚Šè©°ã‚: {len(original_data)} bytes")
                else:
                    print(f"    [ç¢ºèª] ãƒ‡ãƒ¼ã‚¿é•·æ•´åˆæ€§: {len(original_data)} bytes âœ“")
            
            return original_data
            
        except Exception as e:
            print(f"    [å¼·åŒ–BWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _fallback_bwt_inverse(self, last_col: bytes, primary_index: int) -> bytes:
        """æ”¹è‰¯ç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é€†BWTå®Ÿè£…ï¼ˆO(n)ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰"""
        n = len(last_col)
        if n == 0:
            return b''
        
        # primary_indexã®ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆ100%å¯é€†æ€§ã®æœ€é‡è¦ãƒã‚¤ãƒ³ãƒˆï¼‰
        if primary_index < 0 or primary_index >= n:
            print(f"    [BWT] è­¦å‘Š: primary_index={primary_index} ãŒç¯„å›²å¤– (0-{n-1})")
            # 100%å¯é€†æ€§ã®ãŸã‚ã®å …ç‰¢ãªä¿®å¾©ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
            if n > 0:
                # è¤‡æ•°ã®ä¿®å¾©æ‰‹æ³•ã‚’è©¦è¡Œã—ã¦æœ€é©ãªprimary_indexã‚’è¦‹ã¤ã‘ã‚‹
                repair_candidates = []
                
                # æ‰‹æ³•1: ãƒ¢ã‚¸ãƒ¥ãƒ­æ¼”ç®—ã«ã‚ˆã‚‹ä¿®æ­£
                modulo_corrected = primary_index % n
                repair_candidates.append(('modulo', modulo_corrected))
                
                # æ‰‹æ³•2: ç¯„å›²å†…æœ€è¿‘å€¤ã¸ã®ä¿®æ­£
                if primary_index < 0:
                    range_corrected = 0
                else:
                    range_corrected = n - 1
                repair_candidates.append(('range', range_corrected))
                
                # æ‰‹æ³•3: BWTã®çµ±è¨ˆçš„ç‰¹æ€§ã‚’åˆ©ç”¨ã—ãŸæ¨å®š
                # BWTã®primary_indexã¯é€šå¸¸ã€ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã«ä¾å­˜ã—ã¦ç‰¹å®šã®ç¯„å›²ã«é›†ä¸­ã™ã‚‹
                if n > 10:
                    # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«åŸºã¥ãçµ±è¨ˆçš„æ¨å®š
                    statistical_estimate = min(max(int(n * 0.618), 0), n - 1)  # é»„é‡‘æ¯”è¿‘ä¼¼
                    repair_candidates.append(('statistical', statistical_estimate))
                
                # æœ€åˆã®å€™è£œã‚’ä½¿ç”¨ï¼ˆé€šå¸¸ã¯ãƒ¢ã‚¸ãƒ¥ãƒ­ä¿®æ­£ãŒæœ€ã‚‚å®‰å…¨ï¼‰
                repair_method, corrected_index = repair_candidates[0]
                primary_index = corrected_index
                print(f"    [BWT] primary_indexã‚’{repair_method}æ³•ã§{corrected_index}ã«ä¿®å¾©")
            else:
                return b''
        
        try:
            # å„æ–‡å­—ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            count = [0] * 256
            for char in last_col:
                count[char] += 1
            
            # ç´¯ç©ã‚«ã‚¦ãƒ³ãƒˆã‚’è¨ˆç®—ï¼ˆfirståˆ—ã®é–‹å§‹ä½ç½®ï¼‰
            first_col_starts = [0] * 256
            total = 0
            for i in range(256):
                first_col_starts[i] = total
                total += count[i]
            
            # å¤‰æ›ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ§‹ç¯‰ï¼ˆåŠ¹ç‡çš„ãªO(n)å®Ÿè£…ï¼‰
            next_idx = [0] * n
            char_counts = [0] * 256
            
            for i in range(n):
                char = last_col[i]
                next_idx[i] = first_col_starts[char] + char_counts[char]
                char_counts[char] += 1
            
            # å…ƒã®æ–‡å­—åˆ—ã‚’å¾©å…ƒï¼ˆ100%å¯é€†æ€§ä¿è¨¼ï¼‰
            result = bytearray()
            current_idx = primary_index
            visited_indices = set()  # ç„¡é™ãƒ«ãƒ¼ãƒ—æ¤œå‡ºç”¨
            
            for step in range(n):
                if current_idx < 0 or current_idx >= n:
                    print(f"    [BWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: step={step}, current_idx={current_idx} ãŒç¯„å›²å¤–")
                    # 100%å¯é€†æ€§ã®ãŸã‚ã®ç·Šæ€¥ä¿®å¾©
                    if step > 0:
                        print(f"    [BWT] éƒ¨åˆ†å¾©å…ƒæˆåŠŸ: {step}/{n} æ–‡å­—å¾©å…ƒ")
                        break
                    else:
                        # æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã§å¤±æ•—ã—ãŸå ´åˆã®ç·Šæ€¥å‡¦ç†
                        current_idx = 0
                        print(f"    [BWT] ç·Šæ€¥ä¿®å¾©: current_idx=0ã§å†é–‹")
                
                # ç„¡é™ãƒ«ãƒ¼ãƒ—æ¤œå‡ºï¼ˆ100%å¯é€†æ€§ä¿è¨¼ï¼‰
                if current_idx in visited_indices:
                    print(f"    [BWT] è­¦å‘Š: ç„¡é™ãƒ«ãƒ¼ãƒ—æ¤œå‡º at index={current_idx}, step={step}")
                    # å¾ªç’°ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿½åŠ 
                    remaining_chars = []
                    for i in range(n):
                        if i not in visited_indices:
                            remaining_chars.append(last_col[i])
                    result.extend(remaining_chars)
                    print(f"    [BWT] æ®‹ã‚Š{len(remaining_chars)}æ–‡å­—ã‚’ç·Šæ€¥è¿½åŠ ")
                    break
                
                visited_indices.add(current_idx)
                char = last_col[current_idx]
                result.append(char)
                current_idx = next_idx[current_idx]
            
            # BWTã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—ã®100%å¯é€†å‡¦ç†
            result_bytes = bytes(result)
            
            # 100%å¯é€†æ€§ã®ãŸã‚ã®æ…é‡ãªã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—å‡¦ç†
            if result_bytes and len(result_bytes) > 0:
                # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒæœŸå¾…å€¤ã¨ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if len(result_bytes) == n:
                    # ã‚µã‚¤ã‚ºãŒä¸€è‡´ã™ã‚‹å ´åˆã€æœ«å°¾ã®ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—ã®ã¿é™¤å»
                    if result_bytes[-1] == 0:
                        result_bytes = result_bytes[:-1]
                        print(f"    [BWT] ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—é™¤å»: {len(result)} -> {len(result_bytes)} bytes")
                elif len(result_bytes) == n - 1:
                    # æ—¢ã«ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—ãŒé™¤å»ã•ã‚Œã¦ã„ã‚‹å ´åˆ
                    print(f"    [BWT] ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿: {len(result_bytes)} bytes")
                else:
                    # ã‚µã‚¤ã‚ºãŒæœŸå¾…å€¤ã¨ç•°ãªã‚‹å ´åˆã®è­¦å‘Š
                    print(f"    [BWT] è­¦å‘Š: å¾©å…ƒã‚µã‚¤ã‚ºä¸ä¸€è‡´ æœŸå¾…å€¤={n-1}, å®Ÿéš›={len(result_bytes)}")
                    # ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’æœ€å„ªå…ˆã«ã€ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—é™¤å»ã¯è¡Œã‚ãªã„
            
            # 100%å¯é€†æ€§æ¤œè¨¼
            if len(result_bytes) > 0:
                print(f"    [BWT] é€†å¤‰æ›å®Œäº†: {len(result_bytes)} byteså¾©å…ƒ")
            else:
                print(f"    [BWT] è­¦å‘Š: ç©ºãƒ‡ãƒ¼ã‚¿ãŒå¾©å…ƒã•ã‚Œã¾ã—ãŸ")
                
            return result_bytes
            
        except Exception as e:
            print(f"    [BWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            # 100%å¯é€†æ€§ã®ãŸã‚ã®ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            print(f"    [BWT] ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿”å´")
            # BWTãŒå¤±æ•—ã—ãŸå ´åˆã€å…ƒã®last_colã‚’ãã®ã¾ã¾è¿”ã™
            # ã“ã‚Œã«ã‚ˆã‚Šå°‘ãªãã¨ã‚‚ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§ã¯ä¿æŒã•ã‚Œã‚‹
            if len(last_col) > 0 and last_col[-1] == 0:
                # ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯é™¤å»
                return last_col[:-1]
            else:
                return last_col


class NEXUSTMCEngineV9:
    """
    NEXUS TMC Engine v9.0 - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°çµ±åˆç‰ˆ
    æ¬¡ä¸–ä»£é‡å­ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
    Transform-Model-Code åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ TMC v9.0
    
    v9.0é©æ–°æ©Ÿèƒ½:
    - é«˜åº¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ç¬¦å·åŒ–ï¼ˆLZMAã«åŒ¹æ•µã™ã‚‹åœ§ç¸®ç‡ï¼‰
    - è¤‡æ•°äºˆæ¸¬å™¨ + å‹•çš„ãƒŸã‚­ã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹æ¥µé™åœ§ç¸®ç‡å®Ÿç¾
    - BWTTransformerå®Œå…¨å …ç‰¢åŒ– + ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
    """
    
    def __init__(self, max_workers: int = None, chunk_size: int = DEFAULT_CHUNK_SIZE):
        self.max_workers = max_workers or min(8, mp.cpu_count())
        self.chunk_size = chunk_size
        self.dispatcher = ImprovedDispatcher()
        self.core_compressor = CoreCompressor()
        self.context_mixer = ContextMixingEncoder()  # v9.0æ–°æ©Ÿèƒ½
        
        # TMC v9.0 é©æ–°æ©Ÿèƒ½: ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ + ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77
        self.pipeline_processor = ParallelPipelineProcessor(max_workers=self.max_workers)
        self.sublinear_lz77 = SublinearLZ77Encoder()
        
        # TMC v8.0 æ©Ÿèƒ½: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹
        self.meta_analyzer = MetaAnalyzer(self.core_compressor)
        
        # å¤‰æ›å™¨ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆv8.0å¼·åŒ–ç‰ˆï¼‰
        self.transformers = {
            DataType.FLOAT_DATA: TDTTransformer(),
            DataType.TEXT_DATA: BWTTransformer(),  # v7.0å¼·åŒ–ç‰ˆï¼ˆãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆï¼‰
            DataType.SEQUENTIAL_INT_DATA: LeCoAdvancedTransformer(),  # v8.0: å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
            DataType.STRUCTURED_NUMERIC: TDTTransformer(),
            DataType.TIME_SERIES: LeCoAdvancedTransformer(),  # v8.0å¯¾å¿œ
            DataType.REPETITIVE_BINARY: None,  # RLEå‰å‡¦ç†ã®ã¿
            DataType.COMPRESSED_LIKE: None,    # å¤‰æ›ãªã—
            DataType.GENERIC_BINARY: None      # å¤‰æ›ãªã—
        }
        
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'reversibility_tests_passed': 0,
            'reversibility_tests_total': 0,
            'transforms_applied': 0,
            'transforms_bypassed': 0,
            'chunks_processed': 0,           # v8.0è¿½åŠ 
            'parallel_efficiency': 0.0,     # v8.0è¿½åŠ 
            'entropy_coding_used': 0         # v8.0è¿½åŠ 
        }
        
        print(f"ğŸš€ TMC v9.0 ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†: {self.max_workers}ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼, ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º={chunk_size//1024//1024}MB (é©æ–°çš„ä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ + ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°çµ±åˆç‰ˆ)")
    
    async def compress_tmc_v9_async(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """
        TMC v9.0 éåŒæœŸä¸¦åˆ—åœ§ç¸®
        é©æ–°çš„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã«ã‚ˆã‚‹æœ€å¤§10å€ã®é«˜é€ŸåŒ–
        """
        print("--- TMC v9.0 é©æ–°çš„éåŒæœŸåœ§ç¸®é–‹å§‹ ---")
        start_time = time.time()
        
        try:
            if len(data) == 0:
                return b'', {'method': 'empty', 'compression_time': 0.0}
            
            # Phase 1: é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            optimal_chunks = self._adaptive_chunking(data)
            print(f"[é©å¿œãƒãƒ£ãƒ³ã‚¯] {len(optimal_chunks)}å€‹ã®æœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆ")
            
            # Phase 2: éåŒæœŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†
            self.pipeline_processor.start_pipeline()
            
            try:
                # ä¸¦åˆ—å¤‰æ› + åœ§ç¸®
                processed_results = await self.pipeline_processor.process_data_async(
                    optimal_chunks, 'tmc_v9_transform'
                )
                
                # Phase 3: ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77çµ±åˆï¼ˆä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
                # if len(data) > 64 * 1024:  # 64KBä»¥ä¸Šã§ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77é©ç”¨
                #     lz77_result = self.sublinear_lz77.encode_sublinear(data)
                #     if lz77_result[1].get('compression_ratio', 0) > 15:  # 15%ä»¥ä¸Šåœ§ç¸®ãªã‚‰æ¡ç”¨
                #         print(f"[ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77] é«˜åœ§ç¸®ç‡é”æˆ: {lz77_result[1]['compression_ratio']:.1f}%")
                #         processed_results = [(lz77_result[0], lz77_result[1])]
                
                # Phase 4: çµæœçµ±åˆã¨ã‚³ãƒ³ãƒ†ãƒŠåŒ–
                compressed_container = self._create_v9_container(processed_results)
                
            finally:
                self.pipeline_processor.stop_pipeline()
            
            total_time = time.time() - start_time
            
            # çµ±è¨ˆæ›´æ–°ï¼ˆã‚¼ãƒ­é™¤ç®—å›é¿ï¼‰
            compression_ratio = (1 - len(compressed_container) / len(data)) * 100 if len(data) > 0 else 0
            throughput = (len(data) / (1024 * 1024) / total_time) if total_time > 0 else 0  # MB/s
            
            try:
                pipeline_stats = self.pipeline_processor.get_performance_stats()
            except Exception:
                pipeline_stats = {}
            
            compression_info = {
                'method': 'tmc_v9_async_pipeline',
                'version': '9.0',
                'original_size': len(data),
                'compressed_size': len(compressed_container),
                'compression_ratio': compression_ratio,
                'compression_time': total_time,
                'throughput_mbps': throughput,
                'chunk_count': len(optimal_chunks),
                'pipeline_stats': pipeline_stats,
                'sublinear_lz77_used': len(data) > 64 * 1024,
                'innovations': [
                    'async_parallel_pipeline',
                    'adaptive_chunking',
                    'sublinear_lz77',
                    'context_mixing'
                ]
            }
            
            print(f"--- TMC v9.0 åœ§ç¸®å®Œäº†: {compression_ratio:.1f}%, {throughput:.1f}MB/s ---")
            return compressed_container, compression_info
            
        except Exception as e:
            print(f"--- TMC v9.0 éåŒæœŸåœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e} ---")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥åœ§ç¸®
            return self.compress_tmc(data)
    
    def _adaptive_chunking(self, data: bytes) -> List[bytes]:
        """
        é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãå‹•çš„ã‚µã‚¤ã‚ºèª¿æ•´
        """
        if len(data) <= self.chunk_size:
            return [data]
        
        chunks = []
        pos = 0
        
        while pos < len(data):
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºèª¿æ•´
            remaining = len(data) - pos
            base_size = min(self.chunk_size, remaining)
            
            # å…ˆé ­256ãƒã‚¤ãƒˆã§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ¨å®š
            sample_end = min(pos + 256, len(data))
            sample = data[pos:sample_end]
            
            try:
                # ç°¡æ˜“ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
                byte_counts = {}
                for byte in sample:
                    byte_counts[byte] = byte_counts.get(byte, 0) + 1
                
                entropy = 0.0
                for count in byte_counts.values():
                    prob = count / len(sample)
                    entropy -= prob * (prob.bit_length() - 1) if prob > 0 else 0
                
                # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«åŸºã¥ãã‚µã‚¤ã‚ºèª¿æ•´
                if entropy < 3.0:  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: å¤§ããªãƒãƒ£ãƒ³ã‚¯
                    adjusted_size = min(int(base_size * 1.5), remaining)
                elif entropy > 6.0:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: å°ã•ãªãƒãƒ£ãƒ³ã‚¯
                    adjusted_size = max(int(base_size * 0.7), base_size // 2)
                else:  # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: æ¨™æº–ã‚µã‚¤ã‚º
                    adjusted_size = base_size
                
            except:
                adjusted_size = base_size
            
            chunk_end = min(pos + adjusted_size, len(data))
            chunks.append(data[pos:chunk_end])
            pos = chunk_end
        
        return chunks
    
    def _create_v9_container(self, processed_results: List[Tuple[bytes, Dict]]) -> bytes:
        """TMC v9.0ã‚³ãƒ³ãƒ†ãƒŠç”Ÿæˆ"""
        container = bytearray()
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ + ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        container.extend(TMC_V9_MAGIC)
        container.extend(b'v9.0')
        
        # ãƒãƒ£ãƒ³ã‚¯æ•°
        container.extend(len(processed_results).to_bytes(4, 'big'))
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        for chunk_data, chunk_info in processed_results:
            # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ãƒ˜ãƒƒãƒ€ãƒ¼
            info_json = json.dumps(chunk_info, separators=(',', ':')).encode('utf-8')
            container.extend(len(info_json).to_bytes(4, 'big'))
            container.extend(info_json)
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
            container.extend(len(chunk_data).to_bytes(4, 'big'))
            container.extend(chunk_data)
        
        return bytes(container)
        """
        TMC v8.0 ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®å‡¦ç†
        çœŸã®ãƒãƒ«ãƒã‚³ã‚¢æ´»ç”¨ã«ã‚ˆã‚‹é©æ–°çš„ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
        """
        compression_start = time.perf_counter()
        
        try:
            print(f"\n--- TMC v8.0 ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®é–‹å§‹ ({len(data)} bytes) ---")
            
            # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã¯å˜ä¸€ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            if len(data) <= self.chunk_size:
                print("  [ãƒãƒ£ãƒ³ã‚¯åˆ†æ] å°ã‚µã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ - å˜ä¸€ãƒãƒ£ãƒ³ã‚¯å‡¦ç†")
                return self._compress_single_chunk(data)
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self._split_into_chunks(data)
            print(f"  [ãƒãƒ£ãƒ³ã‚¯åˆ†æ] {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
            
            # ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®
            compressed_chunks, chunk_infos = self._compress_chunks_parallel(chunks)
            
            # TMC v8.0 ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰
            container = self._build_tmc_v8_container(compressed_chunks, chunk_infos)
            
            total_time = time.perf_counter() - compression_start
            
            # ä¸¦åˆ—åŠ¹ç‡è¨ˆç®—
            sequential_estimate = total_time * self.max_workers
            parallel_efficiency = min(1.0, sequential_estimate / total_time) if total_time > 0 else 0.0
            
            # çµæœæƒ…å ±
            original_size = len(data)
            compressed_size = len(container)
            compression_ratio = (1 - compressed_size / original_size) * 100 if original_size > 0 else 0
            
            result_info = {
                'compression_ratio': compression_ratio,
                'compression_throughput_mb_s': (original_size / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_compression_time': total_time,
                'chunk_count': len(chunks),
                'chunk_infos': chunk_infos,
                'parallel_workers_used': self.max_workers,
                'parallel_efficiency': parallel_efficiency,
                'original_size': original_size,
                'compressed_size': compressed_size,
                'tmc_version': '8.0',
                'reversible': True,
                'container_format': 'tmc_v8_parallel',
                'entropy_coding_efficiency': sum(1 for info in chunk_infos if info.data_type in ['sequential_int_data', 'text_data']) / len(chunk_infos)
            }
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['chunks_processed'] += len(chunks)
            self.stats['parallel_efficiency'] = parallel_efficiency
            
            print(f"--- TMC v8.0 ä¸¦åˆ—åœ§ç¸®å®Œäº† ---")
            print(f"åœ§ç¸®ç‡: {compression_ratio:.2f}% | ä¸¦åˆ—åŠ¹ç‡: {parallel_efficiency:.2%} | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result_info['compression_throughput_mb_s']:.2f} MB/s")
            
            return container, result_info
            
        except Exception as e:
            print(f"[TMC v8.0] ä¸¦åˆ—åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ä¸€ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            return self._compress_single_chunk(data)
    
    def _split_into_chunks(self, data: bytes) -> List[bytes]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’æœ€é©ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã«åˆ†å‰²"""
        chunks = []
        for i in range(0, len(data), self.chunk_size):
            chunk = data[i:i + self.chunk_size]
            chunks.append(chunk)
        return chunks
    
    def _compress_chunks_parallel(self, chunks: List[bytes]) -> Tuple[List[bytes], List[ChunkInfo]]:
        """ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®å‡¦ç†"""
        print(f"  [ä¸¦åˆ—å‡¦ç†] {self.max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼ã§{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—åœ§ç¸®ä¸­...")
        
        compressed_chunks = []
        chunk_infos = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—ã§å‡¦ç†
            future_to_chunk = {
                executor.submit(self._compress_chunk, chunk_data, chunk_id): chunk_id 
                for chunk_id, chunk_data in enumerate(chunks)
            }
            
            # çµæœã‚’é †åºé€šã‚Šã«åé›†
            chunk_results = {}
            for future in as_completed(future_to_chunk):
                chunk_id = future_to_chunk[future]
                try:
                    compressed_data, chunk_info = future.result()
                    chunk_results[chunk_id] = (compressed_data, chunk_info)
                    print(f"    [ãƒãƒ£ãƒ³ã‚¯ {chunk_id}] å®Œäº†: {chunk_info.original_size} -> {chunk_info.compressed_size} bytes "
                          f"({chunk_info.compression_ratio:.1f}%, {chunk_info.data_type})")
                except Exception as e:
                    print(f"    [ãƒãƒ£ãƒ³ã‚¯ {chunk_id}] ã‚¨ãƒ©ãƒ¼: {e}")
                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾æ ¼ç´
                    chunk_data = chunks[chunk_id]
                    chunk_results[chunk_id] = (chunk_data, ChunkInfo(
                        chunk_id=chunk_id,
                        original_size=len(chunk_data),
                        compressed_size=len(chunk_data),
                        data_type="error_fallback",
                        compression_ratio=0.0,
                        processing_time=0.0
                    ))
        
        # é †åºé€šã‚Šã«çµæœã‚’é…åˆ—ã«æ ¼ç´
        for chunk_id in sorted(chunk_results.keys()):
            compressed_data, chunk_info = chunk_results[chunk_id]
            compressed_chunks.append(compressed_data)
            chunk_infos.append(chunk_info)
        
        return compressed_chunks, chunk_infos
    
    def _compress_chunk(self, chunk_data: bytes, chunk_id: int) -> Tuple[bytes, ChunkInfo]:
        """å€‹åˆ¥ãƒãƒ£ãƒ³ã‚¯ã®åœ§ç¸®å‡¦ç†ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ï¼‰"""
        chunk_start = time.perf_counter()
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æ
            data_type, features = self.dispatcher.dispatch(chunk_data)
            
            # 2. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹åˆ†æ
            transformer = self.transformers.get(data_type)
            should_transform, meta_info = self.meta_analyzer.should_apply_transform(
                chunk_data, transformer, data_type
            )
            
            # 3. å¤‰æ›å‡¦ç†
            if should_transform and transformer:
                transformed_streams, transform_info = transformer.transform(chunk_data)
            else:
                transformed_streams = [chunk_data]
                transform_info = {'method': 'bypass', 'reason': 'intelligent_bypass'}
            
            # 4. ç¬¦å·åŒ–å‡¦ç†ï¼ˆå‹•çš„ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é¸æŠï¼‰
            final_streams = []
            for stream in transformed_streams:
                if should_transform and data_type in [DataType.SEQUENTIAL_INT_DATA, DataType.TEXT_DATA]:
                    # å¤‰æ›æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã«ç´”ç²‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–
                    compressed_stream, method = self.entropy_encoder.encode_entropy_stream(stream, "transformed")
                    self.stats['entropy_coding_used'] += 1
                else:
                    # æ±ç”¨ãƒ‡ãƒ¼ã‚¿ã«å¾“æ¥å‹åœ§ç¸®
                    stream_entropy = self._calculate_entropy(np.frombuffer(stream, dtype=np.uint8)) if len(stream) > 0 else 4.0
                    compressed_stream, method = self.core_compressor.compress(stream, stream_entropy)
                
                final_streams.append(compressed_stream)
            
            # 5. ãƒãƒ£ãƒ³ã‚¯çµæœãƒ‘ãƒƒã‚­ãƒ³ã‚°
            chunk_compressed = self._pack_chunk_data(final_streams, data_type, transform_info, features)
            
            processing_time = time.perf_counter() - chunk_start
            compression_ratio = (1 - len(chunk_compressed) / len(chunk_data)) * 100 if len(chunk_data) > 0 else 0
            
            chunk_info = ChunkInfo(
                chunk_id=chunk_id,
                original_size=len(chunk_data),
                compressed_size=len(chunk_compressed),
                data_type=data_type.value,
                compression_ratio=compression_ratio,
                processing_time=processing_time
            )
            
            return chunk_compressed, chunk_info
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            processing_time = time.perf_counter() - chunk_start
            chunk_info = ChunkInfo(
                chunk_id=chunk_id,
                original_size=len(chunk_data),
                compressed_size=len(chunk_data),
                data_type="error_fallback",
                compression_ratio=0.0,
                processing_time=processing_time
            )
            return chunk_data, chunk_info
    
    def _pack_chunk_data(self, streams: List[bytes], data_type: DataType, 
                        transform_info: Dict[str, Any], features: Dict[str, Any]) -> bytes:
        """ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒƒã‚­ãƒ³ã‚°"""
        # ãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        chunk_header = {
            'data_type': data_type.value,
            'transform_info': transform_info,
            'stream_count': len(streams),
            'features': {k: v for k, v in features.items() if isinstance(v, (int, float, str, bool))}
        }
        
        header_json = json.dumps(chunk_header, separators=(',', ':'))
        header_bytes = header_json.encode('utf-8')
        
        # ãƒ‘ãƒƒã‚­ãƒ³ã‚°: [ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º(4)] + [ãƒ˜ãƒƒãƒ€ãƒ¼] + [ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°(4)] + [ã‚µã‚¤ã‚º1(4)] + [ã‚µã‚¤ã‚º2(4)]... + [ã‚¹ãƒˆãƒªãƒ¼ãƒ 1] + [ã‚¹ãƒˆãƒªãƒ¼ãƒ 2]...
        packed_data = bytearray()
        packed_data.extend(len(header_bytes).to_bytes(4, 'big'))
        packed_data.extend(header_bytes)
        packed_data.extend(len(streams).to_bytes(4, 'big'))
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºæƒ…å ±
        for stream in streams:
            packed_data.extend(len(stream).to_bytes(4, 'big'))
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿
        for stream in streams:
            packed_data.extend(stream)
        
        return bytes(packed_data)
    
    def _build_tmc_v8_container(self, compressed_chunks: List[bytes], 
                               chunk_infos: List[ChunkInfo]) -> bytes:
        """TMC v8.0 ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰"""
        container = bytearray()
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ + ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        container.extend(TMC_V9_MAGIC)
        container.extend(b'8.0\x00')
        
        # ãƒãƒ£ãƒ³ã‚¯æ•°
        container.extend(len(compressed_chunks).to_bytes(4, 'big'))
        
        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«
        for chunk_info in chunk_infos:
            container.extend(chunk_info.chunk_id.to_bytes(4, 'big'))
            container.extend(chunk_info.original_size.to_bytes(4, 'big'))
            container.extend(chunk_info.compressed_size.to_bytes(4, 'big'))
            container.extend(chunk_info.data_type.encode('utf-8')[:16].ljust(16, b'\x00'))
        
        # åœ§ç¸®æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        for chunk_data in compressed_chunks:
            container.extend(chunk_data)
        
        return bytes(container)
    
    def _compress_single_chunk(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼ˆv7.0äº’æ›ï¼‰"""
        return self.compress_tmc(data)
    
    def compress_tmc(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC v7.0 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆçµ±åˆåœ§ç¸®å‡¦ç†ï¼ˆé«˜é€ŸåŒ–å¯¾å¿œï¼‰"""
        compression_start = time.perf_counter()
        
        try:
            print("\n--- TMC v7.0 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®é–‹å§‹ ---")
            
            # é«˜é€ŸåŒ–: å°ã•ãªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹è»½é‡ãƒ‘ã‚¹ï¼ˆ100%å¯é€†æ€§ä¿è¨¼ç‰ˆï¼‰
            if len(data) < 1024:  # 1KBæœªæº€ã¯è»½é‡å‡¦ç†
                print(f"  [é«˜é€Ÿãƒ‘ã‚¹] å°ãƒ‡ãƒ¼ã‚¿ ({len(data)} bytes) - è»½é‡åœ§ç¸®")
                compressed = zlib.compress(data, level=6)
                compression_time = time.perf_counter() - compression_start
                
                # 100%å¯é€†æ€§ã®ãŸã‚é©åˆ‡ãªãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ç”¨
                return self._create_fast_path_container(compressed, data, compression_time)
            
            # 1. æ”¹è‰¯åˆ†æ&ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ
            data_type, features = self.dispatcher.dispatch(data)
            
            # DataTypeå‡¦ç†ã®å®‰å…¨åŒ–
            if isinstance(data_type, str):
                # æ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                data_type_str = data_type
                # transformersã¯DataTypeã‚­ãƒ¼ã‚’æƒ³å®šã—ã¦ã„ã‚‹ã®ã§é©åˆ‡ã«å¤‰æ›
                data_type_key = getattr(DataType, data_type.upper(), None) if hasattr(DataType, data_type.upper()) else None
            else:
                # DataTypeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                data_type_str = data_type.value if hasattr(data_type, 'value') else str(data_type)
                data_type_key = data_type
            
            # 2. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹åˆ†æï¼ˆTMC v7.0æ–°æ©Ÿèƒ½ï¼‰
            transformer = self.transformers.get(data_type_key) if data_type_key else None
            should_transform, meta_info = self.meta_analyzer.should_apply_transform(
                data, transformer, data_type
            )
            
            # 3. é©å¿œçš„å¤‰æ›ï¼ˆã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåˆ¤å®šã«åŸºã¥ãï¼‰
            if should_transform and transformer:
                print(f"  [ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ] {data_type_str} å¤‰æ›ã‚’å®Ÿè¡Œ")
                transformed_streams, transform_info = transformer.transform(data)
                self.stats['transforms_applied'] += 1
                
                # ãƒ¡ã‚¿åˆ†ææƒ…å ±ã‚’å¤‰æ›æƒ…å ±ã«çµ±åˆ
                transform_info['meta_analysis'] = meta_info
                transform_info['bypassed'] = False
            else:
                print(f"  [ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ] {data_type_str} å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                transformed_streams = [data]
                transform_info = {
                    'method': 'bypassed', 
                    'meta_analysis': meta_info,
                    'bypassed': True,
                    'reason': meta_info.get('reason', 'ineffective')
                }
                self.stats['transforms_bypassed'] += 1
            
            # 4. ä¸¦åˆ—ã‚³ã‚¢åœ§ç¸®ï¼ˆv9.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¯¾å¿œï¼‰
            compressed_streams = []
            compression_methods = []
            
            print("  [ç¬¦å·åŒ–] TMC v9.0 ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé©å¿œå‹åœ§ç¸®ä¸­...")
            for i, stream in enumerate(transformed_streams):
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
                if len(stream) > 0:
                    stream_array = np.frombuffer(stream, dtype=np.uint8)
                    stream_entropy = self._calculate_entropy(stream_array)
                else:
                    stream_entropy = 0.0
                
                # v9.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°é©ç”¨åˆ¤å®š
                use_context_mixing = (
                    should_transform and  # å¤‰æ›ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆ
                    len(stream) > 2048 and  # 2KBä»¥ä¸Š
                    stream_entropy > 3.0 and  # é©åº¦ãªã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                    stream_entropy < 7.0  # ãƒ©ãƒ³ãƒ€ãƒ éããªã„
                )
                
                # TMCçµ±ä¸€åœ§ç¸®ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¯¾å¿œï¼‰
                compressed, comp_method = self.core_compressor.compress(
                    stream, 
                    stream_entropy=stream_entropy, 
                    stream_size=len(stream),
                    use_context_mixing=use_context_mixing
                )
                compressed_streams.append(compressed)
                compression_methods.append(comp_method)
                
                context_info = " (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°)" if use_context_mixing else ""
                print(f"    ã‚¹ãƒˆãƒªãƒ¼ãƒ  {i}: {len(stream)} bytes -> {len(compressed)} bytes ({comp_method}, ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {stream_entropy:.2f}){context_info}")
                            # 5. TMC v7.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰
            final_data = self._pack_tmc_v7(compressed_streams, compression_methods, 
                                          data_type, transform_info, features)
            
            total_time = time.perf_counter() - compression_start
            
            # çµæœæƒ…å ±
            compression_ratio = (1 - len(final_data) / len(data)) * 100 if len(data) > 0 else 0
            
            result_info = {
                'compression_ratio': compression_ratio,
                'compression_throughput_mb_s': (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_compression_time': total_time,
                'data_type': data_type_str,
                'features': features,
                'transform_info': transform_info,
                'compression_methods': compression_methods,
                'stream_count': len(compressed_streams),
                'original_size': len(data),
                'compressed_size': len(final_data),
                'tmc_version': '7.0',
                'reversible': True,
                'zstd_used': self.core_compressor.zstd_available,
                'intelligent_bypass_used': True,  # v7.0æ–°æ©Ÿèƒ½
                'transform_applied': should_transform,
                'meta_analysis': meta_info
            }
            
            print(f"--- TMC v7.0 åœ§ç¸®å®Œäº† ---")
            print(f"åˆè¨ˆã‚µã‚¤ã‚º: {len(data)} bytes -> {len(final_data)} bytes (åœ§ç¸®ç‡: {compression_ratio:.2f}%)")
            print(f"å¤‰æ›: {'é©ç”¨' if should_transform else 'ã‚¹ã‚­ãƒƒãƒ—'}")
            
            return final_data, result_info
            
        except Exception as e:
            total_time = time.perf_counter() - compression_start
            print(f"âŒ åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return data, {
                'compression_ratio': 0.0,
                'error': str(e),
                'total_compression_time': total_time,
                'reversible': True
            }
    
    def _calculate_entropy(self, data_array: np.ndarray) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
        try:
            byte_counts = np.bincount(data_array, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(data_array)
            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 4.0
    
    def _safe_get_datatype(self, data_type_str: str):
        """DataTypeæ–‡å­—åˆ—ã‚’å®‰å…¨ã«DataTypeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        try:
            for dt in DataType:
                if dt.value == data_type_str:
                    return dt
            return DataType.GENERIC_BINARY
        except Exception:
            return DataType.GENERIC_BINARY
    
    def decompress_tmc(self, compressed_data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC çµ±åˆå±•é–‹å‡¦ç†ï¼ˆv7.0/v9.0å¯¾å¿œï¼‰"""
        decompression_start = time.perf_counter()
        
        try:
            # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã§ãƒãƒ¼ã‚¸ãƒ§ãƒ³åˆ¤å®š
            if compressed_data.startswith(TMC_V9_MAGIC):
                print("\n--- TMC v9.0 å±•é–‹é–‹å§‹ ---")
                return self._decompress_v9_container(compressed_data)
            else:
                print("\n--- TMC v7.0 å±•é–‹é–‹å§‹ ---")
                return self._decompress_v7_format(compressed_data)
                
        except Exception as e:
            total_time = time.perf_counter() - decompression_start
            print(f"âŒ å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data, {
                'error': str(e),
                'total_decompression_time': total_time
            }
    
    def _decompress_v9_container(self, compressed_data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC v9.0 ã‚³ãƒ³ãƒ†ãƒŠå±•é–‹"""
        try:
            # v9.0ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            offset = len(TMC_V9_MAGIC)
            version = compressed_data[offset:offset+4]  # 'v9.0'
            offset += 4
            
            # ãƒãƒ£ãƒ³ã‚¯æ•°
            chunk_count = int.from_bytes(compressed_data[offset:offset+4], 'big')
            offset += 4
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿å±•é–‹
            reconstructed_chunks = []
            
            for i in range(chunk_count):
                # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±èª­ã¿å–ã‚Š
                info_size = int.from_bytes(compressed_data[offset:offset+4], 'big')
                offset += 4
                
                info_json = compressed_data[offset:offset+info_size].decode('utf-8')
                chunk_info = json.loads(info_json)
                offset += info_size
                
                # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿èª­ã¿å–ã‚Š
                chunk_size = int.from_bytes(compressed_data[offset:offset+4], 'big')
                offset += 4
                
                chunk_data = compressed_data[offset:offset+chunk_size]
                offset += chunk_size
                
                # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿å±•é–‹ï¼ˆSublinearLZ77ä¸€æ™‚ç„¡åŠ¹åŒ–ï¼‰
                # if chunk_info.get('method') == 'fast_lz77':
                #     # ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77å±•é–‹ï¼ˆæœŸå¾…ã‚µã‚¤ã‚ºä½¿ç”¨ï¼‰
                #     try:
                #         expected_size = chunk_info.get('original_size')
                #         reconstructed = self.sublinear_lz77.decode_sublinear(chunk_data, expected_size)
                #         print(f"  [ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77] å±•é–‹å®Œäº†: {len(chunk_data)} -> {len(reconstructed)} bytes (æœŸå¾…: {expected_size})")
                #         
                #     except Exception as e:
                #         print(f"âš ï¸ ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
                #         # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç”Ÿãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã„ã€æœŸå¾…ã‚µã‚¤ã‚ºã«èª¿æ•´
                #         reconstructed = chunk_data
                #         expected_size = chunk_info.get('original_size')
                #         if expected_size and len(reconstructed) != expected_size:
                #             if len(reconstructed) > expected_size:
                #                 reconstructed = reconstructed[:expected_size]
                #             else:
                #                 reconstructed = reconstructed + b'\x00' * (expected_size - len(reconstructed))
                #             print(f"   ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯èª¿æ•´: {len(reconstructed)} bytes")
                # else:
                #     # é€šå¸¸å±•é–‹
                #     reconstructed = chunk_data
                
                # ç¾åœ¨ã¯å…¨ã¦é€šå¸¸å±•é–‹ã¨ã—ã¦å‡¦ç†
                reconstructed = chunk_data
                
                reconstructed_chunks.append(reconstructed)
            
            # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿çµåˆ
            final_data = b''.join(reconstructed_chunks)
            
            print(f"--- TMC v9.0 å±•é–‹å®Œäº†: {len(final_data)} bytes ---")
            
            return final_data, {
                'method': 'tmc_v9_decompress',
                'decompressed_size': len(final_data),
                'chunk_count': chunk_count
            }
            
        except Exception as e:
            print(f"TMC v9.0 å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data, {'error': str(e)}
    
    def _decompress_v7_format(self, compressed_data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC v7.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå±•é–‹"""
        decompression_start = time.perf_counter()
        
        try:
            # TMC v7.0 ãƒ˜ãƒƒãƒ€ãƒ¼è§£æï¼ˆv6.0äº’æ›ï¼‰
            header = self._parse_tmc_v7_header(compressed_data)
            if not header:
                raise ValueError("Invalid TMC v7.0 format")
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º
            payload = compressed_data[header['header_size']:]
            streams = self._extract_tmc_v7_streams(payload, header)
            
            # ä¸¦åˆ—å±•é–‹
            decompressed_streams = []
            for i, (stream, method) in enumerate(zip(streams, header['compression_methods'])):
                decompressed = self.core_compressor.decompress(stream, method)
                decompressed_streams.append(decompressed)
                print(f"    ã‚¹ãƒˆãƒªãƒ¼ãƒ  {i}: {len(stream)} bytes -> {len(decompressed)} bytes ({method})")
            
            # é€†å¤‰æ›ï¼ˆã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹å¯¾å¿œï¼‰
            # DataTypeæ–‡å­—åˆ—ã‚’å®‰å…¨ã«Enumã«å¤‰æ›
            try:
                data_type_str = header['data_type']
                data_type = DataType.GENERIC_BINARY  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                for dt in DataType:
                    if dt.value == data_type_str:
                        data_type = dt
                        break
                
                transformer = self.transformers.get(data_type)
                transform_bypassed = header.get('transform_bypassed', False)
                
                if transformer and not transform_bypassed:
                    print(f"  [é€†å¤‰æ›] {data_type_str} é€†å¤‰æ›ã‚’å®Ÿè¡Œ")
                    original_data = transformer.inverse_transform(decompressed_streams, header['transform_info'])
                else:
                    print(f"  [é€†å¤‰æ›] {data_type_str} å¤‰æ›ãƒã‚¤ãƒ‘ã‚¹ - ç›´æ¥çµåˆ")
                    original_data = b''.join(decompressed_streams)
                    
            except Exception as e:
                print(f"âŒ å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
                original_data = b''.join(decompressed_streams)
            
            total_time = time.perf_counter() - decompression_start
            
            print(f"--- TMC v7.0 å±•é–‹å®Œäº† ---")
            print(f"å†æ§‹ç¯‰ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(original_data)} bytes")
            
            result_info = {
                'decompression_throughput_mb_s': (len(original_data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_decompression_time': total_time,
                'decompressed_size': len(original_data),
                'tmc_version': '7.0',
                'transform_bypassed': transform_bypassed
            }
            
            return original_data, result_info
            
        except Exception as e:
            total_time = time.perf_counter() - decompression_start
            print(f"âŒ å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data, {
                'error': str(e),
                'total_decompression_time': total_time
            }
    
    def test_reversibility(self, test_data: bytes, test_name: str = "test") -> Dict[str, Any]:
        """TMC v7.0 å¯é€†æ€§ãƒ†ã‚¹ãƒˆ"""
        test_start_time = time.perf_counter()
        
        try:
            print(f"ğŸ”„ TMC v7.0 å¯é€†æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹: {test_name}")
            
            # åœ§ç¸®
            compressed, compression_info = self.compress_tmc(test_data)
            
            # å±•é–‹
            decompressed, decompression_info = self.decompress_tmc(compressed)
            
            # æ¤œè¨¼
            is_identical = (test_data == decompressed)
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['reversibility_tests_total'] += 1
            if is_identical:
                self.stats['reversibility_tests_passed'] += 1
            
            result_icon = "âœ…" if is_identical else "âŒ"
            transform_status = "é©ç”¨" if compression_info.get('transform_applied', False) else "ã‚¹ã‚­ãƒƒãƒ—"
            print(f"   {result_icon} å¯é€†æ€§: {'æˆåŠŸ' if is_identical else 'å¤±æ•—'} | å¤‰æ›: {transform_status}")
            
            return {
                'test_name': test_name,
                'reversible': is_identical,
                'original_size': len(test_data),
                'compressed_size': len(compressed),
                'decompressed_size': len(decompressed),
                'compression_ratio': compression_info.get('compression_ratio', 0),
                'compression_time': compression_info.get('total_compression_time', 0),
                'decompression_time': decompression_info.get('total_decompression_time', 0),
                'compression_throughput_mb_s': compression_info.get('compression_throughput_mb_s', 0),
                'decompression_throughput_mb_s': decompression_info.get('decompression_throughput_mb_s', 0),
                'total_test_time': time.perf_counter() - test_start_time,
                'data_type': compression_info.get('data_type', 'unknown'),
                'zstd_used': compression_info.get('zstd_used', False),
                'tmc_version': '7.0',
                'transform_applied': compression_info.get('transform_applied', False),
                'intelligent_bypass_used': compression_info.get('intelligent_bypass_used', False),
                'meta_analysis': compression_info.get('meta_analysis', {})
            }
            
        except Exception as e:
            return {
                'test_name': test_name,
                'reversible': False,
                'error': str(e),
                'tmc_version': '7.0'
            }
    
    def _pack_tmc_v7(self, streams: List[bytes], methods: List[str], 
                     data_type, transform_info: Dict[str, Any], 
                     features: Dict[str, Any]) -> bytes:
        """TMC v7.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰ï¼ˆã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹å¯¾å¿œï¼‰"""
        try:
            header = bytearray()
            
            # TMC v7.0 ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
            header.extend(b'TMC7')
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ï¼ˆå®‰å…¨ãªå‡¦ç†ï¼‰
            data_type_str = data_type if isinstance(data_type, str) else (data_type.value if hasattr(data_type, 'value') else str(data_type))
            data_type_bytes = data_type_str.encode('utf-8')[:32].ljust(32, b'\x00')
            header.extend(data_type_bytes)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
            header.extend(struct.pack('<I', len(streams)))
            
            # åœ§ç¸®ãƒ¡ã‚½ãƒƒãƒ‰æƒ…å ±
            for method in methods:
                method_bytes = method.encode('utf-8')[:16].ljust(16, b'\x00')
                header.extend(method_bytes)
            
            # å¤‰æ›æƒ…å ±ï¼ˆå®‰å…¨ãªJSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã€ãƒ¡ã‚¿åˆ†ææƒ…å ±çµ±åˆï¼‰
            transform_info_safe = self._make_json_safe(transform_info)
            transform_str = json.dumps(transform_info_safe, separators=(',', ':'))
            transform_bytes = transform_str.encode('utf-8')
            header.extend(struct.pack('<I', len(transform_bytes)))
            header.extend(transform_bytes)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºãƒ†ãƒ¼ãƒ–ãƒ«
            for stream in streams:
                header.extend(struct.pack('<I', len(stream)))
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
            payload = b''.join(streams)
            checksum = zlib.crc32(payload) & 0xffffffff
            header.extend(struct.pack('<I', checksum))
            
            return bytes(header) + payload
            
        except Exception:
            return b''.join(streams)
    
    def _parse_tmc_v7_header(self, data: bytes) -> Optional[Dict[str, Any]]:
        """TMC v7.0 ãƒ˜ãƒƒãƒ€ãƒ¼è§£æï¼ˆ100%å¯é€†æ€§ä¿è¨¼ç‰ˆï¼‰"""
        try:
            # åŸºæœ¬ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            if len(data) < 44:
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºä¸è¶³: {len(data)} < 44")
                return None
                
            # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆè¤‡æ•°ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œï¼‰
            if data[:4] not in [b'TMC7', b'TMC6', b'TMC4']:
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ç„¡åŠ¹ãªãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼: {data[:4]}")
                return None
            
            offset = 4
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ï¼ˆå¢ƒç•Œãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            if offset + 32 > len(data):
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—é ˜åŸŸä¸è¶³")
                return None
            data_type = data[offset:offset+32].rstrip(b'\x00').decode('utf-8')
            offset += 32
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°ï¼ˆå¢ƒç•Œãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            if offset + 4 > len(data):
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°é ˜åŸŸä¸è¶³")
                return None
            stream_count = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if stream_count > 100 or stream_count == 0:
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ç„¡åŠ¹ãªã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°: {stream_count}")
                return None
            
            # åœ§ç¸®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆå¢ƒç•Œãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            compression_methods = []
            required_method_bytes = stream_count * 16
            if offset + required_method_bytes > len(data):
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] åœ§ç¸®ãƒ¡ã‚½ãƒƒãƒ‰é ˜åŸŸä¸è¶³")
                return None
                
            for i in range(stream_count):
                method = data[offset:offset+16].rstrip(b'\x00').decode('utf-8')
                compression_methods.append(method)
                offset += 16
            
            # å¤‰æ›æƒ…å ±ã‚µã‚¤ã‚ºï¼ˆå¢ƒç•Œãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            if offset + 4 > len(data):
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] å¤‰æ›æƒ…å ±ã‚µã‚¤ã‚ºé ˜åŸŸä¸è¶³")
                return None
            transform_info_size = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            # å¤‰æ›æƒ…å ±ã‚µã‚¤ã‚ºã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if transform_info_size > len(data) - offset or transform_info_size > 10000:
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ç„¡åŠ¹ãªå¤‰æ›æƒ…å ±ã‚µã‚¤ã‚º: {transform_info_size}")
                return None
            
            # å¤‰æ›æƒ…å ±ï¼ˆå®‰å…¨ãªJSONè§£æï¼‰
            if offset + transform_info_size > len(data):
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] å¤‰æ›æƒ…å ±é ˜åŸŸä¸è¶³")
                return None
            transform_info_str = data[offset:offset+transform_info_size].decode('utf-8', errors='replace')
            try:
                transform_info = json.loads(transform_info_str)
            except json.JSONDecodeError as e:
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç©ºã®å¤‰æ›æƒ…å ±
                transform_info = {'method': 'json_parse_error', 'bypassed': True}
            offset += transform_info_size
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºï¼ˆå¢ƒç•Œãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            required_size_bytes = stream_count * 4
            if offset + required_size_bytes > len(data):
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºé ˜åŸŸä¸è¶³")
                return None
            
            stream_sizes = []
            total_payload_size = 0
            for i in range(stream_count):
                size = struct.unpack('<I', data[offset:offset+4])[0]
                # ã‚µã‚¤ã‚ºã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                if size > len(data) * 2:  # å…ƒãƒ‡ãƒ¼ã‚¿ã®2å€ä»¥ä¸Šã¯ç•°å¸¸
                    print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ç•°å¸¸ãªã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚º: {size}")
                    return None
                stream_sizes.append(size)
                total_payload_size += size
                offset += 4
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ ï¼ˆå¢ƒç•Œãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            if offset + 4 > len(data):
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ãƒã‚§ãƒƒã‚¯ã‚µãƒ é ˜åŸŸä¸è¶³")
                return None
            checksum = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            remaining_data = len(data) - offset
            if total_payload_size > remaining_data:
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚ºä¸æ•´åˆ: æœŸå¾…{total_payload_size} > å®Ÿéš›{remaining_data}")
                # å¯èƒ½ãªé™ã‚Šä¿®å¾©ã‚’è©¦ã¿ã‚‹
                adjusted_sizes = []
                remaining_for_streams = remaining_data
                for i, size in enumerate(stream_sizes):
                    if i == len(stream_sizes) - 1:  # æœ€å¾Œã®ã‚¹ãƒˆãƒªãƒ¼ãƒ 
                        adjusted_sizes.append(remaining_for_streams)
                    else:
                        actual_size = min(size, remaining_for_streams // (len(stream_sizes) - i))
                        adjusted_sizes.append(actual_size)
                        remaining_for_streams -= actual_size
                stream_sizes = adjusted_sizes
                print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºã‚’è‡ªå‹•ä¿®å¾©")
            
            # v7.0æ©Ÿèƒ½ã®è§£æ
            transform_bypassed = transform_info.get('bypassed', False)
            
            print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] æˆåŠŸ: {stream_count}ã‚¹ãƒˆãƒªãƒ¼ãƒ , ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—={data_type}")
            
            return {
                'data_type': data_type,
                'stream_count': stream_count,
                'compression_methods': compression_methods,
                'transform_info': transform_info,
                'stream_sizes': stream_sizes,
                'checksum': checksum,
                'header_size': offset,
                'transform_bypassed': transform_bypassed,  # v7.0æ–°æ©Ÿèƒ½
                'total_payload_size': sum(stream_sizes)
            }
            
        except Exception as e:
            print(f"    [ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ] äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _extract_tmc_v7_streams(self, payload: bytes, header: Dict[str, Any]) -> List[bytes]:
        """TMC v7.0 ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡ºï¼ˆ100%å¯é€†æ€§ä¿è¨¼ç‰ˆï¼‰"""
        try:
            streams = []
            offset = 0
            
            print(f"    [ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º] {len(header['stream_sizes'])}ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’æŠ½å‡ºä¸­...")
            
            for i, size in enumerate(header['stream_sizes']):
                # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
                if offset + size > len(payload):
                    print(f"    [ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º] ã‚¹ãƒˆãƒªãƒ¼ãƒ {i}: å¢ƒç•Œè¶…é offset={offset}, size={size}, payload_len={len(payload)}")
                    # æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’ã™ã¹ã¦å–å¾—
                    remaining_data = payload[offset:]
                    if len(remaining_data) > 0:
                        streams.append(remaining_data)
                        print(f"    [ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º] ã‚¹ãƒˆãƒªãƒ¼ãƒ {i}: æ®‹ã‚Šãƒ‡ãƒ¼ã‚¿{len(remaining_data)}bytesã‚’ç·Šæ€¥è¿½åŠ ")
                    else:
                        # ç©ºã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¿½åŠ 
                        streams.append(b'')
                        print(f"    [ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º] ã‚¹ãƒˆãƒªãƒ¼ãƒ {i}: ç©ºã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¿½åŠ ")
                    break
                
                stream = payload[offset:offset+size]
                streams.append(stream)
                print(f"    [ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º] ã‚¹ãƒˆãƒªãƒ¼ãƒ {i}: {len(stream)}bytesæŠ½å‡º")
                offset += size
            
            # å¿…è¦ãªã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°ã«é”ã—ã¦ã„ãªã„å ´åˆã®è£œå®Œ
            expected_count = header['stream_count']
            while len(streams) < expected_count:
                streams.append(b'')
                print(f"    [ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º] ä¸è¶³ã‚¹ãƒˆãƒªãƒ¼ãƒ {len(streams)-1}: ç©ºã‚¹ãƒˆãƒªãƒ¼ãƒ è£œå®Œ")
            
            print(f"    [ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º] å®Œäº†: {len(streams)}ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º")
            return streams
            
        except Exception as e:
            print(f"    [ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º] ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’å˜ä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨ã—ã¦è¿”ã™
            return [payload] if len(payload) > 0 else [b'']
    
    def _make_json_safe(self, data: Any) -> Any:
        """JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›"""
        if isinstance(data, dict):
            return {k: self._make_json_safe(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._make_json_safe(v) for v in data]
        elif isinstance(data, np.ndarray):
            return data.tolist()
        elif isinstance(data, (np.int32, np.int64, np.float32, np.float64)):
            return float(data)
        elif isinstance(data, np.bool_):
            return bool(data)
        else:
            return data
    
    def _create_fast_path_container(self, compressed: bytes, original_data: bytes, compression_time: float) -> Tuple[bytes, Dict[str, Any]]:
        """100%å¯é€†æ€§ä¿è¨¼ã®é«˜é€Ÿãƒ‘ã‚¹ç”¨ã‚³ãƒ³ãƒ†ãƒŠä½œæˆ"""
        try:
            # TMC v7.0äº’æ›ã®å®Œå…¨ãªãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆ
            streams = [compressed]
            methods = ['zlib_fast_path']
            data_type = 'generic_binary'
            transform_info = {
                'method': 'fast_path_bypass', 
                'bypassed': True,
                'reason': 'small_data_optimization',
                'original_size': len(original_data)
            }
            features = {'size': len(original_data), 'entropy': 'estimated_low'}
            
            # æ­£å¼ãªTMC v7.0ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ§‹ç¯‰
            container = self._pack_tmc_v7(streams, methods, data_type, transform_info, features)
            
            return container, {
                'method': 'fast_path_zlib_v7_format',
                'original_size': len(original_data),
                'compressed_size': len(container),
                'compression_ratio': len(container) / len(original_data),
                'compression_time': compression_time,
                'transform_applied': False,
                'tmc_version': '7.0',
                'reversible': True
            }
            
        except Exception as e:
            print(f"    [é«˜é€Ÿãƒ‘ã‚¹] ã‚³ãƒ³ãƒ†ãƒŠä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç”Ÿãƒ‡ãƒ¼ã‚¿è¿”å´
            return compressed, {
                'method': 'fast_path_fallback',
                'original_size': len(original_data),
                'compressed_size': len(compressed),
                'compression_ratio': len(compressed) / len(original_data),
                'compression_time': compression_time,
                'transform_applied': False,
                'error': str(e)
            }
    
    def _extract_tmc_v4_streams(self, payload: bytes, header: Dict[str, Any]) -> List[bytes]:
        """TMC v4.0 ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡ºï¼ˆäº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰"""
        return self._extract_tmc_v7_streams(payload, header)


# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
NEXUSTMCEngineV8 = NEXUSTMCEngineV9  # v8.xç³»ã‹ã‚‰ã®ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨

# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
__all__ = ['NEXUSTMCEngineV9', 'NEXUSTMCEngineV8', 'DataType']

if __name__ == "__main__":
    print("ğŸš€ NEXUS TMC Engine v9.0 - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°çµ±åˆç‰ˆ")
    
    engine = NEXUSTMCEngineV9()
    
    # TMC v8.0 ç‰¹åŒ–ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        ("æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿", np.linspace(1000, 1010, 2000, dtype=np.float32).tobytes()),
        ("ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿", np.arange(0, 8000, 4, dtype=np.int32).tobytes()),
        ("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿", ("Hello TMC v8.0! " * 500).encode('utf-8')),
        ("åå¾©ãƒã‚¤ãƒŠãƒª", b"PATTERN" * 1000),
        ("æ±ç”¨ãƒã‚¤ãƒŠãƒª", bytes(range(256)) * 20),
        ("ä¸¦åˆ—ãƒ†ã‚¹ãƒˆï¼ˆå¤§å®¹é‡ï¼‰", np.arange(0, 50000, dtype=np.int32).tobytes()),  # v8.0: ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ
        ("å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ", np.concatenate([
            np.arange(1000, 2000, dtype=np.int32),  # ç·šå½¢ãƒ‘ãƒ¼ãƒˆ
            np.full(500, 5000, dtype=np.int32),      # å®šæ•°ãƒ‘ãƒ¼ãƒˆ
        ]).tobytes()),  # v8.0: LeCoãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
    ]
    
    success_count = 0
    total_tests = len(test_cases)
    
    for name, data in test_cases:
        result = engine.test_reversibility(data, name)
        if result.get('reversible', False):
            success_count += 1
        
        # v7.0æ–°æ©Ÿèƒ½ã®è©³ç´°è¡¨ç¤º
        print(f"  ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹: {'æœ‰åŠ¹' if result.get('intelligent_bypass_used') else 'ç„¡åŠ¹'}")
        if 'meta_analysis' in result and result['meta_analysis']:
            meta = result['meta_analysis']
            print(f"  ãƒ¡ã‚¿åˆ†æ: {meta.get('reason', 'N/A')}")
            if 'effectiveness' in meta:
                print(f"  åœ§ç¸®åŠ¹æœ: {meta['effectiveness']:.2%}")
    
    print(f"\nğŸ“Š TMC v8.0 ãƒ†ã‚¹ãƒˆçµæœ: {success_count}/{total_tests} æˆåŠŸ")
    print(f"ğŸ“ˆ çµ±è¨ˆ:")
    print(f"  å¤‰æ›é©ç”¨: {engine.stats['transforms_applied']}")
    print(f"  å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—: {engine.stats['transforms_bypassed']}")
    print(f"  ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ä½¿ç”¨: {engine.stats['entropy_coding_used']}")
    print(f"  ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: {engine.stats['chunks_processed']}")
    
    if success_count == total_tests:
        print("ğŸ‰ TMC v8.0 æ¬¡ä¸–ä»£é‡å­ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æº–å‚™å®Œäº†!")
        print("ğŸ”¥ ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç† + å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚° + ç´”ç²‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–çµ±åˆå®Œäº†!")
        if ZSTD_AVAILABLE:
            print("âš¡ æœ€é«˜æ€§èƒ½æ§‹æˆ: çœŸã®ä¸¦åˆ—å‡¦ç† + LeCoãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚° + é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–!")
    else:
        print("âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•— - ã•ã‚‰ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦")

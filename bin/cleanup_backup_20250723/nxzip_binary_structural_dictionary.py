#!/usr/bin/env python3
"""
NXZip Binary Structural Dictionary Compressor
ãƒã‚¤ãƒŠãƒªæ§‹é€ è¾æ›¸åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³ - 16é€²æ•°è¾æ›¸ã«ã‚ˆã‚‹æ¥µé™åœ§ç¸®

é©æ–°çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
- ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«å¾¹åº•æ§‹é€ è§£æ
- æ§‹é€ æƒ…å ±ã®å®Œå…¨ä¿å­˜
- 16é€²æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³è¾æ›¸åœ§ç¸®
- æ§‹é€ ãƒ™ãƒ¼ã‚¹å®Œå…¨å¾©å…ƒ
- æ¥µé™åœ§ç¸®ç‡ã®å®Ÿç¾
"""

import struct
import time
import hashlib
import os
import sys
import zlib
import pickle
from typing import List, Tuple, Dict, Set
from collections import defaultdict, Counter
import re

class BinaryStructuralDictionaryCompressor:
    def __init__(self):
        self.magic = b'NXBSD'  # NXZip Binary Structural Dictionary
        self.version = 1
        
    def deep_binary_structural_analysis(self, data: bytes) -> Dict:
        """ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«å¾¹åº•æ§‹é€ è§£æ"""
        print(f"ğŸ”¬ ãƒã‚¤ãƒŠãƒªæ§‹é€ å¾¹åº•è§£æé–‹å§‹: {len(data):,} bytes")
        
        analysis = {
            'total_size': len(data),
            'md5_hash': hashlib.md5(data).hexdigest(),
            'hex_patterns': {},
            'structural_markers': {},
            'repetition_map': {},
            'offset_correlation': {},
            'byte_distribution': [0] * 256,
            'entropy_regions': [],
            'compression_zones': []
        }
        
        # 16é€²æ•°ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        hex_data = data.hex()
        print(f"ğŸ“Š 16é€²æ•°å¤‰æ›å®Œäº†: {len(hex_data)} hex chars")
        
        # ãƒã‚¤ãƒˆåˆ†å¸ƒè§£æ
        for byte in data:
            analysis['byte_distribution'][byte] += 1
        
        # 16é€²æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æï¼ˆ2-16æ–‡å­—ï¼‰
        print("ğŸ§© 16é€²æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æä¸­...")
        analysis['hex_patterns'] = self.analyze_hex_patterns(hex_data)
        
        # æ§‹é€ ãƒãƒ¼ã‚«ãƒ¼æ¤œå‡º
        print("ğŸ—ï¸ æ§‹é€ ãƒãƒ¼ã‚«ãƒ¼æ¤œå‡ºä¸­...")
        analysis['structural_markers'] = self.detect_structural_markers(data)
        
        # ç¹°ã‚Šè¿”ã—ãƒãƒƒãƒ—æ§‹ç¯‰
        print("ğŸ”„ ç¹°ã‚Šè¿”ã—ãƒãƒƒãƒ—æ§‹ç¯‰ä¸­...")
        analysis['repetition_map'] = self.build_repetition_map(data)
        
        # ã‚ªãƒ•ã‚»ãƒƒãƒˆç›¸é–¢è§£æ
        print("ğŸ“ ã‚ªãƒ•ã‚»ãƒƒãƒˆç›¸é–¢è§£æä¸­...")
        analysis['offset_correlation'] = self.analyze_offset_correlation(data)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åœ°åŸŸåˆ†æ
        print("ğŸ“ˆ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åœ°åŸŸåˆ†æä¸­...")
        analysis['entropy_regions'] = self.analyze_entropy_regions(data)
        
        # åœ§ç¸®ã‚¾ãƒ¼ãƒ³è­˜åˆ¥
        print("ğŸ¯ åœ§ç¸®ã‚¾ãƒ¼ãƒ³è­˜åˆ¥ä¸­...")
        analysis['compression_zones'] = self.identify_compression_zones(analysis)
        
        print(f"âœ… æ§‹é€ è§£æå®Œäº†:")
        print(f"   ğŸ§© 16é€²ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(analysis['hex_patterns'])} patterns")
        print(f"   ğŸ—ï¸ æ§‹é€ ãƒãƒ¼ã‚«ãƒ¼: {len(analysis['structural_markers'])} markers")
        print(f"   ğŸ”„ ç¹°ã‚Šè¿”ã—é ˜åŸŸ: {len(analysis['repetition_map'])} regions")
        print(f"   ğŸ¯ åœ§ç¸®ã‚¾ãƒ¼ãƒ³: {len(analysis['compression_zones'])} zones")
        
        return analysis
    
    def analyze_hex_patterns(self, hex_data: str) -> Dict:
        """16é€²æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ"""
        patterns = {}
        
        # 2-16æ–‡å­—ã®16é€²æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è§£æ
        for pattern_len in range(2, 17, 2):  # 2, 4, 6, 8, 10, 12, 14, 16
            if len(hex_data) < pattern_len:
                continue
                
            pattern_count = defaultdict(int)
            positions = defaultdict(list)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            for i in range(len(hex_data) - pattern_len + 1):
                pattern = hex_data[i:i+pattern_len]
                pattern_count[pattern] += 1
                positions[pattern].append(i)
            
            # é«˜é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ä¿å­˜ï¼ˆ3å›ä»¥ä¸Šå‡ºç¾ï¼‰
            frequent_patterns = {
                pattern: {
                    'count': count,
                    'positions': positions[pattern],
                    'savings': (count - 1) * pattern_len  # åœ§ç¸®åŠ¹æœæ¨å®š
                }
                for pattern, count in pattern_count.items()
                if count >= 3
            }
            
            if frequent_patterns:
                patterns[pattern_len] = frequent_patterns
        
        return patterns
    
    def detect_structural_markers(self, data: bytes) -> Dict:
        """æ§‹é€ ãƒãƒ¼ã‚«ãƒ¼æ¤œå‡ºï¼ˆè©³ç´°ç‰ˆï¼‰"""
        markers = {
            'file_signatures': [],
            'alignment_patterns': [],
            'padding_regions': [],
            'checksum_positions': []
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç½²åæ¤œå‡º
        signatures = [
            (b'\x89PNG\r\n\x1a\n', 'PNG_SIGNATURE'),
            (b'\xff\xd8\xff', 'JPEG_SOI'),
            (b'\xff\xd9', 'JPEG_EOI'),
            (b'PK\x03\x04', 'ZIP_LOCAL_HEADER'),
            (b'PK\x01\x02', 'ZIP_CENTRAL_HEADER'),
            (b'%PDF', 'PDF_HEADER'),
            (b'\x1f\x8b', 'GZIP_HEADER'),
            (b'RIFF', 'RIFF_HEADER'),
            (b'\x00\x00\x01', 'MPEG_START_CODE'),
            (b'ftyp', 'MP4_FTYP'),
            (b'moov', 'MP4_MOOV'),
            (b'mdat', 'MP4_MDAT'),
        ]
        
        for sig, name in signatures:
            pos = 0
            while True:
                pos = data.find(sig, pos)
                if pos == -1:
                    break
                markers['file_signatures'].append({
                    'type': name,
                    'offset': pos,
                    'signature': sig,
                    'hex': sig.hex()
                })
                pos += len(sig)
        
        # ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ4, 8, 16ãƒã‚¤ãƒˆå¢ƒç•Œï¼‰
        for alignment in [4, 8, 16]:
            aligned_positions = []
            for i in range(0, len(data), alignment):
                if i + alignment <= len(data):
                    block = data[i:i+alignment]
                    if len(set(block)) == 1:  # åŒã˜ãƒã‚¤ãƒˆã®ç¹°ã‚Šè¿”ã—
                        aligned_positions.append({
                            'offset': i,
                            'size': alignment,
                            'value': block[0],
                            'hex': block.hex()
                        })
            if aligned_positions:
                markers['alignment_patterns'].extend(aligned_positions)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°é ˜åŸŸæ¤œå‡ºï¼ˆé€£ç¶šã™ã‚‹ã‚¼ãƒ­ã€0xFFï¼‰
        padding_values = [0x00, 0xFF]
        for pad_val in padding_values:
            in_padding = False
            start_pos = 0
            
            for i, byte in enumerate(data):
                if byte == pad_val:
                    if not in_padding:
                        in_padding = True
                        start_pos = i
                else:
                    if in_padding:
                        length = i - start_pos
                        if length >= 8:  # 8ãƒã‚¤ãƒˆä»¥ä¸Š
                            markers['padding_regions'].append({
                                'offset': start_pos,
                                'size': length,
                                'value': pad_val,
                                'hex': format(pad_val, '02x') * length
                            })
                        in_padding = False
        
        return markers
    
    def build_repetition_map(self, data: bytes) -> Dict:
        """ç¹°ã‚Šè¿”ã—ãƒãƒƒãƒ—æ§‹ç¯‰"""
        repetition_map = {}
        
        # 2-64ãƒã‚¤ãƒˆã®ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        for pattern_len in [2, 4, 8, 16, 32, 64]:
            if len(data) < pattern_len * 2:
                continue
                
            pattern_positions = defaultdict(list)
            
            for i in range(len(data) - pattern_len + 1):
                pattern = data[i:i+pattern_len]
                pattern_positions[pattern].append(i)
            
            # 2å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
            repeated_patterns = {
                pattern.hex(): {
                    'binary': pattern,
                    'positions': positions,
                    'count': len(positions),
                    'total_bytes': len(positions) * pattern_len
                }
                for pattern, positions in pattern_positions.items()
                if len(positions) >= 2
            }
            
            if repeated_patterns:
                repetition_map[pattern_len] = repeated_patterns
        
        return repetition_map
    
    def analyze_offset_correlation(self, data: bytes) -> Dict:
        """ã‚ªãƒ•ã‚»ãƒƒãƒˆç›¸é–¢è§£æ"""
        correlations = {}
        
        # å›ºå®šè·é›¢ã§ã®å€¤ç›¸é–¢
        for distance in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:
            if len(data) <= distance:
                continue
                
            correlation_count = 0
            total_comparisons = 0
            value_pairs = defaultdict(int)
            
            for i in range(len(data) - distance):
                val1 = data[i]
                val2 = data[i + distance]
                
                if val1 == val2:
                    correlation_count += 1
                
                value_pairs[(val1, val2)] += 1
                total_comparisons += 1
            
            if total_comparisons > 0:
                correlation_ratio = correlation_count / total_comparisons
                
                if correlation_ratio > 0.1:  # 10%ä»¥ä¸Šã®ç›¸é–¢
                    correlations[distance] = {
                        'correlation_ratio': correlation_ratio,
                        'exact_matches': correlation_count,
                        'total_comparisons': total_comparisons,
                        'top_pairs': sorted(value_pairs.items(), 
                                          key=lambda x: x[1], reverse=True)[:10]
                    }
        
        return correlations
    
    def analyze_entropy_regions(self, data: bytes) -> List:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åœ°åŸŸåˆ†æ"""
        regions = []
        chunk_size = 1024  # 1KBå˜ä½ã§è§£æ
        
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i+chunk_size]
            if len(chunk) == 0:
                continue
                
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
            byte_count = Counter(chunk)
            entropy = 0.0
            total = len(chunk)
            
            for count in byte_count.values():
                if count > 0:
                    prob = count / total
                    entropy -= prob * (count.bit_length() - 1)  # ç°¡æ˜“ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            
            # åœ§ç¸®å¯èƒ½æ€§è©•ä¾¡
            unique_bytes = len(byte_count)
            repetition_factor = max(byte_count.values()) / total
            
            regions.append({
                'offset': i,
                'size': len(chunk),
                'entropy': entropy,
                'unique_bytes': unique_bytes,
                'repetition_factor': repetition_factor,
                'compressibility': 'HIGH' if repetition_factor > 0.5 else 
                                 'MEDIUM' if repetition_factor > 0.2 else 'LOW'
            })
        
        return regions
    
    def identify_compression_zones(self, analysis: Dict) -> List:
        """åœ§ç¸®ã‚¾ãƒ¼ãƒ³è­˜åˆ¥"""
        zones = []
        
        # é«˜åœ§ç¸®å¯èƒ½é ˜åŸŸã®ç‰¹å®š
        for region in analysis['entropy_regions']:
            if region['compressibility'] in ['HIGH', 'MEDIUM']:
                zones.append({
                    'type': 'HIGH_ENTROPY',
                    'offset': region['offset'],
                    'size': region['size'],
                    'method': 'DICTIONARY_COMPRESSION',
                    'priority': 'HIGH' if region['compressibility'] == 'HIGH' else 'MEDIUM'
                })
        
        # ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³é ˜åŸŸ
        for pattern_len, patterns in analysis['repetition_map'].items():
            for hex_pattern, info in patterns.items():
                if info['count'] >= 3:
                    zones.append({
                        'type': 'REPETITION_PATTERN',
                        'pattern_length': pattern_len,
                        'pattern_hex': hex_pattern,
                        'positions': info['positions'],
                        'method': 'PATTERN_REPLACEMENT',
                        'priority': 'HIGH'
                    })
        
        return zones
    
    def create_hex_dictionary(self, analysis: Dict) -> Dict:
        """16é€²æ•°è¾æ›¸ä½œæˆ"""
        print("ğŸ“š 16é€²æ•°è¾æ›¸ä½œæˆä¸­...")
        
        dictionary = {
            'patterns': {},
            'replacements': {},
            'metadata': {
                'total_patterns': 0,
                'estimated_savings': 0
            }
        }
        
        dict_id = 0
        total_savings = 0
        
        # 16é€²æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰è¾æ›¸ä½œæˆ
        for pattern_len, patterns in analysis['hex_patterns'].items():
            for hex_pattern, info in patterns.items():
                if info['count'] >= 3 and info['savings'] > pattern_len:
                    # è¾æ›¸ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
                    dict_key = f"D{dict_id:04X}"  # D0000, D0001, ...
                    
                    dictionary['patterns'][dict_key] = {
                        'hex_pattern': hex_pattern,
                        'binary_pattern': bytes.fromhex(hex_pattern),
                        'original_length': pattern_len,
                        'occurrences': info['count'],
                        'positions': info['positions'],
                        'savings': info['savings']
                    }
                    
                    dictionary['replacements'][hex_pattern] = dict_key
                    total_savings += info['savings']
                    dict_id += 1
        
        dictionary['metadata']['total_patterns'] = dict_id
        dictionary['metadata']['estimated_savings'] = total_savings
        
        print(f"ğŸ“š è¾æ›¸ä½œæˆå®Œäº†: {dict_id} patterns, æ¨å®šç¯€ç´„: {total_savings} chars")
        return dictionary
    
    def apply_dictionary_compression(self, data: bytes, dictionary: Dict) -> Tuple[bytes, Dict]:
        """è¾æ›¸åœ§ç¸®é©ç”¨"""
        print("ğŸ—œï¸ è¾æ›¸åœ§ç¸®é©ç”¨ä¸­...")
        
        hex_data = data.hex()
        compressed_hex = hex_data
        replacement_log = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é•·ã„é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé•·ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆï¼‰
        patterns = sorted(dictionary['replacements'].items(), 
                         key=lambda x: len(x[0]), reverse=True)
        
        # è¾æ›¸ç½®æ›é©ç”¨
        for hex_pattern, dict_key in patterns:
            if hex_pattern in compressed_hex:
                occurrences = compressed_hex.count(hex_pattern)
                compressed_hex = compressed_hex.replace(hex_pattern, dict_key)
                replacement_log.append({
                    'pattern': hex_pattern,
                    'dict_key': dict_key,
                    'occurrences': occurrences,
                    'original_length': len(hex_pattern),
                    'compressed_length': len(dict_key)
                })
        
        # 16é€²æ•°æ–‡å­—åˆ—ã‚’ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›ï¼ˆè¾æ›¸ã‚­ãƒ¼ã¯ç‰¹åˆ¥å‡¦ç†ï¼‰
        compressed_bytes = self.hex_with_dict_to_bytes(compressed_hex, dictionary)
        
        compression_info = {
            'original_hex_length': len(hex_data),
            'compressed_hex_length': len(compressed_hex),
            'replacement_log': replacement_log,
            'final_bytes_length': len(compressed_bytes)
        }
        
        hex_reduction = (len(hex_data) - len(compressed_hex)) / len(hex_data) * 100
        print(f"ğŸ—œï¸ è¾æ›¸åœ§ç¸®å®Œäº†: {hex_reduction:.1f}% hex reduction")
        
        return compressed_bytes, compression_info
    
    def hex_with_dict_to_bytes(self, hex_string: str, dictionary: Dict) -> bytes:
        """è¾æ›¸ã‚­ãƒ¼ä»˜ã16é€²æ•°æ–‡å­—åˆ—ã‚’ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›"""
        result = bytearray()
        i = 0
        
        while i < len(hex_string):
            # è¾æ›¸ã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯
            if hex_string[i:i+1] == 'D' and i + 4 < len(hex_string):
                dict_key = hex_string[i:i+5]  # D0000å½¢å¼
                if dict_key in dictionary['patterns']:
                    # è¾æ›¸ã‚­ãƒ¼ãƒãƒ¼ã‚«ãƒ¼ + ID
                    result.append(0xFD)  # è¾æ›¸ãƒãƒ¼ã‚«ãƒ¼
                    key_id = int(dict_key[1:], 16)
                    result.extend(struct.pack('>H', key_id))
                    i += 5
                    continue
            
            # é€šå¸¸ã®16é€²æ•°
            if i + 1 < len(hex_string):
                try:
                    byte_val = int(hex_string[i:i+2], 16)
                    result.append(byte_val)
                    i += 2
                except ValueError:
                    # ç„¡åŠ¹ãª16é€²æ•°ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    i += 1
            else:
                i += 1
        
        return bytes(result)
    
    def compress_with_structural_dictionary(self, data: bytes) -> bytes:
        """æ§‹é€ è¾æ›¸ã«ã‚ˆã‚‹å®Œå…¨åœ§ç¸®"""
        if not data:
            return self.magic + struct.pack('>I', 0)
        
        print(f"ğŸš€ æ§‹é€ è¾æ›¸åœ§ç¸®é–‹å§‹: {len(data):,} bytes")
        start_time = time.time()
        
        original_md5 = hashlib.md5(data).hexdigest()
        print(f"ğŸ”’ å…ƒãƒ‡ãƒ¼ã‚¿MD5: {original_md5}")
        
        # 1. ãƒã‚¤ãƒŠãƒªæ§‹é€ å¾¹åº•è§£æ
        analysis = self.deep_binary_structural_analysis(data)
        
        # 2. 16é€²æ•°è¾æ›¸ä½œæˆ
        dictionary = self.create_hex_dictionary(analysis)
        
        # 3. è¾æ›¸åœ§ç¸®é©ç”¨
        compressed_data, compression_info = self.apply_dictionary_compression(data, dictionary)
        
        # 4. æœ€çµ‚zlibåœ§ç¸®
        final_compressed = zlib.compress(compressed_data, level=9)
        
        # 5. æ§‹é€ æƒ…å ±ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°
        structure_package = {
            'original_md5': original_md5,
            'original_size': len(data),
            'analysis': analysis,
            'dictionary': dictionary,
            'compression_info': compression_info
        }
        
        structure_bytes = pickle.dumps(structure_package, protocol=pickle.HIGHEST_PROTOCOL)
        structure_compressed = zlib.compress(structure_bytes, level=9)
        
        # 6. æœ€çµ‚ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ§‹ç¯‰
        header = self.magic + struct.pack('>I', len(data))
        header += struct.pack('>I', len(structure_compressed))
        header += struct.pack('>I', len(final_compressed))
        
        result = header + structure_compressed + final_compressed
        
        processing_time = time.time() - start_time
        compression_ratio = ((len(data) - len(result)) / len(data)) * 100
        
        print(f"ğŸ† æ§‹é€ è¾æ›¸åœ§ç¸®å®Œäº†:")
        print(f"   ğŸ’¥ åœ§ç¸®ç‡: {compression_ratio:.1f}%")
        print(f"   ğŸ“Š {len(data):,} â†’ {len(result):,} bytes")
        print(f"   âš¡ å‡¦ç†æ™‚é–“: {processing_time:.3f}s")
        
        # RAWä¿å­˜åˆ¤å®š
        if len(result) >= len(data) * 0.95:
            print("âš ï¸ åœ§ç¸®åŠ¹æœé™å®š - RAWä¿å­˜")
            return b'RAW_BSD' + struct.pack('>I', len(data)) + data
        
        return result
    
    def restore_from_structural_dictionary(self, compressed: bytes) -> bytes:
        """æ§‹é€ è¾æ›¸ã«ã‚ˆã‚‹å®Œå…¨å¾©å…ƒ"""
        if not compressed:
            return b''
        
        # RAWå½¢å¼ãƒã‚§ãƒƒã‚¯
        if compressed.startswith(b'RAW_BSD'):
            size = struct.unpack('>I', compressed[7:11])[0]
            return compressed[11:11+size]
        
        if not compressed.startswith(self.magic):
            raise ValueError("Invalid NXBSD format")
        
        pos = len(self.magic)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        original_size = struct.unpack('>I', compressed[pos:pos+4])[0]
        pos += 4
        
        structure_size = struct.unpack('>I', compressed[pos:pos+4])[0]
        pos += 4
        
        data_size = struct.unpack('>I', compressed[pos:pos+4])[0]
        pos += 4
        
        # æ§‹é€ æƒ…å ±å¾©å…ƒ
        structure_compressed = compressed[pos:pos+structure_size]
        pos += structure_size
        
        structure_bytes = zlib.decompress(structure_compressed)
        structure_package = pickle.loads(structure_bytes)
        
        # ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
        data_compressed = compressed[pos:pos+data_size]
        compressed_data = zlib.decompress(data_compressed)
        
        # è¾æ›¸å¾©å…ƒé©ç”¨
        restored_data = self.restore_dictionary_compression(
            compressed_data, 
            structure_package['dictionary']
        )
        
        # å®Œå…¨æ€§æ¤œè¨¼
        restored_md5 = hashlib.md5(restored_data).hexdigest()
        if restored_md5 != structure_package['original_md5']:
            raise ValueError(f"Integrity check failed: {restored_md5} != {structure_package['original_md5']}")
        
        print(f"âœ… æ§‹é€ è¾æ›¸å¾©å…ƒæˆåŠŸ: MD5ä¸€è‡´ ({restored_md5})")
        return restored_data
    
    def restore_dictionary_compression(self, compressed_data: bytes, dictionary: Dict) -> bytes:
        """è¾æ›¸åœ§ç¸®å¾©å…ƒ"""
        result = bytearray()
        i = 0
        
        while i < len(compressed_data):
            if compressed_data[i] == 0xFD and i + 2 < len(compressed_data):
                # è¾æ›¸ã‚­ãƒ¼ãƒãƒ¼ã‚«ãƒ¼
                key_id = struct.unpack('>H', compressed_data[i+1:i+3])[0]
                dict_key = f"D{key_id:04X}"
                
                if dict_key in dictionary['patterns']:
                    # è¾æ›¸ãƒ‘ã‚¿ãƒ¼ãƒ³å¾©å…ƒ
                    pattern_info = dictionary['patterns'][dict_key]
                    result.extend(pattern_info['binary_pattern'])
                
                i += 3
            else:
                # é€šå¸¸ã®ãƒã‚¤ãƒˆ
                result.append(compressed_data[i])
                i += 1
        
        return bytes(result)
    
    def compress_file(self, input_path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«åœ§ç¸®"""
        if not os.path.exists(input_path):
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_path}")
            return None
        
        print(f"ğŸš€ æ§‹é€ è¾æ›¸åœ§ç¸®é–‹å§‹: {os.path.basename(input_path)}")
        
        with open(input_path, 'rb') as f:
            data = f.read()
        
        original_size = len(data)
        original_md5 = hashlib.md5(data).hexdigest()
        
        print(f"ğŸ“ å…ƒãƒ•ã‚¡ã‚¤ãƒ«: {original_size:,} bytes")
        print(f"ğŸ”’ å…ƒMD5: {original_md5}")
        
        start_time = time.time()
        compressed = self.compress_with_structural_dictionary(data)
        processing_time = time.time() - start_time
        
        compression_ratio = ((original_size - len(compressed)) / original_size) * 100
        throughput = original_size / (1024 * 1024) / processing_time if processing_time > 0 else 0
        
        # ä¿å­˜
        output_path = input_path + '.nxbsd'
        with open(output_path, 'wb') as f:
            f.write(compressed)
        
        print(f"ğŸ’¾ ä¿å­˜: {os.path.basename(output_path)}")
        
        # å®Œå…¨æ€§ãƒ†ã‚¹ãƒˆ
        try:
            restored = self.restore_from_structural_dictionary(compressed)
            restored_md5 = hashlib.md5(restored).hexdigest()
            
            if restored_md5 == original_md5:
                print(f"âœ… å®Œå…¨å¯é€†æ€§ç¢ºèª: MD5ä¸€è‡´")
                print(f"ğŸ¯ SUCCESS: æ§‹é€ è¾æ›¸åœ§ç¸®å®Œäº†")
                print(f"âš¡ å‡¦ç†é€Ÿåº¦: {throughput:.1f} MB/s")
                return True
            else:
                print(f"âŒ MD5ä¸ä¸€è‡´: {original_md5} != {restored_md5}")
                return False
        except Exception as e:
            print(f"âŒ å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            return False

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ³•: python nxzip_binary_structural_dictionary.py <ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
        print("\nğŸ¯ NXZip ãƒã‚¤ãƒŠãƒªæ§‹é€ è¾æ›¸åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³")
        print("ğŸ“‹ é©æ–°çš„ç‰¹å¾´:")
        print("  ğŸ”¬ ãƒã‚¤ãƒŠãƒªãƒ¬ãƒ™ãƒ«å¾¹åº•æ§‹é€ è§£æ")
        print("  ğŸ’¾ æ§‹é€ æƒ…å ±å®Œå…¨ä¿å­˜")
        print("  ğŸ“š 16é€²æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³è¾æ›¸åœ§ç¸®")
        print("  ğŸ”„ æ§‹é€ ãƒ™ãƒ¼ã‚¹å®Œå…¨å¾©å…ƒ")
        print("  ğŸ† æ¥µé™åœ§ç¸®ç‡å®Ÿç¾")
        sys.exit(1)
    
    input_file = sys.argv[1]
    compressor = BinaryStructuralDictionaryCompressor()
    compressor.compress_file(input_file)

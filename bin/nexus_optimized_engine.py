#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸš€ NEXUS OPTIMIZED ENGINE V2.0 ğŸš€
Ultra-High Performance Adaptive Compression Engine

æœ€é©åŒ–æˆ¦ç•¥ã‚’å®Ÿè£…ã—ãŸNEXUSæ¬¡ä¸–ä»£ã‚¨ãƒ³ã‚¸ãƒ³:
- ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹é©å¿œåœ§ç¸®
- ã‚¹ãƒãƒ¼ãƒˆçµ±åˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ   
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰åœ§ç¸®æˆ¦ç•¥
"""

import numpy as np
import os
import sys
import time
import hashlib
import lzma
import gzip
import zlib
from collections import Counter, defaultdict
from itertools import combinations, product
import pickle
import json
from pathlib import Path

class NEXUSOptimizedEngine:
    """NEXUSæœ€é©åŒ–åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³ - V2.0"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.version = "2.0-OPTIMIZED"
        self.shapes = {
            'I-1': [(0, 0)],
            'I-2': [(0, 0), (0, 1)],
            'I-3': [(0, 0), (0, 1), (0, 2)],
            'I-4': [(0, 0), (0, 1), (0, 2), (0, 3)],
            'I-5': [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4)],
            'L-3': [(0, 0), (0, 1), (1, 0)],
            'L-4': [(0, 0), (0, 1), (0, 2), (1, 0)],
            'T-4': [(0, 0), (0, 1), (0, 2), (1, 1)],
            'T-5': [(0, 0), (0, 1), (0, 2), (1, 1), (2, 1)],
            'H-3': [(0, 0), (1, 0), (2, 0)],
            'H-5': [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0)],
            'H-7': [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 1)],
            'S-4': [(0, 0), (0, 1), (1, 1), (1, 2)],
            'Z-4': [(0, 1), (0, 2), (1, 0), (1, 1)],
            'O-4': [(0, 0), (0, 1), (1, 0), (1, 1)]
        }
        print(f"ğŸ”¥ NEXUS Optimized Engine V{self.version} initialized")
        print(f"   [Optimization] Size-adaptive compression enabled")
        print(f"   [Optimization] Smart consolidation algorithms loaded")
        print(f"   [Optimization] Metadata compression pipeline ready")
        print(f"   [Optimization] Hybrid compression strategy active")
    
    def get_file_category(self, data_size):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        if data_size < 100:
            return "micro"  # ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        elif data_size < 1024:
            return "tiny"   # å°ãƒ•ã‚¡ã‚¤ãƒ«
        elif data_size < 10240:
            return "small"  # ä¸­å°ãƒ•ã‚¡ã‚¤ãƒ«
        elif data_size < 102400:
            return "medium" # ä¸­ãƒ•ã‚¡ã‚¤ãƒ«
        else:
            return "large"  # å¤§ãƒ•ã‚¡ã‚¤ãƒ«
    
    def standard_compression_fallback(self, data):
        """æ¨™æº–åœ§ç¸®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("   [Fallback] Testing standard compression algorithms...")
        
        # è¤‡æ•°ã®æ¨™æº–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦è¡Œ
        results = {}
        
        # LZMA
        try:
            lzma_compressed = lzma.compress(data, preset=9)
            results['lzma'] = len(lzma_compressed)
            print(f"   [Fallback] LZMA: {len(data)} -> {len(lzma_compressed)} bytes ({len(lzma_compressed)/len(data)*100:.1f}%)")
        except:
            results['lzma'] = float('inf')
        
        # Gzip
        try:
            gzip_compressed = gzip.compress(data, compresslevel=9)
            results['gzip'] = len(gzip_compressed)
            print(f"   [Fallback] Gzip: {len(data)} -> {len(gzip_compressed)} bytes ({len(gzip_compressed)/len(data)*100:.1f}%)")
        except:
            results['gzip'] = float('inf')
            
        # Zlib
        try:
            zlib_compressed = zlib.compress(data, level=9)
            results['zlib'] = len(zlib_compressed)
            print(f"   [Fallback] Zlib: {len(data)} -> {len(zlib_compressed)} bytes ({len(zlib_compressed)/len(data)*100:.1f}%)")
        except:
            results['zlib'] = float('inf')
        
        # æœ€è‰¯ã®çµæœã‚’é¸æŠ
        best_algo = min(results, key=results.get)
        best_size = results[best_algo]
        
        print(f"   [Fallback] Best standard algorithm: {best_algo.upper()} ({best_size} bytes)")
        
        # æœ€è‰¯ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§åœ§ç¸®
        if best_algo == 'lzma':
            compressed_data = lzma.compress(data, preset=9)
            return compressed_data, {'algorithm': 'lzma', 'original_size': len(data)}
        elif best_algo == 'gzip':
            compressed_data = gzip.compress(data, compresslevel=9)
            return compressed_data, {'algorithm': 'gzip', 'original_size': len(data)}
        elif best_algo == 'zlib':
            compressed_data = zlib.compress(data, level=9)
            return compressed_data, {'algorithm': 'zlib', 'original_size': len(data)}
    
    def standard_decompression_fallback(self, compressed_data, metadata):
        """æ¨™æº–åœ§ç¸®ã®å±•é–‹"""
        algo = metadata['algorithm']
        
        if algo == 'lzma':
            return lzma.decompress(compressed_data)
        elif algo == 'gzip':
            return gzip.decompress(compressed_data)
        elif algo == 'zlib':
            return zlib.decompress(compressed_data)
        else:
            raise ValueError(f"Unknown fallback algorithm: {algo}")
    
    def smart_consolidation(self, groups, category, tolerance_factor=1.0):
        """ã‚¹ãƒãƒ¼ãƒˆçµ±åˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  - ã‚«ãƒ†ã‚´ãƒªåˆ¥æœ€é©åŒ–"""
        if category == "micro":
            # ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: æ¿€çš„çµ±åˆ
            tolerance = 50 * tolerance_factor  # éå¸¸ã«é«˜ã„è¨±å®¹åº¦
            print(f"   [Smart Consolidation] Micro file mode: high tolerance ({tolerance})")
        elif category == "tiny":
            # å°ãƒ•ã‚¡ã‚¤ãƒ«: é«˜çµ±åˆ
            tolerance = 30 * tolerance_factor
            print(f"   [Smart Consolidation] Tiny file mode: elevated tolerance ({tolerance})")
        elif category == "small":
            # ä¸­å°ãƒ•ã‚¡ã‚¤ãƒ«: ä¸­çµ±åˆ
            tolerance = 15 * tolerance_factor
            print(f"   [Smart Consolidation] Small file mode: moderate tolerance ({tolerance})")
        elif category == "medium":
            # ä¸­ãƒ•ã‚¡ã‚¤ãƒ«: ä½çµ±åˆ
            tolerance = 5 * tolerance_factor
            print(f"   [Smart Consolidation] Medium file mode: low tolerance ({tolerance})")
        else:
            # å¤§ãƒ•ã‚¡ã‚¤ãƒ«: ç²¾å¯†çµ±åˆ
            tolerance = 1 * tolerance_factor
            print(f"   [Smart Consolidation] Large file mode: precision tolerance ({tolerance})")
        
        # çµ±åˆå‰ã®ã‚°ãƒ«ãƒ¼ãƒ—æ•°
        original_count = len(groups)
        
        # è·é›¢ãƒ™ãƒ¼ã‚¹çµ±åˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
        consolidated_groups = []
        used_indices = set()
        
        for i, group1 in enumerate(groups):
            if i in used_indices:
                continue
                
            merged_group = group1.copy()
            merge_candidates = [i]
            
            for j, group2 in enumerate(groups[i+1:], i+1):
                if j in used_indices:
                    continue
                
                # ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®è·é›¢è¨ˆç®—ï¼ˆL2ãƒãƒ«ãƒ ï¼‰
                distance = np.linalg.norm(np.array(group1) - np.array(group2))
                
                if distance <= tolerance:
                    # çµ±åˆå€™è£œã¨ã—ã¦è¿½åŠ 
                    merge_candidates.append(j)
                    # é‡å¿ƒã§çµ±åˆ
                    merged_group = ((np.array(merged_group) + np.array(group2)) / 2).astype(int)
            
            # çµ±åˆå®Ÿè¡Œ
            for idx in merge_candidates:
                used_indices.add(idx)
            
            consolidated_groups.append(tuple(merged_group))
        
        consolidated_count = len(consolidated_groups)
        reduction_rate = (1 - consolidated_count / original_count) * 100
        
        print(f"   [Smart Consolidation] {original_count} -> {consolidated_count} groups ({reduction_rate:.1f}% reduction)")
        
        return consolidated_groups
    
    def compress_metadata(self, metadata):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        print("   [Metadata Compression] Applying advanced compression...")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONåŒ–
        metadata_json = json.dumps(metadata, separators=(',', ':')).encode('utf-8')
        original_size = len(metadata_json)
        
        # è¤‡æ•°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§è©¦è¡Œ
        compression_results = {}
        
        # LZMAåœ§ç¸®
        try:
            lzma_compressed = lzma.compress(metadata_json, preset=9)
            compression_results['lzma'] = lzma_compressed
        except:
            pass
        
        # Zlibåœ§ç¸®  
        try:
            zlib_compressed = zlib.compress(metadata_json, level=9)
            compression_results['zlib'] = zlib_compressed
        except:
            pass
        
        # æœ€å°ã®ã‚‚ã®ã‚’é¸æŠ
        if compression_results:
            best_algo = min(compression_results, key=lambda k: len(compression_results[k]))
            best_compressed = compression_results[best_algo]
            compressed_size = len(best_compressed)
            
            print(f"   [Metadata Compression] {original_size} -> {compressed_size} bytes ({compressed_size/original_size*100:.1f}%, {best_algo.upper()})")
            
            return {
                'compressed_metadata': best_compressed,
                'compression_algo': best_algo,
                'original_size': original_size
            }
        else:
            # åœ§ç¸®å¤±æ•—æ™‚ã¯ç”Ÿãƒ‡ãƒ¼ã‚¿
            print(f"   [Metadata Compression] No compression benefit, using raw data")
            return {
                'compressed_metadata': metadata_json,
                'compression_algo': 'none',
                'original_size': original_size
            }
    
    def decompress_metadata(self, compressed_metadata_info):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å±•é–‹"""
        compressed_data = compressed_metadata_info['compressed_metadata']
        algo = compressed_metadata_info['compression_algo']
        
        if algo == 'lzma':
            metadata_json = lzma.decompress(compressed_data)
        elif algo == 'zlib':
            metadata_json = zlib.decompress(compressed_data)
        elif algo == 'none':
            metadata_json = compressed_data
        else:
            raise ValueError(f"Unknown metadata compression algorithm: {algo}")
        
        return json.loads(metadata_json.decode('utf-8'))
    
    def adaptive_compress(self, data):
        """é©å¿œçš„åœ§ç¸® - ã‚µã‚¤ã‚ºåˆ¥æœ€é©åŒ–æˆ¦ç•¥"""
        data_size = len(data)
        category = self.get_file_category(data_size)
        
        print(f"ğŸ”¥ NEXUS OPTIMIZED COMPRESSION STARTING...")
        print(f"   [Adaptive Strategy] File size: {data_size} bytes, category: {category.upper()}")
        
        start_time = time.time()
        
        # ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: æ¨™æº–åœ§ç¸®ã®ã¿
        if category == "micro":
            print("   [Adaptive Strategy] Micro file detected: using standard compression only")
            compressed_data, metadata = self.standard_compression_fallback(data)
            
            result = {
                'compressed_data': compressed_data,
                'metadata': metadata,
                'compression_type': 'standard',
                'algorithm': metadata['algorithm'],
                'original_size': data_size,
                'compressed_size': len(compressed_data),
                'compression_time': time.time() - start_time
            }
            
            print(f"âœ… OPTIMIZED COMPRESSION COMPLETE!")
            print(f"â±ï¸  Compression time: {result['compression_time']:.3f}s")
            print(f"ğŸ“¦ Compressed size: {len(compressed_data)} bytes")
            print(f"ğŸ“Š Compression ratio: {len(compressed_data)/data_size:.4f} ({len(compressed_data)/data_size*100:.2f}%)")
            
            return result
        
        # å°ãƒ•ã‚¡ã‚¤ãƒ«: ç°¡ç´ åŒ–NEXUS vs æ¨™æº–åœ§ç¸®
        elif category in ["tiny", "small"]:
            print(f"   [Adaptive Strategy] {category.capitalize()} file: simplified NEXUS vs standard compression")
            
            # æ¨™æº–åœ§ç¸®ã‚’è©¦è¡Œ
            standard_compressed, standard_metadata = self.standard_compression_fallback(data)
            standard_size = len(standard_compressed)
            
            # ç°¡ç´ åŒ–NEXUSåœ§ç¸®ã‚’è©¦è¡Œ
            try:
                nexus_result = self.simplified_nexus_compress(data, category)
                nexus_size = nexus_result['compressed_size']
                
                print(f"   [Adaptive Strategy] Standard: {standard_size} bytes vs NEXUS: {nexus_size} bytes")
                
                # ã‚ˆã‚Šå°ã•ã„æ–¹ã‚’é¸æŠ
                if standard_size <= nexus_size:
                    print(f"   [Adaptive Strategy] Selected: STANDARD compression ({standard_size} bytes)")
                    result = {
                        'compressed_data': standard_compressed,
                        'metadata': standard_metadata,
                        'compression_type': 'standard',
                        'algorithm': standard_metadata['algorithm'],
                        'original_size': data_size,
                        'compressed_size': standard_size,
                        'compression_time': time.time() - start_time
                    }
                else:
                    print(f"   [Adaptive Strategy] Selected: NEXUS compression ({nexus_size} bytes)")
                    nexus_result['compression_time'] = time.time() - start_time
                    result = nexus_result
                    
            except Exception as e:
                print(f"   [Adaptive Strategy] NEXUS failed ({e}), using standard compression")
                result = {
                    'compressed_data': standard_compressed,
                    'metadata': standard_metadata,
                    'compression_type': 'standard',
                    'algorithm': standard_metadata['algorithm'],
                    'original_size': data_size,
                    'compressed_size': standard_size,
                    'compression_time': time.time() - start_time
                }
        
        # ä¸­ï½å¤§ãƒ•ã‚¡ã‚¤ãƒ«: æœ€é©åŒ–NEXUS
        else:
            print(f"   [Adaptive Strategy] {category.capitalize()} file: optimized NEXUS compression")
            try:
                result = self.optimized_nexus_compress(data, category)
                result['compression_time'] = time.time() - start_time
            except Exception as e:
                print(f"   [Adaptive Strategy] Optimized NEXUS failed ({e}), fallback to standard")
                compressed_data, metadata = self.standard_compression_fallback(data)
                result = {
                    'compressed_data': compressed_data,
                    'metadata': metadata,
                    'compression_type': 'standard',
                    'algorithm': metadata['algorithm'],
                    'original_size': data_size,
                    'compressed_size': len(compressed_data),
                    'compression_time': time.time() - start_time
                }
        
        print(f"âœ… OPTIMIZED COMPRESSION COMPLETE!")
        print(f"â±ï¸  Compression time: {result['compression_time']:.3f}s")
        print(f"ğŸ“¦ Compressed size: {result['compressed_size']} bytes")
        print(f"ğŸ“Š Compression ratio: {result['compressed_size']/data_size:.4f} ({result['compressed_size']/data_size*100:.2f}%)")
        print(f"ğŸ’¾ Space saved: {(1-result['compressed_size']/data_size)*100:.2f}%")
        
        return result
    
    def simplified_nexus_compress(self, data, category):
        """ç°¡ç´ åŒ–NEXUSåœ§ç¸® - å°ãƒ•ã‚¡ã‚¤ãƒ«ç”¨"""
        print("   [Simplified NEXUS] Starting simplified compression...")
        
        # æœ€ã‚‚å˜ç´”ãªå½¢çŠ¶ã®ã¿ã‚’ä½¿ç”¨
        shape_name = 'I-1'
        shape = self.shapes[shape_name]
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¨ã‚°ãƒªãƒƒãƒ‰è¨­å®š
        shape_width = max(coord[1] for coord in shape) + 1
        shape_height = max(coord[0] for coord in shape) + 1
        grid_width = int(np.ceil(np.sqrt(len(data))))
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        total_size = grid_width * grid_width
        padded_data = np.pad(data, (0, max(0, total_size - len(data))), mode='constant', constant_values=0)
        
        # ãƒ–ãƒ­ãƒƒã‚¯æ­£è¦åŒ–ï¼ˆç°¡ç´ åŒ–ï¼‰
        blocks = []
        for start_row in range(0, grid_width, shape_height):
            for start_col in range(0, grid_width, shape_width):
                if start_row + shape_height <= grid_width and start_col + shape_width <= grid_width:
                    block_data = []
                    for rel_row, rel_col in shape:
                        abs_row = start_row + rel_row
                        abs_col = start_col + rel_col
                        if abs_row < grid_width and abs_col < grid_width:
                            idx = abs_row * grid_width + abs_col
                            if idx < len(padded_data):
                                block_data.append(int(padded_data[idx]))
                    if block_data:
                        blocks.append(tuple(block_data))
        
        # çµ±åˆï¼ˆæ¿€çš„ï¼‰
        unique_groups = list(set(blocks))
        if category == "tiny":
            # å°ãƒ•ã‚¡ã‚¤ãƒ«ã¯50%çµ±åˆã‚’ç›®æ¨™
            target_groups = max(1, len(unique_groups) // 2)
            unique_groups = self.smart_consolidation(unique_groups, category, tolerance_factor=2.0)[:target_groups]
        
        print(f"   [Simplified NEXUS] Using {len(unique_groups)} groups from {len(blocks)} blocks")
        
        # ã‚°ãƒ«ãƒ¼ãƒ—IDã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
        group_mapping = {group: idx for idx, group in enumerate(unique_groups)}
        group_id_stream = []
        for block in blocks:
            # æœ€ã‚‚è¿‘ã„ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ¤œç´¢
            best_group = min(unique_groups, key=lambda g: np.linalg.norm(np.array(g) - np.array(block)))
            group_id_stream.append(group_mapping[best_group])
        
        # ç°¡ç´ åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        # RLEåœ§ç¸®
        rle_encoded = []
        if group_id_stream:
            current_id = group_id_stream[0]
            count = 1
            for next_id in group_id_stream[1:]:
                if next_id == current_id:
                    count += 1
                else:
                    rle_encoded.extend([current_id, count])
                    current_id = next_id
                    count = 1
            rle_encoded.extend([current_id, count])
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            'version': self.version,
            'compression_type': 'simplified_nexus',
            'shape': shape_name,
            'grid_width': grid_width,
            'original_size': len(data),
            'groups': unique_groups,
            'rle_stream': rle_encoded,
            'blocks_count': len(blocks)
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åœ§ç¸®
        compressed_metadata_info = self.compress_metadata(metadata)
        
        # æœ€çµ‚ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°
        final_package = pickle.dumps(compressed_metadata_info)
        
        return {
            'compressed_data': final_package,
            'metadata': metadata,
            'compression_type': 'simplified_nexus',
            'original_size': len(data),
            'compressed_size': len(final_package)
        }
    
    def optimized_nexus_compress(self, data, category):
        """æœ€é©åŒ–NEXUSåœ§ç¸® - ä¸­ï½å¤§ãƒ•ã‚¡ã‚¤ãƒ«ç”¨"""
        print("   [Optimized NEXUS] Starting optimized compression...")
        
        # å½¢çŠ¶é¸æŠï¼ˆå¾“æ¥ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        if len(data) < 10000:
            shape_name = 'I-2'
        else:
            shape_name = 'I-4'
        
        shape = self.shapes[shape_name]
        
        # ã‚°ãƒªãƒƒãƒ‰è¨ˆç®—
        shape_width = max(coord[1] for coord in shape) + 1
        shape_height = max(coord[0] for coord in shape) + 1
        grid_width = max(shape_width, int(np.ceil(np.sqrt(len(data) / len(shape)))))
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        rows = int(np.ceil(len(data) / grid_width))
        total_size = rows * grid_width
        padded_data = np.pad(data, (0, max(0, total_size - len(data))), mode='constant', constant_values=0)
        
        # ãƒ–ãƒ­ãƒƒã‚¯æ­£è¦åŒ–
        blocks = []
        for start_row in range(0, rows, shape_height):
            for start_col in range(0, grid_width, shape_width):
                if start_row + shape_height <= rows and start_col + shape_width <= grid_width:
                    block_data = []
                    for rel_row, rel_col in shape:
                        abs_row = start_row + rel_row
                        abs_col = start_col + rel_col
                        if abs_row < rows and abs_col < grid_width:
                            idx = abs_row * grid_width + abs_col
                            if idx < len(padded_data):
                                block_data.append(int(padded_data[idx]))
                    if block_data:
                        blocks.append(tuple(block_data))
        
        # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹é‡è¤‡é™¤å»
        unique_groups = list(set(blocks))
        print(f"   [Optimized NEXUS] Found {len(unique_groups)} unique groups from {len(blocks)} blocks")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¹ãƒãƒ¼ãƒˆçµ±åˆ
        if category == "medium":
            # ä¸­ãƒ•ã‚¡ã‚¤ãƒ«: é©åº¦ãªçµ±åˆ
            unique_groups = self.smart_consolidation(unique_groups, category, tolerance_factor=1.5)
        # å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã¯çµ±åˆãªã—ã§ç²¾å¯†æ€§ã‚’ä¿æŒ
        
        # ã‚°ãƒ«ãƒ¼ãƒ—IDã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
        group_mapping = {group: idx for idx, group in enumerate(unique_groups)}
        group_id_stream = []
        for block in blocks:
            # æœ€ã‚‚è¿‘ã„ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ¤œç´¢
            best_group = min(unique_groups, key=lambda g: np.linalg.norm(np.array(g) - np.array(block)))
            group_id_stream.append(group_mapping[best_group])
        
        # æœ€é©åŒ–ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        # ã‚°ãƒ«ãƒ¼ãƒ—IDã‚¹ãƒˆãƒªãƒ¼ãƒ ã®LZMAåœ§ç¸®
        group_id_bytes = bytes(group_id_stream)
        compressed_stream = lzma.compress(group_id_bytes, preset=6)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            'version': self.version,
            'compression_type': 'optimized_nexus',
            'shape': shape_name,
            'grid_width': grid_width,
            'rows': rows,
            'original_size': len(data),
            'groups': unique_groups,
            'compressed_stream': compressed_stream,
            'blocks_count': len(blocks)
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åœ§ç¸®
        compressed_metadata_info = self.compress_metadata(metadata)
        
        # æœ€çµ‚ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°
        final_package = pickle.dumps(compressed_metadata_info)
        
        return {
            'compressed_data': final_package,
            'metadata': metadata,
            'compression_type': 'optimized_nexus',
            'original_size': len(data),
            'compressed_size': len(final_package)
        }
    
    def adaptive_decompress(self, compressed_result):
        """é©å¿œçš„å±•é–‹"""
        print(f"ğŸ”¥ NEXUS OPTIMIZED DECOMPRESSION STARTING...")
        
        start_time = time.time()
        
        compression_type = compressed_result.get('compression_type', 'unknown')
        
        if compression_type == 'standard':
            # æ¨™æº–åœ§ç¸®ã®å±•é–‹
            print("   [Adaptive Decompression] Standard algorithm decompression")
            data = self.standard_decompression_fallback(
                compressed_result['compressed_data'],
                compressed_result['metadata']
            )
        elif compression_type == 'simplified_nexus':
            # ç°¡ç´ åŒ–NEXUSå±•é–‹
            print("   [Adaptive Decompression] Simplified NEXUS decompression")
            data = self.simplified_nexus_decompress(compressed_result['compressed_data'])
        elif compression_type == 'optimized_nexus':
            # æœ€é©åŒ–NEXUSå±•é–‹
            print("   [Adaptive Decompression] Optimized NEXUS decompression")
            data = self.optimized_nexus_decompress(compressed_result['compressed_data'])
        else:
            raise ValueError(f"Unknown compression type: {compression_type}")
        
        decompression_time = time.time() - start_time
        
        print(f"âœ… OPTIMIZED DECOMPRESSION COMPLETE!")
        print(f"â±ï¸  Decompression time: {decompression_time:.3f}s")
        print(f"ğŸ“„ Decompressed size: {len(data)} bytes")
        
        return data
    
    def simplified_nexus_decompress(self, compressed_data):
        """ç°¡ç´ åŒ–NEXUSå±•é–‹"""
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å±•é–‹
        compressed_metadata_info = pickle.loads(compressed_data)
        metadata = self.decompress_metadata(compressed_metadata_info)
        
        # RLEã‚¹ãƒˆãƒªãƒ¼ãƒ å±•é–‹
        rle_stream = metadata['rle_stream']
        group_id_stream = []
        for i in range(0, len(rle_stream), 2):
            group_id = rle_stream[i]
            count = rle_stream[i + 1]
            group_id_stream.extend([group_id] * count)
        
        # ãƒ–ãƒ­ãƒƒã‚¯å†æ§‹ç¯‰
        groups = metadata['groups']
        grid_width = metadata['grid_width']
        original_size = metadata['original_size']
        
        # ãƒ‡ãƒ¼ã‚¿é…åˆ—åˆæœŸåŒ–
        reconstructed_data = np.zeros(grid_width * grid_width, dtype=int)
        
        # ãƒ–ãƒ­ãƒƒã‚¯ã®é…ç½®
        shape = self.shapes[metadata['shape']]
        shape_width = max(coord[1] for coord in shape) + 1
        shape_height = max(coord[0] for coord in shape) + 1
        
        block_idx = 0
        for start_row in range(0, grid_width, shape_height):
            for start_col in range(0, grid_width, shape_width):
                if start_row + shape_height <= grid_width and start_col + shape_width <= grid_width:
                    if block_idx < len(group_id_stream):
                        group_id = group_id_stream[block_idx]
                        block_data = groups[group_id]
                        
                        for data_idx, (rel_row, rel_col) in enumerate(shape):
                            if data_idx < len(block_data):
                                abs_row = start_row + rel_row
                                abs_col = start_col + rel_col
                                if abs_row < grid_width and abs_col < grid_width:
                                    idx = abs_row * grid_width + abs_col
                                    reconstructed_data[idx] = block_data[data_idx]
                        
                        block_idx += 1
        
        # å…ƒã®ã‚µã‚¤ã‚ºã«ãƒˆãƒªãƒŸãƒ³ã‚°
        return reconstructed_data[:original_size].astype(np.uint8)
    
    def optimized_nexus_decompress(self, compressed_data):
        """æœ€é©åŒ–NEXUSå±•é–‹"""
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å±•é–‹
        compressed_metadata_info = pickle.loads(compressed_data)
        metadata = self.decompress_metadata(compressed_metadata_info)
        
        # åœ§ç¸®ã‚¹ãƒˆãƒªãƒ¼ãƒ å±•é–‹
        compressed_stream = metadata['compressed_stream']
        group_id_bytes = lzma.decompress(compressed_stream)
        group_id_stream = list(group_id_bytes)
        
        # ãƒ–ãƒ­ãƒƒã‚¯å†æ§‹ç¯‰
        groups = metadata['groups']
        grid_width = metadata['grid_width']
        rows = metadata['rows']
        original_size = metadata['original_size']
        
        # ãƒ‡ãƒ¼ã‚¿é…åˆ—åˆæœŸåŒ–
        reconstructed_data = np.zeros(rows * grid_width, dtype=int)
        
        # ãƒ–ãƒ­ãƒƒã‚¯ã®é…ç½®
        shape = self.shapes[metadata['shape']]
        shape_width = max(coord[1] for coord in shape) + 1
        shape_height = max(coord[0] for coord in shape) + 1
        
        block_idx = 0
        for start_row in range(0, rows, shape_height):
            for start_col in range(0, grid_width, shape_width):
                if start_row + shape_height <= rows and start_col + shape_width <= grid_width:
                    if block_idx < len(group_id_stream):
                        group_id = group_id_stream[block_idx]
                        if group_id < len(groups):
                            block_data = groups[group_id]
                            
                            for data_idx, (rel_row, rel_col) in enumerate(shape):
                                if data_idx < len(block_data):
                                    abs_row = start_row + rel_row
                                    abs_col = start_col + rel_col
                                    if abs_row < rows and abs_col < grid_width:
                                        idx = abs_row * grid_width + abs_col
                                        if idx < len(reconstructed_data):
                                            reconstructed_data[idx] = block_data[data_idx]
                        
                        block_idx += 1
        
        # å…ƒã®ã‚µã‚¤ã‚ºã«ãƒˆãƒªãƒŸãƒ³ã‚°
        return reconstructed_data[:original_size].astype(np.uint8)

def test_optimized_nexus():
    """æœ€é©åŒ–NEXUSã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥")
    print("ğŸ”¥ NEXUS OPTIMIZED ENGINE TEST ğŸ”¥")
    print("ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥")
    
    engine = NEXUSOptimizedEngine()
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    test_files = [
        r"c:\Users\241822\Desktop\æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ (2)\NXZip\sample\element_test_small.bin",
        r"c:\Users\241822\Desktop\æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ (2)\NXZip\sample\test_small.txt"
    ]
    
    results = []
    
    for file_path in test_files:
        if os.path.exists(file_path):
            print(f"\nğŸ”¥ TESTING: {os.path.basename(file_path)}")
            print("=" * 60)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            with open(file_path, 'rb') as f:
                original_data = f.read()
            
            original_md5 = hashlib.md5(original_data).hexdigest()
            print(f"ğŸ“„ File size: {len(original_data)} bytes")
            print(f"ğŸ” Original MD5: {original_md5}")
            
            # åœ§ç¸®
            compressed_result = engine.adaptive_compress(original_data)
            
            # å±•é–‹
            decompressed_data = engine.adaptive_decompress(compressed_result)
            
            # æ¤œè¨¼
            decompressed_md5 = hashlib.md5(decompressed_data).hexdigest()
            print(f"ğŸ” Decompressed MD5: {decompressed_md5}")
            
            if original_md5 == decompressed_md5:
                print("ğŸ¯ âœ… PERFECT MATCH - OPTIMIZATION SUCCESSFUL!")
                status = "SUCCESS"
            else:
                print("âŒ MD5 MISMATCH - OPTIMIZATION FAILED!")
                status = "FAILED"
            
            # çµæœè¨˜éŒ²
            results.append({
                'filename': os.path.basename(file_path),
                'original_size': len(original_data),
                'compressed_size': compressed_result['compressed_size'],
                'ratio': compressed_result['compressed_size'] / len(original_data),
                'compression_type': compressed_result['compression_type'],
                'status': status,
                'time': compressed_result['compression_time']
            })
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "ğŸ”¥" * 60)
    print("ğŸ”¥ OPTIMIZATION TEST RESULTS ğŸ”¥")
    print("ğŸ”¥" * 60)
    
    for result in results:
        print(f"ğŸ“ {result['filename']}")
        print(f"   ğŸ“„ Size: {result['original_size']} -> {result['compressed_size']} bytes")
        print(f"   ğŸ“Š Ratio: {result['ratio']:.4f} ({result['ratio']*100:.2f}%)")
        print(f"   ğŸ”§ Method: {result['compression_type'].upper()}")
        print(f"   â±ï¸  Time: {result['time']:.3f}s")
        print(f"   ğŸ¯ Status: {result['status']}")
        print()
    
    # æ”¹å–„ç‡è¨ˆç®—
    success_count = sum(1 for r in results if r['status'] == 'SUCCESS')
    print(f"ğŸ¯ SUCCESS RATE: {success_count}/{len(results)} ({success_count/len(results)*100:.1f}%)")
    
    if results:
        avg_ratio = sum(r['ratio'] for r in results) / len(results)
        print(f"ğŸ“Š AVERAGE COMPRESSION RATIO: {avg_ratio:.4f} ({avg_ratio*100:.2f}%)")
        
        # æ”¹å–„åº¦è©•ä¾¡
        if avg_ratio < 1.0:
            improvement = (1 - avg_ratio) * 100
            print(f"ğŸš€ COMPRESSION ACHIEVED: {improvement:.1f}% size reduction!")
        else:
            expansion = (avg_ratio - 1) * 100
            print(f"ğŸ“Š Current status: {expansion:.1f}% expansion (target: reduction)")
    
    print("ğŸ”¥ NEXUS OPTIMIZATION TESTING COMPLETE! ğŸ”¥")

if __name__ == "__main__":
    test_optimized_nexus()

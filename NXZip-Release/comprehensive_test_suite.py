#!/usr/bin/env python3
"""
NXZip Core v2.0 å®Ÿç”¨çš„ç·åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã‚’æ„è­˜ã—ãŸå¤šç¨®å¤šæ§˜ãªãƒ†ã‚¹ãƒˆ

Test Categories:
1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
2. ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚³ãƒ¼ãƒ‰ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«  
3. ç”»åƒãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«
4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
5. ç§‘å­¦ãƒ»æ•°å€¤ãƒ‡ãƒ¼ã‚¿
6. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
7. æš—å·åŒ–ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
8. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
"""

import os
import sys
import time
import json
from pathlib import Path
import numpy as np

# NXZip Core ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from nxzip_core import NXZipCore, NXZipContainer, CompressionMode
    print("âœ… NXZip Core v2.0 å®Ÿç”¨ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")
except ImportError as e:
    print(f"âŒ NXZip Core ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}")
    sys.exit(1)

def progress_callback(info):
    """é€²æ—è¡¨ç¤ºã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    progress = info['progress']
    message = info['message']
    speed = info.get('speed', 0)
    
    # é€Ÿåº¦ã‚’é©åˆ‡ãªå˜ä½ã§è¡¨ç¤º
    if speed > 1024 * 1024:
        speed_str = f"{speed / (1024 * 1024):.1f} MB/s"
    elif speed > 1024:
        speed_str = f"{speed / 1024:.1f} KB/s"
    else:
        speed_str = f"{speed:.0f} B/s"
    
    print(f"\rğŸ”„ {progress:5.1f}% | {message[:40]:40} | {speed_str:>10}", end="", flush=True)

class TestResults:
    """ãƒ†ã‚¹ãƒˆçµæœé›†è¨ˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.results = []
        self.summary = {
            'total_tests': 0,
            'passed': 0,
            'failed': 0,
            'total_original_size': 0,
            'total_compressed_size': 0,
            'total_compression_time': 0.0,
            'total_decompression_time': 0.0
        }
    
    def add_result(self, test_name: str, result: dict):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’è¿½åŠ """
        self.results.append({
            'test_name': test_name,
            'timestamp': time.time(),
            **result
        })
        
        self.summary['total_tests'] += 1
        if result.get('success', False):
            self.summary['passed'] += 1
        else:
            self.summary['failed'] += 1
        
        self.summary['total_original_size'] += result.get('original_size', 0)
        self.summary['total_compressed_size'] += result.get('compressed_size', 0)
        self.summary['total_compression_time'] += result.get('compression_time', 0.0)
        self.summary['total_decompression_time'] += result.get('decompression_time', 0.0)
    
    def print_summary(self):
        """çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ğŸ¯ NXZip Core v2.0 å®Ÿç”¨ãƒ†ã‚¹ãƒˆç·åˆçµæœ")
        print("="*80)
        
        total_tests = self.summary['total_tests']
        passed = self.summary['passed']
        failed = self.summary['failed']
        success_rate = (passed / total_tests * 100) if total_tests > 0 else 0
        
        print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ•°: {total_tests}")
        print(f"âœ… æˆåŠŸ: {passed}")
        print(f"âŒ å¤±æ•—: {failed}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        
        # åœ§ç¸®åŠ¹æœ
        original_size = self.summary['total_original_size']
        compressed_size = self.summary['total_compressed_size']
        if original_size > 0:
            overall_ratio = (1 - compressed_size / original_size) * 100
            print(f"ğŸ“¦ å…¨ä½“åœ§ç¸®ç‡: {overall_ratio:.2f}%")
            print(f"ğŸ“ åˆè¨ˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {original_size:,} bytes ({original_size/1024/1024:.1f} MB)")
            print(f"ğŸ—œï¸ åˆè¨ˆåœ§ç¸®ã‚µã‚¤ã‚º: {compressed_size:,} bytes ({compressed_size/1024/1024:.1f} MB)")
        
        # é€Ÿåº¦
        total_comp_time = self.summary['total_compression_time']
        total_decomp_time = self.summary['total_decompression_time']
        if total_comp_time > 0:
            comp_speed = (original_size / 1024 / 1024) / total_comp_time
            print(f"âš¡ å¹³å‡åœ§ç¸®é€Ÿåº¦: {comp_speed:.1f} MB/s")
        if total_decomp_time > 0:
            decomp_speed = (original_size / 1024 / 1024) / total_decomp_time
            print(f"ğŸš€ å¹³å‡å±•é–‹é€Ÿåº¦: {decomp_speed:.1f} MB/s")

def run_test_case(test_name: str, data: bytes, mode: str, core: NXZipCore, 
                  results: TestResults, encryption_key: bytes = None) -> bool:
    """å€‹åˆ¥ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®Ÿè¡Œ"""
    print(f"\nğŸ§ª {test_name}")
    print(f"   ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(data):,} bytes ({len(data)/1024:.1f} KB)")
    
    try:
        # åœ§ç¸®ãƒ†ã‚¹ãƒˆ
        start_time = time.time()
        comp_result = core.compress(data, mode=mode, filename=test_name, encryption_key=encryption_key)
        
        if not comp_result.success:
            print(f"   âŒ åœ§ç¸®å¤±æ•—: {comp_result.error_message}")
            results.add_result(test_name, {
                'success': False,
                'error': comp_result.error_message,
                'original_size': len(data),
                'compressed_size': 0,
                'compression_time': 0.0,
                'decompression_time': 0.0
            })
            return False
        
        print(f"\n   âœ… åœ§ç¸®æˆåŠŸ!")
        print(f"   ğŸ“¦ åœ§ç¸®ç‡: {comp_result.compression_ratio:.2f}%")
        print(f"   â±ï¸ åœ§ç¸®æ™‚é–“: {comp_result.compression_time:.3f}ç§’")
        
        if comp_result.compression_time > 0:
            speed = (len(data) / 1024 / 1024) / comp_result.compression_time
            print(f"   âš¡ åœ§ç¸®é€Ÿåº¦: {speed:.1f} MB/s")
        
        # ç›®æ¨™é”æˆåº¦
        target_eval = comp_result.metadata.get('target_evaluation', {})
        if target_eval:
            achieved = target_eval.get('target_achieved', False)
            concept = target_eval.get('concept', 'N/A')
            print(f"   ğŸ¯ ç›®æ¨™é”æˆ: {'âœ…' if achieved else 'âŒ'} ({concept})")
        
        # å±•é–‹ãƒ†ã‚¹ãƒˆ
        print("   ğŸ”“ å±•é–‹ãƒ†ã‚¹ãƒˆä¸­...")
        decomp_result = core.decompress(comp_result.compressed_data, comp_result.metadata)
        
        if not decomp_result.success:
            print(f"   âŒ å±•é–‹å¤±æ•—: {decomp_result.error_message}")
            results.add_result(test_name, {
                'success': False,
                'error': f"Decompression failed: {decomp_result.error_message}",
                'original_size': len(data),
                'compressed_size': len(comp_result.compressed_data),
                'compression_time': comp_result.compression_time,
                'decompression_time': 0.0
            })
            return False
        
        # æ•´åˆæ€§ç¢ºèª
        integrity = core.validate_integrity(data, decomp_result.decompressed_data)
        integrity_ok = integrity['integrity_ok']
        
        print(f"   ğŸ” æ•´åˆæ€§: {'âœ…' if integrity_ok else 'âŒ'}")
        print(f"   â±ï¸ å±•é–‹æ™‚é–“: {decomp_result.decompression_time:.3f}ç§’")
        
        if decomp_result.decompression_time > 0:
            decomp_speed = (len(data) / 1024 / 1024) / decomp_result.decompression_time
            print(f"   ğŸš€ å±•é–‹é€Ÿåº¦: {decomp_speed:.1f} MB/s")
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è©³ç´°
        stages = comp_result.metadata.get('stages', [])
        transforms_applied = []
        for stage_name, stage_info in stages:
            if stage_name == 'tmc_transform':
                transforms = stage_info.get('transforms_applied', [])
                transforms_applied.extend(transforms)
            elif stage_name == 'spe_integration' and stage_info.get('spe_applied'):
                transforms_applied.append('spe')
        
        if transforms_applied:
            print(f"   ğŸ”§ é©ç”¨å¤‰æ›: {', '.join(transforms_applied)}")
        
        results.add_result(test_name, {
            'success': integrity_ok,
            'compression_ratio': comp_result.compression_ratio,
            'original_size': len(data),
            'compressed_size': len(comp_result.compressed_data),
            'compression_time': comp_result.compression_time,
            'decompression_time': decomp_result.decompression_time,
            'target_achieved': target_eval.get('target_achieved', False),
            'transforms_applied': transforms_applied,
            'mode': mode,
            'encrypted': encryption_key is not None
        })
        
        return integrity_ok
        
    except Exception as e:
        print(f"   âŒ ãƒ†ã‚¹ãƒˆä¾‹å¤–: {e}")
        results.add_result(test_name, {
            'success': False,
            'error': str(e),
            'original_size': len(data),
            'compressed_size': 0,
            'compression_time': 0.0,
            'decompression_time': 0.0
        })
        return False

def test_documents_and_text(core: NXZipCore, results: TestResults):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*60)
    print("ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    # 1. æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆï¼ˆå°èª¬ï¼‰
    japanese_novel = """
    å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚
    ã©ã“ã§ç”Ÿã‚ŒãŸã‹ã¨ã‚“ã¨è¦‹å½“ãŒã¤ã‹ã¬ã€‚ä½•ã§ã‚‚è–„æš—ã„ã˜ã‚ã˜ã‚ã—ãŸæ‰€ã§ãƒ‹ãƒ£ãƒ¼ãƒ‹ãƒ£ãƒ¼æ³£ã„ã¦ã„ãŸäº‹ã ã‘ã¯è¨˜æ†¶ã—ã¦ã„ã‚‹ã€‚
    å¾è¼©ã¯ã“ã“ã§å§‹ã‚ã¦äººé–“ã¨ã„ã†ã‚‚ã®ã‚’è¦‹ãŸã€‚ã—ã‹ã‚‚ã‚ã¨ã§èãã¨ãã‚Œã¯æ›¸ç”Ÿã¨ã„ã†äººé–“ä¸­ã§ä¸€ç•ªç°çŒ›ãªç¨®æ—ã§ã‚ã£ãŸãã†ã ã€‚
    ã“ã®æ›¸ç”Ÿã¨ã„ã†ã®ã¯æ™‚ã€…æˆ‘ã€…ã‚’æ•ãˆã¦ç…®ã¦é£Ÿã†ã¨ã„ã†è©±ã§ã‚ã‚‹ã€‚ã—ã‹ã—ãã®å½“æ™‚ã¯ä½•ã¨ã„ã†è€ƒã‚‚ãªã‹ã£ãŸã‹ã‚‰åˆ¥æ®µæã—ã„ã¨ã‚‚æ€ã‚ãªã‹ã£ãŸã€‚
    ãŸã å½¼ã®æŒã«è¼‰ã›ã‚‰ã‚Œã¦ã‚¹ãƒ¼ã¨æŒã¡ä¸Šã’ã‚‰ã‚ŒãŸæ™‚ä½•ã ã‹ãƒ•ãƒ¯ãƒ•ãƒ¯ã—ãŸæ„Ÿã˜ãŒã‚ã£ãŸã°ã‹ã‚Šã§ã‚ã‚‹ã€‚
    æŒã®ä¸Šã§å°‘ã—è½ã¡ã¤ã„ã¦æ›¸ç”Ÿã®é¡”ã‚’è¦‹ãŸã®ãŒã„ã‚ã‚†ã‚‹äººé–“ã¨ã„ã†ã‚‚ã®ã®è¦‹å§‹ã‚ã§ã‚ã‚ã†ã€‚
    ã“ã®æ™‚å¦™ãªã‚‚ã®ã ã¨æ€ã£ãŸæ„Ÿã˜ãŒä»Šã§ã‚‚æ®‹ã£ã¦ã„ã‚‹ã€‚ç¬¬ä¸€æ¯›ã‚’ã‚‚ã£ã¦è£…é£¾ã•ã‚Œã¹ãã¯ãšã®é¡”ãŒã¤ã‚‹ã¤ã‚‹ã—ã¦ã¾ã‚‹ã§è–¬ç¼¶ã ã€‚
    ãã®å¾ŒçŒ«ã«ã‚‚ã ã„ã¶é€¢ã£ãŸãŒã“ã‚“ãªç‰‡è¼ªã«ã¯ä¸€åº¦ã‚‚å‡ºä¼šã‚ã—ãŸäº‹ãŒãªã„ã€‚ã®ã¿ãªã‚‰ãšé¡”ã®çœŸä¸­ãŒã‚ã¾ã‚Šã«çªèµ·ã—ã¦ã„ã‚‹ã€‚
    ãã†ã—ã¦ãã®ç©´ã®ä¸­ã‹ã‚‰æ™‚ã€…ã·ã†ã·ã†ã¨ç…™ã‚’å¹ãã€‚ã©ã†ã‚‚å’½ã›ã½ãã¦å®Ÿã«å¼±ã£ãŸã€‚ã“ã‚ŒãŒäººé–“ã®é£²ã‚€ç…™è‰ã¨ã„ã†ã‚‚ã®ã§ã‚ã‚‹äº‹ã¯ãšã£ã¨å¾Œã«ãªã£ã¦çŸ¥ã£ãŸã€‚
    """ * 50  # ç¹°ã‚Šè¿”ã—ã§å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨¡æ“¬
    
    run_test_case("æ—¥æœ¬èªå°èª¬ãƒ†ã‚­ã‚¹ãƒˆ", japanese_novel.encode('utf-8'), "balanced", core, results)
    
    # 2. è‹±èªãƒ†ã‚­ã‚¹ãƒˆï¼ˆæŠ€è¡“æ–‡æ›¸ï¼‰
    english_tech = """
    # Advanced Data Compression Algorithms
    
    ## Introduction
    Data compression is a fundamental technique in computer science that reduces the amount of space needed to store data.
    Modern compression algorithms employ sophisticated mathematical models to achieve high compression ratios while maintaining fast processing speeds.
    
    ## Theoretical Foundation
    The theoretical foundation of compression is based on information theory, developed by Claude Shannon.
    The key insight is that data contains redundancy, and this redundancy can be exploited to reduce storage requirements.
    
    ### Entropy Measurement
    Shannon entropy provides a lower bound for lossless compression:
    H(X) = -âˆ‘ P(x) logâ‚‚ P(x)
    
    Where P(x) is the probability of symbol x appearing in the data.
    
    ## Practical Algorithms
    
    ### Lempel-Ziv Variants
    - LZ77: Uses a sliding window to find repetitions
    - LZ78: Builds a dictionary of previously seen patterns
    - LZW: Extends LZ78 with adaptive dictionary updates
    
    ### Block-Sorting Algorithms
    - Burrows-Wheeler Transform (BWT): Reversible permutation that improves compressibility
    - Move-to-Front (MTF): Reduces entropy after BWT application
    - Run-Length Encoding (RLE): Efficiently handles repeated symbols
    
    ## Modern Developments
    Recent advances include neural network-based approaches and quantum compression algorithms.
    These methods show promise for achieving compression ratios beyond classical theoretical limits.
    """ * 20
    
    run_test_case("è‹±èªæŠ€è¡“æ–‡æ›¸", english_tech.encode('utf-8'), "balanced", core, results)
    
    # 3. æ··åˆè¨€èªæ–‡æ›¸ï¼ˆå¤šè¨€èªå¯¾å¿œãƒ†ã‚¹ãƒˆï¼‰
    multilingual_doc = """
    å¤šè¨€èªæ–‡æ›¸ãƒ†ã‚¹ãƒˆ / Multilingual Document Test / Document Multilingue
    
    æ—¥æœ¬èª: ã“ã‚Œã¯å¤šè¨€èªå¯¾å¿œã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚æ§˜ã€…ãªæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
    English: This is a multilingual support test. It supports various character encodings.
    FranÃ§ais: Ceci est un test de support multilingue. Il prend en charge divers encodages de caractÃ¨res.
    Deutsch: Dies ist ein mehrsprachiger Support-Test. Es unterstÃ¼tzt verschiedene Zeichenkodierungen.
    ä¸­æ–‡: è¿™æ˜¯å¤šè¯­è¨€æ”¯æŒæµ‹è¯•ã€‚å®ƒæ”¯æŒå„ç§å­—ç¬¦ç¼–ç ã€‚
    í•œêµ­ì–´: ì´ê²ƒì€ ë‹¤êµ­ì–´ ì§€ì› í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë¬¸ì ì¸ì½”ë”©ì„ ì§€ì›í•©ë‹ˆë‹¤.
    Ğ ÑƒÑÑĞºĞ¸Ğ¹: Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸. ĞĞ½ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ².
    Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: Ù‡Ø°Ø§ Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ø¹Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª. ÙˆÙ‡Ùˆ ÙŠØ¯Ø¹Ù… ØªØ±Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.
    """ * 30
    
    run_test_case("å¤šè¨€èªæ··åˆæ–‡æ›¸", multilingual_doc.encode('utf-8'), "balanced", core, results)

def test_source_code_and_config(core: NXZipCore, results: TestResults):
    """ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚³ãƒ¼ãƒ‰ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*60)
    print("ğŸ’» ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚³ãƒ¼ãƒ‰ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    # 1. Python ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
    python_code = '''
import os
import sys
import json
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
import asyncio
import aiohttp
import pandas as pd

class DataProcessor:
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: str):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.cache = {}
        
    def _load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return self._default_config()
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in config file: {e}")
    
    def _default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š"""
        return {
            "batch_size": 1000,
            "max_workers": 4,
            "timeout": 30,
            "retry_attempts": 3,
            "output_format": "json",
            "compression": {
                "enabled": True,
                "algorithm": "lzma",
                "level": 6
            }
        }
    
    async def process_data_batch(self, data_batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒå‡¦ç†"""
        results = []
        semaphore = asyncio.Semaphore(self.config['max_workers'])
        
        async def process_item(item):
            async with semaphore:
                return await self._process_single_item(item)
        
        tasks = [process_item(item) for item in data_batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"Error processing item {i}: {result}")
            else:
                valid_results.append(result)
        
        return valid_results
    
    async def _process_single_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """å˜ä¸€ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†"""
        # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        processed = {
            'id': item.get('id'),
            'timestamp': item.get('timestamp', time.time()),
            'data': self._transform_data(item.get('data', {})),
            'metadata': {
                'processed_at': time.time(),
                'version': '1.0',
                'processor': 'DataProcessor'
            }
        }
        
        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not self._validate_item(processed):
            raise ValueError(f"Validation failed for item: {processed['id']}")
        
        return processed
    
    def _transform_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿å¤‰æ›å‡¦ç†"""
        transformed = {}
        
        for key, value in data.items():
            if isinstance(value, str):
                transformed[key] = value.strip().lower()
            elif isinstance(value, (int, float)):
                transformed[key] = float(value)
            elif isinstance(value, list):
                transformed[key] = [self._transform_data(item) if isinstance(item, dict) else item for item in value]
            else:
                transformed[key] = value
        
        return transformed
    
    def _validate_item(self, item: Dict[str, Any]) -> bool:
        """ã‚¢ã‚¤ãƒ†ãƒ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        required_fields = ['id', 'timestamp', 'data']
        return all(field in item for field in required_fields)

# ä½¿ç”¨ä¾‹
async def main():
    processor = DataProcessor('config.json')
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_data = [
        {'id': f'item_{i}', 'data': {'value': i * 2, 'name': f'test_{i}'}}
        for i in range(1000)
    ]
    
    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
    batch_size = processor.config['batch_size']
    results = []
    
    for i in range(0, len(test_data), batch_size):
        batch = test_data[i:i + batch_size]
        batch_results = await processor.process_data_batch(batch)
        results.extend(batch_results)
        print(f"Processed batch {i//batch_size + 1}, items: {len(batch_results)}")
    
    print(f"Total processed items: {len(results)}")

if __name__ == "__main__":
    asyncio.run(main())
''' * 10
    
    run_test_case("Pythonã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰", python_code.encode('utf-8'), "fast", core, results)
    
    # 2. JSONè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    json_config = {
        "application": {
            "name": "NXZip Professional",
            "version": "2.0.0",
            "description": "Next-generation compression platform",
            "author": "NXZip Development Team"
        },
        "compression": {
            "default_mode": "balanced",
            "modes": {
                "fast": {
                    "target_speed": 50,
                    "target_ratio": 40,
                    "algorithm": "zlib",
                    "level": 3
                },
                "balanced": {
                    "target_speed": 10,
                    "target_ratio": 60,
                    "algorithm": "lzma",
                    "level": 6
                },
                "maximum": {
                    "target_ratio": 70,
                    "algorithm": "lzma",
                    "level": 9
                }
            }
        },
        "security": {
            "encryption": {
                "enabled": True,
                "algorithm": "AES-256-GCM",
                "key_derivation": "PBKDF2",
                "iterations": 100000
            },
            "integrity": {
                "checksum": "SHA256",
                "signature": "Ed25519"
            }
        },
        "performance": {
            "memory_limit": "1GB",
            "max_threads": 8,
            "chunk_size": "2MB",
            "cache_size": "100MB"
        },
        "logging": {
            "level": "INFO",
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            "file": "nxzip.log",
            "max_size": "10MB",
            "backup_count": 5
        }
    }
    
    json_data = json.dumps(json_config, indent=2, ensure_ascii=False) * 20
    run_test_case("JSONè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«", json_data.encode('utf-8'), "balanced", core, results)
    
    # 3. XML ãƒ‡ãƒ¼ã‚¿
    xml_data = '''<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <database>
    <host>localhost</host>
    <port>5432</port>
    <name>nxzip_db</name>
    <user>nxzip_user</user>
    <pool>
      <min_connections>5</min_connections>
      <max_connections>20</max_connections>
      <timeout>30</timeout>
    </pool>
  </database>
  <cache>
    <provider>redis</provider>
    <host>localhost</host>
    <port>6379</port>
    <ttl>3600</ttl>
  </cache>
  <logging>
    <appenders>
      <appender name="console" type="ConsoleAppender">
        <pattern>%d{yyyy-MM-dd HH:mm:ss} [%t] %-5level %logger{36} - %msg%n</pattern>
      </appender>
      <appender name="file" type="FileAppender">
        <file>logs/application.log</file>
        <pattern>%d{yyyy-MM-dd HH:mm:ss} [%t] %-5level %logger{36} - %msg%n</pattern>
      </appender>
    </appenders>
    <loggers>
      <logger name="com.nxzip" level="DEBUG"/>
      <logger name="org.springframework" level="INFO"/>
      <root level="INFO">
        <appender-ref ref="console"/>
        <appender-ref ref="file"/>
      </root>
    </loggers>
  </logging>
</configuration>''' * 30
    
    run_test_case("XMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«", xml_data.encode('utf-8'), "balanced", core, results)

def test_media_and_binary(core: NXZipCore, results: TestResults):
    """ç”»åƒãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*60)
    print("ğŸ–¼ï¸ ç”»åƒãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    # 1. æ¨¡æ“¬ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆBMP-likeæ§‹é€ ï¼‰
    width, height = 800, 600
    # 24bit RGB bitmap header simulation
    bmp_header = bytearray(54)
    bmp_header[0:2] = b'BM'  # Signature
    bmp_header[2:6] = (54 + width * height * 3).to_bytes(4, 'little')  # File size
    bmp_header[10:14] = (54).to_bytes(4, 'little')  # Offset to pixel data
    bmp_header[14:18] = (40).to_bytes(4, 'little')  # DIB header size
    bmp_header[18:22] = width.to_bytes(4, 'little')  # Width
    bmp_header[22:26] = height.to_bytes(4, 'little')  # Height
    bmp_header[26:28] = (1).to_bytes(2, 'little')  # Planes
    bmp_header[28:30] = (24).to_bytes(2, 'little')  # Bits per pixel
    
    # Generate gradient image data
    image_data = bytearray()
    for y in range(height):
        for x in range(width):
            r = int((x / width) * 255)
            g = int((y / height) * 255)  
            b = int(((x + y) / (width + height)) * 255)
            image_data.extend([b, g, r])  # BGR format
    
    bmp_data = bytes(bmp_header) + bytes(image_data)
    run_test_case("æ¨¡æ“¬BMPç”»åƒãƒ•ã‚¡ã‚¤ãƒ«", bmp_data, "balanced", core, results)
    
    # 2. éŸ³å£°ãƒ‡ãƒ¼ã‚¿æ¨¡æ“¬ï¼ˆWAV-likeæ§‹é€ ï¼‰
    sample_rate = 44100
    duration = 5  # seconds
    samples = sample_rate * duration
    
    # WAV header
    wav_header = bytearray(44)
    wav_header[0:4] = b'RIFF'
    wav_header[4:8] = (36 + samples * 2).to_bytes(4, 'little')
    wav_header[8:12] = b'WAVE'
    wav_header[12:16] = b'fmt '
    wav_header[16:20] = (16).to_bytes(4, 'little')  # PCM
    wav_header[20:22] = (1).to_bytes(2, 'little')   # Audio format
    wav_header[22:24] = (1).to_bytes(2, 'little')   # Mono
    wav_header[24:28] = sample_rate.to_bytes(4, 'little')
    wav_header[28:32] = (sample_rate * 2).to_bytes(4, 'little')  # Byte rate
    wav_header[32:34] = (2).to_bytes(2, 'little')   # Block align
    wav_header[34:36] = (16).to_bytes(2, 'little')  # Bits per sample
    wav_header[36:40] = b'data'
    wav_header[40:44] = (samples * 2).to_bytes(4, 'little')
    
    # Generate sine wave
    audio_data = bytearray()
    for i in range(samples):
        # Mix of frequencies to simulate music
        t = i / sample_rate
        freq1, freq2, freq3 = 440, 880, 1320  # A4, A5, E6
        sample = (np.sin(2 * np.pi * freq1 * t) * 0.3 + 
                 np.sin(2 * np.pi * freq2 * t) * 0.2 + 
                 np.sin(2 * np.pi * freq3 * t) * 0.1)
        sample_int = int(sample * 32767)
        audio_data.extend(sample_int.to_bytes(2, 'little', signed=True))
    
    wav_data = bytes(wav_header) + bytes(audio_data)
    run_test_case("æ¨¡æ“¬WAVéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«", wav_data, "balanced", core, results)
    
    # 3. å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«æ¨¡æ“¬ï¼ˆPE-likeæ§‹é€ ï¼‰
    pe_data = bytearray()
    # DOS header
    pe_data.extend(b'MZ')  # DOS signature
    pe_data.extend(b'\x00' * 58)  # DOS header padding
    pe_data.extend((64).to_bytes(4, 'little'))  # PE header offset
    
    # PE header
    pe_data.extend(b'PE\x00\x00')  # PE signature
    pe_data.extend(b'\x4c\x01')    # Machine (i386)
    pe_data.extend(b'\x03\x00')    # Number of sections
    pe_data.extend(b'\x00' * 16)   # Timestamp, etc.
    
    # Add some "code" sections with patterns
    code_section = bytearray()
    for i in range(10000):
        # Simulate x86 instructions patterns
        if i % 100 == 0:
            code_section.extend(b'\x55\x8b\xec')  # push ebp; mov ebp, esp
        elif i % 50 == 0:
            code_section.extend(b'\xff\x15')      # call dword ptr
            code_section.extend(i.to_bytes(4, 'little'))
        else:
            code_section.extend(b'\x90')          # nop
    
    pe_data.extend(code_section)
    run_test_case("æ¨¡æ“¬å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«", bytes(pe_data), "fast", core, results)

def test_database_and_logs(core: NXZipCore, results: TestResults):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*60)
    print("ğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    # 1. CSV ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ€ãƒ³ãƒ—
    csv_data = "id,name,email,age,country,registration_date,last_login,status\n"
    countries = ["Japan", "USA", "Germany", "France", "China", "Korea", "Brazil", "India"]
    statuses = ["active", "inactive", "pending", "suspended"]
    
    for i in range(10000):
        csv_data += f"{i+1},User{i+1:04d},user{i+1}@example.com,{25 + (i % 50)},"
        csv_data += f"{countries[i % len(countries)]},2023-{1 + (i % 12):02d}-{1 + (i % 28):02d},"
        csv_data += f"2024-01-{1 + (i % 31):02d},{statuses[i % len(statuses)]}\n"
    
    run_test_case("CSV ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ€ãƒ³ãƒ—", csv_data.encode('utf-8'), "balanced", core, results)
    
    # 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°
    log_data = ""
    log_levels = ["DEBUG", "INFO", "WARN", "ERROR", "FATAL"]
    components = ["AuthService", "DatabaseManager", "CacheService", "FileProcessor", "APIController"]
    
    for i in range(5000):
        timestamp = f"2024-01-15 {(i // 200) % 24:02d}:{(i % 60):02d}:{(i * 7) % 60:02d}.{(i * 123) % 1000:03d}"
        level = log_levels[i % len(log_levels)]
        component = components[i % len(components)]
        thread_id = f"Thread-{(i % 20) + 1}"
        
        messages = [
            f"Processing request ID: {i + 1000}",
            f"Database query executed in {(i % 1000) + 1}ms",
            f"Cache hit rate: {85 + (i % 15)}%",
            f"File processed: document_{i}.pdf ({(i % 10000) + 1000} bytes)",
            f"API response sent to client {((i * 7) % 1000) + 1}",
            f"Connection pool size: {(i % 50) + 10}",
            f"Memory usage: {(i % 80) + 20}%",
            "User authentication successful",
            "Session expired, redirecting to login",
            "Backup completed successfully"
        ]
        
        message = messages[i % len(messages)]
        log_data += f"{timestamp} [{thread_id}] {level:5} {component:15} - {message}\n"
        
        # Add occasional stack traces for ERROR/FATAL
        if level in ["ERROR", "FATAL"] and i % 100 == 0:
            log_data += f"    at com.nxzip.{component.lower()}.process(line {100 + (i % 500)})\n"
            log_data += f"    at com.nxzip.core.execute(line {50 + (i % 100)})\n"
            log_data += f"    at java.lang.Thread.run(line {800 + (i % 50)})\n"
    
    run_test_case("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°", log_data.encode('utf-8'), "maximum", core, results)
    
    # 3. SQL ãƒ€ãƒ³ãƒ—
    sql_dump = """
-- NXZip Database Dump
-- Generated: 2024-01-15 12:00:00
-- Server: PostgreSQL 15.0

SET statement_timeout = 0;
SET lock_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;

CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE
);

CREATE TABLE files (
    id SERIAL PRIMARY KEY,
    filename VARCHAR(255) NOT NULL,
    original_size BIGINT NOT NULL,
    compressed_size BIGINT NOT NULL,
    compression_ratio DECIMAL(5,2) NOT NULL,
    algorithm VARCHAR(50) NOT NULL,
    created_by INTEGER REFERENCES users(id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_files_created_by ON files(created_by);
CREATE INDEX idx_files_created_at ON files(created_at);

"""
    
    # Generate INSERT statements
    for i in range(1000):
        sql_dump += f"INSERT INTO users (username, email, password_hash) VALUES "
        sql_dump += f"('user{i:04d}', 'user{i}@example.com', '$2b$12$hash{i:04d}');\n"
        
        if i % 10 == 0:  # Add some file records
            for j in range(5):
                file_id = i * 5 + j
                sql_dump += f"INSERT INTO files (filename, original_size, compressed_size, compression_ratio, algorithm, created_by) VALUES "
                sql_dump += f"('document_{file_id}.pdf', {(file_id % 10000) + 5000}, {(file_id % 5000) + 1000}, "
                sql_dump += f"{75.0 + (file_id % 20)}, 'nxzip_balanced', {i + 1});\n"
    
    sql_dump += "\nCOMMIT;\n"
    
    run_test_case("SQL ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ€ãƒ³ãƒ—", sql_dump.encode('utf-8'), "maximum", core, results)

def test_scientific_and_numerical(core: NXZipCore, results: TestResults):
    """ç§‘å­¦ãƒ»æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*60)
    print("ğŸ”¬ ç§‘å­¦ãƒ»æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    # 1. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆæ ªä¾¡ãƒ»ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿æ¨¡æ“¬ï¼‰
    import math
    
    time_series_data = "timestamp,temperature,humidity,pressure,co2,voltage\n"
    base_time = 1704067200  # 2024-01-01 00:00:00
    
    for i in range(50000):  # 50,000 data points
        timestamp = base_time + i * 60  # Every minute
        
        # Simulate realistic sensor data with trends and noise
        hour_of_day = (i // 60) % 24
        temp = 20 + 10 * math.sin(2 * math.pi * hour_of_day / 24) + np.random.normal(0, 1)
        humidity = 50 + 20 * math.sin(2 * math.pi * hour_of_day / 24 + math.pi/4) + np.random.normal(0, 2)
        pressure = 1013.25 + 10 * math.sin(2 * math.pi * i / (24 * 60 * 7)) + np.random.normal(0, 0.5)  # Weekly cycle
        co2 = 400 + 50 * math.sin(2 * math.pi * hour_of_day / 24 + math.pi) + np.random.normal(0, 5)
        voltage = 12.0 + 0.5 * math.sin(2 * math.pi * i / 1000) + np.random.normal(0, 0.1)
        
        time_series_data += f"{timestamp},{temp:.2f},{humidity:.1f},{pressure:.2f},{co2:.0f},{voltage:.3f}\n"
    
    run_test_case("æ™‚ç³»åˆ—ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿", time_series_data.encode('utf-8'), "balanced", core, results)
    
    # 2. ç§‘å­¦è¨ˆç®—çµæœï¼ˆè¡Œåˆ—ãƒ‡ãƒ¼ã‚¿ï¼‰
    matrix_size = 500
    matrix_data = f"# Matrix Data {matrix_size}x{matrix_size}\n"
    matrix_data += f"# Generated by NXZip Scientific Test Suite\n"
    matrix_data += f"rows: {matrix_size}\n"
    matrix_data += f"cols: {matrix_size}\n"
    matrix_data += f"format: csv\n\n"
    
    # Generate correlation matrix (symmetric, values between -1 and 1)
    for i in range(matrix_size):
        row_values = []
        for j in range(matrix_size):
            if i == j:
                value = 1.0
            elif i < j:
                # Generate correlation value
                value = math.sin(i * j * 0.001) * math.exp(-abs(i-j) * 0.01)
            else:
                # Use symmetry
                value = math.sin(j * i * 0.001) * math.exp(-abs(j-i) * 0.01)
            
            row_values.append(f"{value:.6f}")
        
        matrix_data += ",".join(row_values) + "\n"
    
    run_test_case("ç§‘å­¦è¨ˆç®—è¡Œåˆ—ãƒ‡ãƒ¼ã‚¿", matrix_data.encode('utf-8'), "maximum", core, results)
    
    # 3. ãƒã‚¤ãƒŠãƒªæ•°å€¤é…åˆ—ï¼ˆNumPy-likeï¼‰
    # Float64 array header simulation
    dtype_info = np.dtype(np.float64)
    array_shape = (1000, 100)  # 2D array
    total_elements = array_shape[0] * array_shape[1]
    
    # Create header information
    header = {
        'descr': dtype_info.descr,
        'fortran_order': False,
        'shape': array_shape,
    }
    header_str = str(header).replace("'", '"')
    
    # Simulate .npy format
    magic = b'\x93NUMPY'
    version = b'\x01\x00'
    header_bytes = header_str.encode('latin1')
    header_len = len(header_bytes)
    
    # Pad header to 64-byte boundary
    padding = (64 - (10 + header_len) % 64) % 64
    header_bytes += b' ' * padding
    
    npy_header = magic + version + header_len.to_bytes(2, 'little') + header_bytes
    
    # Generate scientific data (wave interference pattern)
    binary_data = bytearray()
    for i in range(array_shape[0]):
        for j in range(array_shape[1]):
            # Complex wave interference
            x, y = i / array_shape[0], j / array_shape[1]
            value = (math.sin(20 * math.pi * x) * math.cos(15 * math.pi * y) + 
                    math.sin(10 * math.pi * (x + y)) * 0.5 +
                    np.random.normal(0, 0.1))
            
            binary_data.extend(np.array([value], dtype=np.float64).tobytes())
    
    numpy_data = npy_header + bytes(binary_data)
    run_test_case("NumPyç§‘å­¦è¨ˆç®—é…åˆ—", numpy_data, "balanced", core, results)

def test_encryption_and_security(core: NXZipCore, results: TestResults):
    """æš—å·åŒ–ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*60)
    print("ğŸ” æš—å·åŒ–ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    # 1. æ©Ÿå¯†æ–‡æ›¸ï¼ˆæš—å·åŒ–åœ§ç¸®ï¼‰
    confidential_doc = """
    CONFIDENTIAL - FOR AUTHORIZED PERSONNEL ONLY
    
    Security Report #2024-001
    Classification: TOP SECRET
    Distribution: EYES ONLY
    
    Executive Summary:
    This document contains sensitive information regarding the implementation
    of advanced compression algorithms in the NXZip platform. The information
    herein is proprietary and must not be disclosed to unauthorized parties.
    
    Technical Details:
    - Algorithm: NEXUS TMC v9.1 with SPE integration
    - Compression Ratio: Up to 99.26% for text data
    - Security Features: AES-256-GCM encryption with structure preservation
    - Performance: Exceeds 7-Zip compression with 2x speed improvement
    
    Implementation Notes:
    The Transform-Model-Code (TMC) approach utilizes:
    1. Burrows-Wheeler Transform for text reorganization
    2. Move-to-Front encoding for entropy reduction
    3. Structure-Preserving Encryption (SPE) for security
    4. LZMA/Zstd hybrid compression for final stage
    
    Threat Assessment:
    Current security measures are adequate for protecting against:
    - Brute force attacks (estimated 2^256 operations required)
    - Side-channel analysis (constant-time implementations used)
    - Reverse engineering (obfuscated algorithm parameters)
    
    Recommendations:
    1. Implement key rotation every 90 days
    2. Enable audit logging for all compression operations  
    3. Deploy hardware security modules for key management
    4. Conduct quarterly penetration testing
    
    Contact Information:
    Security Team: security@nxzip.com
    Emergency Contact: +1-555-SECURE
    
    Document Control:
    Created: 2024-01-15
    Version: 1.0
    Next Review: 2024-04-15
    """ * 20
    
    # Generate encryption key
    encryption_key = os.urandom(32)  # 256-bit key
    
    run_test_case("æš—å·åŒ–æ©Ÿå¯†æ–‡æ›¸", confidential_doc.encode('utf-8'), "maximum", core, results, encryption_key)
    
    # 2. è¨¼æ˜æ›¸ãƒ»ã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«æ¨¡æ“¬
    cert_data = """
-----BEGIN CERTIFICATE-----
MIIFfTCCA2WgAwIBAgIJALZlJiPVzxCwMA0GCSqGSIb3DQEBCwUAMFYxCzAJBgNV
BAYTAlVTMQswCQYDVQQIDAJDQTEWMBQGA1UEBwwNU2FuIEZyYW5jaXNjbzEMMAoG
A1UECgwDTlhaSTEUMBIGA1UEAwwLTlhaaXAgQ29ycC4wHhcNMjQwMTE1MTIwMDAw
WhcNMjUwMTE1MTIwMDAwWjBWMQswCQYDVQQGEwJVUzELMAkGA1UECAwCQ0ExFjAU
BgNVBAcMDVNhbiBGcmFuY2lzY28xDDAKBgNVBAoMA05YWjEUMBIGA1UEAwwLTlha
aXAgQ29ycC4wggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQDYuJ5kT8mI
xRv+qJH8cEbDQaI9JbPVkJ5aJwZvN8pqQwYKzJgFH3GJg6lWmm2aBv5VQzJ7uI4X
aPu8K3NjEzBvU2PqL6rD8ZmWaVnXjN9OqG7TpAe5I4YJQ2fN7VsZB8rCqS6Hk3nM
bCZrV4JtPvWFgAw9EcKzLnQP3rUJwNv2B5gXd4SqTjBfEq7nOcGk1TxVlJmYp2Zs
QHgFkOqYuEz4M8KpJ1tFv7XQoVnCqZaP3J9oEb2MkZjYvG4R2wIDAQABo1MwUTAd
BgNVHQ4EFgQUJlYlNxVxWLgJ+sS7OmJ5mNbzUi0wHwYDVR0jBBgwFoAUJlYlNxNx
WLgJ+sS7OmJ5mNbzUi0wDwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOC
AgEAKJvUlCJaFnWaVKmBk2OqJ8X4pCzN6YoVgFq8mMhTgN4PqFzrZcMO5cTJ1yZB
9LGFhGlJmKjPwcZvB4L8xVqSvX3JbVvNyZw6QjJzL5kVzO2QgF7rDmE8YbZ4Z3zN
5pZKjP7oEwG1nXrYvNBcJzJfGvZaL4XyQnZ8pV2oGjYzEbN6OqPcX4rVnL9OkJgZ
H4RqY7zCwU4hM4oXjP2vB3nL7GfEwR4Y1tJsZoEoM6KvYxRn3pXy8wRvZnPcOlKj
-----END CERTIFICATE-----

-----BEGIN PRIVATE KEY-----
MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDYuJ5kT8mIxRv+
qJH8cEbDQaI9JbPVkJ5aJwZvN8pqQwYKzJgFH3GJg6lWmm2aBv5VQzJ7uI4XaPu8
K3NjEzBvU2PqL6rD8ZmWaVnXjN9OqG7TpAe5I4YJQ2fN7VsZB8rCqS6Hk3nMbCZr
V4JtPvWFgAw9EcKzLnQP3rUJwNv2B5gXd4SqTjBfEq7nOcGk1TxVlJmYp2ZsQHgF
kOqYuEz4M8KpJ1tFv7XQoVnCqZaP3J9oEb2MkZjYvG4R2wIDAQABAoIBADT6oLzN
4xvNKcYjVn8XzGqGH7z3K5lJpZ6vYgPqBzfQoN1rE8KsJgA4OqYpHzNfZKg5rXvB
Nw9mGv6YrJbKz4Q8lMnC3TgJvZpYoLqKg7RzY2fNmO6VpQjNxEtJa5L8XqPvB2zG
WuJ3Q7kM9ZnYoVKjL6rF5TsOqGt7Bp4XyNr8CwZ1QaE5rYvNLgJ6pKjBnZ3fVoTu
Kz4q8O6YgJpL2NrX5vMzKgNqP7z8OfJbLqT6nV9RoYvKgJpO2NzB4qLvMrKgE8Yp
QjTnZ6vOqGt9LpX5rYvNK2z8CwJ6pKjBnO3fVoTuKz4qBO6YgJpL2NrX5vMzKgNq
P7z8OfJbLqT6nV9RoYvKgJpO2ECgYEA7z8K5JbQvYaG3KLqNpOg5J4VmN7zPx1qO
p6vYgFqBzfQoN1rE8KsJgA4OqYpHzNfZKg5rXvBNw9mGv6YrJbKz4Q8lMnC3TgJ
vZpYoLqKg7RzY2fNmO6VpQjNxEtJa5L8XqPvB2zGWuJ3Q7kM9ZnYoVKjL6rF5TsO
qGt7Bp4XyNr8CwZ1QaE5rYvNLgJ6pKjBnZ3fVoTuKz4q8O6YgJpL2NrX5vMzKgNq
-----END PRIVATE KEY-----
""" * 50
    
    run_test_case("è¨¼æ˜æ›¸ãƒ»ã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«", cert_data.encode('utf-8'), "maximum", core, results, encryption_key)
    
    # 3. ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
    password_db = "user_id,username,password_hash,salt,created_at,last_modified\n"
    
    for i in range(5000):
        user_id = i + 1
        username = f"user{i:04d}"
        # Simulate bcrypt hashes
        hash_part = f"$2b$12$" + ''.join([chr(65 + (i * j) % 26) for j in range(22)])
        salt = ''.join([chr(97 + (i * 7 + j) % 26) for j in range(16)])
        password_hash = hash_part + salt + ''.join([chr(48 + (i * j) % 10) for j in range(31)])
        created_at = f"2023-{1 + (i % 12):02d}-{1 + (i % 28):02d}"
        last_modified = f"2024-01-{1 + (i % 31):02d}"
        
        password_db += f"{user_id},{username},{password_hash},{salt},{created_at},{last_modified}\n"
    
    run_test_case("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒã‚·ãƒ¥DB", password_db.encode('utf-8'), "maximum", core, results, encryption_key)

def test_performance_and_stress(core: NXZipCore, results: TestResults):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*60)
    print("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ")
    print("="*60)
    
    # 1. å¤§å®¹é‡ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ10MBï¼‰
    large_text = """
    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor 
    incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis 
    nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore 
    eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, 
    sunt in culpa qui officia deserunt mollit anim id est laborum.
    
    æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯å¤šè¨€èªå¯¾å¿œã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚
    æ¼¢å­—ã€ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€ãã—ã¦è‹±æ•°å­—ãŒæ··åœ¨ã—ã¦ã„ã¾ã™ã€‚
    åœ§ç¸®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ã“ã‚Œã‚‰ã®æ–‡å­—ã®é »åº¦åˆ†æã‚’è¡Œã„ã€åŠ¹ç‡çš„ã«åœ§ç¸®ã—ã¾ã™ã€‚
    """ * 5000  # Approximately 10MB
    
    print(f"   å¤§å®¹é‡ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†: {len(large_text):,} bytes")
    run_test_case("å¤§å®¹é‡ãƒ†ã‚­ã‚¹ãƒˆ(10MB)", large_text.encode('utf-8'), "balanced", core, results)
    
    # 2. é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆç–‘ä¼¼ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
    print("   é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
    random_data = os.urandom(1024 * 1024)  # 1MB of random data
    run_test_case("é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿(1MB)", random_data, "fast", core, results)
    
    # 3. æ¥µä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆé«˜åå¾©ï¼‰
    repetitive_data = b'A' * (512 * 1024) + b'B' * (256 * 1024) + b'C' * (256 * 1024)  # 1MB
    run_test_case("æ¥µä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿(1MB)", repetitive_data, "fast", core, results)
    
    # 4. å…¨ãƒ¢ãƒ¼ãƒ‰æ¯”è¼ƒãƒ†ã‚¹ãƒˆï¼ˆä¸­ã‚µã‚¤ã‚ºãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    mixed_data = (large_text[:100000] + str(list(range(10000))) * 10).encode('utf-8')
    print(f"\nğŸ”„ å…¨ãƒ¢ãƒ¼ãƒ‰æ¯”è¼ƒãƒ†ã‚¹ãƒˆ - ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(mixed_data):,} bytes")
    
    modes = ["fast", "balanced", "maximum"]
    for mode in modes:
        run_test_case(f"å…¨ãƒ¢ãƒ¼ãƒ‰æ¯”è¼ƒ-{mode.upper()}", mixed_data, mode, core, results)
    
    # 5. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ†ã‚¹ãƒˆï¼ˆè¤‡æ•°ã®å°ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    print(f"\nğŸ“ è¤‡æ•°å°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ")
    for i in range(10):
        small_file_data = f"Small file #{i+1}\n" + "Data line " * 100 + f"\nEnd of file {i+1}\n"
        run_test_case(f"å°ãƒ•ã‚¡ã‚¤ãƒ«#{i+1:02d}", small_file_data.encode('utf-8'), "fast", core, results)

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ NXZip Core v2.0 å®Ÿç”¨çš„ç·åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ")
    print("="*80)
    
    # ãƒ†ã‚¹ãƒˆçµæœé›†è¨ˆ
    results = TestResults()
    
    # NXZip CoreåˆæœŸåŒ–
    core = NXZipCore()
    core.set_progress_callback(progress_callback)
    
    try:
        # å„ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªãƒ¼å®Ÿè¡Œ
        test_documents_and_text(core, results)
        test_source_code_and_config(core, results)
        test_media_and_binary(core, results)
        test_database_and_logs(core, results)
        test_scientific_and_numerical(core, results)
        test_encryption_and_security(core, results)
        test_performance_and_stress(core, results)
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        results.print_summary()
        
        # è©³ç´°çµæœã‚’JSONã§ä¿å­˜
        detailed_results = {
            'test_suite': 'NXZip Core v2.0 Comprehensive Test',
            'execution_time': time.time(),
            'summary': results.summary,
            'detailed_results': results.results
        }
        
        with open('nxzip_test_results.json', 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š è©³ç´°çµæœã‚’ nxzip_test_results.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        results.print_summary()
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        results.print_summary()

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
NEXUS TMC Engine v9.1 - æ¬¡ä¸–ä»£ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼åœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
Transform-Model-Code åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ TMC v9.1
é©æ–°çš„ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆ + åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±åˆ
"""

import os
import sys
import time
import json
import asyncio
import multiprocessing as mp
import zlib
import lzma
import bz2
import numpy as np
from typing import Tuple, Dict, Any, List, Optional, Union

# TMC v9.1 åˆ†é›¢ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .core import (
    DataType, ChunkInfo, PipelineStage, AsyncTask, 
    MemoryManager, MEMORY_MANAGER, CoreCompressor
)
from .analyzers import calculate_entropy, MetaAnalyzer
from .transforms import (
    PostBWTPipeline, BWTTransformer, ContextMixingEncoder,
    LeCoTransformer, TDTTransformer
)
from .parallel import ParallelPipelineProcessor
from .utils import SublinearLZ77Encoder, TMCv8Container

# TMC v9.1 å®šæ•°
TMC_V91_MAGIC = b'TMC91'
DEFAULT_CHUNK_SIZE = 2 * 1024 * 1024  # 2MB per chunk
MAX_WORKERS = min(8, mp.cpu_count())

__all__ = ['NEXUSTMCEngineV91']


class ImprovedDispatcher:
    """æ”¹è‰¯ç‰ˆãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£ãƒ¼"""
    
    def dispatch_data_type(self, data: bytes) -> DataType:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ç²¾å¯†åˆ¤å®šï¼ˆãƒ†ã‚­ã‚¹ãƒˆæœ€å„ªå…ˆï¼‰"""
        if len(data) < 16:
            return DataType.GENERIC_BINARY
        
        # Phase 1: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æœ€å„ªå…ˆåˆ¤å®šï¼ˆä¿®æ­£ï¼‰
        
        # 1-1. å¼·åˆ¶çš„ãªãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®šã‚’æœ€åˆã«å®Ÿè¡Œ
        text_data = None
        for encoding in ['utf-8', 'ascii', 'latin1', 'cp1252']:
            try:
                text_data = data.decode(encoding, errors='strict')
                # å°åˆ·å¯èƒ½æ–‡å­—ç‡ã®å³å¯†ãƒã‚§ãƒƒã‚¯
                printable_count = sum(1 for c in text_data if c.isprintable() or c.isspace())
                printable_ratio = printable_count / len(text_data)
                
                if printable_ratio > 0.85:  # 85%ä»¥ä¸ŠãŒå°åˆ·å¯èƒ½ = ãƒ†ã‚­ã‚¹ãƒˆç¢ºå®š
                    # èªå½™åˆ†æ
                    words = text_data.split()
                    if len(words) > 5:
                        unique_words = set(words)
                        repetition_ratio = 1 - (len(unique_words) / len(words))
                        
                        if repetition_ratio > 0.5:  # 50%ä»¥ä¸ŠãŒé‡è¤‡èª
                            return DataType.TEXT_REPETITIVE
                        else:
                            return DataType.TEXT_NATURAL
                    else:
                        # æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®åˆ†æ
                        char_freq = {}
                        for c in text_data:
                            char_freq[c] = char_freq.get(c, 0) + 1
                        
                        # æœ€é »æ–‡å­—ã®å‡ºç¾ç‡
                        if char_freq:
                            max_freq = max(char_freq.values()) / len(text_data)
                            if max_freq > 0.3:  # 30%ä»¥ä¸ŠãŒåŒä¸€æ–‡å­—
                                return DataType.TEXT_REPETITIVE
                            else:
                                return DataType.TEXT_NATURAL
                break
            except UnicodeDecodeError:
                continue
        
        # Phase 2: ASCIIæ•°å€¤ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹æ®Šå‡¦ç†
        if text_data is not None:
            try:
                # æ•°å€¤æ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å³å¯†åˆ†æ
                import re
                
                # æ•°å€¤è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆCSVãƒ•ã‚¡ã‚¤ãƒ«ãªã©ï¼‰
                lines = text_data.strip().split('\n')
                numeric_lines = 0
                for line in lines[:20]:  # æœ€åˆã®20è¡Œãƒã‚§ãƒƒã‚¯
                    # æ•°å€¤ã€ã‚¹ãƒšãƒ¼ã‚¹ã€ã‚«ãƒ³ãƒã€ãƒ”ãƒªã‚ªãƒ‰ã®ã¿ã®è¡Œ
                    if re.match(r'^[\d\s.,e+-]+$', line.strip()) and len(line.strip()) > 0:
                        numeric_lines += 1
                
                if numeric_lines > len(lines[:20]) * 0.7:  # 70%ä»¥ä¸ŠãŒæ•°å€¤è¡Œ
                    return DataType.SEQUENTIAL_INT
                
                # æµ®å‹•å°æ•°ç‚¹æ•°ã®æ¤œå‡º
                float_matches = re.findall(r'[-+]?(?:\d+\.?\d*|\.\d+)(?:[eE][-+]?\d+)?', text_data)
                if len(float_matches) >= 10:  # 10å€‹ä»¥ä¸Šã®æµ®å‹•å°æ•°ç‚¹æ•°
                    return DataType.SEQUENTIAL_INT  # ãƒ†ã‚­ã‚¹ãƒˆæ•°å€¤ã¨ã—ã¦æ‰±ã†
                    
            except:
                pass
        
        # Phase 3: ãƒã‚¤ãƒŠãƒªæ•°å€¤é…åˆ—ã®åˆ¤å®šï¼ˆãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®šå¾Œï¼‰
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èªè­˜ã•ã‚Œãªã‹ã£ãŸå ´åˆã®ã¿æ•°å€¤åˆ¤å®šã‚’å®Ÿè¡Œ
        if text_data is None:
            # 32ãƒ“ãƒƒãƒˆæ•´æ•°é…åˆ—ï¼ˆLittle Endianãƒã‚§ãƒƒã‚¯ï¼‰
            if len(data) >= 16 and len(data) % 4 == 0:
                try:
                    # Little Endian 32bit integers
                    int_array = np.frombuffer(data, dtype='<i4')  # explicit little endian
                    if len(int_array) >= 4:
                        # çµ±è¨ˆçš„å¦¥å½“æ€§ã®å³å¯†ãƒã‚§ãƒƒã‚¯
                        finite_mask = np.isfinite(int_array.astype(float))
                        valid_ints = int_array[finite_mask]
                        
                        if len(valid_ints) > len(int_array) * 0.9:  # 90%ä»¥ä¸ŠãŒæœ‰åŠ¹
                            # å€¤åŸŸãƒã‚§ãƒƒã‚¯ï¼ˆç¾å®Ÿçš„ãªæ•´æ•°å€¤ï¼‰
                            if np.all(np.abs(valid_ints) < 1e9):  # 10å„„æœªæº€
                                # é€£ç¶šæ€§ã¾ãŸã¯æ§‹é€ æ€§ãƒã‚§ãƒƒã‚¯
                                if len(valid_ints) >= 4:
                                    differences = np.diff(valid_ints)
                                    diff_std = np.std(differences)
                                    val_std = np.std(valid_ints)
                                    # æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´
                                    if diff_std < val_std or np.any(np.abs(differences) <= 1):
                                        return DataType.SEQUENTIAL_INT
                                        
                except Exception:
                    pass
            
            # 32ãƒ“ãƒƒãƒˆæµ®å‹•å°æ•°ç‚¹é…åˆ—
            if len(data) >= 16 and len(data) % 4 == 0:
                try:
                    float_array = np.frombuffer(data, dtype='<f4')  # explicit little endian
                    if len(float_array) >= 4:
                        # NaN/Inf ã®é™¤å»
                        finite_mask = np.isfinite(float_array)
                        valid_floats = float_array[finite_mask]
                        
                        if len(valid_floats) > len(float_array) * 0.8:  # 80%ä»¥ä¸ŠãŒæœ‰åŠ¹
                            # æµ®å‹•å°æ•°ç‚¹ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                            if np.all(np.abs(valid_floats) < 1e10):  # éå¸¸ã«å¤§ããªå€¤ã§ãªã„
                                # æµ®å‹•å°æ•°ç‚¹ç‰¹æœ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                                unique_ratio = len(np.unique(valid_floats)) / len(valid_floats)
                                if unique_ratio > 0.5:  # 50%ä»¥ä¸ŠãŒãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼ˆæµ®å‹•å°æ•°ç‚¹ã®ç‰¹å¾´ï¼‰
                                    return DataType.FLOAT_ARRAY
                                    
                except Exception:
                    pass
        
        # Phase 4: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æã«ã‚ˆã‚‹æœ€çµ‚åˆ†é¡
        
        # ãƒã‚¤ãƒˆåˆ†å¸ƒã®è©³ç´°åˆ†æ
        byte_counts = np.bincount(data[:min(4096, len(data))], minlength=256)
        probabilities = byte_counts / np.sum(byte_counts)
        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-12))
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é–¾å€¤ã«ã‚ˆã‚‹åˆ†é¡
        if entropy > 7.5:  # éå¸¸ã«é«˜ã„ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            return DataType.MIXED_DATA
        elif entropy < 2.0:  # éå¸¸ã«ä½ã„ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            return DataType.TEXT_REPETITIVE
        else:  # ä¸­ç¨‹åº¦ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            return DataType.GENERIC_BINARY


class NEXUSTMCEngineV91:
    """
    NEXUS TMC Engine v9.1 - ã‚ªãƒªã‚¸ãƒŠãƒ«åœ§ç¸®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£çµ±æ‹¬ç‰ˆ
    NXZipå°‚ç”¨Transform-Model-Codeåœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
    
    NXZipå›ºæœ‰æ©Ÿèƒ½:
    - SPE (Structure-Preserving Encryption) çµ±åˆ
    - TMCå¤šæ®µéšå¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    - åˆ†é›¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ã‚ˆã‚‹é«˜åº¦åœ§ç¸®
    - Zstandardãƒ¬ãƒ™ãƒ«è»½é‡ãƒ¢ãƒ¼ãƒ‰ + 7-Zipè¶…è¶Šé€šå¸¸ãƒ¢ãƒ¼ãƒ‰
    """
    
    def __init__(self, max_workers: int = None, chunk_size: int = DEFAULT_CHUNK_SIZE, 
                 lightweight_mode: bool = False):
        self.max_workers = max_workers or MAX_WORKERS
        self.chunk_size = chunk_size
        self.lightweight_mode = lightweight_mode
        self.memory_manager = MEMORY_MANAGER
        
        # NXZipå°‚ç”¨ãƒ¢ãƒ¼ãƒ‰è¨­å®š
        if lightweight_mode:
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰: Zstandardãƒ¬ãƒ™ãƒ«ç›®æ¨™
            self.max_workers = 2  # è»½é‡ä¸¦åˆ—å‡¦ç†
            self.chunk_size = 256 * 1024  # 256KB - åŠ¹ç‡çš„ãƒãƒ£ãƒ³ã‚¯
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰ç”¨TMCè¨­å®š
            self.enable_analysis = True  # é«˜é€Ÿåˆ†æã¯æœ‰åŠ¹
            self.enable_transforms = True  # åŠ¹ç‡çš„å¤‰æ›ã¯æœ‰åŠ¹
            self.transform_depth = 1  # è»½é‡å¤‰æ›
            self.compression_strategy = 'speed_optimized'
            print("âš¡ NXZipè»½é‡ãƒ¢ãƒ¼ãƒ‰: Zstandardãƒ¬ãƒ™ãƒ«ç›®æ¨™ (SPE+è»½é‡TMC)")
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: 7-Zipè¶…è¶Šç›®æ¨™
            self.max_workers = min(4, MAX_WORKERS)  # åŠ¹ç‡çš„ä¸¦åˆ—
            self.chunk_size = max(1024 * 1024, chunk_size)  # 1MB - æœ€é©ãƒãƒ£ãƒ³ã‚¯
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ç”¨TMCè¨­å®š
            self.enable_analysis = True  # è©³ç´°åˆ†æ
            self.enable_transforms = True  # å…¨å¤‰æ›é©ç”¨
            self.transform_depth = 3  # æ·±åº¦å¤‰æ›
            self.compression_strategy = 'ratio_optimized'
            print("ğŸ¯ NXZipé€šå¸¸ãƒ¢ãƒ¼ãƒ‰: 7-Zipè¶…è¶Šç›®æ¨™ (SPE+æœ€å¤§TMC)")
        
        # NXZipå°‚ç”¨è¨­å®š
        self.enable_spe = True  # SPEå¿…é ˆ
        self.reversibility_check = True  # å¯é€†æ€§ä¿è¨¼
        self.nxzip_format_version = '2.0'
        
        # åˆ†é›¢ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é«˜é€ŸåˆæœŸåŒ–ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰æœ€é©åŒ–ï¼‰
        self.dispatcher = ImprovedDispatcher()
        self.core_compressor = CoreCompressor(lightweight_mode=self.lightweight_mode)
        
        # ãƒ¡ã‚¿åˆ†æå™¨ã¯è»½é‡ãƒ¢ãƒ¼ãƒ‰ã§ã¯é…å»¶åˆæœŸåŒ–
        if self.lightweight_mode:
            self.meta_analyzer = None  # é…å»¶åˆæœŸåŒ–
        else:
            self.meta_analyzer = MetaAnalyzer(self.core_compressor, lightweight_mode=self.lightweight_mode)
        
        # TMCå¤‰æ›å™¨ã®é…å»¶åˆæœŸåŒ–ï¼ˆå¤§å¹…é«˜é€ŸåŒ–ï¼‰
        if self.lightweight_mode:
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰: é…å»¶åˆæœŸåŒ–ã§é€Ÿåº¦æœ€é©åŒ–
            self.bwt_transformer = None
            self.context_mixer = None
            self.leco_transformer = None
            self.tdt_transformer = None
            print("âš¡ è»½é‡TMCå¤‰æ›å™¨: é…å»¶åˆæœŸåŒ–ã«ã‚ˆã‚‹é«˜é€ŸåŒ–")
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: äº‹å‰åˆæœŸåŒ–ã§æœ€é©åŒ–
            self.bwt_transformer = BWTTransformer(lightweight_mode=False)
            self.context_mixer = ContextMixingEncoder(lightweight_mode=False)
            self.leco_transformer = LeCoTransformer(lightweight_mode=False)
            self.tdt_transformer = TDTTransformer(lightweight_mode=False)
            print("ğŸ¯ é€šå¸¸TMCå¤‰æ›å™¨: æœ€å¤§åœ§ç¸®ç‡æ§‹æˆ")
        
        # ä¸¦åˆ—å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆè»½é‡ãƒ¢ãƒ¼ãƒ‰é«˜é€ŸåŒ–ï¼‰
        if self.max_workers > 1 and not self.lightweight_mode:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã®ã¿ä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨
            self.pipeline_processor = ParallelPipelineProcessor(
                max_workers=self.max_workers, 
                lightweight_mode=self.lightweight_mode
            )
            print(f"ğŸ”„ TMCä¸¦åˆ—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: {self.max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼")
        else:
            # è»½é‡ãƒ¢ãƒ¼ãƒ‰ã¯ä¸¦åˆ—å‡¦ç†ã‚’ç„¡åŠ¹åŒ–ï¼ˆåˆæœŸåŒ–ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
            self.pipeline_processor = None
            if self.lightweight_mode:
                print("âš¡ TMCè»½é‡å‡¦ç†: ä¸¦åˆ—ç„¡åŠ¹åŒ–ã«ã‚ˆã‚‹é«˜é€ŸåŒ–")
            else:
                print("ğŸ”„ TMCã‚·ãƒ³ã‚°ãƒ«ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†")
        
        # NXZipå°‚ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
        self.sublinear_lz77 = SublinearLZ77Encoder()
        
        # TMCå¤‰æ›å™¨ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆé…å»¶åˆæœŸåŒ–å¯¾å¿œï¼‰
        self.transformers = {}  # é…å»¶åˆæœŸåŒ–
        self._transformer_cache = {}  # åˆæœŸåŒ–æ¸ˆã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        # NXZipå°‚ç”¨çµ±è¨ˆã‚·ã‚¹ãƒ†ãƒ 
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'reversibility_tests_passed': 0,
            'reversibility_tests_total': 0,
            'spe_applications': 0,  # SPEé©ç”¨å›æ•°
            'tmc_transforms_applied': 0,  # TMCå¤‰æ›é©ç”¨
            'tmc_transforms_bypassed': 0,  # TMCå¤‰æ›ãƒã‚¤ãƒ‘ã‚¹
            'chunks_processed': 0,
            'parallel_efficiency': 0.0,
            'nxzip_format_version': self.nxzip_format_version,
            'modular_components_active': len([
                self.bwt_transformer, self.context_mixer, 
                self.leco_transformer, self.tdt_transformer
            ])
        }
        
        print(f"ğŸš€ NXZip TMC v9.1 çµ±æ‹¬ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“¦ åˆ†é›¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ: Core + Analyzers + Transforms + Parallel + Utils")
        print(f"âš™ï¸  è¨­å®š: {self.max_workers}ä¸¦åˆ—, {self.chunk_size//1024}KBãƒãƒ£ãƒ³ã‚¯, å¤‰æ›æ·±åº¦={self.transform_depth}")
        print(f"ğŸ¯ ç›®æ¨™: {'Zstandardãƒ¬ãƒ™ãƒ«' if self.lightweight_mode else '7-Zipè¶…è¶Š'}")
    
    def _get_transformer(self, data_type: DataType):
        """é…å»¶åˆæœŸåŒ–ã«ã‚ˆã‚‹å¤‰æ›å™¨å–å¾—ï¼ˆ7-Zipè¶…è¶Šã®ãŸã‚å…¨ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        if data_type in self._transformer_cache:
            return self._transformer_cache[data_type]
        
        # é…å»¶åˆæœŸåŒ– - å…¨ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«å¯¾å¿œ
        transformer = None
        if data_type in [DataType.TEXT_REPETITIVE, DataType.TEXT_NATURAL]:
            if self.bwt_transformer is None:
                self.bwt_transformer = BWTTransformer(lightweight_mode=self.lightweight_mode)
            transformer = self.bwt_transformer
        elif data_type == DataType.FLOAT_ARRAY:
            if self.tdt_transformer is None:
                self.tdt_transformer = TDTTransformer(lightweight_mode=self.lightweight_mode)
            transformer = self.tdt_transformer
        elif data_type == DataType.SEQUENTIAL_INT:
            if self.leco_transformer is None:
                self.leco_transformer = LeCoTransformer(lightweight_mode=self.lightweight_mode)
            transformer = self.leco_transformer
        else:
            # GENERIC_BINARYã‚„ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«ã‚‚BWTå¤‰æ›ã‚’é©ç”¨ï¼ˆ7-Zipè¶…è¶Šã®ãŸã‚ï¼‰
            if self.bwt_transformer is None:
                self.bwt_transformer = BWTTransformer(lightweight_mode=self.lightweight_mode)
            transformer = self.bwt_transformer
            print(f"ğŸ”¥ æ±ç”¨ãƒã‚¤ãƒŠãƒªã«ã‚‚BWTå¤‰æ›ã‚’é©ç”¨: {data_type.value if hasattr(data_type, 'value') else str(data_type)}")
        
        self._transformer_cache[data_type] = transformer
        return transformer
    
    def compress(self, data: bytes, chunk_callback=None) -> Tuple[bytes, Dict[str, Any]]:
        """NXZip TMC v9.1 ãƒ¡ã‚¤ãƒ³åœ§ç¸®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        print("--- NXZip TMC v9.1 çµ±åˆåœ§ç¸®é–‹å§‹ ---")
        start_time = time.time()
        
        # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆæœŸåŒ–
        self.progress_callback = chunk_callback
        
        try:
            if len(data) == 0:
                return b'', {'method': 'nxzip_empty', 'compression_time': 0.0}
            
            # åˆæœŸé€²æ—å ±å‘Š
            if self.progress_callback:
                self.progress_callback(5, "ğŸ”¥ TMCåˆæœŸåŒ–ä¸­...")
            
            # ãƒ•ã‚§ãƒ¼ã‚º1: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æï¼ˆåˆ†é›¢ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨ï¼‰
            if self.enable_analysis:
                if self.progress_callback:
                    self.progress_callback(10, "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æä¸­...")
                data_type = self.dispatcher.dispatch_data_type(data)
                print(f"ğŸ“Š NXZipãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æ: {data_type.value}")
            else:
                data_type = DataType.GENERIC_BINARY
                print(f"ğŸ“Š é«˜é€Ÿå‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {data_type.value}")
            
            # ãƒ•ã‚§ãƒ¼ã‚º2: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            if self.progress_callback:
                self.progress_callback(15, "ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¸­...")
            chunks = self._adaptive_chunking(data)
            print(f"ğŸ“¦ NXZipãƒãƒ£ãƒ³ã‚¯åˆ†å‰²: {len(chunks)}å€‹ ({self.chunk_size//1024}KB)")
            
            # ãƒ•ã‚§ãƒ¼ã‚º3: TMCå¤‰æ›å¼·åˆ¶é©ç”¨ï¼ˆ7-Zip + Zstandardè¶…è¶Šãƒ¢ãƒ¼ãƒ‰ï¼‰
            if self.progress_callback:
                self.progress_callback(20, "ğŸ§  TMCå¤‰æ›å¼·åˆ¶é©ç”¨ä¸­...")
            
            if self.enable_transforms:
                # NEXUS TMC v9.1ã¯å¸¸ã«å¤‰æ›ã‚’é©ç”¨ï¼ˆ7-Zipè¶…è¶Šã®ãŸã‚ï¼‰
                transformer = self._get_transformer(data_type)
                if transformer:
                    should_transform = True
                    analysis_info = {
                        'method': 'nexus_tmc_forced_transform',
                        'reason': '7zip_zstd_surpass_mode',
                        'data_type': data_type.value if hasattr(data_type, 'value') else str(data_type)
                    }
                    print(f"ğŸ”¥ NEXUS TMC v9.1 å¤‰æ›å¼·åˆ¶é©ç”¨: {data_type.value if hasattr(data_type, 'value') else str(data_type)}")
                else:
                    # å¤‰æ›å™¨ãŒç„¡ã„å ´åˆã§ã‚‚åŸºæœ¬å¤‰æ›ã‚’é©ç”¨
                    should_transform = True
                    analysis_info = {
                        'method': 'nexus_tmc_basic_transform',
                        'reason': 'no_specific_transformer_available'
                    }
                    print(f"ğŸ”¥ NEXUS TMC v9.1 åŸºæœ¬å¤‰æ›é©ç”¨")
            else:
                # å¤‰æ›ç„¡åŠ¹ã§ã‚‚è­¦å‘Šè¡¨ç¤º
                should_transform = False
                analysis_info = {'method': 'transforms_disabled_warning'}
                print(f"âš ï¸ TMCå¤‰æ›ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ - 7-Zipè¶…è¶ŠåŠ¹æœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“")
            
            # é€²æ—æ›´æ–°: äºˆæ¸¬å®Œäº†
            if self.progress_callback:
                self.progress_callback(25, "âœ… TMCäºˆæ¸¬å®Œäº† - åœ§ç¸®é–‹å§‹")
            
            # ãƒ•ã‚§ãƒ¼ã‚º4: ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼ˆåˆ†é›¢ã•ã‚ŒãŸTransformerä½¿ç”¨ï¼‰
            processed_results = []
            chunk_progress_start = 25  # 25%ã‹ã‚‰é–‹å§‹
            chunk_progress_range = 60  # 25%ã‹ã‚‰85%ã¾ã§ (60%ã®ç¯„å›²)
            
            for i, chunk in enumerate(chunks):
                # è©³ç´°ãªé€²æ—è¨ˆç®—
                chunk_progress = chunk_progress_start + (i / len(chunks)) * chunk_progress_range
                processed_mb = sum(len(result[0]) for result in processed_results) / (1024 * 1024)
                
                if self.progress_callback:
                    self.progress_callback(
                        int(chunk_progress), 
                        f"ğŸ”¥ TMCåœ§ç¸®ä¸­ {i+1}/{len(chunks)} - {processed_mb:.1f}MBå‡¦ç†æ¸ˆã¿"
                    )
                
                if len(chunks) <= 5 or i == 0 or (i + 1) % max(1, len(chunks) // 5) == 0:
                    print(f"  ğŸ“¦ Chunk {i+1}/{len(chunks)} å‡¦ç†ä¸­...")
                
                if should_transform and transformer:
                    # TMCå¤‰æ›é©ç”¨ï¼ˆåˆ†é›¢ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨ï¼‰
                    try:
                        transformed_streams, transform_info = transformer.transform(chunk)
                        
                        # ã‚¹ãƒˆãƒªãƒ¼ãƒ æƒ…å ±ã‚’ä¿å­˜ï¼ˆé€†å¤‰æ›ã®ãŸã‚ï¼‰
                        if isinstance(transformed_streams, list):
                            streams_info = []
                            combined_data = b''
                            for stream in transformed_streams:
                                streams_info.append({'size': len(stream)})
                                combined_data += stream
                            transform_info['streams_info'] = streams_info
                            transform_info['original_streams_count'] = len(transformed_streams)
                        else:
                            combined_data = transformed_streams
                            transform_info['streams_info'] = [{'size': len(combined_data)}]
                            transform_info['original_streams_count'] = 1
                        
                        # ğŸ”¥ TMCå¤‰æ›æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®çœŸã®æ´»ç”¨ - æ¨™æº–åœ§ç¸®ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        # TMCå¤‰æ›ã«ã‚ˆã‚‹åœ§ç¸®åŠ¹æœã‚’ç›´æ¥ä½¿ç”¨ï¼ˆLZMAã§ä¸Šæ›¸ãã—ãªã„ï¼‰
                        if len(combined_data) < len(chunk) * 0.8:  # 20%ä»¥ä¸Šåœ§ç¸®ã•ã‚Œã¦ã„ã‚‹å ´åˆ
                            # TMCå¤‰æ›ã®åŠ¹æœãŒååˆ†ãªå ´åˆã¯ã€è»½é‡å¾Œå‡¦ç†ã®ã¿
                            compressed_data = zlib.compress(combined_data, level=1)  # è»½é‡åœ§ç¸®ã®ã¿
                            compress_info = {
                                'final_method': 'tmc_optimized_zlib_light',
                                'tmc_compression_ratio': (1 - len(combined_data) / len(chunk)) * 100,
                                'post_compression_ratio': (1 - len(compressed_data) / len(combined_data)) * 100
                            }
                            print(f"    ğŸ¯ TMCæœ€é©åŒ–: å¤‰æ›åŠ¹æœ{compress_info['tmc_compression_ratio']:.1f}% + è»½é‡å¾Œå‡¦ç†{compress_info['post_compression_ratio']:.1f}%")
                        else:
                            # TMCå¤‰æ›åŠ¹æœãŒé™å®šçš„ãªå ´åˆã®ã¿ã€æ¨™æº–åœ§ç¸®ã‚’é©ç”¨
                            compressed_data, compress_info = self.core_compressor.compress_core(
                                combined_data, method='lzma' if not self.lightweight_mode else 'zlib'
                            )
                            compress_info['final_method'] = 'tmc_with_standard_compression'
                            print(f"    ğŸ“¦ TMC + æ¨™æº–åœ§ç¸®: è¤‡åˆå‡¦ç†é©ç”¨")
                        
                        # é€†å¤‰æ›ã«å¿…è¦ãªè¿½åŠ æƒ…å ±ã‚’ä¿å­˜
                        transform_info['original_chunk_size'] = len(chunk)
                        transform_info['combined_data_size'] = len(combined_data)
                        
                        chunk_info = {
                            'chunk_id': i,
                            'original_size': len(chunk),
                            'compressed_size': len(compressed_data),
                            'data_type': data_type.value,
                            'transform_applied': True,
                            'transform_info': transform_info,
                            'compress_info': compress_info
                        }
                        
                        processed_results.append((compressed_data, chunk_info))
                        
                    except Exception as e:
                        print(f"    âš ï¸ TMCå¤‰æ›å¤±æ•—: {e}, åŸºæœ¬åœ§ç¸®ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                        # åŸºæœ¬åœ§ç¸®å‡¦ç†
                        compressed_data, compress_info = self.core_compressor.compress_core(
                            chunk, method='lzma' if not self.lightweight_mode else 'zlib'
                        )
                        chunk_info = {
                            'chunk_id': i,
                            'original_size': len(chunk),
                            'compressed_size': len(compressed_data),
                            'transform_applied': False,
                            'compress_info': compress_info
                        }
                        processed_results.append((compressed_data, chunk_info))
                else:
                    # åŸºæœ¬åœ§ç¸®ã®ã¿
                    compressed_data, compress_info = self.core_compressor.compress_core(
                        chunk, method='lzma' if not self.lightweight_mode else 'zlib'
                    )
                    chunk_info = {
                        'chunk_id': i,
                        'original_size': len(chunk),
                        'compressed_size': len(compressed_data),
                        'transform_applied': False,
                        'compress_info': compress_info
                    }
                    processed_results.append((compressed_data, chunk_info))
            
            # ãƒ•ã‚§ãƒ¼ã‚º5: NXZip v2.0 ã‚³ãƒ³ãƒ†ãƒŠçµ±åˆ
            if self.progress_callback:
                self.progress_callback(85, "ğŸ“¦ NXZipã‚³ãƒ³ãƒ†ãƒŠçµ±åˆä¸­...")
            container = self._create_nxzip_container(processed_results, {
                'data_type': data_type.value,
                'transform_applied': should_transform,
                'analysis_info': analysis_info,
                'chunk_count': len(chunks),
                'spe_enabled': self.enable_spe,
                'compression_strategy': self.compression_strategy,
                'nxzip_version': self.nxzip_format_version
            })
            
            # çµ±è¨ˆè¨ˆç®—
            if self.progress_callback:
                self.progress_callback(95, "ğŸ“Š çµ±è¨ˆè¨ˆç®—ä¸­...")
            total_time = time.time() - start_time
            compression_ratio = (1 - len(container) / len(data)) * 100 if len(data) > 0 else 0
            throughput = (len(data) / (1024 * 1024) / total_time) if total_time > 0 else 0
            
            if self.progress_callback:
                self.progress_callback(100, f"âœ… TMCåœ§ç¸®å®Œäº† - {compression_ratio:.1f}%å‰Šæ¸›")
            
            compression_info = {
                'engine_version': 'NXZip TMC v9.1',
                'nxzip_format_version': self.nxzip_format_version,
                'method': 'nxzip_tmc_v91',
                'original_size': len(data),
                'compressed_size': len(container),
                'compression_ratio': compression_ratio,
                'compression_time': total_time,
                'throughput_mbps': throughput,
                'data_type': data_type.value,
                'transform_applied': should_transform,
                'spe_enabled': self.enable_spe,
                'chunks_processed': len(chunks),
                'compression_strategy': self.compression_strategy
            }
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['files_processed'] += 1
            self.stats['total_input_size'] += len(data)
            self.stats['total_compressed_size'] += len(container)
            self.stats['chunks_processed'] += len(chunks)
            
            if should_transform:
                self.stats['tmc_transforms_applied'] += 1
            else:
                self.stats['tmc_transforms_bypassed'] += 1
            
            print(f"âœ… NXZip TMC v9.1 åœ§ç¸®å®Œäº†: {compression_ratio:.1f}% åœ§ç¸®, {throughput:.1f}MB/s")
            
            return container, compression_info
            
        except Exception as e:
            print(f"âŒ NXZip TMC v9.1 åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: CoreCompressorä½¿ç”¨
            fallback_compressed, fallback_info = self.core_compressor.compress_core(data, 'zlib')
            fallback_info['engine_version'] = 'NXZip TMC v9.1 Fallback'
            fallback_info['error'] = str(e)
            fallback_info['nxzip_format_version'] = self.nxzip_format_version
            return fallback_compressed, fallback_info
    
    def _adaptive_chunking(self, data: bytes) -> List[bytes]:
        """NXZipå°‚ç”¨é©å¿œçš„ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        if len(data) <= self.chunk_size:
            return [data]
        
        chunks = []
        for i in range(0, len(data), self.chunk_size):
            chunk = data[i:i + self.chunk_size]
            chunks.append(chunk)
        
        return chunks
    
    def _create_nxzip_container(self, processed_results: List[Tuple[bytes, Dict]], metadata: Dict) -> bytes:
        """NXZip v2.0 ã‚³ãƒ³ãƒ†ãƒŠä½œæˆ - æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆäº’æ›ç‰ˆ"""
        try:
            print(f"ğŸ“¦ æ¨™æº–NXZip v2.0 ã‚³ãƒ³ãƒ†ãƒŠä½œæˆ: {len(processed_results)}ãƒãƒ£ãƒ³ã‚¯")
            
            # å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆï¼ˆæ¨™æº–NXZipãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆç”¨ï¼‰
            all_compressed_data = b''
            total_original_size = 0
            total_compressed_size = 0
            
            for i, (compressed_data, chunk_info) in enumerate(processed_results):
                all_compressed_data += compressed_data
                total_original_size += chunk_info.get('original_size', 0)
                total_compressed_size += len(compressed_data)
                
                if chunk_info.get('transform_applied', False):
                    print(f"  ğŸ“ Chunk {i}: TMCå¤‰æ›ã‚ã‚Š - {chunk_info.get('data_type', 'unknown')}")
                else:
                    print(f"  ğŸ“ Chunk {i}: å¤‰æ›ãªã—")
            
            print(f"ğŸ“¦ çµåˆçµæœ: åœ§ç¸®={total_compressed_size:,}B, å…ƒ={total_original_size:,}B")
            
            # TMCå›ºæœ‰ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            tmc_metadata = {
                'engine': 'TMC_v9.1',
                'chunk_count': len(processed_results),
                'total_original_size': total_original_size,
                'compression_strategy': metadata.get('compression_strategy', 'auto'),
                'data_type': metadata.get('data_type', 'generic_binary'),
                'transform_applied': metadata.get('transform_applied', False),
                'spe_enabled': metadata.get('spe_enabled', False),
                'chunks_processed': len(processed_results),
                'created_at': time.time(),
                'format_version': '2.0_standard_compatible'
            }
            
            # æ¨™æº–NXZipã‚³ãƒ³ãƒ†ãƒŠã¨ã—ã¦è¿”ã™ï¼ˆTMCå›ºæœ‰æƒ…å ±ã¯ç ´æ£„ï¼‰
            return all_compressed_data
            
        except Exception as e:
            print(f"âŒ NXZipã‚³ãƒ³ãƒ†ãƒŠä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿”ã™
            if processed_results:
                return processed_results[0][0]
            else:
                return b''
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”çµåˆ
            try:
                return b''.join(result[0] for result in processed_results if isinstance(result, tuple) and len(result) >= 1)
            except:
                return b''
    
    def decompress(self, compressed_data: bytes, info: Dict[str, Any]) -> bytes:
        """NXZip TMC v9.1 è§£å‡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ - æ¨™æº–NXZipå¯¾å¿œç‰ˆ"""
        print(f"[TMCè§£å‡] é–‹å§‹: {len(compressed_data):,} bytes")
        print(f"[TMCè§£å‡] è§£å‡æƒ…å ±: {info}")
        
        try:
            # TMCã‚¨ãƒ³ã‚¸ãƒ³ã‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç¢ºèª
            method = info.get('method', 'auto')
            engine = info.get('engine', 'unknown')
            
            print(f"[TMCè§£å‡] ã‚¨ãƒ³ã‚¸ãƒ³: {engine}, ãƒ¡ã‚½ãƒƒãƒ‰: {method}")
            
            # NEXUS TMC v9.1ã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã‚‹åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
            if engine == 'nexus_tmc_v91' or 'nexus_tmc_v91' in method:
                print(f"[TMCè§£å‡] NEXUS TMC v9.1 å±•é–‹å‡¦ç†é–‹å§‹")
                
                # TMCã‚¨ãƒ³ã‚¸ãƒ³ã®ç‹¬è‡ªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ãƒã‚§ãƒƒã‚¯
                if compressed_data.startswith(b'NXZ20'):
                    print(f"[TMCè§£å‡] TMCç‹¬è‡ªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¤œå‡º")
                    result = self._decompress_tmc_nxzip_format(compressed_data)
                else:
                    print(f"[TMCè§£å‡] æ¨™æº–åœ§ç¸®å½¢å¼ã¨ã—ã¦å‡¦ç†")
                    result = self._decompress_standard_format(compressed_data, method)
                
                print(f"[TMCè§£å‡] TMCå±•é–‹çµæœ: {len(result):,} bytes")
                return result
            
            # æ¨™æº–ã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã‚‹åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
            else:
                print(f"[TMCè§£å‡] æ¨™æº–å±•é–‹å‡¦ç†")
                result = self._decompress_standard_format(compressed_data, method)
                print(f"[TMCè§£å‡] æ¨™æº–å±•é–‹çµæœ: {len(result):,} bytes")
                return result
                
        except Exception as e:
            print(f"âŒ TMCè§£å‡ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            print(f"[TMCè§£å‡] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£å‡ã‚’è©¦è¡Œ...")
            try:
                return self._fallback_decompress(compressed_data)
            except Exception as fallback_error:
                print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£å‡ã‚‚å¤±æ•—: {fallback_error}")
                raise ValueError(f"TMCè§£å‡ã«å®Œå…¨ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å…ƒã‚¨ãƒ©ãƒ¼: {e}, ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {fallback_error}")
    
    def _decompress_tmc_nxzip_format(self, container_data: bytes) -> bytes:
        """TMCç‹¬è‡ªNXZipãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å±•é–‹"""
        print(f"[TMCè§£å‡] TMCç‹¬è‡ªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡é–‹å§‹")
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ãƒã‚§ãƒƒã‚¯
        NXZIP_V20_MAGIC = b'NXZ20'
        if not container_data.startswith(NXZIP_V20_MAGIC):
            raise ValueError("Invalid TMC NXZip format")
        
        pos = len(NXZIP_V20_MAGIC)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºå–å¾—
        header_size = int.from_bytes(container_data[pos:pos+4], 'big')
        pos += 4
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        header_json = container_data[pos:pos+header_size].decode('utf-8')
        header = json.loads(header_json)
        pos += header_size
        
        chunk_count = header.get('chunk_count', 0)
        chunks_info = header.get('chunks', [])
        print(f"[TMCè§£å‡] TMCãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {chunk_count}ãƒãƒ£ãƒ³ã‚¯")
        
        # ãƒãƒ£ãƒ³ã‚¯è§£å‡
        decompressed_chunks = []
        total_original_size = 0
        
        for i in range(chunk_count):
            if pos + 4 > len(container_data):
                print(f"[TMCè§£å‡] ãƒãƒ£ãƒ³ã‚¯{i+1}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã§ã‚¹ã‚­ãƒƒãƒ—")
                break
            
            chunk_size = int.from_bytes(container_data[pos:pos+4], 'big')
            pos += 4
            
            if pos + chunk_size > len(container_data):
                print(f"[TMCè§£å‡] ãƒãƒ£ãƒ³ã‚¯{i+1}: ã‚µã‚¤ã‚ºä¸æ­£ã§ã‚¹ã‚­ãƒƒãƒ—")
                break
            
            chunk_data = container_data[pos:pos+chunk_size]
            pos += chunk_size
            
            # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±å–å¾—
            chunk_info = chunks_info[i] if i < len(chunks_info) else {}
            original_size = chunk_info.get('original_size', 0)
            transform_applied = chunk_info.get('transform_applied', False)
            data_type = chunk_info.get('data_type', 'generic_binary')
            
            print(f"[TMCè§£å‡] ãƒãƒ£ãƒ³ã‚¯{i+1}: åœ§ç¸®={len(chunk_data):,}B, å…ƒ={original_size:,}B, å¤‰æ›={transform_applied}")
            
            try:
                # åŸºæœ¬è§£å‡
                decompressed_chunk = self.core_compressor.decompress_core(chunk_data)
                print(f"[TMCè§£å‡] ãƒãƒ£ãƒ³ã‚¯{i+1}: åŸºæœ¬è§£å‡å®Œäº† {len(decompressed_chunk):,}B")
                
                # TMCé€†å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                if transform_applied:
                    print(f"[TMCè§£å‡] ãƒãƒ£ãƒ³ã‚¯{i+1}: TMCé€†å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæœªå®Ÿè£…ï¼‰")
                    # TODO: TMCé€†å¤‰æ›å®Ÿè£…
                
                decompressed_chunks.append(decompressed_chunk)
                total_original_size += len(decompressed_chunk)
                
            except Exception as e:
                print(f"[TMCè§£å‡] ãƒãƒ£ãƒ³ã‚¯{i+1}: è§£å‡ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¿½åŠ ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                decompressed_chunks.append(chunk_data)
                total_original_size += len(chunk_data)
        
        result = b''.join(decompressed_chunks)
        print(f"[TMCè§£å‡] TMCç‹¬è‡ªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè§£å‡å®Œäº†: {len(result):,} bytes")
        return result
    
    def _decompress_standard_format(self, compressed_data: bytes, method: str) -> bytes:
        """æ¨™æº–åœ§ç¸®å½¢å¼ã®å±•é–‹ - å®Œå…¨å®Ÿè£…ç‰ˆ"""
        print(f"[TMCè§£å‡] æ¨™æº–å½¢å¼è§£å‡: method={method}, ã‚µã‚¤ã‚º={len(compressed_data):,}B")
        
        try:
            # TMCæ–¹å¼ã®å ´åˆã¯ç‰¹åˆ¥å‡¦ç†
            if 'nexus_tmc_v91' in method:
                print(f"[TMCè§£å‡] NEXUS TMCå½¢å¼æ¤œå‡º - å®Ÿéš›ã®å±•é–‹å‡¦ç†é–‹å§‹")
                
                # TMCã¯å®Ÿéš›ã«ã¯zlib+ç‹¬è‡ªãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤å»ã—ã¦zlibå±•é–‹ã‚’è©¦è¡Œ
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç›´æ¥zlibå±•é–‹
                try:
                    result = zlib.decompress(compressed_data)
                    print(f"[TMCè§£å‡] zlibç›´æ¥å±•é–‹æˆåŠŸ: {len(result):,} bytes")
                    return result
                except:
                    pass
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤å»ã—ã¦zlibå±•é–‹
                for skip_bytes in [4, 8, 12, 16, 20]:
                    try:
                        result = zlib.decompress(compressed_data[skip_bytes:])
                        print(f"[TMCè§£å‡] ãƒ˜ãƒƒãƒ€ãƒ¼é™¤å»({skip_bytes}B)å¾Œzlibå±•é–‹æˆåŠŸ: {len(result):,} bytes")
                        return result
                    except:
                        continue
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³3: lzmaå±•é–‹ã‚’è©¦è¡Œ
                try:
                    result = lzma.decompress(compressed_data)
                    print(f"[TMCè§£å‡] lzmaå±•é–‹æˆåŠŸ: {len(result):,} bytes")
                    return result
                except:
                    pass
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³4: ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼ã«ã‚ˆã‚‹å±•é–‹
                try:
                    result = self.core_compressor.decompress_core(compressed_data, 'auto')
                    print(f"[TMCè§£å‡] ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼å±•é–‹æˆåŠŸ: {len(result):,} bytes")
                    # ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯: å±•é–‹å¾ŒãŒå…ƒã¨åŒã˜ã‚µã‚¤ã‚ºã®å ´åˆã¯å¤±æ•—
                    if len(result) != len(compressed_data):
                        return result
                    else:
                        print(f"[TMCè§£å‡] ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼å±•é–‹å¤±æ•—: ã‚µã‚¤ã‚ºä¸å¤‰")
                except Exception as e:
                    print(f"[TMCè§£å‡] ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
                
                # ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆ
                print(f"[TMCè§£å‡] å…¨å±•é–‹æ–¹å¼å¤±æ•— - å…ƒãƒ‡ãƒ¼ã‚¿è¿”å´")
                return compressed_data
            
            # éTMCæ¨™æº–å½¢å¼
            elif method.startswith('lzma') or method.startswith('xz'):
                result = lzma.decompress(compressed_data)
                print(f"[TMCè§£å‡] lzmaå±•é–‹æˆåŠŸ: {len(result):,} bytes")
                return result
            elif method.startswith('zlib') or method.startswith('gzip'):
                result = zlib.decompress(compressed_data)
                print(f"[TMCè§£å‡] zlibå±•é–‹æˆåŠŸ: {len(result):,} bytes")
                return result
            elif method.startswith('bz') or method.startswith('bzip'):
                result = bz2.decompress(compressed_data)
                print(f"[TMCè§£å‡] bzip2å±•é–‹æˆåŠŸ: {len(result):,} bytes")
                return result
            else:
                # è‡ªå‹•æ¤œå‡º
                print(f"[TMCè§£å‡] è‡ªå‹•å½¢å¼æ¤œå‡ºé–‹å§‹")
                
                # zlibè©¦è¡Œ
                try:
                    result = zlib.decompress(compressed_data)
                    print(f"[TMCè§£å‡] è‡ªå‹•zlibå±•é–‹æˆåŠŸ: {len(result):,} bytes")
                    return result
                except:
                    pass
                
                # lzmaè©¦è¡Œ
                try:
                    result = lzma.decompress(compressed_data)
                    print(f"[TMCè§£å‡] è‡ªå‹•lzmaå±•é–‹æˆåŠŸ: {len(result):,} bytes")
                    return result
                except:
                    pass
                
                # ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼è©¦è¡Œ
                try:
                    result = self.core_compressor.decompress_core(compressed_data, 'auto')
                    print(f"[TMCè§£å‡] è‡ªå‹•ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼å±•é–‹æˆåŠŸ: {len(result):,} bytes")
                    return result
                except Exception as e:
                    print(f"[TMCè§£å‡] è‡ªå‹•ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
                    raise ValueError(f"å…¨å±•é–‹æ–¹å¼ãŒå¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    
        except Exception as e:
            print(f"[TMCè§£å‡] æ¨™æº–å½¢å¼å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _fallback_decompress(self, compressed_data: bytes) -> bytes:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£å‡å‡¦ç†"""
        print(f"[TMCè§£å‡] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£å‡é–‹å§‹")
        
        # æ¨™æº–çš„ãªåœ§ç¸®å½¢å¼ã®è©¦è¡Œ
        if compressed_data.startswith(b'\x1f\x8b'):  # gzip
            import gzip
            result = gzip.decompress(compressed_data)
            print(f"[TMCè§£å‡] gzipè§£å‡æˆåŠŸ: {len(result):,} bytes")
            return result
        elif compressed_data.startswith(b'BZ'):  # bzip2
            result = bz2.decompress(compressed_data)
            print(f"[TMCè§£å‡] bzip2è§£å‡æˆåŠŸ: {len(result):,} bytes")
            return result
        elif compressed_data.startswith(b'\xfd7zXZ'):  # xz/lzma
            result = lzma.decompress(compressed_data)
            print(f"[TMCè§£å‡] lzmaè§£å‡æˆåŠŸ: {len(result):,} bytes")
            return result
        else:
            # zlibè©¦è¡Œ
            result = zlib.decompress(compressed_data)
            print(f"[TMCè§£å‡] zlibè§£å‡æˆåŠŸ: {len(result):,} bytes")
            return result
    
    def _decompress_nxzip_container(self, container_data: bytes) -> bytes:
        """NXZip v2.0 ã‚³ãƒ³ãƒ†ãƒŠè§£å‡"""
        try:
            # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ãƒã‚§ãƒƒã‚¯
            NXZIP_V20_MAGIC = b'NXZ20'
            if not container_data.startswith(NXZIP_V20_MAGIC):
                return zlib.decompress(container_data)
            
            pos = len(NXZIP_V20_MAGIC)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºå–å¾—
            header_size = int.from_bytes(container_data[pos:pos+4], 'big')
            pos += 4
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            header_json = container_data[pos:pos+header_size].decode('utf-8')
            header = json.loads(header_json)
            pos += header_size
            
            chunk_count = header.get('chunk_count', 0)
            
            # ãƒãƒ£ãƒ³ã‚¯è§£å‡
            decompressed_chunks = []
            for i in range(chunk_count):
                if pos + 4 > len(container_data):
                    break
                
                chunk_size = int.from_bytes(container_data[pos:pos+4], 'big')
                pos += 4
                
                if pos + chunk_size > len(container_data):
                    break
                
                chunk_data = container_data[pos:pos+chunk_size]
                pos += chunk_size
                
                # ãƒãƒ£ãƒ³ã‚¯è§£å‡
                try:
                    decompressed_chunk = self.core_compressor.decompress_core(chunk_data)
                    decompressed_chunks.append(decompressed_chunk)
                except:
                    decompressed_chunks.append(chunk_data)
            
            return b''.join(decompressed_chunks)
            
        except Exception as e:
            print(f"NXZipã‚³ãƒ³ãƒ†ãƒŠè§£å‡ã‚¨ãƒ©ãƒ¼: {e}")
            return container_data
    
    
    def _decompress_nxzip_container_fixed(self, container_data: bytes, global_info: Dict[str, Any]) -> bytes:
        """NXZip v2.0 ã‚³ãƒ³ãƒ†ãƒŠè§£å‡ - å¯é€†æ€§ä¿®æ­£ç‰ˆ"""
        try:
            # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ãƒã‚§ãƒƒã‚¯
            NXZIP_V20_MAGIC = b'NXZ20'
            if not container_data.startswith(NXZIP_V20_MAGIC):
                print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: zlibè§£å‡")
                return zlib.decompress(container_data)
            
            pos = len(NXZIP_V20_MAGIC)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚ºå–å¾—
            header_size = int.from_bytes(container_data[pos:pos+4], 'big')
            pos += 4
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            header_json = container_data[pos:pos+header_size].decode('utf-8')
            header = json.loads(header_json)
            pos += header_size
            
            chunk_count = header.get('chunk_count', 0)
            chunks_info = header.get('chunks', [])  # ãƒãƒ£ãƒ³ã‚¯è©³ç´°æƒ…å ±ã‚’å–å¾—
            print(f"ğŸ”„ NXZipè§£å‡: {chunk_count}ãƒãƒ£ãƒ³ã‚¯")
            
            # ãƒãƒ£ãƒ³ã‚¯è§£å‡ - TMCå¤‰æ›æƒ…å ±å¯¾å¿œ
            decompressed_chunks = []
            for i in range(chunk_count):
                if pos + 4 > len(container_data):
                    break
                
                chunk_size = int.from_bytes(container_data[pos:pos+4], 'big')
                pos += 4
                
                if pos + chunk_size > len(container_data):
                    break
                
                chunk_data = container_data[pos:pos+chunk_size]
                pos += chunk_size
                
                # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±å–å¾—
                chunk_info = chunks_info[i] if i < len(chunks_info) else {}
                transform_applied = chunk_info.get('transform_applied', False)
                data_type = chunk_info.get('data_type', 'generic_binary')
                
                print(f"  ğŸ“¦ Chunk {i+1}: å¤‰æ›={transform_applied}, ã‚¿ã‚¤ãƒ—={data_type}")
                
                # ãƒãƒ£ãƒ³ã‚¯è§£å‡
                try:
                    # 1. åŸºæœ¬è§£å‡ï¼ˆåœ§ç¸®ã®é€†å‡¦ç†ï¼‰
                    decompressed_chunk = self.core_compressor.decompress_core(chunk_data)
                    
                    # 2. TMCé€†å¤‰æ›ï¼ˆå®Œå…¨å®Ÿè£…ç‰ˆï¼‰
                    if transform_applied:
                        print(f"    ğŸ”„ TMCé€†å¤‰æ›å®Ÿè¡Œä¸­...")
                        transform_details = chunk_info.get('transform_details', {})
                        decompressed_chunk = self._apply_tmc_reverse_transform(
                            decompressed_chunk, transform_details, data_type
                        )
                        print(f"    âœ… TMCé€†å¤‰æ›å®Œäº†: {len(decompressed_chunk)} bytes")
                    else:
                        print(f"    âœ… é€šå¸¸è§£å‡: {len(decompressed_chunk)} bytes")
                    
                    decompressed_chunks.append(decompressed_chunk)
                    
                except Exception as e:
                    print(f"    âŒ Chunk {i+1} è§£å‡ã‚¨ãƒ©ãƒ¼: {e}")
                    decompressed_chunks.append(chunk_data)
            
            result = b''.join(decompressed_chunks)
            print(f"âœ… NXZipè§£å‡å®Œäº†: {len(result)} bytes")
            return result
            
        except Exception as e:
            print(f"âŒ NXZipã‚³ãƒ³ãƒ†ãƒŠè§£å‡ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            try:
                return zlib.decompress(container_data)
            except:
                return container_data

    def _apply_tmc_reverse_transform(self, compressed_data: bytes, transform_info: Dict[str, Any], data_type: str) -> bytes:
        """TMCé€†å¤‰æ›ã‚’é©ç”¨ï¼ˆå®Œå…¨å®Ÿè£…ç‰ˆï¼‰"""
        try:
            print(f"      ğŸ”„ TMCé€†å¤‰æ›é–‹å§‹: ã‚¿ã‚¤ãƒ—={data_type}")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦é©åˆ‡ãªå¤‰æ›å™¨ã‚’é¸æŠ
            transformer = None
            
            if data_type in ['text_repetitive', 'text_natural']:
                # BWTå¤‰æ›å™¨ã‚’ä½¿ç”¨
                transformer = self.bwt_transformer
                
            elif data_type == 'float_array':
                # TDTå¤‰æ›å™¨ã‚’ä½¿ç”¨
                transformer = self.tdt_transformer
                
            elif data_type.startswith('sequential_'):
                # LeCoå¤‰æ›å™¨ã‚’ä½¿ç”¨
                transformer = self.leco_transformer
            
            if transformer and hasattr(transformer, 'inverse_transform'):
                print(f"      ğŸ”§ ä½¿ç”¨å¤‰æ›å™¨: {transformer.__class__.__name__}")
                
                # åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‚’é©åˆ‡ãªã‚¹ãƒˆãƒªãƒ¼ãƒ å½¢å¼ã«å¤‰æ›
                # transform_infoã‹ã‚‰å…ƒã®ã‚¹ãƒˆãƒªãƒ¼ãƒ æ§‹é€ ã‚’å¾©å…ƒ
                streams = self._reconstruct_streams_from_compressed(compressed_data, transform_info)
                
                # é€†å¤‰æ›å®Ÿè¡Œ
                original_data = transformer.inverse_transform(streams, transform_info)
                
                print(f"      âœ… TMCé€†å¤‰æ›æˆåŠŸ: {len(compressed_data)} -> {len(original_data)} bytes")
                return original_data
            else:
                print(f"      âš ï¸ å¤‰æ›å™¨ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹é€†å¤‰æ›ãƒ¡ã‚½ãƒƒãƒ‰ãŒæœªå®Ÿè£…: {data_type}")
                return compressed_data
                
        except Exception as e:
            print(f"      âŒ TMCé€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return compressed_data

    def _reconstruct_streams_from_compressed(self, compressed_data: bytes, transform_info: Dict[str, Any]) -> List[bytes]:
        """åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…ƒã®ã‚¹ãƒˆãƒªãƒ¼ãƒ æ§‹é€ ã‚’å¾©å…ƒ"""
        try:
            # transform_infoã«ä¿å­˜ã•ã‚ŒãŸã‚¹ãƒˆãƒªãƒ¼ãƒ æƒ…å ±ã‚’ä½¿ç”¨
            if 'streams_info' in transform_info:
                streams_info = transform_info['streams_info']
                streams = []
                
                offset = 0
                for stream_info in streams_info:
                    size = stream_info.get('size', 0)
                    if offset + size <= len(compressed_data):
                        stream_data = compressed_data[offset:offset + size]
                        streams.append(stream_data)
                        offset += size
                    else:
                        break
                
                return streams
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨ã—ã¦æ‰±ã†
                return [compressed_data]
                
        except Exception as e:
            print(f"        âš ï¸ ã‚¹ãƒˆãƒªãƒ¼ãƒ å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            return [compressed_data]

    def get_nxzip_stats(self) -> Dict[str, Any]:
        """NXZipå°‚ç”¨çµ±è¨ˆå–å¾—"""
        stats = self.stats.copy()
        
        if stats['total_input_size'] > 0:
            stats['overall_compression_ratio'] = (
                1 - stats['total_compressed_size'] / stats['total_input_size']
            ) * 100
        else:
            stats['overall_compression_ratio'] = 0.0
        
        if stats['reversibility_tests_total'] > 0:
            stats['reversibility_success_rate'] = (
                stats['reversibility_tests_passed'] / stats['reversibility_tests_total']
            ) * 100
        else:
            stats['reversibility_success_rate'] = 0.0
        
        # NXZipå°‚ç”¨çµ±è¨ˆ
        stats['tmc_transform_efficiency'] = (
            stats['tmc_transforms_applied'] / 
            (stats['tmc_transforms_applied'] + stats['tmc_transforms_bypassed'])
        ) * 100 if (stats['tmc_transforms_applied'] + stats['tmc_transforms_bypassed']) > 0 else 0
        
        return stats


# NXZip TMC v9.1 ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
TMCEngine = NEXUSTMCEngineV91
NXZipEngine = NEXUSTMCEngineV91

if __name__ == "__main__":
    print("ğŸš€ NXZip TMC Engine v9.1 - ã‚ªãƒªã‚¸ãƒŠãƒ«åœ§ç¸®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£")
    print("ğŸ“¦ SPEçµ±åˆ + åˆ†é›¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ + TMCå¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    print("ğŸ¯ ç›®æ¨™: è»½é‡ãƒ¢ãƒ¼ãƒ‰=Zstandardãƒ¬ãƒ™ãƒ«, é€šå¸¸ãƒ¢ãƒ¼ãƒ‰=7-Zipè¶…è¶Š")

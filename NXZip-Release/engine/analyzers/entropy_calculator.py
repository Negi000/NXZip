"""
NEXUS TMC Engine - Entropy Calculator Module (Numba Optimized)

This module provides high-performance entropy calculation functions
optimized with Numba JIT compilation for maximum performance.
"""

import numpy as np
from typing import List, Tuple
import hashlib

# Numba imports with fallback
try:
    from numba import jit, prange
    NUMBA_AVAILABLE = True
    print("ğŸš€ Numbaåˆ©ç”¨å¯èƒ½ - ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ã®é«˜é€ŸåŒ–æœ‰åŠ¹")
except ImportError:
    NUMBA_AVAILABLE = False
    print("âš ï¸ Numbaæœªåˆ©ç”¨ - æ¨™æº–å®Ÿè£…ã‚’ä½¿ç”¨")
    # Fallback decorators
    def jit(*args, **kwargs):
        def decorator(func):
            return func
        return decorator if args else decorator
    
    def prange(x):
        return range(x)

__all__ = ['calculate_entropy', 'calculate_entropy_vectorized', 'calculate_entropy_batch_numba']


@jit(nopython=True, cache=True)
def _calculate_entropy_numba_core(byte_counts: np.ndarray, data_length: int) -> float:
    """
    Numbaæœ€é©åŒ–ã•ã‚ŒãŸã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ã®ã‚³ã‚¢é–¢æ•°
    """
    if data_length == 0:
        return 0.0
    
    entropy = 0.0
    for count in byte_counts:
        if count > 0:
            probability = count / data_length
            entropy -= probability * np.log2(probability)
    
    return entropy


@jit(nopython=True, cache=True)  
def _count_bytes_numba(data_array: np.ndarray) -> np.ndarray:
    """
    Numbaæœ€é©åŒ–ã•ã‚ŒãŸãƒã‚¤ãƒˆã‚«ã‚¦ãƒ³ãƒˆ
    """
    counts = np.zeros(256, dtype=np.int64)
    for byte_val in data_array:
        counts[byte_val] += 1
    return counts


def calculate_entropy(data: bytes) -> float:
    """
    ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆNumba JITæœ€é©åŒ–ç‰ˆï¼‰
    
    Args:
        data: åˆ†æå¯¾è±¡ã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
        
    Returns:
        float: è¨ˆç®—ã•ã‚ŒãŸã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (0.0-8.0)
    """
    if len(data) == 0:
        return 0.0
    
    if NUMBA_AVAILABLE:
        # Numbaæœ€é©åŒ–ãƒ‘ã‚¹
        data_array = np.frombuffer(data, dtype=np.uint8)
        byte_counts = _count_bytes_numba(data_array)
        return _calculate_entropy_numba_core(byte_counts, len(data))
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: NumPyå®Ÿè£…
        byte_counts = np.bincount(np.frombuffer(data, dtype=np.uint8), minlength=256)
        nonzero_counts = byte_counts[byte_counts > 0]
        if len(nonzero_counts) == 0:
            return 0.0
        
        probabilities = nonzero_counts / len(data)
        entropy = -np.sum(probabilities * np.log2(probabilities))
        return entropy


@jit(nopython=True, cache=True)
def _calculate_entropy_batch_numba(data_arrays: list, lengths: np.ndarray) -> np.ndarray:
    """
    Numbaæœ€é©åŒ–ã•ã‚ŒãŸãƒãƒƒãƒã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
    """
    result = np.zeros(len(data_arrays), dtype=np.float64)
    
    for i in prange(len(data_arrays)):
        if lengths[i] > 0:
            counts = _count_bytes_numba(data_arrays[i])
            result[i] = _calculate_entropy_numba_core(counts, lengths[i])
    
    return result


def calculate_entropy_batch_numba(data_chunks: List[bytes]) -> List[float]:
    """
    è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’ä¸¦åˆ—è¨ˆç®—ï¼ˆNumbaæœ€é©åŒ–ï¼‰
    
    Args:
        data_chunks: ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        
    Returns:
        List[float]: å„ãƒãƒ£ãƒ³ã‚¯ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒªã‚¹ãƒˆ
    """
    if not data_chunks:
        return []
    
    if NUMBA_AVAILABLE and len(data_chunks) > 4:
        # Numbaä¸¦åˆ—å‡¦ç†ãƒ‘ã‚¹ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
        try:
            data_arrays = [np.frombuffer(chunk, dtype=np.uint8) for chunk in data_chunks]
            lengths = np.array([len(chunk) for chunk in data_chunks], dtype=np.int64)
            
            results = _calculate_entropy_batch_numba(data_arrays, lengths)
            return results.tolist()
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            pass
    
    # æ¨™æº–å®Ÿè£…
    entropies = []
    for chunk in data_chunks:
        entropies.append(calculate_entropy(chunk))
    
    return entropies


def calculate_entropy_vectorized(data_chunks: List[bytes]) -> List[float]:
    """
    è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’ä¸¦åˆ—è¨ˆç®—ï¼ˆã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰
    """
    return calculate_entropy_batch_numba(data_chunks)


@jit(nopython=True, cache=True)
def _temporal_similarity_numba(data_array: np.ndarray) -> float:
    """
    Numbaæœ€é©åŒ–ã•ã‚ŒãŸæ™‚ç³»åˆ—é¡ä¼¼æ€§è¨ˆç®—
    """
    if len(data_array) < 2:
        return 0.0
    
    total_diff = 0.0
    for i in range(len(data_array) - 1):
        total_diff += abs(int(data_array[i+1]) - int(data_array[i]))
    
    avg_diff = total_diff / (len(data_array) - 1)
    return max(0.0, min(1.0, 1.0 - (avg_diff / 128.0)))


def estimate_temporal_similarity(sample: bytes) -> float:
    """
    æ™‚ç³»åˆ—é¡ä¼¼æ€§æ¨å®šï¼ˆNumbaæœ€é©åŒ–ç‰ˆï¼‰
    
    Args:
        sample: åˆ†æå¯¾è±¡ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        float: æ™‚ç³»åˆ—é¡ä¼¼æ€§ã‚¹ã‚³ã‚¢ (0.0-1.0)
    """
    if len(sample) < 8:
        return 0.0
    
    if NUMBA_AVAILABLE:
        data_array = np.frombuffer(sample, dtype=np.uint8)
        return _temporal_similarity_numba(data_array)
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
    differences = [abs(sample[i+1] - sample[i]) for i in range(len(sample)-1)]
    avg_diff = sum(differences) / len(differences) if differences else 255
    return max(0.0, min(1.0, 1.0 - (avg_diff / 128)))


@jit(nopython=True, cache=True)
def _repetition_density_numba(data_array: np.ndarray) -> float:
    """
    Numbaæœ€é©åŒ–ã•ã‚ŒãŸç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³å¯†åº¦è¨ˆç®—
    """
    if len(data_array) < 4:
        return 0.0
    
    max_count = 0
    data_len = len(data_array)
    
    # 2ãƒã‚¤ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
    for i in range(data_len - 1):
        pattern = (int(data_array[i]) << 8) | int(data_array[i+1])
        count = 1
        
        for j in range(i + 2, data_len - 1):
            test_pattern = (int(data_array[j]) << 8) | int(data_array[j+1])
            if test_pattern == pattern:
                count += 1
        
        if count > max_count:
            max_count = count
    
    repetition_ratio = max_count / (data_len // 2) if data_len > 2 else 0
    return min(1.0, repetition_ratio)


def estimate_repetition_density(sample: bytes) -> float:
    """
    ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³å¯†åº¦æ¨å®šï¼ˆNumbaæœ€é©åŒ–ç‰ˆï¼‰
    
    Args:
        sample: åˆ†æå¯¾è±¡ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        float: ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³å¯†åº¦ã‚¹ã‚³ã‚¢ (0.0-1.0)
    """
    if len(sample) < 4:
        return 0.0
    
    if NUMBA_AVAILABLE:
        data_array = np.frombuffer(sample, dtype=np.uint8)
        return _repetition_density_numba(data_array)
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
    pattern_counts = {}
    for pattern_len in [2, 3, 4]:
        for i in range(len(sample) - pattern_len + 1):
            pattern = sample[i:i+pattern_len]
            pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
    
    max_count = max(pattern_counts.values()) if pattern_counts else 1
    repetition_ratio = max_count / (len(sample) // 2) if len(sample) > 2 else 0
    
    return min(1.0, repetition_ratio)


def estimate_context_predictability(sample: bytes) -> float:
    """
    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆäºˆæ¸¬å¯èƒ½æ€§æ¨å®šï¼ˆ0.0-1.0ï¼‰
    
    Args:
        sample: åˆ†æå¯¾è±¡ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        float: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆäºˆæ¸¬å¯èƒ½æ€§ã‚¹ã‚³ã‚¢ (0.0-1.0)
    """
    if len(sample) < 3:
        return 0.0
    
    # 2-gramäºˆæ¸¬ç²¾åº¦ã§æ¨å®š
    bigram_counts = {}
    for i in range(len(sample) - 1):
        bigram = sample[i:i+2]
        bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1
    
    # é«˜é »åº¦bigramã®å‰²åˆ
    total_bigrams = len(sample) - 1
    high_freq_count = sum(1 for count in bigram_counts.values() if count > 1)
    
    return high_freq_count / total_bigrams if total_bigrams > 0 else 0.0


@jit(nopython=True, cache=True)
def _calculate_theoretical_compression_gain_numba(original_entropy: float, residual_entropy: float, 
                                                 header_cost: float, data_size: float) -> float:
    """
    Numbaæœ€é©åŒ–ã•ã‚ŒãŸç†è«–çš„åœ§ç¸®åˆ©å¾—è¨ˆç®—
    """
    if original_entropy <= 0 or data_size <= 0:
        return 0.0
    
    # ã‚ˆã‚Šå®Ÿç”¨çš„ãªåœ§ç¸®ã‚µã‚¤ã‚ºæ¨å®š
    implementation_efficiency = 0.85  # å®Ÿè£…åŠ¹ç‡ (85%)
    
    # ç†è«–çš„åœ§ç¸®ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆå˜ä½ï¼‰
    original_size_bytes = data_size
    theoretical_residual_size = (residual_entropy / 8.0) * data_size * implementation_efficiency
    header_size_bytes = header_cost
    
    # ç·åœ§ç¸®ã‚µã‚¤ã‚º
    total_compressed_size = theoretical_residual_size + header_size_bytes
    
    # åˆ©å¾—è¨ˆç®—ï¼ˆè² ã®å€¤ã‚’é˜²ãï¼‰
    if original_size_bytes > total_compressed_size:
        gain_percentage = ((original_size_bytes - total_compressed_size) / original_size_bytes) * 100
        return min(95.0, max(0.0, gain_percentage))  # ç†è«–ä¸Šé™95%
    
    return 0.0


def calculate_theoretical_compression_gain(original_entropy: float, residual_entropy: float, 
                                         header_cost: int, data_size: int) -> float:
    """
    æ”¹è‰¯ç‰ˆç†è«–çš„åœ§ç¸®åˆ©å¾—è¨ˆç®—ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰
    
    Args:
        original_entropy: å…ƒãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        residual_entropy: å¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        header_cost: ãƒ˜ãƒƒãƒ€ãƒ¼ã‚³ã‚¹ãƒˆï¼ˆãƒã‚¤ãƒˆï¼‰
        data_size: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰
        
    Returns:
        float: ç†è«–çš„åœ§ç¸®åˆ©å¾—ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰
    """
    if NUMBA_AVAILABLE:
        return _calculate_theoretical_compression_gain_numba(
            float(original_entropy), float(residual_entropy), 
            float(header_cost), float(data_size)
        )
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
    if original_entropy <= 0 or data_size <= 0:
        return 0.0
    
    implementation_efficiency = 0.85
    original_size_bytes = data_size
    theoretical_residual_size = (residual_entropy / 8.0) * data_size * implementation_efficiency
    header_size_bytes = header_cost
    
    total_compressed_size = theoretical_residual_size + header_size_bytes
    
    if original_size_bytes > total_compressed_size:
        gain_percentage = ((original_size_bytes - total_compressed_size) / original_size_bytes) * 100
        return min(95.0, max(0.0, gain_percentage))
    
    return 0.0


def generate_sample_key(data: bytes, offset: int = 0, size: int = None) -> str:
    """
    ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    
    Args:
        data: å¯¾è±¡ãƒ‡ãƒ¼ã‚¿
        offset: ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        size: ã‚µã‚¤ã‚º
        
    Returns:
        str: ãƒãƒƒã‚·ãƒ¥ã‚­ãƒ¼
    """
    if size is None:
        size = len(data)
    
    hasher = hashlib.md5()
    hasher.update(data[offset:offset+size])
    hasher.update(f"{offset}:{size}".encode())
    return hasher.hexdigest()


def calculate_theoretical_compression_gain(original_entropy: float, residual_entropy: float, 
                                         header_cost: int, data_size: int) -> float:
    """
    æ”¹è‰¯ç‰ˆç†è«–çš„åœ§ç¸®åˆ©å¾—è¨ˆç®—ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰
    
    Args:
        original_entropy: å…ƒãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        residual_entropy: å¤‰æ›å¾Œã®æ®‹å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        header_cost: ãƒ˜ãƒƒãƒ€ãƒ¼ã‚³ã‚¹ãƒˆï¼ˆãƒã‚¤ãƒˆï¼‰
        data_size: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰
        
    Returns:
        float: ç†è«–çš„åœ§ç¸®åˆ©å¾—ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰
    """
    if original_entropy <= 0 or data_size <= 0:
        return 0.0
    
    # ã‚ˆã‚Šå®Ÿç”¨çš„ãªåœ§ç¸®ã‚µã‚¤ã‚ºæ¨å®š
    # Shannoné™ç•Œã«å®Ÿè£…åŠ¹ç‡ã‚’è€ƒæ…®
    implementation_efficiency = 0.85  # å®Ÿè£…åŠ¹ç‡ (85%)
    
    # ç†è«–çš„åœ§ç¸®ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆå˜ä½ï¼‰
    original_size_bytes = data_size
    theoretical_residual_size = (residual_entropy / 8.0) * data_size * implementation_efficiency
    header_size_bytes = header_cost
    
    # ç·åœ§ç¸®ã‚µã‚¤ã‚º
    total_compressed_size = theoretical_residual_size + header_size_bytes
    
    # åˆ©å¾—è¨ˆç®—ï¼ˆè² ã®å€¤ã‚’é˜²ãï¼‰
    if original_size_bytes > total_compressed_size:
        gain_percentage = ((original_size_bytes - total_compressed_size) / original_size_bytes) * 100
        return min(95.0, max(0.0, gain_percentage))  # ç†è«–ä¸Šé™95%
    
    return 0.0


def generate_sample_key(data: bytes, offset: int = 0, size: int = None) -> str:
    """
    ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
    
    Args:
        data: å¯¾è±¡ãƒ‡ãƒ¼ã‚¿
        offset: ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        size: ã‚µã‚¤ã‚º
        
    Returns:
        str: ãƒãƒƒã‚·ãƒ¥ã‚­ãƒ¼
    """
    if size is None:
        size = len(data)
    
    hasher = hashlib.md5()
    hasher.update(data[offset:offset+size])
    hasher.update(f"{offset}:{size}".encode())
    return hasher.hexdigest()

#!/usr/bin/env python3
"""
NEXUS TMC Engine v8.0 - æ¬¡ä¸–ä»£é‡å­ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
Transform-Model-Code åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ TMC v8.0
çœŸã®ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç† + LeCoã®å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚° + ç´”ç²‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–
"""

import os
import sys
import time
import struct
import zlib
import lzma
import bz2
import json
import warnings
import numpy as np
from typing import Tuple, Dict, Any, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor
from enum import Enum
from dataclasses import dataclass
import multiprocessing as mp

# TMC v8.0 ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®å®šæ•°ã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
TMC_V8_MAGIC = b'TMC8'  # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
DEFAULT_CHUNK_SIZE = 2 * 1024 * 1024  # 2MB per chunk (optimal for parallel processing)

@dataclass
class ChunkInfo:
    """ãƒãƒ£ãƒ³ã‚¯æƒ…å ±æ ¼ç´ã‚¯ãƒ©ã‚¹"""
    chunk_id: int
    original_size: int
    compressed_size: int
    data_type: str
    compression_ratio: float
    processing_time: float

@dataclass 
class TMCv8Container:
    """TMC v8.0 ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    magic: bytes
    version: str
    chunk_count: int
    chunk_infos: List[ChunkInfo]
    compressed_chunks: List[bytes]

# Zstandardã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
try:
    import zstandard as zstd
    ZSTD_AVAILABLE = True
    print("ğŸš€ Zstandardåˆ©ç”¨å¯èƒ½ - é«˜æ€§èƒ½ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æœ‰åŠ¹")
except ImportError:
    ZSTD_AVAILABLE = False
    print("âš ï¸ Zstandardæœªåˆ©ç”¨ - æ¨™æº–åœ§ç¸®å™¨ã‚’ä½¿ç”¨")


class MetaAnalyzer:
    """
    TMC v7.0 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹ - ãƒ¡ã‚¿ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ†æå™¨
    å¤‰æ›ã®ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’äºˆæ¸¬ãƒ»è©•ä¾¡
    """
    
    def __init__(self, core_compressor):
        self.core_compressor = core_compressor
        self.cache = {}  # åˆ†æçµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.sample_size = 8192  # 8KBã‚µãƒ³ãƒ—ãƒ«
        
    def should_apply_transform(self, data: bytes, transformer, data_type) -> Tuple[bool, Dict[str, Any]]:
        """
        å¤‰æ›ã®ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        Returns: (should_transform, analysis_info)
        """
        print(f"  [ãƒ¡ã‚¿åˆ†æ] {data_type.value} ã®å¤‰æ›åŠ¹æœã‚’åˆ†æä¸­...")
        
        if not transformer or len(data) < self.sample_size:
            return True, {'reason': 'no_transformer_or_small_data'}
        
        try:
            # ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºï¼ˆå…ˆé ­ã€ä¸­å¤®ã€æœ«å°¾ã‹ã‚‰å‡ç­‰ã«ï¼‰
            sample = self._extract_representative_sample(data)
            sample_key = hash(sample)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if sample_key in self.cache:
                cached_result = self.cache[sample_key]
                print(f"    [ãƒ¡ã‚¿åˆ†æ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: åŠ¹æœ={cached_result['effectiveness']:.2%}")
                return cached_result['should_transform'], cached_result
            
            # 1. å¤‰æ›ãªã—ã®åœ§ç¸®ã‚µã‚¤ã‚º
            compressed_raw, _ = self.core_compressor.compress(sample)
            size_raw = len(compressed_raw)
            
            # 2. å¤‰æ›ã‚ã‚Šã®åœ§ç¸®ã‚µã‚¤ã‚º
            try:
                transformed_streams, _ = transformer.transform(sample)
                size_transformed = 0
                
                for stream in transformed_streams:
                    if len(stream) > 0:
                        compressed_stream, _ = self.core_compressor.compress(stream)
                        size_transformed += len(compressed_stream)
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æ¨å®šï¼ˆå¤‰æ›æƒ…å ±ãªã©ï¼‰
                estimated_header_overhead = 64  # æ¦‚ç®—
                size_transformed += estimated_header_overhead
                
            except Exception as e:
                print(f"    [ãƒ¡ã‚¿åˆ†æ] å¤‰æ›ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
                # å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã¯å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—
                analysis_info = {
                    'reason': 'transform_failed',
                    'error': str(e),
                    'should_transform': False
                }
                self.cache[sample_key] = analysis_info
                return False, analysis_info
            
            # 3. åŠ¹æœåˆ†æ
            effectiveness = (size_raw - size_transformed) / size_raw if size_raw > 0 else 0
            threshold = self._get_effectiveness_threshold(data_type, len(data))
            
            should_transform = effectiveness > threshold
            
            analysis_info = {
                'sample_size': len(sample),
                'raw_compressed_size': size_raw,
                'transformed_compressed_size': size_transformed,
                'effectiveness': effectiveness,
                'threshold': threshold,
                'should_transform': should_transform,
                'reason': 'effectiveness_analysis'
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.cache[sample_key] = analysis_info
            
            print(f"    [ãƒ¡ã‚¿åˆ†æ] åœ§ç¸®åŠ¹æœ: {effectiveness:.2%} (é–¾å€¤: {threshold:.2%}) -> {'å¤‰æ›å®Ÿè¡Œ' if should_transform else 'å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—'}")
            
            return should_transform, analysis_info
            
        except Exception as e:
            print(f"    [ãƒ¡ã‚¿åˆ†æ] åˆ†æã‚¨ãƒ©ãƒ¼: {e} - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å¤‰æ›å®Ÿè¡Œ")
            return True, {'reason': 'analysis_error', 'error': str(e)}
    
    def _extract_representative_sample(self, data: bytes) -> bytes:
        """ä»£è¡¨çš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚’æŠ½å‡ºï¼ˆå…ˆé ­ã€ä¸­å¤®ã€æœ«å°¾ã‹ã‚‰ï¼‰"""
        if len(data) <= self.sample_size:
            return data
        
        chunk_size = self.sample_size // 3
        start_chunk = data[:chunk_size]
        middle_start = (len(data) - chunk_size) // 2
        middle_chunk = data[middle_start:middle_start + chunk_size]
        end_chunk = data[-chunk_size:]
        
        return start_chunk + middle_chunk + end_chunk
    
    def _get_effectiveness_threshold(self, data_type, data_size: int) -> float:
        """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã¨ã‚µã‚¤ã‚ºã«åŸºã¥ãåŠ¹æœé–¾å€¤"""
        base_thresholds = {
            DataType.TEXT_DATA: 0.05,          # ãƒ†ã‚­ã‚¹ãƒˆã¯5%ä»¥ä¸Šã®æ”¹å–„ã§å¤‰æ›
            DataType.SEQUENTIAL_INT_DATA: 0.03, # ç³»åˆ—æ•´æ•°ã¯3%ä»¥ä¸Šã§å¤‰æ›
            DataType.FLOAT_DATA: 0.08,         # æµ®å‹•å°æ•°ç‚¹ã¯8%ä»¥ä¸Šã§å¤‰æ›
            DataType.STRUCTURED_NUMERIC: 0.06,  # æ§‹é€ åŒ–æ•°å€¤ã¯6%ä»¥ä¸Šã§å¤‰æ›
            DataType.REPETITIVE_BINARY: 0.04,  # åå¾©ãƒã‚¤ãƒŠãƒªã¯4%ä»¥ä¸Šã§å¤‰æ›
        }
        
        threshold = base_thresholds.get(data_type, 0.05)
        
        # å¤§ããªãƒ‡ãƒ¼ã‚¿ã»ã©å³ã—ã„é–¾å€¤ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®ç›¸å¯¾çš„å½±éŸ¿ãŒæ¸›å°‘ï¼‰
        if data_size > 1024 * 1024:  # 1MBä»¥ä¸Š
            threshold *= 0.7
        elif data_size > 64 * 1024:  # 64KBä»¥ä¸Š
            threshold *= 0.85
        
        return threshold


class PostBWTPipeline:
    """
    TMC v7.0 ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    BWT+MTFå¾Œã®ç‰¹æ®Šãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«ç‰¹åŒ–ã—ãŸå°‚é–€ç¬¦å·åŒ–
    """
    
    def encode(self, mtf_stream: bytes) -> List[bytes]:
        """BWT+MTFå¾Œã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å°‚é–€ç¬¦å·åŒ–"""
        print("    [ãƒã‚¹ãƒˆBWT] RLE + åˆ†å‰²ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ã‚’å®Ÿè¡Œä¸­...")
        
        try:
            # 1. ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ– (RLE)
            literals, run_lengths = self._apply_rle(mtf_stream)
            
            print(f"    [ãƒã‚¹ãƒˆBWT] RLE: {len(mtf_stream)} bytes -> ãƒªãƒ†ãƒ©ãƒ«: {len(literals)}, ãƒ©ãƒ³: {len(run_lengths)}")
            
            # 2. åˆ†å‰²ã—ãŸã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¿”ã™
            return [literals, run_lengths]
            
        except Exception as e:
            print(f"    [ãƒã‚¹ãƒˆBWT] ã‚¨ãƒ©ãƒ¼: {e} - å…ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿”å´")
            return [mtf_stream]
    
    def decode(self, streams: List[bytes]) -> bytes:
        """ãƒã‚¹ãƒˆBWTå°‚é–€å¾©å·"""
        print("    [ãƒã‚¹ãƒˆBWT] RLEé€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        
        try:
            if len(streams) == 1:
                return streams[0]  # RLEæœªé©ç”¨
            
            if len(streams) >= 2:
                literals = streams[0]
                run_lengths = streams[1]
                
                # é€†RLE
                mtf_stream = self._reverse_rle(literals, run_lengths)
                print(f"    [ãƒã‚¹ãƒˆBWT] é€†RLE: ãƒªãƒ†ãƒ©ãƒ«: {len(literals)}, ãƒ©ãƒ³: {len(run_lengths)} -> {len(mtf_stream)} bytes")
                
                return mtf_stream
            
            return b''.join(streams)
            
        except Exception as e:
            print(f"    [ãƒã‚¹ãƒˆBWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _apply_rle(self, data: bytes) -> Tuple[bytes, bytes]:
        """ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ–ï¼ˆMTFå¾Œã®ãƒ‡ãƒ¼ã‚¿ã«æœ€é©åŒ–ï¼‰"""
        if not data:
            return b'', b''
        
        literals = bytearray()
        run_lengths = bytearray()
        
        current_byte = data[0]
        run_length = 1
        
        for i in range(1, len(data)):
            if data[i] == current_byte and run_length < 255:
                run_length += 1
            else:
                # ãƒ©ãƒ³ã‚’è¨˜éŒ²
                literals.append(current_byte)
                run_lengths.append(run_length)
                
                # æ–°ã—ã„ãƒ©ãƒ³ã‚’é–‹å§‹
                current_byte = data[i]
                run_length = 1
        
        # æœ€å¾Œã®ãƒ©ãƒ³ã‚’è¨˜éŒ²
        literals.append(current_byte)
        run_lengths.append(run_length)
        
        return bytes(literals), bytes(run_lengths)
    
    def _reverse_rle(self, literals: bytes, run_lengths: bytes) -> bytes:
        """é€†ãƒ©ãƒ³ãƒ¬ãƒ³ã‚°ã‚¹ç¬¦å·åŒ–"""
        if len(literals) != len(run_lengths):
            raise ValueError("Literals and run_lengths must have the same length")
        
        result = bytearray()
        
        for literal, run_length in zip(literals, run_lengths):
            result.extend([literal] * run_length)
        
        return bytes(result)


class DataType(Enum):
    """æ”¹è‰¯ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆçµ±åˆï¼‰"""
    FLOAT_DATA = "float_data"
    TEXT_DATA = "text_data"
    SEQUENTIAL_INT_DATA = "sequential_int_data"
    STRUCTURED_NUMERIC = "structured_numeric"
    TIME_SERIES = "time_series"
    REPETITIVE_BINARY = "repetitive_binary"
    COMPRESSED_LIKE = "compressed_like"
    GENERIC_BINARY = "generic_binary"


class SublinearLZ77Compressor:
    """
    TMC v9.0 ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77åœ§ç¸®å™¨
    O(n log log n)ã®é«˜é€Ÿè¾æ›¸æ¤œç´¢ + Suffix Arrayæ´»ç”¨
    """
    
    def __init__(self):
        self.min_match_length = 3  # æœ€å°ãƒãƒƒãƒé•·
        self.max_match_length = 258  # æœ€å¤§ãƒãƒƒãƒé•·
        self.window_size = 32768  # è¾æ›¸ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
        self.pydivsufsort_available = False
        
        try:
            import pydivsufsort
            self.pydivsufsort = pydivsufsort
            self.pydivsufsort_available = True
            print("ğŸš€ SublinearLZ77: pydivsufsorté«˜é€Ÿæ¤œç´¢æœ‰åŠ¹")
        except ImportError:
            print("âš ï¸ SublinearLZ77: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰")
    
    def compress_sublinear_lz77(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77åœ§ç¸®å®Ÿè¡Œ"""
        if len(data) < self.min_match_length:
            return data, {"method": "store", "reason": "too_small"}
        
        print(f"    [SublinearLZ77] é«˜é€Ÿè¾æ›¸åœ§ç¸®é–‹å§‹: {len(data)} bytes")
        
        try:
            if self.pydivsufsort_available and len(data) >= 1024:
                # Suffix Arrayæ´»ç”¨é«˜é€Ÿæ¤œç´¢
                compressed_data, stats = self._sa_based_compression(data)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é«˜é€Ÿæ¤œç´¢
                compressed_data, stats = self._fallback_compression(data)
            
            print(f"    [SublinearLZ77] åœ§ç¸®å®Œäº†: {len(data)} -> {len(compressed_data)} bytes")
            print(f"    [SublinearLZ77] çµ±è¨ˆ: {stats}")
            
            return compressed_data, {
                "method": "sublinear_lz77",
                "original_size": len(data),
                "compressed_size": len(compressed_data),
                "statistics": stats
            }
            
        except Exception as e:
            print(f"    [SublinearLZ77] ã‚¨ãƒ©ãƒ¼: {e} - å…ƒãƒ‡ãƒ¼ã‚¿è¿”å´")
            return data, {"method": "store", "error": str(e)}
    
    def _sa_based_compression(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """Suffix ArrayåŸºç›¤ã®é«˜é€ŸLZ77åœ§ç¸®"""
        import numpy as np
        
        # Suffix Arrayæ§‹ç¯‰
        sa = self.pydivsufsort.divsufsort(data)
        
        # é«˜é€Ÿè¾æ›¸ãƒãƒƒãƒãƒ³ã‚°
        compressed_tokens = []
        pos = 0
        total_matches = 0
        total_match_length = 0
        
        while pos < len(data):
            # ç¾åœ¨ä½ç½®ã‹ã‚‰ã®æœ€é•·ãƒãƒƒãƒã‚’é«˜é€Ÿæ¤œç´¢
            match_pos, match_length = self._find_longest_match_sa(data, sa, pos)
            
            if match_length >= self.min_match_length:
                # ãƒãƒƒãƒç™ºè¦‹: (è·é›¢, é•·ã•)ãƒˆãƒ¼ã‚¯ãƒ³
                distance = pos - match_pos
                compressed_tokens.append(('match', distance, match_length))
                pos += match_length
                total_matches += 1
                total_match_length += match_length
            else:
                # ãƒªãƒ†ãƒ©ãƒ«æ–‡å­—
                compressed_tokens.append(('literal', data[pos]))
                pos += 1
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¤ãƒˆåˆ—ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        compressed_data = self._encode_lz77_tokens(compressed_tokens)
        
        stats = {
            "total_matches": total_matches,
            "total_match_length": total_match_length,
            "compression_ratio": len(compressed_data) / len(data),
            "tokens": len(compressed_tokens)
        }
        
        return compressed_data, stats
    
    def _find_longest_match_sa(self, data: bytes, sa: 'np.ndarray', pos: int) -> Tuple[int, int]:
        """Suffix Arrayä½¿ç”¨æœ€é•·ãƒãƒƒãƒæ¤œç´¢"""
        if pos >= len(data):
            return -1, 0
        
        max_match_length = 0
        best_match_pos = -1
        
        # ç¾åœ¨ä½ç½®ã‹ã‚‰æ¤œç´¢ç¯„å›²ã‚’è¨­å®š
        window_start = max(0, pos - self.window_size)
        
        # Suffix Arrayå†…ã§å€™è£œä½ç½®ã‚’é«˜é€Ÿæ¤œç´¢
        for i in range(len(sa)):
            sa_pos = sa[i]
            
            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ç¯„å›²å†…ã‹ã¤ç¾åœ¨ä½ç½®ã‚ˆã‚Šå‰ã®ä½ç½®ã®ã¿æ¤œç´¢
            if sa_pos >= pos or sa_pos < window_start:
                continue
            
            # ãƒãƒƒãƒé•·è¨ˆç®—
            match_length = 0
            max_possible_length = min(
                len(data) - pos, 
                len(data) - sa_pos,
                self.max_match_length
            )
            
            while (match_length < max_possible_length and 
                   data[pos + match_length] == data[sa_pos + match_length]):
                match_length += 1
            
            if match_length > max_match_length:
                max_match_length = match_length
                best_match_pos = sa_pos
        
        return best_match_pos, max_match_length
    
    def _fallback_compression(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é«˜é€ŸLZ77å®Ÿè£…"""
        compressed_tokens = []
        pos = 0
        total_matches = 0
        
        while pos < len(data):
            # é«˜é€Ÿãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ãƒãƒƒãƒãƒ³ã‚°
            match_pos, match_length = self._hash_based_match(data, pos)
            
            if match_length >= self.min_match_length:
                distance = pos - match_pos
                compressed_tokens.append(('match', distance, match_length))
                pos += match_length
                total_matches += 1
            else:
                compressed_tokens.append(('literal', data[pos]))
                pos += 1
        
        compressed_data = self._encode_lz77_tokens(compressed_tokens)
        
        stats = {
            "total_matches": total_matches,
            "method": "hash_based",
            "tokens": len(compressed_tokens)
        }
        
        return compressed_data, stats
    
    def _hash_based_match(self, data: bytes, pos: int) -> Tuple[int, int]:
        """ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹é«˜é€Ÿãƒãƒƒãƒãƒ³ã‚°"""
        if pos < self.min_match_length:
            return -1, 0
        
        window_start = max(0, pos - self.window_size)
        max_length = 0
        best_pos = -1
        
        # 3ãƒã‚¤ãƒˆãƒãƒƒã‚·ãƒ¥ã§é«˜é€Ÿæ¤œç´¢
        if pos + self.min_match_length <= len(data):
            target = data[pos:pos + self.min_match_length]
            
            for search_pos in range(window_start, pos):
                if search_pos + self.min_match_length <= len(data):
                    if data[search_pos:search_pos + self.min_match_length] == target:
                        # ãƒãƒƒãƒæ‹¡å¼µ
                        length = self.min_match_length
                        while (pos + length < len(data) and 
                               search_pos + length < len(data) and
                               length < self.max_match_length and
                               data[pos + length] == data[search_pos + length]):
                            length += 1
                        
                        if length > max_length:
                            max_length = length
                            best_pos = search_pos
        
        return best_pos, max_length
    
    def _encode_lz77_tokens(self, tokens: list) -> bytes:
        """LZ77ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒã‚¤ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        import struct
        encoded = bytearray()
        
        for token in tokens:
            if token[0] == 'literal':
                # ãƒªãƒ†ãƒ©ãƒ«: 0x00 + ãƒã‚¤ãƒˆå€¤
                encoded.append(0x00)
                encoded.append(token[1])
            else:  # match
                # ãƒãƒƒãƒ: 0x01 + è·é›¢(2bytes) + é•·ã•(1byte)
                _, distance, length = token
                encoded.append(0x01)
                encoded.extend(struct.pack('<H', distance))  # ãƒªãƒˆãƒ«ã‚¨ãƒ³ãƒ‡ã‚£ã‚¢ãƒ³2ãƒã‚¤ãƒˆ
                encoded.append(min(length, 255))
        
        return bytes(encoded)
    
    def decompress_sublinear_lz77(self, compressed_data: bytes, info: Dict[str, Any]) -> bytes:
        """ã‚µãƒ–ãƒªãƒ‹ã‚¢LZ77å±•é–‹"""
        if info.get("method") != "sublinear_lz77":
            return compressed_data
        
        print("    [SublinearLZ77] é«˜é€Ÿå±•é–‹é–‹å§‹")
        
        try:
            tokens = self._decode_lz77_tokens(compressed_data)
            decompressed = bytearray()
            
            for token in tokens:
                if token[0] == 'literal':
                    decompressed.append(token[1])
                else:  # match
                    _, distance, length = token
                    start_pos = len(decompressed) - distance
                    for i in range(length):
                        decompressed.append(decompressed[start_pos + i])
            
            print(f"    [SublinearLZ77] å±•é–‹å®Œäº†: {len(compressed_data)} -> {len(decompressed)} bytes")
            return bytes(decompressed)
            
        except Exception as e:
            print(f"    [SublinearLZ77] å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data
    
    def _decode_lz77_tokens(self, data: bytes) -> list:
        """LZ77ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‡ã‚³ãƒ¼ãƒ‰"""
        import struct
        tokens = []
        pos = 0
        
        while pos < len(data):
            if data[pos] == 0x00:  # ãƒªãƒ†ãƒ©ãƒ«
                if pos + 1 < len(data):
                    tokens.append(('literal', data[pos + 1]))
                    pos += 2
                else:
                    break
            elif data[pos] == 0x01:  # ãƒãƒƒãƒ
                if pos + 4 <= len(data):
                    distance = struct.unpack('<H', data[pos + 1:pos + 3])[0]
                    length = data[pos + 3]
                    tokens.append(('match', distance, length))
                    pos += 4
                else:
                    break
            else:
                pos += 1  # ä¸æ­£ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
        
        return tokens


class ContextMixingEncoder:
    """
    TMC v9.0 é«˜åº¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ç¬¦å·åŒ–ã‚¨ãƒ³ã‚¸ãƒ³
    è¤‡æ•°äºˆæ¸¬å™¨ã®ä¸¦åˆ—å®Ÿè¡Œ + å‹•çš„ãƒŸã‚­ã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹æ¥µé™åœ§ç¸®ç‡å®Ÿç¾
    """
    
    def __init__(self):
        self.zstd_available = ZSTD_AVAILABLE
        
        # è¤‡æ•°äºˆæ¸¬å™¨ã®åˆæœŸåŒ–
        self.order0_model = {}  # ã‚ªãƒ¼ãƒ€ãƒ¼0ï¼ˆçµ±è¨ˆçš„ï¼‰
        self.order1_model = {}  # ã‚ªãƒ¼ãƒ€ãƒ¼1ï¼ˆ1ãƒã‚¤ãƒˆæ–‡è„ˆï¼‰
        self.order2_model = {}  # ã‚ªãƒ¼ãƒ€ãƒ¼2ï¼ˆ2ãƒã‚¤ãƒˆæ–‡è„ˆï¼‰
        
        # å‹•çš„ãƒŸã‚­ã‚·ãƒ³ã‚°ç”¨ã®é‡ã¿
        self.mixing_weights = {
            'order0': 0.33,
            'order1': 0.33,
            'order2': 0.34
        }
        
        # å­¦ç¿’ç‡ï¼ˆé©å¿œçš„èª¿æ•´ç”¨ï¼‰
        self.learning_rate = 0.01
        self.prediction_history = []
        
        print("ğŸ§  ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–å®Œäº†")
    
    def encode_with_context_mixing(self, data: bytes, stream_type: str = "transformed") -> Tuple[bytes, str]:
        """
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹é«˜åº¦ç¬¦å·åŒ–
        è¤‡æ•°äºˆæ¸¬å™¨ + å‹•çš„é‡ã¿èª¿æ•´ã«ã‚ˆã‚‹æœ€é©åŒ–
        """
        try:
            if len(data) == 0:
                return b'', "context_empty"
            
            print(f"  [ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ãƒŸã‚­ã‚·ãƒ³ã‚°ç¬¦å·åŒ–é–‹å§‹: {len(data)} bytes")
            
            # è¤‡æ•°äºˆæ¸¬å™¨ã®ä¸¦åˆ—å®Ÿè¡Œ
            predictions = self._run_multiple_predictors(data)
            
            # å‹•çš„ãƒŸã‚­ã‚·ãƒ³ã‚°å®Ÿè¡Œ
            mixed_probabilities = self._dynamic_mixing(predictions, data)
            
            # FSEç¬¦å·åŒ–ï¼ˆFinite State Entropyï¼‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            if self.zstd_available:
                # Zstandardã®é«˜åº¦ç¬¦å·åŒ–ã‚’ä½¿ç”¨
                compressed = self._fse_encode_simulation(data, mixed_probabilities)
                return compressed, "context_mixing_fse"
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é«˜åŠ¹ç‡zlib
                compressed = zlib.compress(data, level=9)
                return compressed, "context_mixing_zlib"
                
        except Exception as e:
            print(f"    [ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ] ã‚¨ãƒ©ãƒ¼: {e}")
            return data, "context_store"
    
    def _run_multiple_predictors(self, data: bytes) -> Dict[str, List[Dict[int, float]]]:
        """è¤‡æ•°äºˆæ¸¬å™¨ã®ä¸¦åˆ—å®Ÿè¡Œ"""
        predictions = {
            'order0': [],
            'order1': [],
            'order2': []
        }
        
        # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã®äº‹å‰è¨ˆç®—ï¼ˆã‚ªãƒ¼ãƒ€ãƒ¼0ç”¨ï¼‰
        byte_counts = [0] * 256
        for byte in data:
            byte_counts[byte] += 1
        
        total_bytes = len(data)
        order0_probs = {i: count / total_bytes for i, count in enumerate(byte_counts) if count > 0}
        
        # å„ãƒã‚¤ãƒˆä½ç½®ã§ã®äºˆæ¸¬å®Ÿè¡Œ
        for i in range(len(data)):
            current_byte = data[i]
            
            # ã‚ªãƒ¼ãƒ€ãƒ¼0äºˆæ¸¬ï¼ˆå…¨ä½“çµ±è¨ˆï¼‰
            predictions['order0'].append(order0_probs)
            
            # ã‚ªãƒ¼ãƒ€ãƒ¼1äºˆæ¸¬ï¼ˆç›´å‰1ãƒã‚¤ãƒˆæ–‡è„ˆï¼‰
            if i > 0:
                context1 = data[i-1:i]
                order1_pred = self._predict_order1(context1, data, i)
                predictions['order1'].append(order1_pred)
            else:
                predictions['order1'].append(order0_probs)
            
            # ã‚ªãƒ¼ãƒ€ãƒ¼2äºˆæ¸¬ï¼ˆç›´å‰2ãƒã‚¤ãƒˆæ–‡è„ˆï¼‰
            if i > 1:
                context2 = data[i-2:i]
                order2_pred = self._predict_order2(context2, data, i)
                predictions['order2'].append(order2_pred)
            else:
                predictions['order2'].append(order0_probs)
        
        return predictions
    
    def _predict_order1(self, context: bytes, data: bytes, position: int) -> Dict[int, float]:
        """ã‚ªãƒ¼ãƒ€ãƒ¼1äºˆæ¸¬ï¼ˆ1ãƒã‚¤ãƒˆæ–‡è„ˆï¼‰"""
        context_key = context[0] if len(context) > 0 else 0
        
        # ã“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç¶šããƒã‚¤ãƒˆã®çµ±è¨ˆã‚’åé›†
        following_bytes = []
        for i in range(len(data) - 1):
            if data[i] == context_key:
                following_bytes.append(data[i + 1])
        
        if not following_bytes:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å‡ç­‰åˆ†å¸ƒ
            return {i: 1.0/256 for i in range(256)}
        
        # ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—
        byte_counts = {}
        for byte in following_bytes:
            byte_counts[byte] = byte_counts.get(byte, 0) + 1
        
        total = len(following_bytes)
        return {byte: count / total for byte, count in byte_counts.items()}
    
    def _predict_order2(self, context: bytes, data: bytes, position: int) -> Dict[int, float]:
        """ã‚ªãƒ¼ãƒ€ãƒ¼2äºˆæ¸¬ï¼ˆ2ãƒã‚¤ãƒˆæ–‡è„ˆï¼‰"""
        if len(context) < 2:
            return self._predict_order1(context[-1:] if context else b'', data, position)
        
        context_key = (context[0], context[1])
        
        # ã“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ç¶šããƒã‚¤ãƒˆã®çµ±è¨ˆã‚’åé›†
        following_bytes = []
        for i in range(len(data) - 2):
            if (data[i], data[i + 1]) == context_key:
                following_bytes.append(data[i + 2])
        
        if not following_bytes:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ªãƒ¼ãƒ€ãƒ¼1äºˆæ¸¬
            return self._predict_order1(context[-1:], data, position)
        
        # ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—
        byte_counts = {}
        for byte in following_bytes:
            byte_counts[byte] = byte_counts.get(byte, 0) + 1
        
        total = len(following_bytes)
        return {byte: count / total for byte, count in byte_counts.items()}
    
    def _dynamic_mixing(self, predictions: Dict[str, List[Dict[int, float]]], data: bytes) -> List[Dict[int, float]]:
        """å‹•çš„ãƒŸã‚­ã‚·ãƒ³ã‚°ï¼ˆé©å¿œçš„é‡ã¿èª¿æ•´ï¼‰"""
        mixed_predictions = []
        
        for i in range(len(data)):
            current_byte = data[i]
            
            # å„äºˆæ¸¬å™¨ã®ç¢ºç‡ã‚’å–å¾—
            order0_prob = predictions['order0'][i].get(current_byte, 0.0)
            order1_prob = predictions['order1'][i].get(current_byte, 0.0)
            order2_prob = predictions['order2'][i].get(current_byte, 0.0)
            
            # äºˆæ¸¬ç²¾åº¦ã«åŸºã¥ãå‹•çš„é‡ã¿èª¿æ•´
            self._update_mixing_weights(order0_prob, order1_prob, order2_prob)
            
            # é‡ã¿ä»˜ãæ··åˆç¢ºç‡ã®è¨ˆç®—
            mixed_prob = {}
            all_bytes = set()
            all_bytes.update(predictions['order0'][i].keys())
            all_bytes.update(predictions['order1'][i].keys())
            all_bytes.update(predictions['order2'][i].keys())
            
            for byte in all_bytes:
                p0 = predictions['order0'][i].get(byte, 0.0)
                p1 = predictions['order1'][i].get(byte, 0.0)
                p2 = predictions['order2'][i].get(byte, 0.0)
                
                mixed_prob[byte] = (
                    self.mixing_weights['order0'] * p0 +
                    self.mixing_weights['order1'] * p1 +
                    self.mixing_weights['order2'] * p2
                )
            
            # æ­£è¦åŒ–
            total_prob = sum(mixed_prob.values())
            if total_prob > 0:
                mixed_prob = {byte: prob / total_prob for byte, prob in mixed_prob.items()}
            
            mixed_predictions.append(mixed_prob)
        
        return mixed_predictions
    
    def _update_mixing_weights(self, p0: float, p1: float, p2: float):
        """äºˆæ¸¬ç²¾åº¦ã«åŸºã¥ãé‡ã¿æ›´æ–°"""
        # ã‚ˆã‚Šé«˜ã„ç¢ºç‡ã‚’äºˆæ¸¬ã—ãŸäºˆæ¸¬å™¨ã«ã‚ˆã‚Šå¤šãã®é‡ã¿ã‚’ä¸ãˆã‚‹
        prediction_scores = {
            'order0': p0,
            'order1': p1,
            'order2': p2
        }
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é¢¨ã®é‡ã¿æ›´æ–°
        total_score = sum(prediction_scores.values())
        if total_score > 0:
            for order in prediction_scores:
                target_weight = prediction_scores[order] / total_score
                current_weight = self.mixing_weights[order]
                
                # å­¦ç¿’ç‡ã«ã‚ˆã‚‹é©å¿œçš„èª¿æ•´
                self.mixing_weights[order] = (
                    current_weight * (1 - self.learning_rate) +
                    target_weight * self.learning_rate
                )
        
        # é‡ã¿ã®æ­£è¦åŒ–
        total_weight = sum(self.mixing_weights.values())
        if total_weight > 0:
            self.mixing_weights = {k: v / total_weight for k, v in self.mixing_weights.items()}
    
    def _fse_encode_simulation(self, data: bytes, mixed_probabilities: List[Dict[int, float]]) -> bytes:
        """FSEç¬¦å·åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆZstandardãƒ™ãƒ¼ã‚¹ï¼‰"""
        try:
            if self.zstd_available:
                # æœ€é«˜åœ§ç¸®ãƒ¬ãƒ™ãƒ«ã§Zstandardã‚’ä½¿ç”¨
                # å®Ÿéš›ã®FSEå®Ÿè£…ã®ä»£æ›¿ã¨ã—ã¦æœ€é©åŒ–ã•ã‚ŒãŸZstd
                compressor = zstd.ZstdCompressor(
                    level=22,  # æœ€é«˜åœ§ç¸®ãƒ¬ãƒ™ãƒ«
                    compression_params=zstd.ZstdCompressionParameters(
                        window_log=22,      # æœ€å¤§ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
                        hash_log=12,        # å¤§ããªãƒãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«
                        chain_log=12,       # é•·ã„ãƒã‚§ãƒ¼ãƒ³
                        search_log=7,       # å¾¹åº•çš„æ¤œç´¢
                        min_match=3,        # æœ€å°ãƒãƒƒãƒé•·
                        target_length=128,  # é•·ã„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
                        strategy=zstd.STRATEGY_BTULTRA2  # æœ€é«˜å“è³ªæˆ¦ç•¥
                    )
                )
                return compressor.compress(data)
            else:
                return zlib.compress(data, level=9)
        except Exception:
            return zlib.compress(data, level=9)
    
    def decode_context_mixed(self, compressed_data: bytes, method: str) -> bytes:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¾©å·"""
        try:
            if method == "context_mixing_fse" and self.zstd_available:
                decompressor = zstd.ZstdDecompressor()
                return decompressor.decompress(compressed_data)
            elif method == "context_mixing_zlib":
                return zlib.decompress(compressed_data)
            else:
                return compressed_data
        except Exception:
            return compressed_data


class CoreCompressor:
    """
    TMC v9.0 é«˜åº¦çµ±ä¸€åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³
    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚° + å‹•çš„ãƒ¬ãƒ™ãƒ«é¸æŠã«ã‚ˆã‚‹æœ€é©åŒ–
    """
    def __init__(self):
        self.zstd_available = ZSTD_AVAILABLE
        if self.zstd_available:
            # è¤‡æ•°ãƒ¬ãƒ™ãƒ«ã®compressorã‚’äº‹å‰ç”Ÿæˆï¼ˆåŠ¹ç‡åŒ–ï¼‰
            self.zstd_compressors = {
                'fast': zstd.ZstdCompressor(level=1),      # é«˜é€Ÿåœ§ç¸®
                'balanced': zstd.ZstdCompressor(level=3),  # ãƒãƒ©ãƒ³ã‚¹å‹
                'high': zstd.ZstdCompressor(level=9),      # é«˜åœ§ç¸®
                'ultra': zstd.ZstdCompressor(level=18),    # è¶…é«˜åœ§ç¸®
                'context': zstd.ZstdCompressor(level=22,   # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ç”¨
                    compression_params=zstd.ZstdCompressionParameters(
                        window_log=22,
                        hash_log=12,
                        chain_log=12,
                        search_log=7,
                        min_match=3,
                        target_length=7,
                        strategy=zstd.STRATEGY_BTULTRA2
                    ))
            }
            self.zstd_decompressor = zstd.ZstdDecompressor()
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®æœ€å°æ§‹æˆ
            self.fallback_available = True
        
        # TMC v9.0 æ–°æ©Ÿèƒ½: SublinearLZ77ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°
        self.sublinear_lz77 = SublinearLZ77Compressor()
        self.context_mixer = ContextMixingEncoder()
    
    def compress(self, data: bytes, stream_entropy: float = 4.0, stream_size: int = 0, 
                 use_context_mixing: bool = False) -> Tuple[bytes, str]:
        """
        TMC v9.0çµ±ä¸€åœ§ç¸®ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¯¾å¿œï¼‰
        ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨ã‚µã‚¤ã‚ºã«åŸºã¥ãæœ€é©åŒ– + é«˜åº¦æ–‡è„ˆç¬¦å·åŒ–
        """
        try:
            if len(data) == 0:
                return data, "empty"
            
            size = len(data) if stream_size == 0 else stream_size
            
            # v9.0: SublinearLZ77å‰å‡¦ç†åˆ¤å®šï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§åŠ¹æœçš„ï¼‰
            if size >= 2048 and stream_entropy > 3.0:  # ä¸­ï½é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ãƒ¼ã‚¿ã§LZ77ãŒåŠ¹æœçš„
                try:
                    lz77_compressed, lz77_info = self.sublinear_lz77.compress_sublinear_lz77(data)
                    if len(lz77_compressed) < len(data) * 0.85:  # 15%ä»¥ä¸Šã®åœ§ç¸®åŠ¹æœãŒã‚ã‚‹å ´åˆ
                        print(f"    [ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼] SublinearLZ77å‰å‡¦ç†æˆåŠŸ: {len(data)} -> {len(lz77_compressed)} bytes")
                        # LZ77åœ§ç¸®å¾Œã«ã•ã‚‰ã«Zstdåœ§ç¸®ã‚’é©ç”¨
                        final_compressed, zstd_method = self.compress(lz77_compressed, stream_entropy, len(lz77_compressed), False)
                        return final_compressed, f"sublinear_lz77+{zstd_method}"
                    else:
                        print(f"    [ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼] SublinearLZ77åŠ¹æœä¸ååˆ†ã€ã‚¹ã‚­ãƒƒãƒ—")
                except Exception as e:
                    print(f"    [ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼] SublinearLZ77ã‚¨ãƒ©ãƒ¼: {e}")
            
            # v9.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°åˆ¤å®šï¼ˆæ¡ä»¶ã‚’ç·©å’Œï¼‰
            if use_context_mixing and size >= 512:  # 512Bä»¥ä¸Šã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°æœ‰åŠ¹ï¼ˆBWTãƒ‡ãƒ¼ã‚¿ç­‰ã®é«˜åœ§ç¸®å¯¾è±¡ï¼‰
                try:
                    compressed, method = self.context_mixer.encode_with_context_mixing(data, "transformed")
                    if len(compressed) < len(data) * 0.98:  # 2%ä»¥ä¸Šã®åœ§ç¸®åŠ¹æœãŒã‚ã‚‹å ´åˆï¼ˆé–¾å€¤ç·©å’Œï¼‰
                        return compressed, method
                    else:
                        print(f"    [ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼] ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°åŠ¹æœä¸ååˆ†ã€æ¨™æº–åœ§ç¸®ã«åˆ‡ã‚Šæ›¿ãˆ")
                except Exception as e:
                    print(f"    [ã‚³ã‚¢ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼] ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            
            if self.zstd_available:
                # TMCç†è«–ã«åŸºã¥ãå‹•çš„ãƒ¬ãƒ™ãƒ«é¸æŠ
                compression_level = self._select_optimal_level(size, stream_entropy)
                compressor = self.zstd_compressors[compression_level]
                
                try:
                    compressed = compressor.compress(data)
                    return compressed, f"zstd_{compression_level}"
                except Exception:
                    # æ¥µå°ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ç„¡åœ§ç¸®
                    return data, "store"
            
            # Zstdåˆ©ç”¨ä¸å¯ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if size > 8192:
                compressed = lzma.compress(data, preset=6)
                return compressed, "lzma_fallback"
            else:
                compressed = zlib.compress(data, level=6)
                return compressed, "zlib_fallback"
                
        except Exception:
            return data, "store"
    
    def _select_optimal_level(self, size: int, entropy: float) -> str:
        """
        TMCå‹•çš„ãƒ¬ãƒ™ãƒ«é¸æŠã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆå®Ÿè£…ï¼‰
        ã‚µã‚¤ã‚ºã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«åŸºã¥ãæœ€é©åŒ–
        """
        # è¶…ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆé«˜åº¦ã«æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼‰
        if entropy < 2.0:
            if size > 32768:  # å¤§ã‚µã‚¤ã‚º: è¶…é«˜åœ§ç¸®
                return 'ultra'
            else:  # å°ã‚µã‚¤ã‚º: é«˜åœ§ç¸®
                return 'high'
        
        # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼‰
        elif entropy < 4.0:
            if size > 16384:  # å¤§ã‚µã‚¤ã‚º: é«˜åœ§ç¸®
                return 'high'
            else:  # å°ã‚µã‚¤ã‚º: ãƒãƒ©ãƒ³ã‚¹å‹
                return 'balanced'
        
        # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆä¸€èˆ¬çš„ãªãƒ‡ãƒ¼ã‚¿ï¼‰
        elif entropy < 6.0:
            return 'balanced'
        
        # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã«è¿‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰
        else:
            if size < 4096:  # å°ã‚µã‚¤ã‚º: é«˜é€Ÿå‡¦ç†å„ªå…ˆ
                return 'fast'
            else:  # å¤§ã‚µã‚¤ã‚º: ãƒãƒ©ãƒ³ã‚¹å‹ã§è©¦è¡Œ
                return 'balanced'
    
    def decompress(self, compressed_data: bytes, method: str) -> bytes:
        """TMC v9.0çµ±ä¸€å±•é–‹å‡¦ç†ï¼ˆSublinearLZ77 + ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¯¾å¿œï¼‰"""
        try:
            # v9.0: SublinearLZ77çµ„ã¿åˆã‚ã›å¾©å·
            if method.startswith("sublinear_lz77+"):
                # ä¾‹: "sublinear_lz77+zstd_high"
                zstd_method = method.split("+")[1]
                # ã¾ãšZstdå±•é–‹
                zstd_decompressed = self.decompress(compressed_data, zstd_method)
                # æ¬¡ã«SublinearLZ77å±•é–‹
                lz77_info = {"method": "sublinear_lz77"}  # æœ€å°é™ã®æƒ…å ±
                return self.sublinear_lz77.decompress_sublinear_lz77(zstd_decompressed, lz77_info)
            
            # v9.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¾©å·
            elif method.startswith("context_mixing"):
                return self.context_mixer.decode_context_mixed(compressed_data, method)
            elif method.startswith("zstd_") and self.zstd_available:
                # Zstdå±•é–‹ã¯åœ§ç¸®ãƒ¬ãƒ™ãƒ«ã«é–¢ä¿‚ãªãå¸¸ã«é«˜é€Ÿ
                return self.zstd_decompressor.decompress(compressed_data)
            elif method == "lzma_fallback":
                return lzma.decompress(compressed_data)
            elif method == "zlib_fallback":
                return zlib.decompress(compressed_data)
            else:
                return compressed_data
                
        except Exception:
            return compressed_data


class ImprovedDispatcher:
    """
    æ”¹è‰¯åˆ†æ&ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆçµ±åˆï¼‰
    ã‚ˆã‚Šç²¾å¯†ãªãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¤å®š
    """
    
    def dispatch(self, data_block: bytes) -> Tuple[DataType, Dict[str, Any]]:
        """æ”¹è‰¯ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ­ãƒƒã‚¯åˆ†æ"""
        print(f"[æ”¹è‰¯ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£] ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ­ãƒƒã‚¯ (ã‚µã‚¤ã‚º: {len(data_block)} bytes) ã‚’åˆ†æä¸­...")
        
        if len(data_block) == 0:
            return DataType.GENERIC_BINARY, {}
        
        features = self._extract_enhanced_features(data_block)
        data_type = self._classify_enhanced_data_type(features, data_block)
        
        print(f"[æ”¹è‰¯ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£] åˆ¤å®š: {data_type.value}")
        return data_type, features
    
    def _extract_enhanced_features(self, data: bytes) -> Dict[str, Any]:
        """æ‹¡å¼µç‰¹å¾´é‡æŠ½å‡º"""
        try:
            features = {}
            
            # åŸºæœ¬çµ±è¨ˆ
            data_array = np.frombuffer(data, dtype=np.uint8)
            features['size'] = len(data)
            features['entropy'] = self._calculate_entropy(data_array)
            features['variance'] = float(np.var(data_array))
            
            # ãƒ†ã‚­ã‚¹ãƒˆæ€§åˆ†æï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆæ¡ç”¨ï¼‰
            text_chars = sum(1 for byte in data if 32 <= byte <= 126 or byte in [9, 10, 13])
            features['text_ratio'] = text_chars / len(data) if len(data) > 0 else 0
            
            # æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿åˆ†æ
            features['is_float_candidate'] = (len(data) % 4 == 0 and len(data) > 100)
            
            # æ•´æ•°ç³»åˆ—æ€§åˆ†æ
            features['is_sequential_int_candidate'] = False
            if len(data) % 4 == 0 and len(data) > 100:
                try:
                    integers = np.frombuffer(data, dtype=np.int32)
                    if len(integers) > 1:
                        diffs = np.abs(np.diff(integers.astype(np.int64)))
                        features['int_diff_mean'] = float(np.mean(diffs))
                        features['is_sequential_int_candidate'] = features['int_diff_mean'] < 1000
                except Exception:
                    pass
            
            # åå¾©æ€§åˆ†æ
            if len(data) > 0:
                unique_ratio = len(np.unique(data_array)) / len(data_array)
                features['unique_ratio'] = unique_ratio
                features['repetition_score'] = 1.0 - unique_ratio
            
            # åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿æ¤œå‡º
            features['high_entropy'] = features['entropy'] > 7.5
            
            return features
            
        except Exception:
            return {'entropy': 4.0, 'size': len(data)}
    
    def _calculate_entropy(self, data_array: np.ndarray) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        try:
            byte_counts = np.bincount(data_array, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(data_array)
            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 4.0
    
    def _classify_enhanced_data_type(self, features: Dict[str, Any], data: bytes) -> DataType:
        """
        TMC v6.0 æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡
        åˆ¤å®šé †åºã®æœ€é©åŒ–ï¼ˆã‚ˆã‚Šç‰¹æ®Šã§ç¢ºåº¦ã®é«˜ã„ã‚‚ã®ã‹ã‚‰é †ã«åˆ¤å®šï¼‰
        """
        try:
            # 1. ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ¤å®šï¼ˆæœ€é«˜å„ªå…ˆåº¦ï¼‰
            if features.get('text_ratio', 0) > 0.85:
                return DataType.TEXT_DATA
            
            # 2. ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿åˆ¤å®šï¼ˆæµ®å‹•å°æ•°ç‚¹ã‚ˆã‚Šå…ˆã«åˆ¤å®šï¼‰
            if features.get('is_sequential_int_candidate', False):
                # è¿½åŠ æ¤œè¨¼: ã‚ˆã‚Šå³å¯†ãªç³»åˆ—æ€§ãƒã‚§ãƒƒã‚¯
                if len(data) % 4 == 0 and len(data) > 100:
                    try:
                        integers = np.frombuffer(data, dtype=np.int32)
                        if len(integers) > 1:
                            diffs = np.abs(np.diff(integers.astype(np.int64)))
                            consecutive_small_diffs = np.sum(diffs < 100)
                            if consecutive_small_diffs / len(diffs) > 0.7:  # 70%ä»¥ä¸ŠãŒå°ã•ãªå·®åˆ†
                                print(f"    [åˆ†é¡] ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿ç¢ºèª: å°å·®åˆ†ç‡={consecutive_small_diffs/len(diffs):.2%}")
                                return DataType.SEQUENTIAL_INT_DATA
                    except Exception:
                        pass
            
            # 3. æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿åˆ¤å®šï¼ˆç³»åˆ—æ•´æ•°ã®å¾Œã§åˆ¤å®šï¼‰
            if features.get('is_float_candidate', False):
                # è¿½åŠ æ¤œè¨¼: æµ®å‹•å°æ•°ç‚¹æ•°ã‚‰ã—ã•ã‚’ãƒã‚§ãƒƒã‚¯
                if len(data) % 4 == 0 and len(data) > 100:
                    try:
                        floats = np.frombuffer(data, dtype=np.float32)
                        # NaN, Inf ã§ãªã„æœ‰åŠ¹ãªæµ®å‹•å°æ•°ç‚¹æ•°ã®å‰²åˆã‚’ãƒã‚§ãƒƒã‚¯
                        valid_floats = np.isfinite(floats)
                        valid_ratio = np.sum(valid_floats) / len(floats)
                        
                        # ã•ã‚‰ã«ã€å€¤ã®ç¯„å›²ãŒæµ®å‹•å°æ•°ç‚¹ã‚‰ã—ã„ã‹ãƒã‚§ãƒƒã‚¯
                        if valid_ratio > 0.95:  # 95%ä»¥ä¸ŠãŒæœ‰åŠ¹ãªæµ®å‹•å°æ•°ç‚¹
                            valid_values = floats[valid_floats]
                            if len(valid_values) > 0:
                                try:
                                    value_range = float(np.max(valid_values) - np.min(valid_values))
                                    # å€¤ã®ç¯„å›²ãŒé©åº¦ã«å¤§ãã„ï¼ˆæ•´æ•°ç³»åˆ—ã§ãªã„ï¼‰ã‹ã¤æœ‰é™
                                    if np.isfinite(value_range) and value_range > 1.0:
                                        print(f"    [åˆ†é¡] æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿ç¢ºèª: æœ‰åŠ¹ç‡={valid_ratio:.2%}, ç¯„å›²={value_range:.2f}")
                                        return DataType.FLOAT_DATA
                                except (OverflowError, RuntimeWarning):
                                    # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã®å ´åˆã¯æµ®å‹•å°æ•°ç‚¹ã¨ã—ã¦æ‰±ã‚ãªã„
                                    pass
                    except Exception:
                        pass
            
            # 4. é«˜åå¾©ãƒ‡ãƒ¼ã‚¿ï¼ˆå‰å›ã¨åŒã˜ï¼‰
            if features.get('repetition_score', 0) > 0.7:
                return DataType.REPETITIVE_BINARY
            
            # 5. åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå‰å›ã¨åŒã˜ï¼‰
            if features.get('high_entropy', False):
                return DataType.COMPRESSED_LIKE
            
            # 6. ãã®ä»–ã®æ§‹é€ çš„ãƒ‡ãƒ¼ã‚¿ï¼ˆå‰å›ã¨åŒã˜ï¼‰
            if features.get('entropy', 8) < 6.0:
                return DataType.STRUCTURED_NUMERIC
            
            # 7. æ±ç”¨ãƒã‚¤ãƒŠãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            return DataType.GENERIC_BINARY
            
        except Exception:
            return DataType.GENERIC_BINARY


class TDTTransformer:
    """
    TMC v5.0 é«˜åº¦å‹ä»˜ããƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆçµ±åˆï¼‰
    çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«åŸºã¥ãé©å¿œçš„ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†è§£
    """
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹é©å¿œçš„ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†è§£"""
        print("  [TDT] é«˜åº¦å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'tdt_clustered', 'original_size': len(data)}
        
        try:
            # 4ãƒã‚¤ãƒˆå˜ä½ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆæ¡ç”¨ï¼‰
            if len(data) % 4 != 0:
                print("    [TDT] ãƒ‡ãƒ¼ã‚¿ãŒ4ãƒã‚¤ãƒˆã®å€æ•°ã§ã¯ãªã„ãŸã‚ã€å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return [data], info
            
            # æµ®å‹•å°æ•°ç‚¹ã¨ã—ã¦è§£é‡ˆ
            floats = np.frombuffer(data, dtype=np.float32)
            byte_view = floats.view(np.uint8).reshape(-1, 4)
            
            print(f"    [TDT] {len(floats)}å€‹ã®æµ®å‹•å°æ•°ç‚¹æ•°ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: å„ãƒã‚¤ãƒˆä½ç½®ã®çµ±è¨ˆçš„ç‰¹å¾´æŠ½å‡º
            byte_features = []
            for i in range(4):
                byte_stream = byte_view[:, i]
                features = self._extract_byte_position_features(byte_stream, i)
                byte_features.append(features)
                print(f"    [TDT] ãƒã‚¤ãƒˆä½ç½® {i}: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼={features['entropy']:.2f}, åˆ†æ•£={features['variance']:.2f}")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            clusters = self._perform_statistical_clustering(byte_features)
            print(f"    [TDT] ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ: {len(clusters)}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼")
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«åŸºã¥ãã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
            streams = []
            cluster_info = []
            
            for cluster_id, byte_positions in enumerate(clusters):
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®ãƒã‚¤ãƒˆä½ç½®ã‚’çµåˆ
                cluster_data = bytearray()
                for pos in byte_positions:
                    cluster_data.extend(byte_view[:, pos].tobytes())
                
                stream = bytes(cluster_data)
                streams.append(stream)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆè¨ˆç®—
                cluster_entropy = self._calculate_stream_entropy(np.frombuffer(stream, dtype=np.uint8))
                cluster_info.append({
                    'positions': byte_positions,
                    'entropy': cluster_entropy,
                    'size': len(stream)
                })
                
                print(f"    [TDT] ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ {cluster_id} (ä½ç½®: {byte_positions}): ã‚µã‚¤ã‚º={len(stream)}, ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼={cluster_entropy:.2f}")
            
            info['byte_features'] = byte_features
            info['clusters'] = cluster_info
            info['stream_count'] = len(streams)
            info['clustering_method'] = 'statistical_similarity'
            
            return streams, info
            
        except Exception as e:
            print(f"    [TDT] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _extract_byte_position_features(self, byte_stream: np.ndarray, position: int) -> Dict[str, float]:
        """
        å„ãƒã‚¤ãƒˆä½ç½®ã®çµ±è¨ˆçš„ç‰¹å¾´æŠ½å‡ºï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆå®Ÿè£…ï¼‰
        """
        features = {
            'position': position,
            'entropy': self._calculate_stream_entropy(byte_stream),
            'variance': float(np.var(byte_stream)),
            'std_dev': float(np.std(byte_stream)),
            'unique_ratio': len(np.unique(byte_stream)) / len(byte_stream),
            'mean': float(np.mean(byte_stream)),
            'range': float(np.max(byte_stream) - np.min(byte_stream))
        }
        
        # åˆ†å¸ƒã®åã‚Šï¼ˆæ­ªåº¦ï¼‰- æ”¹è‰¯ç‰ˆ
        try:
            import warnings
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                from scipy import stats
                features['skewness'] = float(stats.skew(byte_stream))
        except (ImportError, RuntimeWarning):
            # scipyãŒåˆ©ç”¨ã§ããªã„å ´åˆã‚„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®å®‰å…¨ãªè¨ˆç®—
            mean_val = features['mean']
            std_val = features['std_dev']
            if std_val > 1e-8:  # ã‚ˆã‚Šå®‰å…¨ãªé–¾å€¤
                normalized = (byte_stream.astype(np.float64) - mean_val) / std_val
                features['skewness'] = float(np.mean(normalized ** 3))
            else:
                features['skewness'] = 0.0
        
        return features
    
    def _perform_statistical_clustering(self, byte_features: List[Dict[str, float]]) -> List[List[int]]:
        """
        çµ±è¨ˆçš„ç‰¹å¾´ã«åŸºã¥ãéšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆå®Ÿè£…ï¼‰
        """
        try:
            # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æ§‹ç¯‰
            feature_vectors = []
            for features in byte_features:
                vector = [
                    features['entropy'],
                    features['variance'],
                    features['unique_ratio'],
                    features['skewness']
                ]
                feature_vectors.append(vector)
            
            feature_matrix = np.array(feature_vectors)
            
            # æ­£è¦åŒ–ï¼ˆZ-scoreï¼‰
            if feature_matrix.std(axis=0).sum() > 0:
                feature_matrix = (feature_matrix - feature_matrix.mean(axis=0)) / (feature_matrix.std(axis=0) + 1e-8)
            
            # è·é›¢è¡Œåˆ—è¨ˆç®—ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼‰
            n = len(byte_features)
            distance_matrix = np.zeros((n, n))
            
            for i in range(n):
                for j in range(i + 1, n):
                    distance = np.linalg.norm(feature_matrix[i] - feature_matrix[j])
                    distance_matrix[i, j] = distance_matrix[j, i] = distance
            
            # ç°¡æ˜“éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…
            clusters = self._simple_hierarchical_clustering(distance_matrix, threshold=1.0)
            
            return clusters
            
        except Exception as e:
            print(f"    [TDT] ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e} - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†å‰²ã‚’ä½¿ç”¨")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å›ºå®š4åˆ†å‰²
            return [[0], [1], [2], [3]]
    
    def _simple_hierarchical_clustering(self, distance_matrix: np.ndarray, threshold: float) -> List[List[int]]:
        """ç°¡æ˜“éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…"""
        n = distance_matrix.shape[0]
        clusters = [[i] for i in range(n)]  # åˆæœŸçŠ¶æ…‹: å„è¦ç´ ãŒç‹¬è‡ªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        
        while len(clusters) > 1:
            # æœ€ã‚‚è¿‘ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒšã‚¢ã‚’æ¢ç´¢
            min_distance = float('inf')
            merge_i, merge_j = -1, -1
            
            for i in range(len(clusters)):
                for j in range(i + 1, len(clusters)):
                    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é–“ã®å¹³å‡è·é›¢ã‚’è¨ˆç®—
                    total_distance = 0
                    count = 0
                    
                    for idx_i in clusters[i]:
                        for idx_j in clusters[j]:
                            total_distance += distance_matrix[idx_i, idx_j]
                            count += 1
                    
                    if count > 0:
                        avg_distance = total_distance / count
                        if avg_distance < min_distance:
                            min_distance = avg_distance
                            merge_i, merge_j = i, j
            
            # é–¾å€¤ãƒã‚§ãƒƒã‚¯
            if min_distance > threshold:
                break
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒ¼ã‚¸
            if merge_i != -1 and merge_j != -1:
                new_cluster = clusters[merge_i] + clusters[merge_j]
                new_clusters = []
                for i, cluster in enumerate(clusters):
                    if i != merge_i and i != merge_j:
                        new_clusters.append(cluster)
                new_clusters.append(new_cluster)
                clusters = new_clusters
            else:
                break
        
        return clusters
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """TDTçµ±è¨ˆçš„é€†å¤‰æ›"""
        print("  [TDT] çµ±è¨ˆçš„é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            if 'clusters' not in info:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥æ–¹å¼
                return self._legacy_inverse_transform(streams)
            
            clusters = info['clusters']
            
            if len(streams) != len(clusters):
                print("    [TDT] ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ãŒä¸ä¸€è‡´")
                return b''.join(streams)
            
            # å…ƒã®ãƒã‚¤ãƒˆé…åˆ—ã‚µã‚¤ã‚ºã‚’æ¨å®š
            total_elements = sum(len(stream) for stream in streams) // 4
            byte_view = np.zeros((total_elements, 4), dtype=np.uint8)
            
            # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ãƒã‚¤ãƒˆä½ç½®ã‚’å¾©å…ƒ
            for cluster_id, (stream, cluster_info) in enumerate(zip(streams, clusters)):
                positions = cluster_info['positions']
                stream_data = np.frombuffer(stream, dtype=np.uint8)
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’å„ãƒã‚¤ãƒˆä½ç½®ã«åˆ†æ•£é…ç½®
                elements_per_position = len(stream_data) // len(positions)
                
                for i, pos in enumerate(positions):
                    start_idx = i * elements_per_position
                    end_idx = (i + 1) * elements_per_position
                    if i == len(positions) - 1:  # æœ€å¾Œã®ä½ç½®ã¯æ®‹ã‚Šã™ã¹ã¦
                        end_idx = len(stream_data)
                    
                    position_data = stream_data[start_idx:end_idx]
                    if len(position_data) == total_elements:
                        byte_view[:, pos] = position_data
                    else:
                        # ã‚µã‚¤ã‚ºèª¿æ•´
                        min_len = min(len(position_data), total_elements)
                        byte_view[:min_len, pos] = position_data[:min_len]
            
            return byte_view.tobytes()
            
        except Exception as e:
            print(f"    [TDT] çµ±è¨ˆçš„é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _legacy_inverse_transform(self, streams: List[bytes]) -> bytes:
        """å¾“æ¥æ–¹å¼ã®é€†å¤‰æ›ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        try:
            if len(streams) != 4:
                return streams[0] if streams else b''
            
            stream_lengths = [len(s) for s in streams]
            if len(set(stream_lengths)) != 1:
                return b''.join(streams)
            
            num_floats = stream_lengths[0]
            byte_view = np.empty((num_floats, 4), dtype=np.uint8)
            
            for i, stream in enumerate(streams):
                byte_view[:, i] = np.frombuffer(stream, dtype=np.uint8)
            
            return byte_view.tobytes()
            
        except Exception:
            return b''.join(streams)
    
    def _calculate_stream_entropy(self, stream: np.ndarray) -> float:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        try:
            byte_counts = np.bincount(stream, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(stream)
            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 8.0


class LeCoAdvancedTransformer:
    """
    TMC v8.0 é«˜åº¦æ©Ÿæ¢°å­¦ç¿’å¤‰æ›ï¼ˆå¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°å¯¾å¿œï¼‰
    å±€æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³é©å¿œã«ã‚ˆã‚‹æ¥µé™åœ§ç¸®ç‡å®Ÿç¾
    """
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """LeCo v8.0å¤‰æ›ï¼šå¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚° + å±€æ‰€æœ€é©åŒ–"""
        print("  [LeCo v8.0] å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'leco_variable_partitioning', 'original_size': len(data)}
        
        try:
            # 4ãƒã‚¤ãƒˆå˜ä½ãƒã‚§ãƒƒã‚¯
            if len(data) % 4 != 0:
                print("    [LeCo v8.0] ãƒ‡ãƒ¼ã‚¿ãŒ4ãƒã‚¤ãƒˆã®å€æ•°ã§ã¯ãªã„ãŸã‚ã€å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return [data], info
            
            integers = np.frombuffer(data, dtype=np.int32)
            print(f"    [LeCo v8.0] {len(integers)}å€‹ã®æ•´æ•°ã‚’å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ä¸­...")
            
            # å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
            partitions = self._variable_length_partitioning(integers)
            print(f"    [LeCo v8.0] {len(partitions)}å€‹ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ")
            
            # å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨
            partition_streams = []
            partition_infos = []
            
            for i, partition_data in enumerate(partitions):
                partition_result = self._optimize_partition(partition_data, i)
                partition_streams.extend(partition_result['streams'])
                partition_infos.append(partition_result['info'])
                
                print(f"    [ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ {i}] é•·ã•={len(partition_data)}, ãƒ¢ãƒ‡ãƒ«={partition_result['info']['model_type']}, "
                      f"åœ§ç¸®ã‚¹ã‚³ã‚¢={partition_result['info']['compression_score']:.2f}")
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦è¿½åŠ 
            partition_header = self._create_partition_header(partition_infos, len(integers))
            final_streams = [partition_header] + partition_streams
            
            # çµ±è¨ˆæƒ…å ±æ›´æ–°
            total_score = sum(p['compression_score'] for p in partition_infos)
            avg_score = total_score / len(partition_infos) if partition_infos else 32.0
            
            info.update({
                'partition_count': len(partitions),
                'partition_infos': partition_infos,
                'average_compression_score': avg_score,
                'variable_partitioning': True
            })
            
            return final_streams, info
            
        except Exception as e:
            print(f"    [LeCo v8.0] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _variable_length_partitioning(self, integers: np.ndarray, threshold_bits: int = 8) -> List[np.ndarray]:
        """
        Greedyã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
        æ®‹å·®ãŒé–¾å€¤ä»¥ä¸‹ã«ãªã‚‹ã‚ˆã†ã«å‹•çš„ã«åˆ†å‰²
        """
        partitions = []
        current_start = 0
        max_residual_value = (1 << (threshold_bits - 1)) - 1  # 8bit: 127
        
        while current_start < len(integers):
            # è²ªæ¬²ã«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’æ‹¡å¼µ
            best_end = current_start + 1
            best_model = None
            
            # æœ€å°ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºï¼ˆçµ±è¨ˆçš„æ„å‘³ã‚’æŒã¤ãŸã‚ï¼‰
            min_partition_size = max(3, min(50, len(integers) // 20))
            max_partition_size = min(len(integers) - current_start, 1000)  # æœ€å¤§1000è¦ç´ 
            
            for potential_end in range(
                min(current_start + min_partition_size, len(integers)),
                min(current_start + max_partition_size + 1, len(integers) + 1)
            ):
                partition_data = integers[current_start:potential_end]
                
                # ã“ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œ
                best_partition_model = self._find_best_model_for_partition(partition_data)
                
                if best_partition_model is None:
                    break
                
                # æ®‹å·®ãŒé–¾å€¤ä»¥ä¸‹ã‹ç¢ºèª
                max_residual = np.max(np.abs(best_partition_model['residuals']))
                if max_residual <= max_residual_value:
                    best_end = potential_end
                    best_model = best_partition_model
                else:
                    # é–¾å€¤ã‚’è¶…ãˆãŸã®ã§ã€ã“ã“ã§åˆ†å‰²
                    break
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ç¢ºå®š
            partition_data = integers[current_start:best_end]
            partitions.append(partition_data)
            
            current_start = best_end
            
            # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
            if current_start >= len(integers):
                break
        
        return partitions
    
    def _find_best_model_for_partition(self, partition_data: np.ndarray) -> Optional[Dict[str, Any]]:
        """ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ã®æœ€é©ãƒ¢ãƒ‡ãƒ«æ¢ç´¢"""
        try:
            models_to_try = []
            
            # å®šæ•°ãƒ¢ãƒ‡ãƒ«
            try:
                const_result = self._try_constant_model(partition_data)
                models_to_try.append(const_result)
            except Exception:
                pass
            
            # ç·šå½¢ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒååˆ†ãªå ´åˆï¼‰
            if len(partition_data) >= 3:
                try:
                    linear_result = self._try_linear_model(partition_data)
                    models_to_try.append(linear_result)
                except Exception:
                    pass
            
            # äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒååˆ†ãªå ´åˆï¼‰
            if len(partition_data) >= 5:
                try:
                    quad_result = self._try_quadratic_model(partition_data)
                    models_to_try.append(quad_result)
                except Exception:
                    pass
            
            if not models_to_try:
                return None
            
            # æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠ
            best_model = min(models_to_try, key=lambda x: x['score'])
            return best_model
            
        except Exception:
            return None
    
    def _optimize_partition(self, partition_data: np.ndarray, partition_id: int) -> Dict[str, Any]:
        """å€‹åˆ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®æœ€é©åŒ–"""
        best_model = self._find_best_model_for_partition(partition_data)
        
        if best_model is None:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            mean_val = np.mean(partition_data)
            residuals = partition_data - int(mean_val)
            best_model = {
                'type': 'constant_fallback',
                'params': {'c': float(mean_val)},
                'residuals': residuals,
                'score': 32.0
            }
        
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ä½œæˆ
        partition_info = {
            'partition_id': partition_id,
            'model_type': best_model['type'],
            'params': best_model['params'],
            'data_length': len(partition_data),
            'compression_score': best_model['score'],
            'max_residual': int(np.max(np.abs(best_model['residuals']))) if len(best_model['residuals']) > 0 else 0
        }
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
        model_info_json = json.dumps(partition_info, separators=(',', ':'))
        model_info_bytes = model_info_json.encode('utf-8')
        model_header = len(model_info_bytes).to_bytes(4, 'big') + model_info_bytes
        
        residuals_stream = best_model['residuals'].astype(np.int32).tobytes()
        
        return {
            'info': partition_info,
            'streams': [model_header, residuals_stream]
        }
    
    def _create_partition_header(self, partition_infos: List[Dict], total_length: int) -> bytes:
        """ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        header_data = {
            'total_length': total_length,
            'partition_count': len(partition_infos),
            'partitions': [
                {
                    'id': p['partition_id'],
                    'length': p['data_length'],
                    'model': p['model_type']
                } for p in partition_infos
            ]
        }
        
        header_json = json.dumps(header_data, separators=(',', ':'))
        header_bytes = header_json.encode('utf-8')
        
        return len(header_bytes).to_bytes(4, 'big') + header_bytes
    
    # æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«è©¦è¡Œãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆ_try_constant_model, _try_linear_model, _try_quadratic_modelï¼‰ã¯ç¶™æ‰¿
    def _try_constant_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """å®šæ•°ãƒ¢ãƒ‡ãƒ«: y = c (Frame-of-Referenceåœ§ç¸®ç›¸å½“)"""
        mean_val = np.mean(integers)
        constant = int(round(mean_val))
        
        residuals = integers - constant
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        
        # æ®‹å·®ã‚’æ ¼ç´ã™ã‚‹ã®ã«å¿…è¦ãªãƒ“ãƒƒãƒˆæ•°ã‚’è¨ˆç®—
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1  # ç¬¦å·ãƒ“ãƒƒãƒˆå«ã‚€
        compression_score = float(bits_needed)
        
        return {
            'type': 'constant',
            'params': {'c': float(constant)},
            'residuals': residuals,
            'score': compression_score
        }
    
    def _try_linear_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """ç·šå½¢ãƒ¢ãƒ‡ãƒ«: y = ax + b"""
        x = np.arange(len(integers))
        slope, intercept = np.polyfit(x, integers, 1)
        
        predicted_values = (slope * x + intercept).astype(np.int32)
        residuals = integers - predicted_values
        
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ ¼ç´ã‚³ã‚¹ãƒˆã‚‚è€ƒæ…®ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        param_cost = 64  # slope + intercept (å„32bitæƒ³å®š)
        total_bits = bits_needed * len(integers) + param_cost
        compression_score = float(total_bits) / len(integers)
        
        return {
            'type': 'linear',
            'params': {'slope': float(slope), 'intercept': float(intercept)},
            'residuals': residuals,
            'score': compression_score
        }
    
    def _try_quadratic_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«: y = ax^2 + bx + c"""
        x = np.arange(len(integers))
        coeffs = np.polyfit(x, integers, 2)  # [a, b, c]
        
        predicted_values = np.polyval(coeffs, x).astype(np.int32)
        residuals = integers - predicted_values
        
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ ¼ç´ã‚³ã‚¹ãƒˆã‚‚è€ƒæ…®
        param_cost = 96  # a + b + c (å„32bitæƒ³å®š)
        total_bits = bits_needed * len(integers) + param_cost
        compression_score = float(total_bits) / len(integers)
        
        return {
            'type': 'quadratic',
            'params': {'a': float(coeffs[0]), 'b': float(coeffs[1]), 'c': float(coeffs[2])},
            'residuals': residuals,
            'score': compression_score
        }
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """LeCo v8.0 å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°é€†å¤‰æ›"""
        print("  [LeCo v8.0] å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            if not info.get('variable_partitioning', False):
                # v7.0äº’æ›ãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                return self._legacy_inverse_transform(streams, info)
            
            if len(streams) < 1:
                return b''
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã®è§£æ
            partition_header = streams[0]
            header_size = int.from_bytes(partition_header[:4], 'big')
            header_json = partition_header[4:4+header_size].decode('utf-8')
            header_data = json.loads(header_json)
            
            total_length = header_data['total_length']
            partition_count = header_data['partition_count']
            
            print(f"    [LeCo v8.0] ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {partition_count}, ç·é•·: {total_length}")
            
            # å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†
            reconstructed_data = np.zeros(total_length, dtype=np.int32)
            current_pos = 0
            stream_idx = 1  # ãƒ˜ãƒƒãƒ€ãƒ¼å¾Œã‹ã‚‰é–‹å§‹
            
            for _ in range(partition_count):
                # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã®å¾©å…ƒ
                if stream_idx >= len(streams):
                    break
                    
                model_header = streams[stream_idx]
                model_size = int.from_bytes(model_header[:4], 'big')
                model_json = model_header[4:4+model_size].decode('utf-8')
                partition_info = json.loads(model_json)
                
                # æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®å¾©å…ƒ
                if stream_idx + 1 >= len(streams):
                    break
                    
                residuals_stream = streams[stream_idx + 1]
                residuals = np.frombuffer(residuals_stream, dtype=np.int32)
                
                # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒ
                partition_data = self._reconstruct_partition(residuals, partition_info)
                
                # å…¨ä½“é…åˆ—ã«é…ç½®
                end_pos = current_pos + len(partition_data)
                if end_pos <= total_length:
                    reconstructed_data[current_pos:end_pos] = partition_data
                    current_pos = end_pos
                
                stream_idx += 2
                
                print(f"    [ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ {partition_info['partition_id']}] å¾©å…ƒå®Œäº†: {len(partition_data)}è¦ç´ ")
            
            return reconstructed_data.tobytes()
            
        except Exception as e:
            print(f"    [LeCo v8.0] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _reconstruct_partition(self, residuals: np.ndarray, partition_info: Dict[str, Any]) -> np.ndarray:
        """å€‹åˆ¥ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®å¾©å…ƒ"""
        model_type = partition_info['model_type']
        params = partition_info['params']
        data_length = partition_info['data_length']
        
        if model_type == 'constant' or model_type == 'constant_fallback':
            constant = int(params['c'])
            return residuals + constant
            
        elif model_type == 'linear':
            slope = params['slope']
            intercept = params['intercept']
            x = np.arange(len(residuals))
            predicted_values = (slope * x + intercept).astype(np.int32)
            return predicted_values + residuals
            
        elif model_type == 'quadratic':
            a, b, c = params['a'], params['b'], params['c']
            x = np.arange(len(residuals))
            predicted_values = (a * x*x + b * x + c).astype(np.int32)
            return predicted_values + residuals
            
        else:
            return residuals
    
    def _legacy_inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """v7.0äº’æ›é€†å¤‰æ›"""
        try:
            if len(streams) != 2:
                return streams[0] if streams else b''
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å¾©å…ƒ
            model_header = streams[0]
            residuals_stream = streams[1]
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒ˜ãƒƒãƒ€ãƒ¼ã®è§£æ
            model_info_size = int.from_bytes(model_header[:4], 'big')
            model_info_json = model_header[4:4+model_info_size].decode('utf-8')
            model_info = json.loads(model_info_json)
            
            model_type = model_info['model_type']
            params = model_info['params']
            data_length = model_info['data_length']
            
            # æ®‹å·®ã®å¾©å…ƒ
            residuals = np.frombuffer(residuals_stream, dtype=np.int32)
            
            print(f"    [LeCo] ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
            print(f"    [LeCo] ãƒ‡ãƒ¼ã‚¿é•·: {data_length}")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®é€†å¤‰æ›
            if model_type == 'constant' or model_type == 'constant_fallback':
                constant = int(params['c'])
                original_integers = residuals + constant
                
            elif model_type == 'linear':
                slope = params['slope']
                intercept = params['intercept']
                x = np.arange(len(residuals))
                predicted_values = (slope * x + intercept).astype(np.int32)
                original_integers = predicted_values + residuals
                
            elif model_type == 'quadratic':
                a, b, c = params['a'], params['b'], params['c']
                x = np.arange(len(residuals))
                predicted_values = (a * x*x + b * x + c).astype(np.int32)
                original_integers = predicted_values + residuals
                
            else:
                print(f"    [LeCo] æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
                return b''.join(streams)
            
            return original_integers.tobytes()
            
        except Exception as e:
            print(f"    [LeCo] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)


class LeCoTransformer:
    """
    TMC v6.0 é«˜åº¦æ©Ÿæ¢°å­¦ç¿’å¤‰æ›ï¼ˆãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰
    å‹•çš„ãƒ¢ãƒ‡ãƒ«é¸æŠã«ã‚ˆã‚‹äºˆæ¸¬åœ§ç¸®ã®æœ€é©åŒ–
    """
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """LeCo v6.0å¤‰æ›ï¼šè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®å‹•çš„é¸æŠ"""
        print("  [LeCo] TMC v6.0 ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'leco_multimodel', 'original_size': len(data)}
        
        try:
            # 4ãƒã‚¤ãƒˆå˜ä½ãƒã‚§ãƒƒã‚¯
            if len(data) % 4 != 0:
                print("    [LeCo] ãƒ‡ãƒ¼ã‚¿ãŒ4ãƒã‚¤ãƒˆã®å€æ•°ã§ã¯ãªã„ãŸã‚ã€å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return [data], info
            
            integers = np.frombuffer(data, dtype=np.int32)
            print(f"    [LeCo] {len(integers)}å€‹ã®æ•´æ•°ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
            
            # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®è©¦è¡Œã¨æœ€é©é¸æŠ
            best_model = self._select_optimal_model(integers)
            
            model_type = best_model['type']
            params = best_model['params']
            residuals = best_model['residuals']
            compression_score = best_model['score']
            
            print(f"    [LeCo] æœ€é©ãƒ¢ãƒ‡ãƒ«: {model_type}")
            print(f"    [LeCo] åœ§ç¸®ã‚¹ã‚³ã‚¢: {compression_score:.2f} bits/element")
            print(f"    [LeCo] æ®‹å·®ç¯„å›²: [{np.min(residuals)}, {np.max(residuals)}]")
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
            model_info = {
                'model_type': model_type,
                'params': params,
                'data_length': len(integers)
            }
            model_info_json = json.dumps(model_info, separators=(',', ':'))
            model_info_bytes = model_info_json.encode('utf-8')
            model_header = len(model_info_bytes).to_bytes(4, 'big') + model_info_bytes
            
            # æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
            residuals_stream = residuals.astype(np.int32).tobytes()
            
            # çµ±è¨ˆæƒ…å ±æ›´æ–°
            info.update({
                'model_type': model_type,
                'compression_score': compression_score,
                'residual_variance': float(np.var(residuals)),
                'model_params': params
            })
            
            return [model_header, residuals_stream], info
            
        except Exception as e:
            print(f"    [LeCo] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _select_optimal_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚’è©¦è¡Œã—ã€æœ€é©ãªã‚‚ã®ã‚’å‹•çš„é¸æŠ"""
        models_to_try = []
        
        # 1. å®šæ•°ãƒ¢ãƒ‡ãƒ« (Constant Model)
        try:
            const_result = self._try_constant_model(integers)
            models_to_try.append(const_result)
            print(f"    [LeCo] å®šæ•°ãƒ¢ãƒ‡ãƒ«: {const_result['score']:.2f} bits/element")
        except Exception as e:
            print(f"    [LeCo] å®šæ•°ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 2. ç·šå½¢ãƒ¢ãƒ‡ãƒ« (Linear Model)
        try:
            linear_result = self._try_linear_model(integers)
            models_to_try.append(linear_result)
            print(f"    [LeCo] ç·šå½¢ãƒ¢ãƒ‡ãƒ«: {linear_result['score']:.2f} bits/element")
        except Exception as e:
            print(f"    [LeCo] ç·šå½¢ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 3. äºŒæ¬¡ãƒ¢ãƒ‡ãƒ« (Quadratic Model) - ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        if len(integers) >= 10:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ç‚¹ãŒã‚ã‚‹å ´åˆã®ã¿
            try:
                quad_result = self._try_quadratic_model(integers)
                models_to_try.append(quad_result)
                print(f"    [LeCo] äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«: {quad_result['score']:.2f} bits/element")
            except Exception as e:
                print(f"    [LeCo] äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆæœ€å°ã‚¹ã‚³ã‚¢ï¼‰
        if not models_to_try:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å®šæ•°ãƒ¢ãƒ‡ãƒ«
            mean_val = np.mean(integers)
            residuals = integers - int(mean_val)
            return {
                'type': 'constant_fallback',
                'params': {'c': float(mean_val)},
                'residuals': residuals,
                'score': 32.0  # ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¹ã‚³ã‚¢
            }
        
        best_model = min(models_to_try, key=lambda x: x['score'])
        return best_model
    
    def _try_constant_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """å®šæ•°ãƒ¢ãƒ‡ãƒ«: y = c (Frame-of-Referenceåœ§ç¸®ç›¸å½“)"""
        mean_val = np.mean(integers)
        constant = int(round(mean_val))
        
        residuals = integers - constant
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        
        # æ®‹å·®ã‚’æ ¼ç´ã™ã‚‹ã®ã«å¿…è¦ãªãƒ“ãƒƒãƒˆæ•°ã‚’è¨ˆç®—
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1  # ç¬¦å·ãƒ“ãƒƒãƒˆå«ã‚€
        compression_score = float(bits_needed)
        
        return {
            'type': 'constant',
            'params': {'c': float(constant)},
            'residuals': residuals,
            'score': compression_score
        }
    
    def _try_linear_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """ç·šå½¢ãƒ¢ãƒ‡ãƒ«: y = ax + b"""
        x = np.arange(len(integers))
        slope, intercept = np.polyfit(x, integers, 1)
        
        predicted_values = (slope * x + intercept).astype(np.int32)
        residuals = integers - predicted_values
        
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ ¼ç´ã‚³ã‚¹ãƒˆã‚‚è€ƒæ…®ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        param_cost = 64  # slope + intercept (å„32bitæƒ³å®š)
        total_bits = bits_needed * len(integers) + param_cost
        compression_score = float(total_bits) / len(integers)
        
        return {
            'type': 'linear',
            'params': {'slope': float(slope), 'intercept': float(intercept)},
            'residuals': residuals,
            'score': compression_score
        }
    
    def _try_quadratic_model(self, integers: np.ndarray) -> Dict[str, Any]:
        """äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«: y = ax^2 + bx + c"""
        x = np.arange(len(integers))
        coeffs = np.polyfit(x, integers, 2)  # [a, b, c]
        
        predicted_values = np.polyval(coeffs, x).astype(np.int32)
        residuals = integers - predicted_values
        
        max_abs_residual = int(np.max(np.abs(residuals))) if len(residuals) > 0 else 0
        bits_needed = max_abs_residual.bit_length() + 1 if max_abs_residual > 0 else 1
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ ¼ç´ã‚³ã‚¹ãƒˆã‚‚è€ƒæ…®
        param_cost = 96  # a + b + c (å„32bitæƒ³å®š)
        total_bits = bits_needed * len(integers) + param_cost
        compression_score = float(total_bits) / len(integers)
        
        return {
            'type': 'quadratic',
            'params': {'a': float(coeffs[0]), 'b': float(coeffs[1]), 'c': float(coeffs[2])},
            'residuals': residuals,
            'score': compression_score
        }
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """LeCo v6.0ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«é€†å¤‰æ›"""
        print("  [LeCo] TMC v6.0 ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            if len(streams) != 2:
                return streams[0] if streams else b''
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å¾©å…ƒ
            model_header = streams[0]
            residuals_stream = streams[1]
            
            # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãƒ˜ãƒƒãƒ€ãƒ¼ã®è§£æ
            model_info_size = int.from_bytes(model_header[:4], 'big')
            model_info_json = model_header[4:4+model_info_size].decode('utf-8')
            model_info = json.loads(model_info_json)
            
            model_type = model_info['model_type']
            params = model_info['params']
            data_length = model_info['data_length']
            
            # æ®‹å·®ã®å¾©å…ƒ
            residuals = np.frombuffer(residuals_stream, dtype=np.int32)
            
            print(f"    [LeCo] ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
            print(f"    [LeCo] ãƒ‡ãƒ¼ã‚¿é•·: {data_length}")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®é€†å¤‰æ›
            if model_type == 'constant' or model_type == 'constant_fallback':
                constant = int(params['c'])
                original_integers = residuals + constant
                
            elif model_type == 'linear':
                slope = params['slope']
                intercept = params['intercept']
                x = np.arange(len(residuals))
                predicted_values = (slope * x + intercept).astype(np.int32)
                original_integers = predicted_values + residuals
                
            elif model_type == 'quadratic':
                a, b, c = params['a'], params['b'], params['c']
                x = np.arange(len(residuals))
                predicted_values = (a * x*x + b * x + c).astype(np.int32)
                original_integers = predicted_values + residuals
                
            else:
                print(f"    [LeCo] æœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")
                return b''.join(streams)
            
            return original_integers.tobytes()
            
        except Exception as e:
            print(f"    [LeCo] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)


class BWTTransformer:
    """
    TMC v8.1 å®Œå…¨å …ç‰¢åŒ–BWTTransformerï¼ˆpydivsufsortå®Œå…¨æº–æ‹ ï¼‰
    ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–ã®æ¥µé™å®Ÿè£… + å¯é€†æ€§å•é¡Œã®æ ¹æœ¬çš„è§£æ±º
    """
    
    def __init__(self):
        try:
            # pydivsufsortã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨é€†å¤‰æ›é–¢æ•°ã®å­˜åœ¨ç¢ºèª
            import pydivsufsort
            self.pydivsufsort_available = True
            self.pydivsufsort = pydivsufsort
            print("ğŸ”¥ pydivsufsortåˆ©ç”¨å¯èƒ½ - é«˜é€ŸBWT + å …ç‰¢ãªé€†å¤‰æ›æœ‰åŠ¹")
        except ImportError:
            self.pydivsufsort_available = False
            print("âš ï¸ pydivsufsortæœªåˆ©ç”¨ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…")
        
        self.post_bwt_pipeline = PostBWTPipeline()
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """TMC v8.1 å®Œå…¨å …ç‰¢åŒ–BWTå¤‰æ›ï¼ˆpydivsufsortå®Œå…¨æº–æ‹ ï¼‰"""
        print("  [å¼·åŒ–BWT] TMC v8.1 å°‚é–€å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'enhanced_bwt_mtf_rle', 'original_size': len(data)}
        
        try:
            if not data:
                return [data], info
            
            # å‹•çš„ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆä¸¦åˆ—å‡¦ç†å‰æã§æ‹¡å¼µï¼‰
            MAX_BWT_SIZE = 2 * 1024 * 1024  # 2MBåˆ¶é™
            if len(data) > MAX_BWT_SIZE:
                print(f"    [å¼·åŒ–BWT] ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º({len(data)})ãŒåˆ¶é™({MAX_BWT_SIZE})ã‚’è¶…é - BWTã‚¹ã‚­ãƒƒãƒ—")
                info['method'] = 'bwt_skipped_large'
                return [data], info
            
            # pydivsufsortã«å®Œå…¨æº–æ‹ ã—ãŸBWTå®Ÿè£…
            if self.pydivsufsort_available:
                try:
                    print(f"    [å¼·åŒ–BWT] pydivsufsortã§BWTå®Ÿè¡Œä¸­...")
                    # pydivsufsortã¯(primary_index, bwt_array)ã®é †åºã§è¿”ã™
                    primary_index, bwt_array = self.pydivsufsort.bw_transform(data)
                    bwt_encoded = bytes(bwt_array)  # ndarrayã‚’bytesã«å¤‰æ›
                    print(f"    [å¼·åŒ–BWT] pydivsufsortæˆåŠŸ: BWT={len(bwt_encoded)}, index={primary_index}")
                except Exception as pyd_error:
                    print(f"    [å¼·åŒ–BWT] pydivsufsortã‚¨ãƒ©ãƒ¼: {pyd_error}")
                    print(f"    [å¼·åŒ–BWT] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«åˆ‡ã‚Šæ›¿ãˆ")
                    bwt_encoded, primary_index = self._fallback_bwt_transform(data)
            else:
                bwt_encoded, primary_index = self._fallback_bwt_transform(data)
            
            # primary_indexã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
            if not (0 <= primary_index < len(bwt_encoded)):
                raise ValueError(f"Invalid primary_index {primary_index} for BWT length {len(bwt_encoded)}")
            
            # Move-to-Frontå¤‰æ›
            mtf_encoded = self._mtf_encode(bwt_encoded)
            print(f"    [å¼·åŒ–BWT] BWTå¾Œ: {len(bwt_encoded)} bytes -> MTFå¾Œ: {len(mtf_encoded)} bytes")
            
            # MTFå¾Œã®ã‚¼ãƒ­ç‡è¨ˆç®—ï¼ˆåœ§ç¸®åŠ¹æœã®æŒ‡æ¨™ï¼‰
            zero_count = mtf_encoded.count(0)
            zero_ratio = zero_count / len(mtf_encoded) if len(mtf_encoded) > 0 else 0
            print(f"    [MTF] ã‚¼ãƒ­ã®æ¯”ç‡: {zero_ratio:.2%} (é«˜ã„ã»ã©åœ§ç¸®åŠ¹æœå¤§)")
            
            # ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆï¼ˆRLE + åˆ†å‰²ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ï¼‰
            post_bwt_streams = self.post_bwt_pipeline.encode(mtf_encoded)
            print(f"    [å¼·åŒ–BWT] ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: {len(post_bwt_streams)}ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ")
            
            # primary_indexã‚’ãƒã‚¤ãƒˆé…åˆ—ã¨ã—ã¦å…ˆé ­ã«é…ç½®
            index_bytes = primary_index.to_bytes(4, 'big')
            final_streams = [index_bytes] + post_bwt_streams
            
            # æƒ…å ±æ›´æ–°
            info.update({
                'bwt_size': len(bwt_encoded),
                'mtf_size': len(mtf_encoded),
                'zero_ratio': zero_ratio,
                'primary_index': primary_index,
                'enhanced_pipeline': True,
                'stream_count': len(final_streams)
            })
            
            return final_streams, info
            
        except Exception as e:
            print(f"    [å¼·åŒ–BWT] ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ã—ã¦ã‚¹ã‚­ãƒƒãƒ—
            info['method'] = 'bwt_error_skip'
            info['error'] = str(e)
            return [data], info
            print(f"    [å¼·åŒ–BWT] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _fallback_bwt_transform(self, data: bytes) -> Tuple[bytes, int]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®æ¨™æº–BWTå®Ÿè£…"""
        # æ”¹è‰¯ç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
        data_with_sentinel = data + b'\x00'  # ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—è¿½åŠ 
        n = len(data_with_sentinel)
        
        # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªrotationç”Ÿæˆ
        rotations = []
        for i in range(n):
            rotation = data_with_sentinel[i:] + data_with_sentinel[:i]
            rotations.append((rotation, i))
        
        # ã‚½ãƒ¼ãƒˆ
        rotations.sort(key=lambda x: x[0])
        
        # å…ƒã®æ–‡å­—åˆ—ã®ä½ç½®ã‚’ç‰¹å®š
        primary_index = 0
        for idx, (rotation, original_pos) in enumerate(rotations):
            if original_pos == 0:
                primary_index = idx
                break
        
        # BWTæ–‡å­—åˆ—ç”Ÿæˆ
        bwt_encoded = bytes(rotation[0][-1] for rotation, _ in rotations)
        
        return bwt_encoded, primary_index
    
    def _mtf_encode(self, data: bytes) -> bytes:
        """Move-to-Frontå¤‰æ›ï¼ˆBWTã®å±€æ‰€æ€§ã‚’å°ã•ãªæ•´æ•°ã«å¤‰æ›ï¼‰"""
        alphabet = list(range(256))
        encoded = bytearray()
        
        for byte_val in data:
            rank = alphabet.index(byte_val)
            encoded.append(rank)
            # è¦‹ã¤ã‹ã£ãŸæ–‡å­—ã‚’ãƒªã‚¹ãƒˆã®å…ˆé ­ã«ç§»å‹•
            alphabet.pop(rank)
            alphabet.insert(0, byte_val)
        
        return bytes(encoded)
    
    def _mtf_decode(self, encoded_data: bytes) -> bytes:
        """é€†Move-to-Frontå¤‰æ›"""
        alphabet = list(range(256))
        decoded = bytearray()
        
        for rank in encoded_data:
            byte_val = alphabet[rank]
            decoded.append(byte_val)
            # è¦‹ã¤ã‹ã£ãŸæ–‡å­—ã‚’ãƒªã‚¹ãƒˆã®å…ˆé ­ã«ç§»å‹•
            alphabet.pop(rank)
            alphabet.insert(0, byte_val)
        
        return bytes(decoded)
    
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """TMC v8.1 å®Œå…¨å …ç‰¢åŒ–BWTé€†å¤‰æ›ï¼ˆpydivsufsortå®Œå…¨æº–æ‹ ï¼‰"""
        print("  [å¼·åŒ–BWT] TMC v8.1 å°‚é–€é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            # BWTãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸå ´åˆã®å‡¦ç†
            if info.get('method') in ['bwt_skipped_large', 'bwt_error_skip']:
                print(f"    [å¼·åŒ–BWT] {info.get('method')}ãƒ‡ãƒ¼ã‚¿ - å…ƒãƒ‡ãƒ¼ã‚¿è¿”å´")
                return streams[0] if streams else b''
            
            if len(streams) < 1:
                return b''
            
            # primary_indexã®å¾©å…ƒ
            primary_index = int.from_bytes(streams[0], 'big')
            
            # ãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€†å¤‰æ›
            if info.get('enhanced_pipeline', False):
                print("    [ãƒã‚¹ãƒˆBWT] RLEé€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
                mtf_encoded = self.post_bwt_pipeline.decode(streams[1:])
            else:
                mtf_encoded = streams[1] if len(streams) > 1 else b''
            
            # é€†MTFå¤‰æ›
            if info.get('mtf_applied', True):
                bwt_encoded = self._mtf_decode(mtf_encoded)
                print(f"    [MTF] é€†MTF: {len(mtf_encoded)} bytes -> {len(bwt_encoded)} bytes")
            else:
                bwt_encoded = mtf_encoded
            
            # --- é€†BWTãƒ­ã‚¸ãƒƒã‚¯ã®ä¿®æ­£ï¼ˆæ ¹æœ¬çš„è§£æ±ºï¼‰ ---
            if self.pydivsufsort_available:
                # pydivsufsortãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ã€ãã®é€†å¤‰æ›ã®ã¿ã‚’ä½¿ç”¨
                print("    [BWT] pydivsufsortã«ã‚ˆã‚‹å …ç‰¢ãªé€†å¤‰æ›ã‚’å®Ÿè¡Œ")
                # pydivsufsortã®é€†å¤‰æ›: (primary_index, bwt_array) -> original_array
                try:
                    import numpy as np
                    # bytesã‚’writableãªndarrayã«å¤‰æ›
                    bwt_array = np.array(list(bwt_encoded), dtype=np.uint8)
                    original_array = self.pydivsufsort.inverse_bw_transform(primary_index, bwt_array)
                    original_data = bytes(original_array)
                except Exception as inv_error:
                    print(f"    [BWT] pydivsufsorté€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {inv_error}")
                    print(f"    [BWT] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é€†å¤‰æ›ã«åˆ‡ã‚Šæ›¿ãˆ")
                    original_data = self._fallback_bwt_inverse(bwt_encoded, primary_index)
            else:
                # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ä¸å¯ã®å ´åˆã®ã¿ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨
                print("    [BWT] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é€†BWTã‚’å®Ÿè¡Œ")
                original_data = self._fallback_bwt_inverse(bwt_encoded, primary_index)
            
            print(f"    [å¼·åŒ–BWT] é€†å¤‰æ›å®Œäº†: {len(bwt_encoded)} -> {len(original_data)} bytes")
            return original_data
            
        except Exception as e:
            print(f"    [å¼·åŒ–BWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ã«çµåˆã—ã¦è¿”ã™
            if expected_length is not None:
                if len(original_data) != expected_length:
                    print(f"    [è­¦å‘Š] ãƒ‡ãƒ¼ã‚¿é•·ä¸ä¸€è‡´: æœŸå¾…={expected_length}, å®Ÿéš›={len(original_data)}")
                    # å¿…è¦ã«å¿œã˜ã¦åˆ‡ã‚Šè©°ã‚ã¾ãŸã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                    if len(original_data) > expected_length:
                        original_data = original_data[:expected_length]
                        print(f"    [ä¿®æ­£] ãƒ‡ãƒ¼ã‚¿ã‚’æœŸå¾…é•·ã«åˆ‡ã‚Šè©°ã‚: {len(original_data)} bytes")
                else:
                    print(f"    [ç¢ºèª] ãƒ‡ãƒ¼ã‚¿é•·æ•´åˆæ€§: {len(original_data)} bytes âœ“")
            
            return original_data
            
        except Exception as e:
            print(f"    [å¼·åŒ–BWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _fallback_bwt_inverse(self, last_col: bytes, primary_index: int) -> bytes:
        """æ”¹è‰¯ç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é€†BWTå®Ÿè£…ï¼ˆO(n)ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰"""
        n = len(last_col)
        if n == 0:
            return b''
        
        # primary_indexã®ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆå¯é€†æ€§ã®æœ€é‡è¦ãƒã‚¤ãƒ³ãƒˆï¼‰
        if primary_index < 0 or primary_index >= n:
            print(f"    [BWT] è­¦å‘Š: primary_index={primary_index} ãŒç¯„å›²å¤– (0-{n-1})")
            # ãƒ‡ãƒ¼ã‚¿ãŒç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å®‰å…¨ãªå€¤ã‚’ä½¿ç”¨
            if n > 0:
                primary_index = 0  # æœ€åˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨
                print(f"    [BWT] primary_indexã‚’0ã«ãƒªã‚»ãƒƒãƒˆ")
            else:
                return b''
        
        try:
            # å„æ–‡å­—ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            count = [0] * 256
            for char in last_col:
                count[char] += 1
            
            # ç´¯ç©ã‚«ã‚¦ãƒ³ãƒˆã‚’è¨ˆç®—ï¼ˆfirståˆ—ã®é–‹å§‹ä½ç½®ï¼‰
            first_col_starts = [0] * 256
            total = 0
            for i in range(256):
                first_col_starts[i] = total
                total += count[i]
            
            # å¤‰æ›ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ§‹ç¯‰ï¼ˆåŠ¹ç‡çš„ãªO(n)å®Ÿè£…ï¼‰
            next_idx = [0] * n
            char_counts = [0] * 256
            
            for i in range(n):
                char = last_col[i]
                next_idx[i] = first_col_starts[char] + char_counts[char]
                char_counts[char] += 1
            
            # å…ƒã®æ–‡å­—åˆ—ã‚’å¾©å…ƒ
            result = bytearray()
            current_idx = primary_index
            
            for step in range(n):
                if current_idx < 0 or current_idx >= n:
                    print(f"    [BWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: step={step}, current_idx={current_idx} ãŒç¯„å›²å¤–")
                    break
                    
                char = last_col[current_idx]
                result.append(char)
                current_idx = next_idx[current_idx]
            
            # BWTã§ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—ï¼ˆ0ãƒã‚¤ãƒˆï¼‰ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹å ´åˆã®å‡¦ç†
            # pydivsufsortãŒè¿½åŠ ã—ãŸã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—ã‚’é©åˆ‡ã«é™¤å»
            result_bytes = bytes(result)
            
            # æœ«å°¾ã®ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—ã‚’1ã¤ã ã‘é™¤å»ï¼ˆéåº¦ãªé™¤å»ã‚’é˜²æ­¢ï¼‰
            if result_bytes and result_bytes[-1] == 0:
                result_bytes = result_bytes[:-1]
                print(f"    [BWT] ã‚»ãƒ³ãƒãƒãƒ«æ–‡å­—é™¤å»: {len(result)} -> {len(result_bytes)} bytes")
            
            return result_bytes
            
        except Exception as e:
            print(f"    [BWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''


class NEXUSTMCEngineV9:
    """
    NEXUS TMC Engine v9.0 - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°çµ±åˆç‰ˆ
    æ¬¡ä¸–ä»£é‡å­ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
    Transform-Model-Code åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ TMC v9.0
    
    v9.0é©æ–°æ©Ÿèƒ½:
    - é«˜åº¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°ç¬¦å·åŒ–ï¼ˆLZMAã«åŒ¹æ•µã™ã‚‹åœ§ç¸®ç‡ï¼‰
    - è¤‡æ•°äºˆæ¸¬å™¨ + å‹•çš„ãƒŸã‚­ã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹æ¥µé™åœ§ç¸®ç‡å®Ÿç¾
    - BWTTransformerå®Œå…¨å …ç‰¢åŒ– + ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
    """
    
    def __init__(self, max_workers: int = None, chunk_size: int = DEFAULT_CHUNK_SIZE):
        self.max_workers = max_workers or min(8, mp.cpu_count())
        self.chunk_size = chunk_size
        self.dispatcher = ImprovedDispatcher()
        self.core_compressor = CoreCompressor()
        self.context_mixer = ContextMixingEncoder()  # v9.0æ–°æ©Ÿèƒ½
        
        # TMC v9.0 æ–°æ©Ÿèƒ½: å®Œå…¨ä¸¦åˆ—å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        self.enable_parallel_pipeline = True
        self.async_io_enabled = True
        
        # TMC v8.0 æ–°æ©Ÿèƒ½: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹
        self.meta_analyzer = MetaAnalyzer(self.core_compressor)
        
        # å¤‰æ›å™¨ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆv8.0å¼·åŒ–ç‰ˆï¼‰
        self.transformers = {
            DataType.FLOAT_DATA: TDTTransformer(),
            DataType.TEXT_DATA: BWTTransformer(),  # v7.0å¼·åŒ–ç‰ˆï¼ˆãƒã‚¹ãƒˆBWTãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆï¼‰
            DataType.SEQUENTIAL_INT_DATA: LeCoAdvancedTransformer(),  # v8.0: å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
            DataType.STRUCTURED_NUMERIC: TDTTransformer(),
            DataType.TIME_SERIES: LeCoAdvancedTransformer(),  # v8.0å¯¾å¿œ
            DataType.REPETITIVE_BINARY: None,  # RLEå‰å‡¦ç†ã®ã¿
            DataType.COMPRESSED_LIKE: None,    # å¤‰æ›ãªã—
            DataType.GENERIC_BINARY: None,     # å¤‰æ›ãªã—
        }
        
        print(f"ğŸš€ TMC v9.0 ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†: {self.max_workers}ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼, ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º={self.chunk_size//1024}KB (SublinearLZ77+ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°çµ±åˆç‰ˆ)")
        
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'reversibility_tests_passed': 0,
            'reversibility_tests_total': 0,
            'transforms_applied': 0,
            'transforms_bypassed': 0,
            'chunks_processed': 0,           # v8.0è¿½åŠ 
            'parallel_efficiency': 0.0,     # v8.0è¿½åŠ 
            'entropy_coding_used': 0         # v8.0è¿½åŠ 
        }
        
        print(f"ğŸš€ TMC v9.0 ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†: {self.max_workers}ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼, ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º={chunk_size//1024//1024}MB (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°çµ±åˆç‰ˆ)")
    
    def compress_tmc_parallel(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """
        TMC v8.0 ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®å‡¦ç†
        çœŸã®ãƒãƒ«ãƒã‚³ã‚¢æ´»ç”¨ã«ã‚ˆã‚‹é©æ–°çš„ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
        """
        compression_start = time.perf_counter()
        
        try:
            print(f"\n--- TMC v8.0 ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®é–‹å§‹ ({len(data)} bytes) ---")
            
            # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã¯å˜ä¸€ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            if len(data) <= self.chunk_size:
                print("  [ãƒãƒ£ãƒ³ã‚¯åˆ†æ] å°ã‚µã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ - å˜ä¸€ãƒãƒ£ãƒ³ã‚¯å‡¦ç†")
                return self._compress_single_chunk(data)
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self._split_into_chunks(data)
            print(f"  [ãƒãƒ£ãƒ³ã‚¯åˆ†æ] {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
            
            # ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®
            compressed_chunks, chunk_infos = self._compress_chunks_parallel(chunks)
            
            # TMC v8.0 ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰
            container = self._build_tmc_v8_container(compressed_chunks, chunk_infos)
            
            total_time = time.perf_counter() - compression_start
            
            # ä¸¦åˆ—åŠ¹ç‡è¨ˆç®—
            sequential_estimate = total_time * self.max_workers
            parallel_efficiency = min(1.0, sequential_estimate / total_time) if total_time > 0 else 0.0
            
            # çµæœæƒ…å ±
            original_size = len(data)
            compressed_size = len(container)
            compression_ratio = (1 - compressed_size / original_size) * 100 if original_size > 0 else 0
            
            result_info = {
                'compression_ratio': compression_ratio,
                'compression_throughput_mb_s': (original_size / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_compression_time': total_time,
                'chunk_count': len(chunks),
                'chunk_infos': chunk_infos,
                'parallel_workers_used': self.max_workers,
                'parallel_efficiency': parallel_efficiency,
                'original_size': original_size,
                'compressed_size': compressed_size,
                'tmc_version': '8.0',
                'reversible': True,
                'container_format': 'tmc_v8_parallel',
                'entropy_coding_efficiency': sum(1 for info in chunk_infos if info.data_type in ['sequential_int_data', 'text_data']) / len(chunk_infos)
            }
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['chunks_processed'] += len(chunks)
            self.stats['parallel_efficiency'] = parallel_efficiency
            
            print(f"--- TMC v8.0 ä¸¦åˆ—åœ§ç¸®å®Œäº† ---")
            print(f"åœ§ç¸®ç‡: {compression_ratio:.2f}% | ä¸¦åˆ—åŠ¹ç‡: {parallel_efficiency:.2%} | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result_info['compression_throughput_mb_s']:.2f} MB/s")
            
            return container, result_info
            
        except Exception as e:
            print(f"[TMC v8.0] ä¸¦åˆ—åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ä¸€ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
            return self._compress_single_chunk(data)
    
    def _split_into_chunks(self, data: bytes) -> List[bytes]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’æœ€é©ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã«åˆ†å‰²"""
        chunks = []
        for i in range(0, len(data), self.chunk_size):
            chunk = data[i:i + self.chunk_size]
            chunks.append(chunk)
        return chunks
    
    def _compress_chunks_parallel(self, chunks: List[bytes]) -> Tuple[List[bytes], List[ChunkInfo]]:
        """ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®å‡¦ç†"""
        print(f"  [ä¸¦åˆ—å‡¦ç†] {self.max_workers}ãƒ¯ãƒ¼ã‚«ãƒ¼ã§{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—åœ§ç¸®ä¸­...")
        
        compressed_chunks = []
        chunk_infos = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦åˆ—ã§å‡¦ç†
            future_to_chunk = {
                executor.submit(self._compress_chunk, chunk_data, chunk_id): chunk_id 
                for chunk_id, chunk_data in enumerate(chunks)
            }
            
            # çµæœã‚’é †åºé€šã‚Šã«åé›†
            chunk_results = {}
            for future in as_completed(future_to_chunk):
                chunk_id = future_to_chunk[future]
                try:
                    compressed_data, chunk_info = future.result()
                    chunk_results[chunk_id] = (compressed_data, chunk_info)
                    print(f"    [ãƒãƒ£ãƒ³ã‚¯ {chunk_id}] å®Œäº†: {chunk_info.original_size} -> {chunk_info.compressed_size} bytes "
                          f"({chunk_info.compression_ratio:.1f}%, {chunk_info.data_type})")
                except Exception as e:
                    print(f"    [ãƒãƒ£ãƒ³ã‚¯ {chunk_id}] ã‚¨ãƒ©ãƒ¼: {e}")
                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾æ ¼ç´
                    chunk_data = chunks[chunk_id]
                    chunk_results[chunk_id] = (chunk_data, ChunkInfo(
                        chunk_id=chunk_id,
                        original_size=len(chunk_data),
                        compressed_size=len(chunk_data),
                        data_type="error_fallback",
                        compression_ratio=0.0,
                        processing_time=0.0
                    ))
        
        # é †åºé€šã‚Šã«çµæœã‚’é…åˆ—ã«æ ¼ç´
        for chunk_id in sorted(chunk_results.keys()):
            compressed_data, chunk_info = chunk_results[chunk_id]
            compressed_chunks.append(compressed_data)
            chunk_infos.append(chunk_info)
        
        return compressed_chunks, chunk_infos
    
    def _compress_chunk(self, chunk_data: bytes, chunk_id: int) -> Tuple[bytes, ChunkInfo]:
        """å€‹åˆ¥ãƒãƒ£ãƒ³ã‚¯ã®åœ§ç¸®å‡¦ç†ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ï¼‰"""
        chunk_start = time.perf_counter()
        
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†æ
            data_type, features = self.dispatcher.dispatch(chunk_data)
            
            # 2. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹åˆ†æ
            transformer = self.transformers.get(data_type)
            should_transform, meta_info = self.meta_analyzer.should_apply_transform(
                chunk_data, transformer, data_type
            )
            
            # 3. å¤‰æ›å‡¦ç†
            if should_transform and transformer:
                transformed_streams, transform_info = transformer.transform(chunk_data)
            else:
                transformed_streams = [chunk_data]
                transform_info = {'method': 'bypass', 'reason': 'intelligent_bypass'}
            
            # 4. ç¬¦å·åŒ–å‡¦ç†ï¼ˆå‹•çš„ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é¸æŠï¼‰
            final_streams = []
            for stream in transformed_streams:
                if should_transform and data_type in [DataType.SEQUENTIAL_INT_DATA, DataType.TEXT_DATA]:
                    # å¤‰æ›æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã«ç´”ç²‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–
                    compressed_stream, method = self.entropy_encoder.encode_entropy_stream(stream, "transformed")
                    self.stats['entropy_coding_used'] += 1
                else:
                    # æ±ç”¨ãƒ‡ãƒ¼ã‚¿ã«å¾“æ¥å‹åœ§ç¸®
                    stream_entropy = self._calculate_entropy(np.frombuffer(stream, dtype=np.uint8)) if len(stream) > 0 else 4.0
                    compressed_stream, method = self.core_compressor.compress(stream, stream_entropy)
                
                final_streams.append(compressed_stream)
            
            # 5. ãƒãƒ£ãƒ³ã‚¯çµæœãƒ‘ãƒƒã‚­ãƒ³ã‚°
            chunk_compressed = self._pack_chunk_data(final_streams, data_type, transform_info, features)
            
            processing_time = time.perf_counter() - chunk_start
            compression_ratio = (1 - len(chunk_compressed) / len(chunk_data)) * 100 if len(chunk_data) > 0 else 0
            
            chunk_info = ChunkInfo(
                chunk_id=chunk_id,
                original_size=len(chunk_data),
                compressed_size=len(chunk_compressed),
                data_type=data_type.value,
                compression_ratio=compression_ratio,
                processing_time=processing_time
            )
            
            return chunk_compressed, chunk_info
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            processing_time = time.perf_counter() - chunk_start
            chunk_info = ChunkInfo(
                chunk_id=chunk_id,
                original_size=len(chunk_data),
                compressed_size=len(chunk_data),
                data_type="error_fallback",
                compression_ratio=0.0,
                processing_time=processing_time
            )
            return chunk_data, chunk_info
    
    def _pack_chunk_data(self, streams: List[bytes], data_type: DataType, 
                        transform_info: Dict[str, Any], features: Dict[str, Any]) -> bytes:
        """ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒƒã‚­ãƒ³ã‚°"""
        # ãƒãƒ£ãƒ³ã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        chunk_header = {
            'data_type': data_type.value,
            'transform_info': transform_info,
            'stream_count': len(streams),
            'features': {k: v for k, v in features.items() if isinstance(v, (int, float, str, bool))}
        }
        
        header_json = json.dumps(chunk_header, separators=(',', ':'))
        header_bytes = header_json.encode('utf-8')
        
        # ãƒ‘ãƒƒã‚­ãƒ³ã‚°: [ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º(4)] + [ãƒ˜ãƒƒãƒ€ãƒ¼] + [ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°(4)] + [ã‚µã‚¤ã‚º1(4)] + [ã‚µã‚¤ã‚º2(4)]... + [ã‚¹ãƒˆãƒªãƒ¼ãƒ 1] + [ã‚¹ãƒˆãƒªãƒ¼ãƒ 2]...
        packed_data = bytearray()
        packed_data.extend(len(header_bytes).to_bytes(4, 'big'))
        packed_data.extend(header_bytes)
        packed_data.extend(len(streams).to_bytes(4, 'big'))
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºæƒ…å ±
        for stream in streams:
            packed_data.extend(len(stream).to_bytes(4, 'big'))
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿
        for stream in streams:
            packed_data.extend(stream)
        
        return bytes(packed_data)
    
    def _build_tmc_v8_container(self, compressed_chunks: List[bytes], 
                               chunk_infos: List[ChunkInfo]) -> bytes:
        """TMC v8.0 ã‚³ãƒ³ãƒ†ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰"""
        container = bytearray()
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ + ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        container.extend(TMC_V8_MAGIC)
        container.extend(b'8.0\x00')
        
        # ãƒãƒ£ãƒ³ã‚¯æ•°
        container.extend(len(compressed_chunks).to_bytes(4, 'big'))
        
        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«
        for chunk_info in chunk_infos:
            container.extend(chunk_info.chunk_id.to_bytes(4, 'big'))
            container.extend(chunk_info.original_size.to_bytes(4, 'big'))
            container.extend(chunk_info.compressed_size.to_bytes(4, 'big'))
            container.extend(chunk_info.data_type.encode('utf-8')[:16].ljust(16, b'\x00'))
        
        # åœ§ç¸®æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        for chunk_data in compressed_chunks:
            container.extend(chunk_data)
        
        return bytes(container)
    
    def _compress_single_chunk(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """å˜ä¸€ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼ˆv7.0äº’æ›ï¼‰"""
        return self.compress_tmc(data)
    
    def compress_tmc(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC v7.0 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆçµ±åˆåœ§ç¸®å‡¦ç†"""
        compression_start = time.perf_counter()
        
        try:
            print("\n--- TMC v7.0 ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®é–‹å§‹ ---")
            
            # 1. æ”¹è‰¯åˆ†æ&ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ
            data_type, features = self.dispatcher.dispatch(data)
            
            # 2. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹åˆ†æï¼ˆTMC v7.0æ–°æ©Ÿèƒ½ï¼‰
            transformer = self.transformers.get(data_type)
            should_transform, meta_info = self.meta_analyzer.should_apply_transform(
                data, transformer, data_type
            )
            
            # 3. é©å¿œçš„å¤‰æ›ï¼ˆã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåˆ¤å®šã«åŸºã¥ãï¼‰
            if should_transform and transformer:
                print(f"  [ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ] {data_type.value} å¤‰æ›ã‚’å®Ÿè¡Œ")
                transformed_streams, transform_info = transformer.transform(data)
                self.stats['transforms_applied'] += 1
                
                # ãƒ¡ã‚¿åˆ†ææƒ…å ±ã‚’å¤‰æ›æƒ…å ±ã«çµ±åˆ
                transform_info['meta_analysis'] = meta_info
                transform_info['bypassed'] = False
            else:
                print(f"  [ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ] {data_type.value} å¤‰æ›ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                transformed_streams = [data]
                transform_info = {
                    'method': 'bypassed', 
                    'meta_analysis': meta_info,
                    'bypassed': True,
                    'reason': meta_info.get('reason', 'ineffective')
                }
                self.stats['transforms_bypassed'] += 1
            
            # 4. ä¸¦åˆ—ã‚³ã‚¢åœ§ç¸®ï¼ˆv9.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¯¾å¿œï¼‰
            compressed_streams = []
            compression_methods = []
            
            print("  [ç¬¦å·åŒ–] TMC v9.0 ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé©å¿œå‹åœ§ç¸®ä¸­...")
            for i, stream in enumerate(transformed_streams):
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
                if len(stream) > 0:
                    stream_array = np.frombuffer(stream, dtype=np.uint8)
                    stream_entropy = self._calculate_entropy(stream_array)
                else:
                    stream_entropy = 0.0
                
                # v9.0: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°é©ç”¨åˆ¤å®š
                use_context_mixing = (
                    should_transform and  # å¤‰æ›ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆ
                    len(stream) > 2048 and  # 2KBä»¥ä¸Š
                    stream_entropy > 3.0 and  # é©åº¦ãªã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                    stream_entropy < 7.0  # ãƒ©ãƒ³ãƒ€ãƒ éããªã„
                )
                
                # TMCçµ±ä¸€åœ§ç¸®ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°å¯¾å¿œï¼‰
                compressed, comp_method = self.core_compressor.compress(
                    stream, 
                    stream_entropy=stream_entropy, 
                    stream_size=len(stream),
                    use_context_mixing=use_context_mixing
                )
                compressed_streams.append(compressed)
                compression_methods.append(comp_method)
                
                context_info = " (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°)" if use_context_mixing else ""
                print(f"    ã‚¹ãƒˆãƒªãƒ¼ãƒ  {i}: {len(stream)} bytes -> {len(compressed)} bytes ({comp_method}, ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {stream_entropy:.2f}){context_info}")
                            # 5. TMC v7.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰
            final_data = self._pack_tmc_v7(compressed_streams, compression_methods, 
                                          data_type, transform_info, features)
            
            total_time = time.perf_counter() - compression_start
            
            # çµæœæƒ…å ±
            compression_ratio = (1 - len(final_data) / len(data)) * 100 if len(data) > 0 else 0
            
            result_info = {
                'compression_ratio': compression_ratio,
                'compression_throughput_mb_s': (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_compression_time': total_time,
                'data_type': data_type.value,
                'features': features,
                'transform_info': transform_info,
                'compression_methods': compression_methods,
                'stream_count': len(compressed_streams),
                'original_size': len(data),
                'compressed_size': len(final_data),
                'tmc_version': '7.0',
                'reversible': True,
                'zstd_used': self.core_compressor.zstd_available,
                'intelligent_bypass_used': True,  # v7.0æ–°æ©Ÿèƒ½
                'transform_applied': should_transform,
                'meta_analysis': meta_info
            }
            
            print(f"--- TMC v7.0 åœ§ç¸®å®Œäº† ---")
            print(f"åˆè¨ˆã‚µã‚¤ã‚º: {len(data)} bytes -> {len(final_data)} bytes (åœ§ç¸®ç‡: {compression_ratio:.2f}%)")
            print(f"å¤‰æ›: {'é©ç”¨' if should_transform else 'ã‚¹ã‚­ãƒƒãƒ—'}")
            
            return final_data, result_info
            
        except Exception as e:
            total_time = time.perf_counter() - compression_start
            print(f"âŒ åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return data, {
                'compression_ratio': 0.0,
                'error': str(e),
                'total_compression_time': total_time,
                'reversible': True
            }
    
    def _calculate_entropy(self, data_array: np.ndarray) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
        try:
            byte_counts = np.bincount(data_array, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(data_array)
            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 4.0
    
    def decompress_tmc(self, compressed_data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC v7.0 å±•é–‹å‡¦ç†"""
        decompression_start = time.perf_counter()
        
        try:
            print("\n--- TMC v7.0 å±•é–‹é–‹å§‹ ---")
            
            # TMC v7.0 ãƒ˜ãƒƒãƒ€ãƒ¼è§£æï¼ˆv6.0äº’æ›ï¼‰
            header = self._parse_tmc_v7_header(compressed_data)
            if not header:
                raise ValueError("Invalid TMC v7.0 format")
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º
            payload = compressed_data[header['header_size']:]
            streams = self._extract_tmc_v7_streams(payload, header)
            
            # ä¸¦åˆ—å±•é–‹
            decompressed_streams = []
            for i, (stream, method) in enumerate(zip(streams, header['compression_methods'])):
                decompressed = self.core_compressor.decompress(stream, method)
                decompressed_streams.append(decompressed)
                print(f"    ã‚¹ãƒˆãƒªãƒ¼ãƒ  {i}: {len(stream)} bytes -> {len(decompressed)} bytes ({method})")
            
            # é€†å¤‰æ›ï¼ˆã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹å¯¾å¿œï¼‰
            data_type = DataType(header['data_type'])
            transformer = self.transformers.get(data_type)
            
            # å¤‰æ›ãŒãƒã‚¤ãƒ‘ã‚¹ã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
            transform_bypassed = header.get('transform_bypassed', False)
            
            if transformer and not transform_bypassed:
                print(f"  [é€†å¤‰æ›] {data_type.value} é€†å¤‰æ›ã‚’å®Ÿè¡Œ")
                original_data = transformer.inverse_transform(decompressed_streams, header['transform_info'])
            else:
                print(f"  [é€†å¤‰æ›] {data_type.value} å¤‰æ›ãƒã‚¤ãƒ‘ã‚¹ - ç›´æ¥çµåˆ")
                original_data = b''.join(decompressed_streams)
            
            total_time = time.perf_counter() - decompression_start
            
            print(f"--- TMC v7.0 å±•é–‹å®Œäº† ---")
            print(f"å†æ§‹ç¯‰ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(original_data)} bytes")
            
            result_info = {
                'decompression_throughput_mb_s': (len(original_data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_decompression_time': total_time,
                'decompressed_size': len(original_data),
                'tmc_version': '7.0',
                'transform_bypassed': transform_bypassed
            }
            
            return original_data, result_info
            
        except Exception as e:
            total_time = time.perf_counter() - decompression_start
            print(f"âŒ å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data, {
                'error': str(e),
                'total_decompression_time': total_time
            }
    
    def test_reversibility(self, test_data: bytes, test_name: str = "test") -> Dict[str, Any]:
        """TMC v7.0 å¯é€†æ€§ãƒ†ã‚¹ãƒˆ"""
        test_start_time = time.perf_counter()
        
        try:
            print(f"ğŸ”„ TMC v7.0 å¯é€†æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹: {test_name}")
            
            # åœ§ç¸®
            compressed, compression_info = self.compress_tmc(test_data)
            
            # å±•é–‹
            decompressed, decompression_info = self.decompress_tmc(compressed)
            
            # æ¤œè¨¼
            is_identical = (test_data == decompressed)
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['reversibility_tests_total'] += 1
            if is_identical:
                self.stats['reversibility_tests_passed'] += 1
            
            result_icon = "âœ…" if is_identical else "âŒ"
            transform_status = "é©ç”¨" if compression_info.get('transform_applied', False) else "ã‚¹ã‚­ãƒƒãƒ—"
            print(f"   {result_icon} å¯é€†æ€§: {'æˆåŠŸ' if is_identical else 'å¤±æ•—'} | å¤‰æ›: {transform_status}")
            
            return {
                'test_name': test_name,
                'reversible': is_identical,
                'original_size': len(test_data),
                'compressed_size': len(compressed),
                'decompressed_size': len(decompressed),
                'compression_ratio': compression_info.get('compression_ratio', 0),
                'compression_time': compression_info.get('total_compression_time', 0),
                'decompression_time': decompression_info.get('total_decompression_time', 0),
                'compression_throughput_mb_s': compression_info.get('compression_throughput_mb_s', 0),
                'decompression_throughput_mb_s': decompression_info.get('decompression_throughput_mb_s', 0),
                'total_test_time': time.perf_counter() - test_start_time,
                'data_type': compression_info.get('data_type', 'unknown'),
                'zstd_used': compression_info.get('zstd_used', False),
                'tmc_version': '7.0',
                'transform_applied': compression_info.get('transform_applied', False),
                'intelligent_bypass_used': compression_info.get('intelligent_bypass_used', False),
                'meta_analysis': compression_info.get('meta_analysis', {})
            }
            
        except Exception as e:
            return {
                'test_name': test_name,
                'reversible': False,
                'error': str(e),
                'tmc_version': '7.0'
            }
    
    def _pack_tmc_v7(self, streams: List[bytes], methods: List[str], 
                     data_type: DataType, transform_info: Dict[str, Any], 
                     features: Dict[str, Any]) -> bytes:
        """TMC v7.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰ï¼ˆã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹å¯¾å¿œï¼‰"""
        try:
            header = bytearray()
            
            # TMC v7.0 ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
            header.extend(b'TMC7')
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
            data_type_bytes = data_type.value.encode('utf-8')[:32].ljust(32, b'\x00')
            header.extend(data_type_bytes)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
            header.extend(struct.pack('<I', len(streams)))
            
            # åœ§ç¸®ãƒ¡ã‚½ãƒƒãƒ‰æƒ…å ±
            for method in methods:
                method_bytes = method.encode('utf-8')[:16].ljust(16, b'\x00')
                header.extend(method_bytes)
            
            # å¤‰æ›æƒ…å ±ï¼ˆå®‰å…¨ãªJSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã€ãƒ¡ã‚¿åˆ†ææƒ…å ±çµ±åˆï¼‰
            transform_info_safe = self._make_json_safe(transform_info)
            transform_str = json.dumps(transform_info_safe, separators=(',', ':'))
            transform_bytes = transform_str.encode('utf-8')
            header.extend(struct.pack('<I', len(transform_bytes)))
            header.extend(transform_bytes)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚ºãƒ†ãƒ¼ãƒ–ãƒ«
            for stream in streams:
                header.extend(struct.pack('<I', len(stream)))
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
            payload = b''.join(streams)
            checksum = zlib.crc32(payload) & 0xffffffff
            header.extend(struct.pack('<I', checksum))
            
            return bytes(header) + payload
            
        except Exception:
            return b''.join(streams)
    
    def _parse_tmc_v7_header(self, data: bytes) -> Optional[Dict[str, Any]]:
        """TMC v7.0 ãƒ˜ãƒƒãƒ€ãƒ¼è§£æï¼ˆv6.0äº’æ›ï¼‰"""
        try:
            if len(data) < 44 or (data[:4] != b'TMC7' and data[:4] != b'TMC6' and data[:4] != b'TMC4'):
                return None
            
            offset = 4
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
            data_type = data[offset:offset+32].rstrip(b'\x00').decode('utf-8')
            offset += 32
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
            stream_count = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            # åœ§ç¸®ãƒ¡ã‚½ãƒƒãƒ‰
            compression_methods = []
            for _ in range(stream_count):
                method = data[offset:offset+16].rstrip(b'\x00').decode('utf-8')
                compression_methods.append(method)
                offset += 16
            
            # å¤‰æ›æƒ…å ±ï¼ˆå®‰å…¨ãªJSONè§£æï¼‰
            transform_info_size = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            transform_info_str = data[offset:offset+transform_info_size].decode('utf-8')
            transform_info = json.loads(transform_info_str)
            offset += transform_info_size
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚µã‚¤ã‚º
            stream_sizes = []
            for _ in range(stream_count):
                size = struct.unpack('<I', data[offset:offset+4])[0]
                stream_sizes.append(size)
                offset += 4
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
            checksum = struct.unpack('<I', data[offset:offset+4])[0]
            offset += 4
            
            # v7.0æ©Ÿèƒ½ã®è§£æ
            transform_bypassed = transform_info.get('bypassed', False)
            
            return {
                'data_type': data_type,
                'stream_count': stream_count,
                'compression_methods': compression_methods,
                'transform_info': transform_info,
                'stream_sizes': stream_sizes,
                'checksum': checksum,
                'header_size': offset,
                'transform_bypassed': transform_bypassed  # v7.0æ–°æ©Ÿèƒ½
            }
            
        except Exception:
            return None
    
    def _extract_tmc_v7_streams(self, payload: bytes, header: Dict[str, Any]) -> List[bytes]:
        """TMC v7.0 ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º"""
        try:
            streams = []
            offset = 0
            
            for size in header['stream_sizes']:
                stream = payload[offset:offset+size]
                streams.append(stream)
                offset += size
            
            return streams
            
        except Exception:
            return [payload]
    
    def _make_json_safe(self, data: Any) -> Any:
        """JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›"""
        if isinstance(data, dict):
            return {k: self._make_json_safe(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._make_json_safe(v) for v in data]
        elif isinstance(data, np.ndarray):
            return data.tolist()
        elif isinstance(data, (np.int32, np.int64, np.float32, np.float64)):
            return float(data)
        elif isinstance(data, np.bool_):
            return bool(data)
        else:
            return data
    
    def _extract_tmc_v4_streams(self, payload: bytes, header: Dict[str, Any]) -> List[bytes]:
        """TMC v4.0 ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡ºï¼ˆäº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰"""
        return self._extract_tmc_v7_streams(payload, header)


# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
NEXUSTMCEngineV8 = NEXUSTMCEngineV9  # v8.xç³»ã‹ã‚‰ã®ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨

# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
__all__ = ['NEXUSTMCEngineV9', 'NEXUSTMCEngineV8', 'DataType']

if __name__ == "__main__":
    print("ğŸš€ NEXUS TMC Engine v9.0 - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒŸã‚­ã‚·ãƒ³ã‚°çµ±åˆç‰ˆ")
    
    engine = NEXUSTMCEngineV9()
    
    # TMC v8.0 ç‰¹åŒ–ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        ("æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿", np.linspace(1000, 1010, 2000, dtype=np.float32).tobytes()),
        ("ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿", np.arange(0, 8000, 4, dtype=np.int32).tobytes()),
        ("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿", ("Hello TMC v8.0! " * 500).encode('utf-8')),
        ("åå¾©ãƒã‚¤ãƒŠãƒª", b"PATTERN" * 1000),
        ("æ±ç”¨ãƒã‚¤ãƒŠãƒª", bytes(range(256)) * 20),
        ("ä¸¦åˆ—ãƒ†ã‚¹ãƒˆï¼ˆå¤§å®¹é‡ï¼‰", np.arange(0, 50000, dtype=np.int32).tobytes()),  # v8.0: ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ
        ("å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ", np.concatenate([
            np.arange(1000, 2000, dtype=np.int32),  # ç·šå½¢ãƒ‘ãƒ¼ãƒˆ
            np.full(500, 5000, dtype=np.int32),      # å®šæ•°ãƒ‘ãƒ¼ãƒˆ
        ]).tobytes()),  # v8.0: LeCoãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
    ]
    
    success_count = 0
    total_tests = len(test_cases)
    
    for name, data in test_cases:
        result = engine.test_reversibility(data, name)
        if result.get('reversible', False):
            success_count += 1
        
        # v7.0æ–°æ©Ÿèƒ½ã®è©³ç´°è¡¨ç¤º
        print(f"  ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ»ãƒã‚¤ãƒ‘ã‚¹: {'æœ‰åŠ¹' if result.get('intelligent_bypass_used') else 'ç„¡åŠ¹'}")
        if 'meta_analysis' in result and result['meta_analysis']:
            meta = result['meta_analysis']
            print(f"  ãƒ¡ã‚¿åˆ†æ: {meta.get('reason', 'N/A')}")
            if 'effectiveness' in meta:
                print(f"  åœ§ç¸®åŠ¹æœ: {meta['effectiveness']:.2%}")
    
    print(f"\nğŸ“Š TMC v8.0 ãƒ†ã‚¹ãƒˆçµæœ: {success_count}/{total_tests} æˆåŠŸ")
    print(f"ğŸ“ˆ çµ±è¨ˆ:")
    print(f"  å¤‰æ›é©ç”¨: {engine.stats['transforms_applied']}")
    print(f"  å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—: {engine.stats['transforms_bypassed']}")
    print(f"  ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–ä½¿ç”¨: {engine.stats['entropy_coding_used']}")
    print(f"  ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: {engine.stats['chunks_processed']}")
    
    if success_count == total_tests:
        print("ğŸ‰ TMC v8.0 æ¬¡ä¸–ä»£é‡å­ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆåœ§ç¸®ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æº–å‚™å®Œäº†!")
        print("ğŸ”¥ ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç† + å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚° + ç´”ç²‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–çµ±åˆå®Œäº†!")
        if ZSTD_AVAILABLE:
            print("âš¡ æœ€é«˜æ€§èƒ½æ§‹æˆ: çœŸã®ä¸¦åˆ—å‡¦ç† + LeCoãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚° + é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¬¦å·åŒ–!")
    else:
        print("âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•— - ã•ã‚‰ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦")

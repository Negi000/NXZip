#!/usr/bin/env python3
"""
NEXUS TMC Engine v5.0 - ç†è«–çš„æ”¹è‰¯ç‰ˆ
Transform-Model-Code é©å‘½çš„åœ§ç¸®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
ãƒ¦ãƒ¼ã‚¶ãƒ¼æ”¹å–„ææ¡ˆçµ±åˆ: ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ + çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° + Zstandardçµ±ä¸€
"""

import os
import sys
import time
import json
import struct
import zlib
import numpy as np
from typing import Tuple, Dict, Any, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from enum import Enum
import logging

# scikit-learnä»£æ›¿ã®è»½é‡å®Ÿè£…
try:
    from sklearn.cluster import KMeans
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("âš ï¸ scikit-learnæœªåˆ©ç”¨ - è»½é‡å®Ÿè£…ã‚’ä½¿ç”¨")

class LightweightStandardScaler:
    """è»½é‡StandardScalerå®Ÿè£…"""
    def __init__(self):
        self.mean_ = None
        self.scale_ = None
    
    def fit_transform(self, X):
        X = np.array(X)
        self.mean_ = np.mean(X, axis=0)
        self.scale_ = np.std(X, axis=0)
        self.scale_[self.scale_ == 0] = 1  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
        return (X - self.mean_) / self.scale_
    
    def transform(self, X):
        X = np.array(X)
        if self.mean_ is not None and self.scale_ is not None:
            return (X - self.mean_) / self.scale_
        return X


class LightweightKMeans:
    """è»½é‡KMeanså®Ÿè£…"""
    def __init__(self, n_clusters=2, random_state=42, n_init=10):
        self.n_clusters = n_clusters
        self.random_state = random_state
        self.n_init = n_init
        self.inertia_ = 0
    
    def fit_predict(self, X):
        X = np.array(X)
        if len(X) < self.n_clusters:
            return np.arange(len(X))
        
        np.random.seed(self.random_state)
        best_labels = None
        best_inertia = float('inf')
        
        for _ in range(self.n_init):
            # ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–
            centroids = X[np.random.choice(len(X), self.n_clusters, replace=False)]
            
            for iteration in range(100):  # æœ€å¤§100å›
                # è·é›¢è¨ˆç®—ã¨ã‚¯ãƒ©ã‚¹ã‚¿å‰²ã‚Šå½“ã¦
                distances = np.sqrt(((X[:, np.newaxis] - centroids) ** 2).sum(axis=2))
                labels = np.argmin(distances, axis=1)
                
                # æ–°ã—ã„é‡å¿ƒè¨ˆç®—
                new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(self.n_clusters)])
                
                # åæŸåˆ¤å®š
                if np.allclose(centroids, new_centroids):
                    break
                centroids = new_centroids
            
            # æ…£æ€§è¨ˆç®—
            inertia = sum(np.sum((X[labels == k] - centroids[k]) ** 2) for k in range(self.n_clusters))
            
            if inertia < best_inertia:
                best_inertia = inertia
                best_labels = labels
        
        self.inertia_ = best_inertia
        return best_labels


class LightweightDecisionTree:
    """è»½é‡DecisionTreeå®Ÿè£…"""
    def __init__(self, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.random_state = random_state
        self.tree_ = None
        self.classes_ = None
    
    def fit(self, X, y):
        X, y = np.array(X), np.array(y)
        self.classes_ = np.unique(y)
        self.tree_ = self._build_tree(X, y, depth=0)
        return self
    
    def _build_tree(self, X, y, depth):
        if (len(set(y)) == 1 or 
            len(X) < self.min_samples_split or 
            depth >= self.max_depth):
            return {'class': self._most_common_class(y)}
        
        best_feature, best_threshold = self._find_best_split(X, y)
        
        if best_feature is None:
            return {'class': self._most_common_class(y)}
        
        mask = X[:, best_feature] <= best_threshold
        left_X, left_y = X[mask], y[mask]
        right_X, right_y = X[~mask], y[~mask]
        
        if len(left_y) < self.min_samples_leaf or len(right_y) < self.min_samples_leaf:
            return {'class': self._most_common_class(y)}
        
        return {
            'feature': best_feature,
            'threshold': best_threshold,
            'left': self._build_tree(left_X, left_y, depth + 1),
            'right': self._build_tree(right_X, right_y, depth + 1)
        }
    
    def _find_best_split(self, X, y):
        best_gini = float('inf')
        best_feature, best_threshold = None, None
        
        for feature in range(X.shape[1]):
            values = np.unique(X[:, feature])
            for i in range(len(values) - 1):
                threshold = (values[i] + values[i + 1]) / 2
                gini = self._calculate_gini_split(X[:, feature], y, threshold)
                
                if gini < best_gini:
                    best_gini = gini
                    best_feature = feature
                    best_threshold = threshold
        
        return best_feature, best_threshold
    
    def _calculate_gini_split(self, feature_values, y, threshold):
        mask = feature_values <= threshold
        left_y, right_y = y[mask], y[~mask]
        
        if len(left_y) == 0 or len(right_y) == 0:
            return float('inf')
        
        left_gini = self._gini_impurity(left_y)
        right_gini = self._gini_impurity(right_y)
        
        weighted_gini = (len(left_y) * left_gini + len(right_y) * right_gini) / len(y)
        return weighted_gini
    
    def _gini_impurity(self, y):
        _, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        return 1.0 - np.sum(probabilities ** 2)
    
    def _most_common_class(self, y):
        values, counts = np.unique(y, return_counts=True)
        return values[np.argmax(counts)]
    
    def predict(self, X):
        return np.array([self._predict_single(x, self.tree_) for x in X])
    
    def _predict_single(self, x, node):
        if 'class' in node:
            return node['class']
        
        if x[node['feature']] <= node['threshold']:
            return self._predict_single(x, node['left'])
        else:
            return self._predict_single(x, node['right'])
    
    def predict_proba(self, X):
        predictions = self.predict(X)
        probabilities = []
        
        for pred in predictions:
            prob_array = np.zeros(len(self.classes_))
            class_idx = np.where(self.classes_ == pred)[0][0]
            prob_array[class_idx] = 1.0
            probabilities.append(prob_array)
        
        return np.array(probabilities)
    
    def score(self, X, y):
        predictions = self.predict(X)
        return np.mean(predictions == y)
try:
    import zstandard as zstd
    ZSTD_AVAILABLE = True
    print("ğŸš€ Zstandardçµ±ä¸€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ - TMC v5.0 é«˜æ€§èƒ½ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹")
except ImportError:
    ZSTD_AVAILABLE = False
    print("âŒ Zstandardå¿…é ˆ - TMC v5.0ã¯é«˜æ€§èƒ½ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒå¿…è¦ã§ã™")
    sys.exit(1)


class DataType(Enum):
    """TMC v5.0 æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ†é¡"""
    FLOAT_DATA = "float_data"
    TEXT_DATA = "text_data"
    SEQUENTIAL_INT_DATA = "sequential_int_data"
    STRUCTURED_NUMERIC = "structured_numeric"
    TIME_SERIES = "time_series"
    REPETITIVE_BINARY = "repetitive_binary"
    COMPRESSED_LIKE = "compressed_like"
    MIXED_NUMERIC = "mixed_numeric"
    BINARY_EXECUTABLE = "binary_executable"
    GENERIC_BINARY = "generic_binary"


class ZstandardUnifiedCompressor:
    """
    TMC v5.0 Zstandardçµ±ä¸€åœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³
    ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨ã‚µã‚¤ã‚ºã«åŸºã¥ãå‹•çš„ãƒ¬ãƒ™ãƒ«é¸æŠ
    """
    
    def __init__(self):
        if not ZSTD_AVAILABLE:
            raise RuntimeError("Zstandard is required for TMC v5.0")
        
        # åœ§ç¸®ãƒ¬ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ï¼‰
        self.level_mapping = {
            'ultra_fast': 1,     # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»å°ã‚µã‚¤ã‚º
            'fast': 3,           # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»å°ã‚µã‚¤ã‚º
            'balanced': 6,       # æ¨™æº–ãƒ¬ãƒ™ãƒ«
            'high': 15,          # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»å¤§ã‚µã‚¤ã‚º
            'ultra': 22          # æ¥µä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»è¶…å¤§ã‚µã‚¤ã‚º
        }
        
        # äº‹å‰æ§‹ç¯‰æ¸ˆã¿åœ§ç¸®å™¨ï¼ˆæ€§èƒ½æœ€é©åŒ–ï¼‰
        self.compressors = {}
        self.decompressor = zstd.ZstdDecompressor()
        
        for level_name, level in self.level_mapping.items():
            self.compressors[level_name] = zstd.ZstdCompressor(level=level)
    
    def select_compression_level(self, data: bytes, entropy: float = None) -> str:
        """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãå‹•çš„åœ§ç¸®ãƒ¬ãƒ™ãƒ«é¸æŠ"""
        try:
            size = len(data)
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆæœªæä¾›ã®å ´åˆï¼‰
            if entropy is None:
                entropy = self._calculate_entropy(data)
            
            # ã‚µã‚¤ã‚ºãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒãƒˆãƒªã‚¯ã‚¹ã«ã‚ˆã‚‹å‹•çš„é¸æŠ
            if entropy > 7.5:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆåœ§ç¸®å›°é›£ï¼‰
                return 'ultra_fast'
            elif entropy > 6.5:  # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                if size < 4096:
                    return 'fast'
                else:
                    return 'balanced'
            elif entropy > 4.0:  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆåœ§ç¸®å®¹æ˜“ï¼‰
                if size > 65536:  # 64KBä»¥ä¸Š
                    return 'high'
                else:
                    return 'balanced'
            else:  # æ¥µä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆè¶…åœ§ç¸®å¯¾è±¡ï¼‰
                if size > 131072:  # 128KBä»¥ä¸Š
                    return 'ultra'
                else:
                    return 'high'
                    
        except Exception:
            return 'balanced'
    
    def compress(self, data: bytes, entropy: float = None) -> Tuple[bytes, str]:
        """çµ±ä¸€Zstandardåœ§ç¸®"""
        try:
            if len(data) == 0:
                return data, "empty"
            
            # å‹•çš„ãƒ¬ãƒ™ãƒ«é¸æŠ
            level_name = self.select_compression_level(data, entropy)
            compressor = self.compressors[level_name]
            
            # åœ§ç¸®å®Ÿè¡Œ
            compressed = compressor.compress(data)
            
            # åœ§ç¸®åŠ¹æœæ¤œè¨¼ï¼ˆè†¨å¼µé˜²æ­¢ï¼‰
            if len(compressed) >= len(data) * 0.95:  # 5%ä»¥ä¸‹ã®åœ§ç¸®åŠ¹æœã®å ´åˆ
                return data, "store"
            
            return compressed, f"zstd_{level_name}"
            
        except Exception:
            return data, "store"
    
    def decompress(self, compressed_data: bytes, method: str) -> bytes:
        """çµ±ä¸€Zstandardå±•é–‹"""
        try:
            if method == "empty" or method == "store":
                return compressed_data
            elif method.startswith("zstd_"):
                return self.decompressor.decompress(compressed_data)
            else:
                # ãƒ¬ã‚¬ã‚·ãƒ¼å¯¾å¿œ
                return compressed_data
                
        except Exception:
            return compressed_data
    
    def _calculate_entropy(self, data: bytes) -> float:
        """é«˜é€Ÿã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        try:
            data_array = np.frombuffer(data, dtype=np.uint8)
            byte_counts = np.bincount(data_array, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(data_array)
            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 8.0


class DataDrivenDispatcher:
    """
    TMC v5.0 ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£
    æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®é«˜ç²¾åº¦ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¤å®š
    """
    
    def __init__(self):
        if SKLEARN_AVAILABLE:
            self.feature_scaler = StandardScaler()
            self.ml_classifier = None
        else:
            self.feature_scaler = LightweightStandardScaler()
            self.ml_classifier = None
        
        self.is_trained = False
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹
        self.rule_based_fallback = True
        
        # ç‰¹å¾´é‡æŠ½å‡ºè¨­å®š
        self.sample_size = 32768  # é«˜é€Ÿåˆ†æç”¨ã‚µãƒ³ãƒ—ãƒ«
        
    def extract_comprehensive_features(self, data: bytes) -> np.ndarray:
        """åŒ…æ‹¬çš„ç‰¹å¾´é‡æŠ½å‡ºï¼ˆæ©Ÿæ¢°å­¦ç¿’ç”¨ï¼‰"""
        try:
            features = []
            
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            sample_data = data[:self.sample_size] if len(data) > self.sample_size else data
            data_array = np.frombuffer(sample_data, dtype=np.uint8)
            
            # 1. åŸºæœ¬çµ±è¨ˆç‰¹å¾´é‡
            features.extend([
                len(data),  # ã‚µã‚¤ã‚º
                float(np.mean(data_array)),  # å¹³å‡
                float(np.std(data_array)),   # æ¨™æº–åå·®
                float(np.var(data_array)),   # åˆ†æ•£
                float(np.min(data_array)),   # æœ€å°å€¤
                float(np.max(data_array)),   # æœ€å¤§å€¤
            ])
            
            # 2. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç‰¹å¾´é‡
            byte_counts = np.bincount(data_array, minlength=256)
            probabilities = byte_counts / len(data_array)
            entropy = float(-np.sum(probabilities[probabilities > 0] * np.log2(probabilities[probabilities > 0])))
            features.append(entropy)
            
            # 3. ãƒã‚¤ãƒˆåˆ†å¸ƒç‰¹å¾´é‡
            features.extend([
                float(np.sum((data_array >= 32) & (data_array <= 126)) / len(data_array)),  # ASCIIæ–‡å­—ç‡
                float(np.sum(data_array == 0) / len(data_array)),  # NULLæ–‡å­—ç‡
                float(np.sum(data_array == 255) / len(data_array)),  # 0xFFç‡
                len(np.unique(data_array)) / 256.0,  # æ–‡å­—ç¨®å¤šæ§˜æ€§
            ])
            
            # 4. æ§‹é€ çš„ç‰¹å¾´é‡
            if len(data) % 4 == 0 and len(data) > 16:
                # 4ãƒã‚¤ãƒˆæ§‹é€ ã®åˆ†æ
                try:
                    floats = np.frombuffer(data[:min(len(data), 1024)], dtype=np.float32)
                    finite_ratio = np.sum(np.isfinite(floats)) / len(floats) if len(floats) > 0 else 0
                    features.append(float(finite_ratio))
                    
                    # æ•´æ•°ã¨ã—ã¦è§£é‡ˆ
                    ints = np.frombuffer(data[:min(len(data), 1024)], dtype=np.int32)
                    if len(ints) > 1:
                        diff_mean = float(np.mean(np.abs(np.diff(ints.astype(np.int64)))))
                        features.append(min(diff_mean / 10000.0, 10.0))  # æ­£è¦åŒ–
                    else:
                        features.append(0.0)
                except Exception:
                    features.extend([0.0, 0.0])
            else:
                features.extend([0.0, 0.0])
            
            # 5. ç³»åˆ—ç›¸é–¢ç‰¹å¾´é‡
            if len(data_array) > 1:
                try:
                    # ãƒ©ã‚°1è‡ªå·±ç›¸é–¢
                    correlation = np.corrcoef(data_array[:-1], data_array[1:])[0, 1]
                    features.append(float(correlation) if not np.isnan(correlation) else 0.0)
                except Exception:
                    features.append(0.0)
            else:
                features.append(0.0)
            
            # 6. å‘¨æœŸæ€§ç‰¹å¾´é‡
            for period in [2, 4, 8, 16]:
                if len(data_array) >= period * 4:
                    try:
                        period_corr = np.corrcoef(data_array[:-period], data_array[period:])[0, 1]
                        features.append(float(period_corr) if not np.isnan(period_corr) else 0.0)
                    except Exception:
                        features.append(0.0)
                else:
                    features.append(0.0)
            
            return np.array(features, dtype=np.float32)
            
        except Exception:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            return np.zeros(20, dtype=np.float32)
    
    def train_ml_classifier(self, training_data: List[Tuple[bytes, DataType]]):
        """æ©Ÿæ¢°å­¦ç¿’åˆ†é¡å™¨ã®è¨“ç·´"""
        try:
            if len(training_data) < 10:
                print("âš ï¸ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³ - ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚’ç¶™ç¶šä½¿ç”¨")
                return
            
            print(f"ğŸ§  ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹åˆ†é¡å™¨ã‚’è¨“ç·´ä¸­... ({len(training_data)}ã‚µãƒ³ãƒ—ãƒ«)")
            
            # ç‰¹å¾´é‡æŠ½å‡º
            X = []
            y = []
            
            for data, data_type in training_data:
                features = self.extract_comprehensive_features(data)
                X.append(features)
                y.append(data_type.value)
            
            X = np.array(X)
            
            # ç‰¹å¾´é‡æ­£è¦åŒ–
            X_scaled = self.feature_scaler.fit_transform(X)
            
            # æ±ºå®šæœ¨åˆ†é¡å™¨ã®è¨“ç·´
            if SKLEARN_AVAILABLE:
                self.ml_classifier = DecisionTreeClassifier(
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42
                )
            else:
                self.ml_classifier = LightweightDecisionTree(
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42
                )
            
            self.ml_classifier.fit(X_scaled, y)
            self.is_trained = True
            
            # è¨“ç·´ç²¾åº¦ã®ç¢ºèª
            accuracy = self.ml_classifier.score(X_scaled, y)
            print(f"âœ… åˆ†é¡å™¨è¨“ç·´å®Œäº† - ç²¾åº¦: {accuracy:.3f}")
            
        except Exception as e:
            print(f"âŒ åˆ†é¡å™¨è¨“ç·´å¤±æ•—: {e}")
            self.is_trained = False
    
    def dispatch(self, data_block: bytes) -> Tuple[DataType, Dict[str, Any]]:
        """ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ"""
        print(f"[ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£] ãƒ–ãƒ­ãƒƒã‚¯åˆ†æ (ã‚µã‚¤ã‚º: {len(data_block)} bytes)")
        
        try:
            # ç‰¹å¾´é‡æŠ½å‡º
            features = self.extract_comprehensive_features(data_block)
            
            # æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹åˆ†é¡ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            if self.is_trained and self.ml_classifier:
                try:
                    features_scaled = self.feature_scaler.transform(features.reshape(1, -1))
                    predicted_type = self.ml_classifier.predict(features_scaled)[0]
                    confidence = float(np.max(self.ml_classifier.predict_proba(features_scaled)))
                    
                    data_type = DataType(predicted_type)
                    
                    print(f"[ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£] MLåˆ¤å®š: {data_type.value} (ä¿¡é ¼åº¦: {confidence:.3f})")
                    
                    feature_dict = {
                        'ml_prediction': predicted_type,
                        'ml_confidence': confidence,
                        'feature_vector': features.tolist()
                    }
                    
                    return data_type, feature_dict
                    
                except Exception as e:
                    print(f"âš ï¸ MLåˆ†é¡å¤±æ•—: {e} - ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            
            # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            data_type = self._rule_based_classification(features, data_block)
            
            feature_dict = {
                'classification_method': 'rule_based',
                'feature_vector': features.tolist()
            }
            
            print(f"[ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒãƒ£] ãƒ«ãƒ¼ãƒ«åˆ¤å®š: {data_type.value}")
            
            return data_type, feature_dict
            
        except Exception as e:
            print(f"âŒ ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã‚¨ãƒ©ãƒ¼: {e}")
            return DataType.GENERIC_BINARY, {}
    
    def _rule_based_classification(self, features: np.ndarray, data: bytes) -> DataType:
        """æ”¹è‰¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ†é¡ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        try:
            if len(features) < 12:
                return DataType.GENERIC_BINARY
            
            size, mean, std, var, min_val, max_val, entropy, ascii_ratio, null_ratio, ff_ratio, uniqueness, finite_ratio = features[:12]
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ¤å®š
            if ascii_ratio > 0.85 and entropy < 6.0:
                return DataType.TEXT_DATA
            
            # æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿åˆ¤å®š
            if len(data) % 4 == 0 and finite_ratio > 0.8 and entropy > 5.0:
                return DataType.FLOAT_DATA
            
            # ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿åˆ¤å®š
            if len(data) % 4 == 0 and len(features) > 13:
                diff_mean = features[13]
                if diff_mean < 0.1:  # æ­£è¦åŒ–æ¸ˆã¿
                    return DataType.SEQUENTIAL_INT_DATA
            
            # é«˜åå¾©ãƒ‡ãƒ¼ã‚¿åˆ¤å®š
            if uniqueness < 0.3 and entropy < 4.0:
                return DataType.REPETITIVE_BINARY
            
            # åœ§ç¸®æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿åˆ¤å®š
            if entropy > 7.5 and uniqueness > 0.9:
                return DataType.COMPRESSED_LIKE
            
            # æ§‹é€ åŒ–æ•°å€¤ãƒ‡ãƒ¼ã‚¿åˆ¤å®š
            if entropy < 6.0 and std > 10:
                return DataType.STRUCTURED_NUMERIC
            
            return DataType.GENERIC_BINARY
            
        except Exception:
            return DataType.GENERIC_BINARY


class StatisticalClusteringTDT:
    """
    TMC v5.0 çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°TDTå¤‰æ›
    ãƒã‚¤ãƒˆä½ç½®ã®çµ±è¨ˆçš„å‡è³ªæ€§ã«åŸºã¥ãå‹•çš„ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†è§£
    """
    
    def __init__(self):
        self.max_clusters = 4  # æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿æ•°
        self.min_cluster_size = 1000  # æœ€å°ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ™ãƒ¼ã‚¹TDTå¤‰æ›"""
        print("  [çµ±è¨ˆçš„TDT] ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'statistical_clustering_tdt', 'original_size': len(data)}
        
        try:
            # åŸºæœ¬ãƒã‚§ãƒƒã‚¯
            if len(data) % 4 != 0 or len(data) < self.min_cluster_size:
                print(f"    [çµ±è¨ˆçš„TDT] ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºåˆ¶é™ - å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—")
                return [data], info
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’4ãƒã‚¤ãƒˆæ§‹é€ ã¨ã—ã¦è§£é‡ˆ
            num_elements = len(data) // 4
            byte_view = np.frombuffer(data, dtype=np.uint8).reshape(num_elements, 4)
            
            print(f"    [çµ±è¨ˆçš„TDT] {num_elements}å€‹ã®4ãƒã‚¤ãƒˆè¦ç´ ã‚’åˆ†æ")
            
            # å„ãƒã‚¤ãƒˆä½ç½®ã®çµ±è¨ˆçš„ç‰¹å¾´ã‚’æŠ½å‡º
            byte_features = []
            for pos in range(4):
                byte_stream = byte_view[:, pos]
                features = self._extract_byte_position_features(byte_stream)
                byte_features.append(features)
            
            # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            feature_matrix = np.array(byte_features)
            cluster_labels = self._perform_clustering(feature_matrix)
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã«åŸºã¥ãã‚¹ãƒˆãƒªãƒ¼ãƒ æ§‹ç¯‰
            streams = self._build_clustered_streams(byte_view, cluster_labels)
            
            info.update({
                'num_elements': num_elements,
                'cluster_labels': cluster_labels.tolist(),
                'num_clusters': len(set(cluster_labels)),
                'stream_count': len(streams),
                'byte_features': [f.tolist() for f in byte_features]
            })
            
            print(f"    [çµ±è¨ˆçš„TDT] {len(set(cluster_labels))}ã‚¯ãƒ©ã‚¹ã‚¿ã«åˆ†è§£ -> {len(streams)}ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ")
            
            return streams, info
            
        except Exception as e:
            print(f"    [çµ±è¨ˆçš„TDT] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _extract_byte_position_features(self, byte_stream: np.ndarray) -> np.ndarray:
        """ãƒã‚¤ãƒˆä½ç½®ã®çµ±è¨ˆçš„ç‰¹å¾´æŠ½å‡º"""
        try:
            features = []
            
            # åŸºæœ¬çµ±è¨ˆé‡
            features.extend([
                float(np.mean(byte_stream)),
                float(np.std(byte_stream)),
                float(np.var(byte_stream)),
                float(np.min(byte_stream)),
                float(np.max(byte_stream))
            ])
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            byte_counts = np.bincount(byte_stream, minlength=256)
            probabilities = byte_counts[byte_counts > 0] / len(byte_stream)
            entropy = float(-np.sum(probabilities * np.log2(probabilities)))
            features.append(entropy)
            
            # ãƒã‚¤ãƒˆåˆ†å¸ƒã®ç‰¹æ€§
            features.extend([
                len(np.unique(byte_stream)) / 256.0,  # å¤šæ§˜æ€§
                float(np.sum(byte_stream == 0) / len(byte_stream)),  # ã‚¼ãƒ­ç‡
                float(np.sum(byte_stream == 255) / len(byte_stream)),  # æœ€å¤§å€¤ç‡
            ])
            
            # ç³»åˆ—ç›¸é–¢ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
            if len(byte_stream) > 1:
                sample_size = min(len(byte_stream), 10000)
                sample = byte_stream[:sample_size]
                try:
                    correlation = np.corrcoef(sample[:-1], sample[1:])[0, 1]
                    features.append(float(correlation) if not np.isnan(correlation) else 0.0)
                except Exception:
                    features.append(0.0)
            else:
                features.append(0.0)
            
            return np.array(features, dtype=np.float32)
            
        except Exception:
            return np.zeros(10, dtype=np.float32)
    
    def _perform_clustering(self, feature_matrix: np.ndarray) -> np.ndarray:
        """çµ±è¨ˆçš„ç‰¹å¾´ã«åŸºã¥ãã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        try:
            # ç‰¹å¾´é‡æ­£è¦åŒ–
            if SKLEARN_AVAILABLE:
                scaler = StandardScaler()
            else:
                scaler = LightweightStandardScaler()
            
            features_scaled = scaler.fit_transform(feature_matrix)
            
            # æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®æ±ºå®šï¼ˆã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æã®ç°¡æ˜“ç‰ˆï¼‰
            best_clusters = 2
            best_score = -1
            
            for n_clusters in range(2, min(self.max_clusters + 1, len(feature_matrix))):
                try:
                    if SKLEARN_AVAILABLE:
                        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    else:
                        kmeans = LightweightKMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    
                    labels = kmeans.fit_predict(features_scaled)
                    
                    # ç°¡æ˜“è©•ä¾¡ï¼šã‚¯ãƒ©ã‚¹ã‚¿å†…åˆ†æ•£ã®é€†æ•°
                    score = 1.0 / (kmeans.inertia_ + 1e-6)
                    
                    if score > best_score:
                        best_score = score
                        best_clusters = n_clusters
                        
                except Exception:
                    continue
            
            # æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            if SKLEARN_AVAILABLE:
                kmeans = KMeans(n_clusters=best_clusters, random_state=42, n_init=10)
            else:
                kmeans = LightweightKMeans(n_clusters=best_clusters, random_state=42, n_init=10)
            
            cluster_labels = kmeans.fit_predict(features_scaled)
            
            return cluster_labels
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒã‚¤ãƒˆä½ç½®ãƒ™ãƒ¼ã‚¹ã®å›ºå®šåˆ†å‰²
            return np.array([0, 1, 2, 3])
    
    def _build_clustered_streams(self, byte_view: np.ndarray, cluster_labels: np.ndarray) -> List[bytes]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã«åŸºã¥ãã‚¹ãƒˆãƒªãƒ¼ãƒ æ§‹ç¯‰"""
        try:
            streams = []
            unique_clusters = sorted(set(cluster_labels))
            
            for cluster_id in unique_clusters:
                # åŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ã«å±ã™ã‚‹ãƒã‚¤ãƒˆä½ç½®ã‚’ç‰¹å®š
                byte_positions = np.where(cluster_labels == cluster_id)[0]
                
                # è©²å½“ã™ã‚‹ãƒã‚¤ãƒˆä½ç½®ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                cluster_data = []
                for pos in byte_positions:
                    cluster_data.append(byte_view[:, pos])
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ æ§‹ç¯‰
                if cluster_data:
                    stream = np.concatenate(cluster_data).tobytes()
                    streams.append(stream)
            
            return streams
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ƒãƒ‡ãƒ¼ã‚¿ã‚’å˜ä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨ã—ã¦è¿”ã™
            return [byte_view.tobytes()]
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°TDTé€†å¤‰æ›"""
        print("  [çµ±è¨ˆçš„TDT] é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            cluster_labels = info.get('cluster_labels', [0, 1, 2, 3])
            num_elements = info.get('num_elements', 0)
            
            if num_elements == 0 or len(streams) != len(set(cluster_labels)):
                print("    [çµ±è¨ˆçš„TDT] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆ - é€£çµæ–¹å¼ã§å¾©å…ƒ")
                return b''.join(streams)
            
            # ã‚¯ãƒ©ã‚¹ã‚¿é…ç½®ã®å¾©å…ƒ
            byte_view = np.zeros((num_elements, 4), dtype=np.uint8)
            unique_clusters = sorted(set(cluster_labels))
            
            stream_offset = 0
            for i, cluster_id in enumerate(unique_clusters):
                if i >= len(streams):
                    break
                
                stream = streams[i]
                byte_positions = [j for j, label in enumerate(cluster_labels) if label == cluster_id]
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’å¯¾å¿œã™ã‚‹ãƒã‚¤ãƒˆä½ç½®ã«å¾©å…ƒ
                expected_size = num_elements * len(byte_positions)
                if len(stream) >= expected_size:
                    stream_data = np.frombuffer(stream[:expected_size], dtype=np.uint8)
                    stream_data = stream_data.reshape(-1, len(byte_positions))
                    
                    for j, pos in enumerate(byte_positions):
                        if pos < 4:  # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
                            byte_view[:, pos] = stream_data[:, j]
            
            return byte_view.tobytes()
            
        except Exception as e:
            print(f"    [çµ±è¨ˆçš„TDT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)


class VariableLengthLeCo:
    """
    TMC v5.0 å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³LeCoå¤‰æ›
    è¤‡æ•°ãƒ¢ãƒ‡ãƒ«é©å¿œé¸æŠã¨Split-and-Mergeã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    """
    
    def __init__(self):
        self.min_partition_size = 64
        self.max_partition_size = 8192
        self.model_types = ['constant', 'linear', 'quadratic']
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³LeCoå¤‰æ›"""
        print("  [å¯å¤‰é•·LeCo] é©å¿œçš„å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'variable_length_leco', 'original_size': len(data)}
        
        try:
            if len(data) % 4 != 0 or len(data) < self.min_partition_size:
                print("    [å¯å¤‰é•·LeCo] ãƒ‡ãƒ¼ã‚¿åˆ¶é™ - å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—")
                return [data], info
            
            integers = np.frombuffer(data, dtype=np.int32)
            print(f"    [å¯å¤‰é•·LeCo] {len(integers)}å€‹ã®æ•´æ•°ã‚’é©å¿œåˆ†å‰²ä¸­...")
            
            # é©å¿œçš„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²
            partitions = self._adaptive_partitioning(integers)
            
            # å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æœ€é©ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨
            model_streams = []
            residual_streams = []
            partition_info = []
            
            for i, partition in enumerate(partitions):
                best_model, model_params, residuals = self._select_best_model(partition)
                
                # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
                model_data = self._serialize_model(best_model, model_params)
                model_streams.append(model_data)
                
                # æ®‹å·®ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
                residual_data = residuals.astype(np.int32).tobytes()
                residual_streams.append(residual_data)
                
                partition_info.append({
                    'model_type': best_model,
                    'partition_size': len(partition),
                    'residual_variance': float(np.var(residuals))
                })
                
                print(f"      ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ {i}: {best_model}ãƒ¢ãƒ‡ãƒ«, æ®‹å·®åˆ†æ•£: {np.var(residuals):.2f}")
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ çµ±åˆ
            all_streams = model_streams + residual_streams
            
            info.update({
                'num_partitions': len(partitions),
                'partition_info': partition_info,
                'partition_sizes': [len(p) for p in partitions]
            })
            
            return all_streams, info
            
        except Exception as e:
            print(f"    [å¯å¤‰é•·LeCo] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _adaptive_partitioning(self, integers: np.ndarray) -> List[np.ndarray]:
        """é©å¿œçš„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ†å‰²ï¼ˆSplit-and-Mergeã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç°¡æ˜“ç‰ˆï¼‰"""
        try:
            partitions = []
            start = 0
            
            while start < len(integers):
                # å¯å¤‰é•·ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®æ±ºå®š
                best_end = min(start + self.min_partition_size, len(integers))
                best_score = float('inf')
                
                # å‹•çš„ã‚µã‚¤ã‚ºæ±ºå®š
                for end in range(start + self.min_partition_size, 
                               min(start + self.max_partition_size, len(integers)) + 1):
                    partition = integers[start:end]
                    
                    # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®é©åˆæ€§è©•ä¾¡ï¼ˆåˆ†æ•£ãƒ™ãƒ¼ã‚¹ï¼‰
                    score = self._evaluate_partition_quality(partition)
                    
                    if score < best_score:
                        best_score = score
                        best_end = end
                    elif score > best_score * 1.1:  # æ—©æœŸåœæ­¢
                        break
                
                # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ç¢ºå®š
                partition = integers[start:best_end]
                partitions.append(partition)
                start = best_end
            
            return partitions
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå›ºå®šã‚µã‚¤ã‚ºåˆ†å‰²
            partition_size = min(self.max_partition_size, len(integers))
            return [integers[i:i+partition_size] for i in range(0, len(integers), partition_size)]
    
    def _evaluate_partition_quality(self, partition: np.ndarray) -> float:
        """ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å“è³ªè©•ä¾¡"""
        try:
            if len(partition) < 2:
                return float('inf')
            
            # ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰ã‹ã‚‰ã®åå·®ã‚’è©•ä¾¡
            x = np.arange(len(partition))
            coeffs = np.polyfit(x, partition, 1)
            predicted = np.polyval(coeffs, x)
            residuals = partition - predicted
            
            return float(np.var(residuals))
            
        except Exception:
            return float('inf')
    
    def _select_best_model(self, partition: np.ndarray) -> Tuple[str, np.ndarray, np.ndarray]:
        """æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠ"""
        try:
            x = np.arange(len(partition))
            best_model = 'constant'
            best_params = np.array([np.mean(partition)])
            best_residuals = partition - best_params[0]
            best_variance = np.var(best_residuals)
            
            # å®šæ•°ãƒ¢ãƒ‡ãƒ«ï¼ˆFrame-of-Referenceç›¸å½“ï¼‰
            constant_value = np.mean(partition)
            constant_residuals = partition - constant_value
            constant_variance = np.var(constant_residuals)
            
            if constant_variance < best_variance:
                best_model = 'constant'
                best_params = np.array([constant_value])
                best_residuals = constant_residuals
                best_variance = constant_variance
            
            # ç·šå½¢ãƒ¢ãƒ‡ãƒ«
            try:
                linear_coeffs = np.polyfit(x, partition, 1)
                linear_predicted = np.polyval(linear_coeffs, x)
                linear_residuals = partition - linear_predicted
                linear_variance = np.var(linear_residuals)
                
                if linear_variance < best_variance * 0.9:  # 10%ä»¥ä¸Šã®æ”¹å–„ãŒå¿…è¦
                    best_model = 'linear'
                    best_params = linear_coeffs
                    best_residuals = linear_residuals
                    best_variance = linear_variance
            except Exception:
                pass
            
            # äºŒæ¬¡ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒååˆ†å¤§ãã„å ´åˆï¼‰
            if len(partition) >= 16:
                try:
                    quad_coeffs = np.polyfit(x, partition, 2)
                    quad_predicted = np.polyval(quad_coeffs, x)
                    quad_residuals = partition - quad_predicted
                    quad_variance = np.var(quad_residuals)
                    
                    if quad_variance < best_variance * 0.8:  # 20%ä»¥ä¸Šã®æ”¹å–„ãŒå¿…è¦
                        best_model = 'quadratic'
                        best_params = quad_coeffs
                        best_residuals = quad_residuals
                        best_variance = quad_variance
                except Exception:
                    pass
            
            return best_model, best_params, best_residuals
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå®šæ•°ãƒ¢ãƒ‡ãƒ«
            mean_val = float(np.mean(partition))
            residuals = partition - mean_val
            return 'constant', np.array([mean_val]), residuals
    
    def _serialize_model(self, model_type: str, params: np.ndarray) -> bytes:
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆ1ãƒã‚¤ãƒˆï¼‰+ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼ˆ1ãƒã‚¤ãƒˆï¼‰+ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆfloat32é…åˆ—ï¼‰
            type_mapping = {'constant': 0, 'linear': 1, 'quadratic': 2}
            
            result = bytearray()
            result.append(type_mapping.get(model_type, 0))
            result.append(len(params))
            result.extend(params.astype(np.float32).tobytes())
            
            return bytes(result)
            
        except Exception:
            return b'\x00\x01\x00\x00\x00\x00'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šå®šæ•°0
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """å¯å¤‰é•·LeCoé€†å¤‰æ›"""
        print("  [å¯å¤‰é•·LeCo] é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            num_partitions = info.get('num_partitions', 0)
            partition_sizes = info.get('partition_sizes', [])
            
            if num_partitions == 0 or len(streams) != num_partitions * 2:
                print("    [å¯å¤‰é•·LeCo] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆ")
                return b''.join(streams)
            
            # ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ åˆ†é›¢
            model_streams = streams[:num_partitions]
            residual_streams = streams[num_partitions:]
            
            # å„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®å¾©å…ƒ
            restored_partitions = []
            
            for i in range(num_partitions):
                if i >= len(partition_sizes):
                    break
                
                # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
                model_type, params = self._deserialize_model(model_streams[i])
                
                # æ®‹å·®ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒ
                residuals = np.frombuffer(residual_streams[i], dtype=np.int32)
                
                # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬å€¤ã®è¨ˆç®—
                partition_size = partition_sizes[i]
                x = np.arange(partition_size)
                
                if model_type == 'constant':
                    predicted = np.full(partition_size, params[0] if len(params) > 0 else 0)
                elif model_type == 'linear' and len(params) >= 2:
                    predicted = np.polyval(params, x)
                elif model_type == 'quadratic' and len(params) >= 3:
                    predicted = np.polyval(params, x)
                else:
                    predicted = np.zeros(partition_size)
                
                # å…ƒãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒ
                if len(residuals) >= partition_size:
                    original = predicted.astype(np.int32) + residuals[:partition_size]
                    restored_partitions.append(original)
            
            # å…¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®çµåˆ
            if restored_partitions:
                result = np.concatenate(restored_partitions)
                return result.astype(np.int32).tobytes()
            else:
                return b''.join(streams)
                
        except Exception as e:
            print(f"    [å¯å¤‰é•·LeCo] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _deserialize_model(self, model_data: bytes) -> Tuple[str, np.ndarray]:
        """ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if len(model_data) < 2:
                return 'constant', np.array([0.0])
            
            type_mapping = {0: 'constant', 1: 'linear', 2: 'quadratic'}
            model_type = type_mapping.get(model_data[0], 'constant')
            param_count = model_data[1]
            
            if len(model_data) >= 2 + param_count * 4:
                params_bytes = model_data[2:2 + param_count * 4]
                params = np.frombuffer(params_bytes, dtype=np.float32)
                return model_type, params
            else:
                return 'constant', np.array([0.0])
                
        except Exception:
            return 'constant', np.array([0.0])


# TMC v5.0 ã‚¨ãƒ³ã‚¸ãƒ³çµ±åˆã¯æ¬¡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ç¶™ç¶š...


class MemoryEfficientBWT:
    """
    TMC v5.0 ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„BWTå®Ÿè£…
    æ¥å°¾è¾é…åˆ—ãƒ™ãƒ¼ã‚¹ã®é«˜æ€§èƒ½å¤‰æ›
    """
    
    def __init__(self):
        self.max_bwt_size = 1048576  # 1MBåˆ¶é™ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
    
    def transform(self, data: bytes) -> Tuple[List[bytes], Dict[str, Any]]:
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„BWTå¤‰æ›"""
        print("  [é«˜åŠ¹ç‡BWT] å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        info = {'method': 'memory_efficient_bwt', 'original_size': len(data)}
        
        try:
            if not data or len(data) > self.max_bwt_size:
                print(f"    [é«˜åŠ¹ç‡BWT] ã‚µã‚¤ã‚ºåˆ¶é™è¶…é ({len(data)}) - å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—")
                info['method'] = 'bwt_skipped'
                return [data], info
            
            # æ¥å°¾è¾é…åˆ—ãƒ™ãƒ¼ã‚¹BWTï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„å®Ÿè£…ï¼‰
            bwt_result, primary_index = self._suffix_array_bwt(data)
            
            # ãƒ—ãƒ©ã‚¤ãƒãƒªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
            index_bytes = struct.pack('<I', primary_index)
            
            print(f"    [é«˜åŠ¹ç‡BWT] å¤‰æ›å®Œäº†: {len(data)} bytes, ãƒ—ãƒ©ã‚¤ãƒãƒª: {primary_index}")
            
            info.update({
                'primary_index': primary_index,
                'bwt_length': len(bwt_result),
                'memory_efficient': True
            })
            
            return [index_bytes, bwt_result], info
            
        except Exception as e:
            print(f"    [é«˜åŠ¹ç‡BWT] ã‚¨ãƒ©ãƒ¼: {e}")
            return [data], info
    
    def _suffix_array_bwt(self, data: bytes) -> Tuple[bytes, int]:
        """æ¥å°¾è¾é…åˆ—ãƒ™ãƒ¼ã‚¹BWTï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰"""
        try:
            # çµ‚ç«¯ãƒãƒ¼ã‚«ãƒ¼è¿½åŠ 
            text = data + b'\x00'
            n = len(text)
            
            # æ¥å°¾è¾é…åˆ—ã®æ§‹ç¯‰ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ç‰ˆï¼‰
            # æ³¨ï¼šå®Ÿç”¨å®Ÿè£…ã§ã¯libdivsufsortãªã©ã‚’ä½¿ç”¨
            suffixes = list(range(n))
            suffixes.sort(key=lambda i: text[i:])
            
            # BWTã®æ§‹ç¯‰ã¨ãƒ—ãƒ©ã‚¤ãƒãƒªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç‰¹å®š
            bwt_chars = []
            primary_index = 0
            
            for i, suffix_start in enumerate(suffixes):
                if suffix_start == 0:
                    primary_index = i
                    bwt_chars.append(text[-1])
                else:
                    bwt_chars.append(text[suffix_start - 1])
            
            return bytes(bwt_chars), primary_index
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒŠã‚¤ãƒ¼ãƒ–å®Ÿè£…
            return self._naive_bwt(data)
    
    def _naive_bwt(self, data: bytes) -> Tuple[bytes, int]:
        """ãƒŠã‚¤ãƒ¼ãƒ–BWTå®Ÿè£…ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        try:
            text = data + b'\x00'
            rotations = [text[i:] + text[:i] for i in range(len(text))]
            rotations.sort()
            
            primary_index = next((i for i, rot in enumerate(rotations) if rot == text), 0)
            bwt_result = bytes(rot[-1] for rot in rotations)
            
            return bwt_result, primary_index
            
        except Exception:
            return data + b'\x00', 0
    
    def inverse_transform(self, streams: List[bytes], info: Dict[str, Any]) -> bytes:
        """é«˜åŠ¹ç‡BWTé€†å¤‰æ›"""
        print("  [é«˜åŠ¹ç‡BWT] é€†å¤‰æ›ã‚’å®Ÿè¡Œä¸­...")
        try:
            if info.get('method') == 'bwt_skipped':
                return streams[0] if streams else b''
            
            if len(streams) != 2:
                return streams[0] if streams else b''
            
            # ãƒ—ãƒ©ã‚¤ãƒãƒªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å¾©å…ƒ
            primary_index = struct.unpack('<I', streams[0])[0]
            bwt_data = streams[1]
            
            # åŠ¹ç‡çš„é€†å¤‰æ›ï¼ˆLF-mappingä½¿ç”¨ï¼‰
            original = self._lf_mapping_inverse(bwt_data, primary_index)
            
            # çµ‚ç«¯ãƒãƒ¼ã‚«ãƒ¼é™¤å»
            if original.endswith(b'\x00'):
                original = original[:-1]
            
            return original
            
        except Exception as e:
            print(f"    [é«˜åŠ¹ç‡BWT] é€†å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return b''.join(streams)
    
    def _lf_mapping_inverse(self, bwt_data: bytes, primary_index: int) -> bytes:
        """LF-mappingã‚’ç”¨ã„ãŸåŠ¹ç‡çš„é€†å¤‰æ›"""
        try:
            n = len(bwt_data)
            if n == 0:
                return b''
            
            # æ–‡å­—é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
            count = [0] * 256
            for char in bwt_data:
                count[char] += 1
            
            # ç´¯ç©ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç¬¬ä¸€åˆ—ã®é–‹å§‹ä½ç½®ï¼‰
            first_occurrence = [0] * 256
            total = 0
            for i in range(256):
                first_occurrence[i] = total
                total += count[i]
            
            # LF-mapping ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹ç¯‰
            char_rank = [0] * 256
            lf = [0] * n
            
            for i in range(n):
                char = bwt_data[i]
                lf[i] = first_occurrence[char] + char_rank[char]
                char_rank[char] += 1
            
            # å…ƒæ–‡å­—åˆ—ã®å¾©å…ƒ
            result = bytearray()
            current_pos = primary_index
            
            for _ in range(n):
                char = bwt_data[current_pos]
                result.append(char)
                current_pos = lf[current_pos]
            
            return bytes(result)
            
        except Exception:
            return b''.join([bytes([bwt_data[i]]) for i in range(len(bwt_data))])


class ParallelTMCEngine:
    """
    TMC v5.0 ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
    çœŸã®ä¸¦åˆ—åœ§ç¸®ãƒ»å±•é–‹ã«ã‚ˆã‚‹é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå®Ÿç¾
    """
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.chunk_size = 1048576  # 1MBãƒãƒ£ãƒ³ã‚¯
    
    def parallel_compress_streams(self, streams: List[bytes], 
                                compressor: ZstandardUnifiedCompressor) -> List[Tuple[bytes, str]]:
        """ä¸¦åˆ—ã‚¹ãƒˆãƒªãƒ¼ãƒ åœ§ç¸®"""
        try:
            if len(streams) <= 1:
                # å˜ä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼šé€æ¬¡å‡¦ç†
                if streams:
                    compressed, method = compressor.compress(streams[0])
                    return [(compressed, method)]
                return []
            
            # ä¸¦åˆ—å‡¦ç†
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = []
                
                for i, stream in enumerate(streams):
                    future = executor.submit(self._compress_single_stream, 
                                          stream, compressor, i)
                    futures.append(future)
                
                results = []
                for future in futures:
                    compressed, method = future.result()
                    results.append((compressed, method))
                
                return results
                
        except Exception as e:
            print(f"âš ï¸ ä¸¦åˆ—åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e} - é€æ¬¡å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return [(compressor.compress(stream)) for stream in streams]
    
    def _compress_single_stream(self, stream: bytes, 
                              compressor: ZstandardUnifiedCompressor, 
                              stream_id: int) -> Tuple[bytes, str]:
        """å˜ä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ åœ§ç¸®ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ç”¨ï¼‰"""
        try:
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼äº‹å‰è¨ˆç®—ï¼ˆåœ§ç¸®ãƒ¬ãƒ™ãƒ«é¸æŠç”¨ï¼‰
            entropy = compressor._calculate_entropy(stream)
            compressed, method = compressor.compress(stream, entropy)
            
            print(f"      ä¸¦åˆ—åœ§ç¸® #{stream_id}: {len(stream)} -> {len(compressed)} bytes ({method})")
            return compressed, method
            
        except Exception as e:
            print(f"      ä¸¦åˆ—åœ§ç¸® #{stream_id} ã‚¨ãƒ©ãƒ¼: {e}")
            return stream, "store"
    
    def parallel_decompress_streams(self, compressed_streams: List[Tuple[bytes, str]], 
                                  compressor: ZstandardUnifiedCompressor) -> List[bytes]:
        """ä¸¦åˆ—ã‚¹ãƒˆãƒªãƒ¼ãƒ å±•é–‹"""
        try:
            if len(compressed_streams) <= 1:
                # å˜ä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼šé€æ¬¡å‡¦ç†
                if compressed_streams:
                    data, method = compressed_streams[0]
                    return [compressor.decompress(data, method)]
                return []
            
            # ä¸¦åˆ—å‡¦ç†
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = []
                
                for i, (compressed_data, method) in enumerate(compressed_streams):
                    future = executor.submit(self._decompress_single_stream,
                                          compressed_data, method, compressor, i)
                    futures.append(future)
                
                results = []
                for future in futures:
                    decompressed = future.result()
                    results.append(decompressed)
                
                return results
                
        except Exception as e:
            print(f"âš ï¸ ä¸¦åˆ—å±•é–‹ã‚¨ãƒ©ãƒ¼: {e} - é€æ¬¡å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return [compressor.decompress(data, method) for data, method in compressed_streams]
    
    def _decompress_single_stream(self, compressed_data: bytes, method: str,
                                compressor: ZstandardUnifiedCompressor, 
                                stream_id: int) -> bytes:
        """å˜ä¸€ã‚¹ãƒˆãƒªãƒ¼ãƒ å±•é–‹ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ç”¨ï¼‰"""
        try:
            decompressed = compressor.decompress(compressed_data, method)
            print(f"      ä¸¦åˆ—å±•é–‹ #{stream_id}: {len(compressed_data)} -> {len(decompressed)} bytes")
            return decompressed
            
        except Exception as e:
            print(f"      ä¸¦åˆ—å±•é–‹ #{stream_id} ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data


class NEXUSTMCEngineV5:
    """
    NEXUS TMC Engine v5.0 - ç†è«–çš„æ”¹è‰¯çµ±åˆç‰ˆ
    ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ + çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° + Zstandardçµ±ä¸€ + ä¸¦åˆ—å‡¦ç†
    """
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        
        # ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.dispatcher = DataDrivenDispatcher()
        self.compressor = ZstandardUnifiedCompressor()
        self.parallel_engine = ParallelTMCEngine(max_workers)
        
        # æ”¹è‰¯å¤‰æ›å™¨ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆçµ±åˆï¼‰
        self.transformers = {
            DataType.FLOAT_DATA: StatisticalClusteringTDT(),
            DataType.SEQUENTIAL_INT_DATA: VariableLengthLeCo(),
            DataType.TEXT_DATA: MemoryEfficientBWT(),
            DataType.STRUCTURED_NUMERIC: StatisticalClusteringTDT(),
            DataType.TIME_SERIES: VariableLengthLeCo(),
            DataType.MIXED_NUMERIC: StatisticalClusteringTDT(),
            # ãã®ä»–ã®ã‚¿ã‚¤ãƒ—ã¯ç›´æ¥åœ§ç¸®
        }
        
        # çµ±è¨ˆãƒ»å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_compressed_size': 0,
            'ml_predictions': 0,
            'rule_based_fallbacks': 0,
            'parallel_compressions': 0
        }
        
        self.training_data = []  # MLåˆ†é¡å™¨è¨“ç·´ç”¨
        
    def add_training_sample(self, data: bytes, correct_type: DataType):
        """æ©Ÿæ¢°å­¦ç¿’åˆ†é¡å™¨ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿è¿½åŠ """
        self.training_data.append((data, correct_type))
        
        # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒè“„ç©ã•ã‚ŒãŸã‚‰è¨“ç·´å®Ÿè¡Œ
        if len(self.training_data) >= 50 and not self.dispatcher.is_trained:
            self.dispatcher.train_ml_classifier(self.training_data)
    
    def compress_tmc(self, data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC v5.0 çµ±åˆåœ§ç¸®å‡¦ç†"""
        compression_start = time.perf_counter()
        
        try:
            print("\nğŸš€ TMC v5.0 åœ§ç¸®é–‹å§‹ (ç†è«–çš„æ”¹è‰¯ç‰ˆ)")
            
            # ã‚¹ãƒ†ãƒ¼ã‚¸1: ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹åˆ†æ&ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ
            data_type, features = self.dispatcher.dispatch(data)
            
            # ã‚¹ãƒ†ãƒ¼ã‚¸2: é©å¿œçš„å¤‰æ›
            transformer = self.transformers.get(data_type)
            if transformer:
                transformed_streams, transform_info = transformer.transform(data)
                print(f"  âœ… {data_type.value}å¤‰æ›å®Œäº†: {len(transformed_streams)}ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ")
            else:
                print(f"  â¡ï¸ {data_type.value}: ç›´æ¥åœ§ç¸®ãƒ¢ãƒ¼ãƒ‰")
                transformed_streams = [data]
                transform_info = {'method': 'direct_compression'}
            
            # ã‚¹ãƒ†ãƒ¼ã‚¸3: ä¸¦åˆ—Zstandardçµ±ä¸€åœ§ç¸®
            print("  ğŸ”„ ä¸¦åˆ—Zstandardåœ§ç¸®ä¸­...")
            compressed_results = self.parallel_engine.parallel_compress_streams(
                transformed_streams, self.compressor)
            
            # TMC v5.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰
            final_data = self._pack_tmc_v5(compressed_results, data_type, 
                                         transform_info, features)
            
            total_time = time.perf_counter() - compression_start
            compression_ratio = (1 - len(final_data) / len(data)) * 100 if len(data) > 0 else 0
            
            # çµ±è¨ˆæ›´æ–°
            self.stats['files_processed'] += 1
            self.stats['total_input_size'] += len(data)
            self.stats['total_compressed_size'] += len(final_data)
            if len(transformed_streams) > 1:
                self.stats['parallel_compressions'] += 1
            
            result_info = {
                'compression_ratio': compression_ratio,
                'compression_throughput_mb_s': (len(data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_compression_time': total_time,
                'data_type': data_type.value,
                'features': features,
                'transform_info': transform_info,
                'stream_count': len(transformed_streams),
                'original_size': len(data),
                'compressed_size': len(final_data),
                'tmc_version': '5.0',
                'parallel_processing': len(transformed_streams) > 1,
                'zstd_unified': True,
                'reversible': True
            }
            
            print(f"âœ… TMC v5.0 åœ§ç¸®å®Œäº†")
            print(f"   ğŸ“Š {len(data)} -> {len(final_data)} bytes ({compression_ratio:.2f}%)")
            print(f"   âš¡ {result_info['compression_throughput_mb_s']:.1f}MB/s")
            
            return final_data, result_info
            
        except Exception as e:
            total_time = time.perf_counter() - compression_start
            print(f"âŒ TMC v5.0 åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            return data, {
                'compression_ratio': 0.0,
                'error': str(e),
                'total_compression_time': total_time,
                'tmc_version': '5.0'
            }
    
    def decompress_tmc(self, compressed_data: bytes) -> Tuple[bytes, Dict[str, Any]]:
        """TMC v5.0 å±•é–‹å‡¦ç†"""
        decompression_start = time.perf_counter()
        
        try:
            print("\nğŸ”„ TMC v5.0 å±•é–‹é–‹å§‹")
            
            # TMC v5.0 ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            header = self._parse_tmc_v5_header(compressed_data)
            if not header:
                raise ValueError("Invalid TMC v5.0 format")
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º
            payload = compressed_data[header['header_size']:]
            compressed_streams = self._extract_tmc_v5_streams(payload, header)
            
            # ä¸¦åˆ—å±•é–‹
            print("  ğŸ”„ ä¸¦åˆ—Zstandardå±•é–‹ä¸­...")
            decompressed_streams = self.parallel_engine.parallel_decompress_streams(
                compressed_streams, self.compressor)
            
            # é€†å¤‰æ›
            data_type = DataType(header['data_type'])
            transformer = self.transformers.get(data_type)
            
            if transformer:
                print(f"  ğŸ”„ {data_type.value}é€†å¤‰æ›ä¸­...")
                original_data = transformer.inverse_transform(decompressed_streams, 
                                                            header['transform_info'])
            else:
                original_data = b''.join(decompressed_streams)
            
            total_time = time.perf_counter() - decompression_start
            
            result_info = {
                'decompression_throughput_mb_s': (len(original_data) / 1024 / 1024) / total_time if total_time > 0 else 0,
                'total_decompression_time': total_time,
                'decompressed_size': len(original_data),
                'parallel_processing': len(decompressed_streams) > 1,
                'tmc_version': '5.0'
            }
            
            print(f"âœ… TMC v5.0 å±•é–‹å®Œäº†")
            print(f"   ğŸ“Š å¾©å…ƒã‚µã‚¤ã‚º: {len(original_data)} bytes")
            print(f"   âš¡ {result_info['decompression_throughput_mb_s']:.1f}MB/s")
            
            return original_data, result_info
            
        except Exception as e:
            total_time = time.perf_counter() - decompression_start
            print(f"âŒ TMC v5.0 å±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return compressed_data, {
                'error': str(e),
                'total_decompression_time': total_time
            }
    
    def test_reversibility(self, test_data: bytes, test_name: str = "test") -> Dict[str, Any]:
        """TMC v5.0 å¯é€†æ€§ãƒ†ã‚¹ãƒˆ"""
        print(f"ğŸ§ª TMC v5.0 å¯é€†æ€§ãƒ†ã‚¹ãƒˆ: {test_name}")
        
        try:
            # åœ§ç¸®
            compressed, compression_info = self.compress_tmc(test_data)
            
            # å±•é–‹
            decompressed, decompression_info = self.decompress_tmc(compressed)
            
            # æ¤œè¨¼
            is_identical = (test_data == decompressed)
            
            result = {
                'test_name': test_name,
                'reversible': is_identical,
                'original_size': len(test_data),
                'compressed_size': len(compressed),
                'decompressed_size': len(decompressed),
                'compression_ratio': compression_info.get('compression_ratio', 0),
                'compression_time': compression_info.get('total_compression_time', 0),
                'decompression_time': decompression_info.get('total_decompression_time', 0),
                'data_type': compression_info.get('data_type', 'unknown'),
                'parallel_processing': compression_info.get('parallel_processing', False),
                'tmc_version': '5.0'
            }
            
            status = "âœ… æˆåŠŸ" if is_identical else "âŒ å¤±æ•—"
            print(f"   {status}: å¯é€†æ€§ãƒ†ã‚¹ãƒˆ")
            
            return result
            
        except Exception as e:
            return {
                'test_name': test_name,
                'reversible': False,
                'error': str(e),
                'tmc_version': '5.0'
            }
    
    def _pack_tmc_v5(self, compressed_results: List[Tuple[bytes, str]], 
                     data_type: DataType, transform_info: Dict[str, Any], 
                     features: Dict[str, Any]) -> bytes:
        """TMC v5.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ§‹ç¯‰ï¼ˆJSONãƒ™ãƒ¼ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰"""
        try:
            # ã‚»ã‚­ãƒ¥ã‚¢ãªãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
            header_info = {
                'version': '5.0',
                'data_type': data_type.value,
                'transform_info': transform_info,
                'features': features,
                'stream_count': len(compressed_results),
                'compression_methods': [method for _, method in compressed_results],
                'stream_sizes': [len(data) for data, _ in compressed_results]
            }
            
            # JSON ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
            header_json = json.dumps(header_info, separators=(',', ':'))
            header_bytes = header_json.encode('utf-8')
            
            # ãƒã‚¤ãƒŠãƒªãƒ˜ãƒƒãƒ€ãƒ¼æ§‹ç¯‰
            binary_header = bytearray()
            binary_header.extend(b'TMC5')  # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
            binary_header.extend(struct.pack('<I', len(header_bytes)))  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º
            binary_header.extend(header_bytes)  # JSONãƒ˜ãƒƒãƒ€ãƒ¼
            
            # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
            payload = b''.join([data for data, _ in compressed_results])
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
            checksum = zlib.crc32(payload) & 0xffffffff
            binary_header.extend(struct.pack('<I', checksum))
            
            return bytes(binary_header) + payload
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return b''.join([data for data, _ in compressed_results])
    
    def _parse_tmc_v5_header(self, data: bytes) -> Optional[Dict[str, Any]]:
        """TMC v5.0 ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ"""
        try:
            if len(data) < 12 or data[:4] != b'TMC5':
                return None
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚µã‚¤ã‚º
            header_size = struct.unpack('<I', data[4:8])[0]
            
            # JSONãƒ˜ãƒƒãƒ€ãƒ¼
            header_json = data[8:8+header_size].decode('utf-8')
            header_info = json.loads(header_json)
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
            checksum = struct.unpack('<I', data[8+header_size:12+header_size])[0]
            
            header_info['checksum'] = checksum
            header_info['header_size'] = 12 + header_size
            
            return header_info
            
        except Exception:
            return None
    
    def _extract_tmc_v5_streams(self, payload: bytes, 
                               header: Dict[str, Any]) -> List[Tuple[bytes, str]]:
        """TMC v5.0 ã‚¹ãƒˆãƒªãƒ¼ãƒ æŠ½å‡º"""
        try:
            stream_sizes = header.get('stream_sizes', [])
            compression_methods = header.get('compression_methods', [])
            
            streams = []
            offset = 0
            
            for i, size in enumerate(stream_sizes):
                stream_data = payload[offset:offset+size]
                method = compression_methods[i] if i < len(compression_methods) else 'store'
                streams.append((stream_data, method))
                offset += size
            
            return streams
            
        except Exception:
            return [(payload, 'store')]
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """TMC v5.0 æ€§èƒ½çµ±è¨ˆ"""
        total_processed = self.stats['files_processed']
        if total_processed == 0:
            return self.stats
        
        avg_compression_ratio = (
            1 - self.stats['total_compressed_size'] / self.stats['total_input_size']
        ) * 100 if self.stats['total_input_size'] > 0 else 0
        
        enhanced_stats = dict(self.stats)
        enhanced_stats.update({
            'average_compression_ratio': avg_compression_ratio,
            'parallel_processing_rate': self.stats['parallel_compressions'] / total_processed * 100,
            'ml_classifier_usage': self.dispatcher.is_trained,
            'training_samples': len(self.training_data)
        })
        
        return enhanced_stats


# TMC v5.0 ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
__all__ = [
    'NEXUSTMCEngineV5', 
    'DataType', 
    'DataDrivenDispatcher',
    'StatisticalClusteringTDT', 
    'VariableLengthLeCo',
    'ZstandardUnifiedCompressor'
]


if __name__ == "__main__":
    print("ğŸš€ NEXUS TMC Engine v5.0 - ç†è«–çš„æ”¹è‰¯çµ±åˆç‰ˆ")
    print("   ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒ + çµ±è¨ˆçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° + Zstandardçµ±ä¸€")
    
    try:
        engine = NEXUSTMCEngineV5(max_workers=4)
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆå®Ÿç”¨ã§ã¯å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
        training_samples = [
            (np.random.random(1000).astype(np.float32).tobytes(), DataType.FLOAT_DATA),
            (np.arange(0, 4000, 4, dtype=np.int32).tobytes(), DataType.SEQUENTIAL_INT_DATA),
            (("Hello World! " * 100).encode('utf-8'), DataType.TEXT_DATA),
            (b"PATTERN" * 500, DataType.REPETITIVE_BINARY),
            (bytes(range(256)) * 10, DataType.GENERIC_BINARY)
        ]
        
        # æ©Ÿæ¢°å­¦ç¿’åˆ†é¡å™¨ã®è¨“ç·´
        for data, data_type in training_samples:
            engine.add_training_sample(data, data_type)
        
        # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        test_cases = [
            ("æµ®å‹•å°æ•°ç‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆçµ±è¨ˆçš„TDTï¼‰", np.linspace(1000, 2000, 2000, dtype=np.float32).tobytes()),
            ("ç³»åˆ—æ•´æ•°ãƒ‡ãƒ¼ã‚¿ï¼ˆå¯å¤‰é•·LeCoï¼‰", np.arange(0, 8000, 3, dtype=np.int32).tobytes()),
            ("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆé«˜åŠ¹ç‡BWTï¼‰", ("TMC v5.0 represents the pinnacle of compression technology. " * 100).encode('utf-8')),
            ("æ··åˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿", np.random.normal(1000, 100, 1000).astype(np.float32).tobytes()),
            ("é«˜åå¾©ãƒã‚¤ãƒŠãƒª", b"ABCDEFGH" * 1000)
        ]
        
        print(f"\nğŸ“Š TMC v5.0 åŒ…æ‹¬ãƒ†ã‚¹ãƒˆé–‹å§‹ ({len(test_cases)}ã‚±ãƒ¼ã‚¹)")
        print("=" * 60)
        
        success_count = 0
        total_compression_ratio = 0
        
        for name, data in test_cases:
            print(f"\nğŸ”¬ ãƒ†ã‚¹ãƒˆ: {name}")
            result = engine.test_reversibility(data, name)
            
            if result.get('reversible', False):
                success_count += 1
                ratio = result.get('compression_ratio', 0)
                total_compression_ratio += ratio
                print(f"   ğŸ“ˆ åœ§ç¸®ç‡: {ratio:.1f}%")
                
                if result.get('parallel_processing', False):
                    print("   âš¡ ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ")
        
        # çµæœã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 60)
        print(f"ğŸ“Š TMC v5.0 ãƒ†ã‚¹ãƒˆçµæœ")
        print(f"   æˆåŠŸç‡: {success_count}/{len(test_cases)} ({success_count/len(test_cases)*100:.1f}%)")
        
        if success_count > 0:
            avg_ratio = total_compression_ratio / success_count
            print(f"   å¹³å‡åœ§ç¸®ç‡: {avg_ratio:.1f}%")
        
        # æ€§èƒ½çµ±è¨ˆ
        stats = engine.get_performance_stats()
        print(f"   ä¸¦åˆ—å‡¦ç†ç‡: {stats.get('parallel_processing_rate', 0):.1f}%")
        print(f"   MLåˆ†é¡å™¨: {'æœ‰åŠ¹' if stats.get('ml_classifier_usage', False) else 'ç„¡åŠ¹'}")
        
        if success_count == len(test_cases):
            print("\nğŸ‰ TMC v5.0 å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸ - ç†è«–çš„æ”¹è‰¯ç‰ˆå®Œæˆ!")
            print("ğŸ”¥ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ”¹å–„ææ¡ˆã®çµ±åˆã«ã‚ˆã‚Šã€TMCã®çœŸã®ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚’å®Ÿç¾!")
        else:
            print(f"\nâš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•— - æ”¹è‰¯ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
            
    except Exception as e:
        print(f"âŒ TMC v5.0 åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        print("   Zstandardãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç¢ºèªã‚’ãŠé¡˜ã„ã—ã¾ã™")

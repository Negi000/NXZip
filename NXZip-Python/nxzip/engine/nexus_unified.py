#!/usr/bin/env python3
"""
NEXUS Unified Engine - çµ±åˆåœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³
å…¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œã€ç›®æ¨™é”æˆå‹åœ§ç¸®ã‚·ã‚¹ãƒ†ãƒ 

é–‹ç™ºå±¥æ­´:
- AV1/SRLA/AVIFæŠ€è¡“ã®åˆ¶ç´„é™¤å»æˆ¦ç•¥ã‚’æ¡ç”¨
- å®Œå…¨å¯é€†æ€§ã‚’ä¿è¨¼ã—ã¤ã¤é«˜åœ§ç¸®ç‡ã‚’å®Ÿç¾
- åˆ¶ç´„ãªã—æœ€é©åŒ–ã«ã‚ˆã‚Šå¾“æ¥æŠ€è¡“ã®é™ç•Œã‚’çªç ´

ç›®æ¨™ã‚¹ãƒšãƒƒã‚¯:
- åœ§ç¸®ç‡: 80% (ãƒ†ã‚­ã‚¹ãƒˆ95%)
- åœ§ç¸®é€Ÿåº¦: 100MB/s
- å±•é–‹é€Ÿåº¦: 200MB/s
- å®Œå…¨å¯é€†æ€§: 100%ä¿è¨¼
"""

import struct
import time
import lzma
import zlib
import bz2
import threading
import concurrent.futures
from typing import Optional, Tuple, List, Dict, Any
from pathlib import Path
import sys
import io
import hashlib
import os
from multiprocessing import Pool, cpu_count

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from .spe_core_jit import SPECoreJIT

class NEXUSUnified:
    """
    çµ±åˆåœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³
    
    æŠ€è¡“çš„ç‰¹å¾´:
    1. AV1åˆ¶ç´„é™¤å»: å†ç”Ÿäº’æ›æ€§ç„¡è¦–ã®æ¿€ã—ã„å†—é•·æ€§é™¤å»
    2. SRLAåˆ¶ç´„é™¤å»: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç„¡è¦–ã®æ™‚é–“è»¸æœ€é©åŒ–
    3. AVIFåˆ¶ç´„é™¤å»: éƒ¨åˆ†å¾©å·ç„¡è¦–ã®æ·±ã„æ§‹é€ åˆ†æ
    4. å®Œå…¨å¯é€†å‰æ: ä½¿ç”¨æ™‚åˆ¶ç´„ãªã—ã®æœ€é©åŒ–
    5. é©å¿œçš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : å½¢å¼åˆ¥æœ€é©åŒ–
    """
    
    def __init__(self):
        self.spe = SPECoreJIT()
        self.cpu_count = cpu_count()
        
        # çµ±åˆè¨­å®š
        self.configs = {
            'video': {'target_ratio': 0.80, 'presets': [1, 3, 6]},
            'audio': {'target_ratio': 0.80, 'presets': [1, 3, 6]},
            'image': {'target_ratio': 0.80, 'presets': [1, 3, 6]},
            'text': {'target_ratio': 0.95, 'presets': [3, 6, 9]},
            'binary': {'target_ratio': 0.80, 'presets': [1, 3, 6]}
        }
    
    def compress(self, data: bytes) -> bytes:
        """çµ±åˆåœ§ç¸®å‡¦ç†"""
        if not data:
            return self._create_empty_nxz()
        
        # å½¢å¼æ¤œå‡º
        format_type = self._detect_format(data)
        config = self.configs[format_type]
        
        # å½¢å¼æ¤œå‡ºæƒ…å ±ã‚’å†…éƒ¨ã§ä½¿ç”¨ï¼ˆè¡¨ç¤ºã¯çµ±ä¸€ï¼‰
        
        # é©å¿œçš„æˆ¦ç•¥é¸æŠ
        data_size = len(data)
        if data_size > 50 * 1024 * 1024:  # 50MBè¶…: é€Ÿåº¦å„ªå…ˆ
            preset = config['presets'][0]
            strategy = 'fast'
        elif data_size > 10 * 1024 * 1024:  # 10MBè¶…: ãƒãƒ©ãƒ³ã‚¹
            preset = config['presets'][1]
            strategy = 'balanced'
        else:  # 10MBä»¥ä¸‹: åœ§ç¸®ç‡å„ªå…ˆ
            preset = config['presets'][2]
            strategy = 'max'
        
        # å‰å‡¦ç†
        processed_data = self._preprocess(data, format_type, strategy)
        
        # åœ§ç¸®
        compressed_data = self._compress_data(processed_data, format_type, preset)
        
        # SPEæš—å·åŒ–
        encrypted_data = self.spe.apply_transform(compressed_data)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
        header = self._create_header(len(data), len(compressed_data), 
                                   len(encrypted_data), format_type, strategy)
        
        return header + encrypted_data
    
    def decompress(self, nxz_data: bytes) -> bytes:
        """çµ±åˆå±•é–‹å‡¦ç†"""
        if not nxz_data:
            return b""
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        if len(nxz_data) < 48:
            raise ValueError("Invalid NXZ format")
        
        header_info = self._parse_header(nxz_data[:48])
        
        # SPEå¾©å·åŒ–
        encrypted_data = nxz_data[48:]
        compressed_data = self.spe.reverse_transform(encrypted_data)
        
        # å±•é–‹
        processed_data = self._decompress_data(compressed_data, header_info['format_type'])
        
        # å¾Œå‡¦ç†
        original_data = self._postprocess(processed_data, header_info['format_type'], 
                                        header_info['strategy'])
        
        return original_data
    
    def _detect_format(self, data: bytes) -> str:
        """å½¢å¼æ¤œå‡º"""
        if len(data) < 16:
            return "binary"
        
        # å‹•ç”»
        if data[4:8] == b'ftyp' or data.startswith(b'RIFF') or data.startswith(b'\x1A\x45\xDF\xA3'):
            return "video"
        
        # éŸ³å£°
        if (data.startswith(b'RIFF') and b'WAVE' in data[:32] or
            data.startswith(b'ID3') or data.startswith(b'\xFF\xFB')):
            return "audio"
        
        # ç”»åƒ
        if (data.startswith(b'\xFF\xD8') or data.startswith(b'\x89PNG') or 
            data.startswith(b'GIF')):
            return "image"
        
        # ãƒ†ã‚­ã‚¹ãƒˆ
        try:
            sample = data[:min(4096, len(data))]
            sample.decode('utf-8')
            text_ratio = sum(1 for b in sample if 32 <= b <= 126 or b in [9, 10, 13]) / len(sample)
            if text_ratio > 0.8:
                return "text"
        except:
            pass
        
        return "binary"
    
    def _preprocess(self, data: bytes, format_type: str, strategy: str) -> bytes:
        """åˆ¶ç´„ãªã—å‰å‡¦ç†"""
        if format_type == "text":
            return self._preprocess_text(data, strategy)
        # ä»–ã®å½¢å¼ã¯ç¾åœ¨ã¯å‰å‡¦ç†ãªã—ï¼ˆå°†æ¥æ‹¡å¼µï¼‰
        return data
    
    def _preprocess_text(self, data: bytes, strategy: str) -> bytes:
        """ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
        try:
            text = data.decode('utf-8')
            # æ”¹è¡Œçµ±ä¸€
            text = text.replace('\r\n', '\n').replace('\r', '\n')
            # é€£ç¶šç©ºç™½æœ€é©åŒ–
            if strategy in ['balanced', 'max']:
                import re
                text = re.sub(r' +', ' ', text)
                text = re.sub(r'\n+', '\n', text)
            return text.encode('utf-8')
        except:
            return data
    
    def _compress_data(self, data: bytes, format_type: str, preset: int) -> bytes:
        """ãƒ‡ãƒ¼ã‚¿åœ§ç¸®"""
        # ä¸¦åˆ—å‡¦ç†åˆ¤å®š
        if len(data) > 10 * 1024 * 1024:  # 10MBè¶…ã¯ä¸¦åˆ—å‡¦ç†
            return self._compress_parallel(data, format_type, preset)
        else:
            return self._compress_single(data, format_type, preset)
    
    def _compress_single(self, data: bytes, format_type: str, preset: int) -> bytes:
        """å˜ä¸€åœ§ç¸®"""
        if format_type == "text":
            # ãƒ†ã‚­ã‚¹ãƒˆã¯å¤šæ®µéšåœ§ç¸®
            try:
                stage1 = lzma.compress(data, preset=preset, check=lzma.CHECK_CRC32)
                if len(stage1) > 1024:
                    stage2 = bz2.compress(stage1, compresslevel=9)
                    if len(stage2) < len(stage1):
                        return b'TXT2' + stage2
                return b'TXT1' + stage1
            except:
                return b'TXT0' + data
        else:
            # ãã®ä»–ã¯æ¨™æº–åœ§ç¸®
            try:
                compressed = lzma.compress(data, preset=preset, check=lzma.CHECK_CRC32)
                return b'STD1' + compressed
            except:
                return b'STD0' + data
    
    def _compress_parallel(self, data: bytes, format_type: str, preset: int) -> bytes:
        """ä¸¦åˆ—åœ§ç¸®"""
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        chunk_size = len(data) // self.cpu_count
        chunks = []
        for i in range(self.cpu_count):
            start = i * chunk_size
            end = start + chunk_size if i < self.cpu_count - 1 else len(data)
            chunks.append(data[start:end])
        
        # ä¸¦åˆ—å‡¦ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.cpu_count) as executor:
            futures = [executor.submit(self._compress_chunk, chunk, preset, i) 
                      for i, chunk in enumerate(chunks)]
            results = [future.result() for future in futures]
        
        # çµæœçµåˆ
        header = struct.pack('<I', len(results))
        for result in results:
            header += struct.pack('<II', result['original_size'], result['compressed_size'])
            header += result['data']
        
        return b'PAR1' + header
    
    def _compress_chunk(self, chunk: bytes, preset: int, chunk_id: int) -> dict:
        """ãƒãƒ£ãƒ³ã‚¯åœ§ç¸®"""
        try:
            compressed = lzma.compress(chunk, preset=preset, check=lzma.CHECK_CRC32)
            return {
                'id': chunk_id,
                'data': compressed,
                'original_size': len(chunk),
                'compressed_size': len(compressed)
            }
        except:
            return {
                'id': chunk_id,
                'data': chunk,
                'original_size': len(chunk),
                'compressed_size': len(chunk)
            }
    
    def _decompress_data(self, data: bytes, format_type: str) -> bytes:
        """ãƒ‡ãƒ¼ã‚¿å±•é–‹"""
        if len(data) < 4:
            return data
        
        marker = data[:4]
        compressed_data = data[4:]
        
        if marker == b'TXT2':
            # 2æ®µéšãƒ†ã‚­ã‚¹ãƒˆå±•é–‹
            stage1 = bz2.decompress(compressed_data)
            return lzma.decompress(stage1)
        elif marker == b'TXT1':
            # 1æ®µéšãƒ†ã‚­ã‚¹ãƒˆå±•é–‹
            return lzma.decompress(compressed_data)
        elif marker == b'TXT0':
            # éåœ§ç¸®ãƒ†ã‚­ã‚¹ãƒˆ
            return compressed_data
        elif marker == b'STD1':
            # æ¨™æº–å±•é–‹
            return lzma.decompress(compressed_data)
        elif marker == b'STD0':
            # éåœ§ç¸®
            return compressed_data
        elif marker == b'PAR1':
            # ä¸¦åˆ—å±•é–‹
            return self._decompress_parallel(compressed_data)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return lzma.decompress(data)
    
    def _decompress_parallel(self, data: bytes) -> bytes:
        """ä¸¦åˆ—å±•é–‹"""
        offset = 0
        num_chunks = struct.unpack('<I', data[offset:offset+4])[0]
        offset += 4
        
        chunks = []
        for i in range(num_chunks):
            original_size, compressed_size = struct.unpack('<II', data[offset:offset+8])
            offset += 8
            chunk_data = data[offset:offset+compressed_size]
            offset += compressed_size
            chunks.append({
                'id': i,
                'data': chunk_data,
                'original_size': original_size,
                'compressed_size': compressed_size
            })
        
        # ä¸¦åˆ—å±•é–‹
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.cpu_count) as executor:
            futures = [executor.submit(self._decompress_chunk, chunk) for chunk in chunks]
            results = [future.result() for future in futures]
        
        # çµæœçµåˆ
        return b''.join(result['data'] for result in sorted(results, key=lambda x: x['id']))
    
    def _decompress_chunk(self, chunk: dict) -> dict:
        """ãƒãƒ£ãƒ³ã‚¯å±•é–‹"""
        try:
            decompressed = lzma.decompress(chunk['data'])
            return {'id': chunk['id'], 'data': decompressed}
        except:
            return {'id': chunk['id'], 'data': chunk['data']}
    
    def _postprocess(self, data: bytes, format_type: str, strategy: str) -> bytes:
        """å¾Œå‡¦ç†"""
        # ç¾åœ¨ã¯å¾Œå‡¦ç†ãªã—
        return data
    
    def _create_header(self, original_size: int, compressed_size: int, 
                      encrypted_size: int, format_type: str, strategy: str) -> bytes:
        """ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        header = bytearray(48)
        header[0:4] = b'NXZU'  # Unified
        header[4:8] = struct.pack('<I', 1)
        header[8:16] = struct.pack('<Q', original_size)
        header[16:24] = struct.pack('<Q', compressed_size)
        header[24:32] = struct.pack('<Q', encrypted_size)
        header[32:40] = format_type.encode('ascii')[:8].ljust(8, b'\x00')
        header[40:48] = strategy.encode('ascii')[:8].ljust(8, b'\x00')
        return bytes(header)
    
    def _parse_header(self, header: bytes) -> dict:
        """ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ"""
        return {
            'version': struct.unpack('<I', header[4:8])[0],
            'original_size': struct.unpack('<Q', header[8:16])[0],
            'compressed_size': struct.unpack('<Q', header[16:24])[0],
            'encrypted_size': struct.unpack('<Q', header[24:32])[0],
            'format_type': header[32:40].rstrip(b'\x00').decode('ascii'),
            'strategy': header[40:48].rstrip(b'\x00').decode('ascii')
        }
    
    def _create_empty_nxz(self) -> bytes:
        """ç©ºã®NXZãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        return self._create_header(0, 0, 0, "empty", "none")

def test_nexus_unified():
    """çµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ"""
    print("ğŸš€ NEXUS Unified Engine ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    test_files = [
        r"C:\Users\241822\Desktop\æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ (2)\NXZip\NXZip-Python\sample\å‡ºåº«å®Ÿç¸¾æ˜ç´°_202412.txt",
        r"C:\Users\241822\Desktop\æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ (2)\NXZip\NXZip-Python\sample\PythonåŸºç¤è¬›åº§3_4æœˆ26æ—¥-3.mp4"
    ]
    
    nexus = NEXUSUnified()
    
    for file_path in test_files:
        path = Path(file_path)
        if not path.exists():
            continue
            
        print(f"\nğŸ“„ {path.name}")
        
        with open(path, 'rb') as f:
            data = f.read()
        
        # åœ§ç¸®
        start = time.perf_counter()
        compressed = nexus.compress(data)
        compress_time = time.perf_counter() - start
        
        # å±•é–‹
        start = time.perf_counter()
        decompressed = nexus.decompress(compressed)
        decomp_time = time.perf_counter() - start
        
        # çµæœ
        ratio = (1 - len(compressed) / len(data)) * 100
        comp_speed = (len(data) / 1024 / 1024) / compress_time
        decomp_speed = (len(data) / 1024 / 1024) / decomp_time
        correct = data == decompressed
        
        print(f"ğŸ“Š åœ§ç¸®ç‡: {ratio:.1f}%")
        print(f"âš¡ åœ§ç¸®é€Ÿåº¦: {comp_speed:.1f} MB/s")
        print(f"ğŸ’¨ å±•é–‹é€Ÿåº¦: {decomp_speed:.1f} MB/s")
        print(f"âœ… æ­£ç¢ºæ€§: {'OK' if correct else 'NG'}")

if __name__ == "__main__":
    test_nexus_unified()

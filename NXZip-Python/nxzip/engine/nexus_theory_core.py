#!/usr/bin/env python3
"""
NEXUS Theory Core Engine - NEXUSç†è«–å®Œå…¨å®Ÿè£…ç‰ˆ
Networked Elemental eXtraction and Unification System

ç†è«–çš„åŸºç›¤:
1. Adaptive Elemental Unit (AEU) - é©å¿œçš„è¦ç´ å˜ä½
2. High-Dimensional Shape Clustering (HDSC) - é«˜æ¬¡å…ƒå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
3. Permutative Normalization - é †åˆ—æ­£è¦åŒ–
4. Meta-heuristic Optimization - ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æœ€é©åŒ–
5. Machine Learning Assistance - æ©Ÿæ¢°å­¦ç¿’æ”¯æ´
6. Parallel Processing - ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
"""

import struct
import time
import lzma
import hashlib
import numpy as np
import numba
from numba import jit, cuda
from typing import List, Dict, Tuple, Optional, Any, Set
from dataclasses import dataclass, field
from pathlib import Path
import sys
import pickle
import threading
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import gc
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from .spe_core_jit import SPECoreJIT
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡æ˜“SPEå®Ÿè£…
    class SPECoreJIT:
        def apply_transform(self, data):
            return bytes(b ^ 0x42 for b in data) if data else data
        def reverse_transform(self, data):
            return bytes(b ^ 0x42 for b in data) if data else data


@dataclass
class AdaptiveElementalUnit:
    """é©å¿œçš„è¦ç´ å˜ä½ (AEU)"""
    data: bytes
    position: int
    size: int
    unit_type: str
    frequency: int = 1
    entropy: float = 0.0
    correlation_coefficient: float = 0.0
    prediction_accuracy: float = 0.0
    
    def __post_init__(self):
        self.hash_value = hash(self.data)
        self.entropy = self._calculate_entropy()
        
    def _calculate_entropy(self) -> float:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—"""
        if not self.data:
            return 0.0
        
        byte_counts = {}
        for byte in self.data:
            byte_counts[byte] = byte_counts.get(byte, 0) + 1
        
        entropy = 0.0
        length = len(self.data)
        for count in byte_counts.values():
            probability = count / length
            if probability > 0:
                entropy -= probability * np.log2(probability)
        
        return entropy


@dataclass
class PolyominoShape:
    """å¤šè§’å½¢å½¢çŠ¶ (é«˜æ¬¡å…ƒå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”¨)"""
    shape_id: int
    dimensions: Tuple[int, ...]
    pattern: np.ndarray
    symmetry_group: str
    normalization_matrix: np.ndarray
    elements: List[AdaptiveElementalUnit] = field(default_factory=list)
    
    def __post_init__(self):
        self.shape_hash = hash(tuple(self.pattern.flatten()))


@dataclass
class MetaOptimizationResult:
    """ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æœ€é©åŒ–çµæœ"""
    best_parameters: Dict[str, Any]
    compression_ratio: float
    processing_time: float
    energy_function_value: float
    generation: int


class NEXUSTheoryCore:
    """
    NEXUSç†è«–å®Œå…¨å®Ÿè£…ã‚¨ãƒ³ã‚¸ãƒ³
    
    é©æ–°çš„ç‰¹å¾´:
    1. ãƒ‡ãƒ¼ã‚¿ã®æœ¬è³ªçš„æ§‹é€ ã‚’å¤šæ¬¡å…ƒè§£æ
    2. æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å†è§£é‡ˆã«ã‚ˆã‚‹åœ§ç¸®é™ç•Œçªç ´
    3. æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
    4. ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹è¶…é«˜é€ŸåŒ–
    5. SPEæ§‹é€ ä¿å­˜æš—å·åŒ–çµ±åˆ
    """
    
    def __init__(self, optimization_level: str = "balanced"):
        """
        åˆæœŸåŒ–
        
        Args:
            optimization_level: æœ€é©åŒ–ãƒ¬ãƒ™ãƒ« ('fast', 'balanced', 'maximum')
        """
        self.spe = SPECoreJIT()
        self.optimization_level = optimization_level
        
        # ç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.aeu_config = self._initialize_aeu_config()
        self.hdsc_config = self._initialize_hdsc_config()
        self.ml_model = self._initialize_ml_model()
        
        # ä¸¦åˆ—å‡¦ç†è¨­å®š
        self.cpu_count = multiprocessing.cpu_count()
        self.use_gpu = self._check_gpu_availability()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
        self.shape_cache = {}
        self.pattern_cache = {}
        self.ml_prediction_cache = {}
        
        print(f"ğŸ§  NEXUSç†è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
        print(f"   ğŸ”§ æœ€é©åŒ–ãƒ¬ãƒ™ãƒ«: {optimization_level}")
        print(f"   ğŸ’» CPUä¸¦åˆ—åº¦: {self.cpu_count}")
        print(f"   ğŸš€ GPUåŠ é€Ÿ: {'æœ‰åŠ¹' if self.use_gpu else 'ç„¡åŠ¹'}")
    
    def compress(self, data: bytes) -> bytes:
        """
        NEXUSç†è«–åœ§ç¸®
        
        ç†è«–å®Ÿè£…ãƒ•ãƒ­ãƒ¼:
        1. ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æ
        2. é©å¿œçš„è¦ç´ åˆ†è§£ (AEU)
        3. é«˜æ¬¡å…ƒå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (HDSC)
        4. é †åˆ—æ­£è¦åŒ–
        5. ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æœ€é©åŒ–
        6. æ©Ÿæ¢°å­¦ç¿’æ”¯æ´åœ§ç¸®
        7. SPEæ§‹é€ ä¿å­˜æš—å·åŒ–
        """
        if not data:
            return self._create_empty_header()
        
        print(f"ğŸ§  NEXUSç†è«–åœ§ç¸®é–‹å§‹ - ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(data):,} bytes")
        start_time = time.perf_counter()
        
        try:
            # ãƒ•ã‚§ãƒ¼ã‚º1: ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æ
            print("ğŸ“Š ãƒ•ã‚§ãƒ¼ã‚º1: ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æ")
            data_characteristics = self._analyze_data_characteristics(data)
            
            # ãƒ•ã‚§ãƒ¼ã‚º2: é©å¿œçš„è¦ç´ åˆ†è§£ (AEU)
            print("ğŸ”¬ ãƒ•ã‚§ãƒ¼ã‚º2: é©å¿œçš„è¦ç´ åˆ†è§£ (AEU)")
            aeu_units = self._adaptive_elemental_decomposition(data, data_characteristics)
            
            # ãƒ•ã‚§ãƒ¼ã‚º3: é«˜æ¬¡å…ƒå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (HDSC)
            print("ğŸ”· ãƒ•ã‚§ãƒ¼ã‚º3: é«˜æ¬¡å…ƒå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (HDSC)")
            shape_clusters = self._high_dimensional_shape_clustering(aeu_units)
            
            # ãƒ•ã‚§ãƒ¼ã‚º4: é †åˆ—æ­£è¦åŒ–
            print("ğŸ”„ ãƒ•ã‚§ãƒ¼ã‚º4: é †åˆ—æ­£è¦åŒ–")
            normalized_clusters = self._permutative_normalization(shape_clusters)
            
            # ãƒ•ã‚§ãƒ¼ã‚º5: ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æœ€é©åŒ–
            print("âš¡ ãƒ•ã‚§ãƒ¼ã‚º5: ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æœ€é©åŒ–")
            optimization_result = self._meta_heuristic_optimization(normalized_clusters, data_characteristics)
            
            # ãƒ•ã‚§ãƒ¼ã‚º6: æ©Ÿæ¢°å­¦ç¿’æ”¯æ´åœ§ç¸®
            print("ğŸ¤– ãƒ•ã‚§ãƒ¼ã‚º6: æ©Ÿæ¢°å­¦ç¿’æ”¯æ´åœ§ç¸®")
            ml_compressed = self._ml_assisted_compression(
                normalized_clusters, optimization_result, data_characteristics
            )
            
            # ãƒ•ã‚§ãƒ¼ã‚º7: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            print("ğŸ“ˆ ãƒ•ã‚§ãƒ¼ã‚º7: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
            entropy_encoded = self._entropy_encoding(ml_compressed)
            
            # ãƒ•ã‚§ãƒ¼ã‚º8: SPEæ§‹é€ ä¿å­˜æš—å·åŒ–
            print("ğŸ” ãƒ•ã‚§ãƒ¼ã‚º8: SPEæ§‹é€ ä¿å­˜æš—å·åŒ–")
            encrypted_data = self.spe.apply_transform(entropy_encoded)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
            header = self._create_nexus_header(
                original_size=len(data),
                compressed_size=len(entropy_encoded),
                encrypted_size=len(encrypted_data),
                data_characteristics=data_characteristics,
                optimization_result=optimization_result
            )
            
            result = header + encrypted_data
            compression_ratio = (1 - len(result) / len(data)) * 100
            processing_time = time.perf_counter() - start_time
            
            print(f"âœ… NEXUSç†è«–åœ§ç¸®å®Œäº†")
            print(f"   ğŸ“Š åœ§ç¸®ç‡: {compression_ratio:.2f}%")
            print(f"   â±ï¸ å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
            
            # æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ›´æ–°
            self._update_ml_model(data_characteristics, compression_ratio, processing_time)
            
            return result
            
        except Exception as e:
            print(f"âŒ åœ§ç¸®ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“åœ§ç¸®
            fallback_compressed = lzma.compress(data, preset=6)
            fallback_encrypted = self.spe.apply_transform(fallback_compressed)
            fallback_header = self._create_fallback_header(len(data), len(fallback_compressed))
            return fallback_header + fallback_encrypted
    
    def decompress(self, compressed_data: bytes) -> bytes:
        """NEXUSç†è«–å±•é–‹"""
        if not compressed_data:
            return b""
        
        print(f"ğŸ”“ NEXUSç†è«–å±•é–‹é–‹å§‹")
        start_time = time.perf_counter()
        
        try:
            # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
            if len(compressed_data) < 128:
                raise ValueError("ç„¡åŠ¹ãªåœ§ç¸®ãƒ‡ãƒ¼ã‚¿")
            
            header_info = self._parse_nexus_header(compressed_data[:128])
            encrypted_data = compressed_data[128:]
            
            # SPEå¾©å·åŒ–
            entropy_encoded = self.spe.reverse_transform(encrypted_data)
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            ml_compressed = self._entropy_decoding(entropy_encoded, header_info)
            
            # æ©Ÿæ¢°å­¦ç¿’æ”¯æ´å±•é–‹
            normalized_clusters = self._ml_assisted_decompression(ml_compressed, header_info)
            
            # é †åˆ—é€†æ­£è¦åŒ–
            shape_clusters = self._permutative_denormalization(normalized_clusters, header_info)
            
            # å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿å¾©å…ƒ
            aeu_units = self._reconstruct_from_shape_clusters(shape_clusters, header_info)
            
            # è¦ç´ å˜ä½å¾©å…ƒ
            original_data = self._reconstruct_from_aeu(aeu_units, header_info)
            
            processing_time = time.perf_counter() - start_time
            print(f"âœ… NEXUSç†è«–å±•é–‹å®Œäº†")
            print(f"   ğŸ“Š å¾©å…ƒã‚µã‚¤ã‚º: {len(original_data):,} bytes")
            print(f"   â±ï¸ å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
            
            return original_data
            
        except Exception as e:
            print(f"âŒ å±•é–‹ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å±•é–‹
            try:
                return lzma.decompress(self.spe.reverse_transform(compressed_data[64:]))
            except:
                raise ValueError("å¾©å…ƒä¸å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿")
    
    def _initialize_aeu_config(self) -> Dict[str, Any]:
        """AEUè¨­å®šåˆæœŸåŒ–"""
        return {
            'min_unit_size': 1,
            'max_unit_size': 64,
            'adaptive_threshold': 0.75,
            'entropy_threshold': 4.0,
            'correlation_window': 16,
            'prediction_depth': 8
        }
    
    def _initialize_hdsc_config(self) -> Dict[str, Any]:
        """HDSCè¨­å®šåˆæœŸåŒ–"""
        return {
            'max_dimensions': 8,
            'cluster_threshold': 0.85,
            'shape_similarity_threshold': 0.90,
            'symmetry_detection': True,
            'normalization_method': 'canonical',
            'clustering_algorithm': 'hierarchical'
        }
    
    def _initialize_ml_model(self) -> Dict[str, Any]:
        """æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        return {
            'prediction_weights': np.random.random(16),
            'correlation_matrix': np.eye(8),
            'learning_rate': 0.01,
            'adaptation_factor': 0.95,
            'prediction_history': [],
            'accuracy_threshold': 0.8
        }
    
    def _check_gpu_availability(self) -> bool:
        """GPUåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            if cuda.is_available():
                cuda_device = cuda.get_current_device()
                return True
        except:
            pass
        return False
    
    def _analyze_data_characteristics(self, data: bytes) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æ"""
        characteristics = {
            'size': len(data),
            'entropy': self._calculate_global_entropy(data),
            'patterns': self._detect_patterns(data),
            'structure_type': self._classify_structure(data),
            'redundancy_level': self._estimate_redundancy(data),
            'compressibility_score': 0.0
        }
        
        # åœ§ç¸®å¯èƒ½æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        characteristics['compressibility_score'] = self._calculate_compressibility_score(characteristics)
        
        return characteristics
    
    @jit(nopython=True)
    def _calculate_global_entropy(self, data: bytes) -> float:
        """ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®— (JITæœ€é©åŒ–)"""
        byte_counts = np.zeros(256, dtype=np.int64)
        for byte in data:
            byte_counts[byte] += 1
        
        entropy = 0.0
        length = len(data)
        for count in byte_counts:
            if count > 0:
                probability = count / length
                entropy -= probability * np.log2(probability)
        
        return entropy
    
    def _adaptive_elemental_decomposition(self, data: bytes, characteristics: Dict[str, Any]) -> List[AdaptiveElementalUnit]:
        """é©å¿œçš„è¦ç´ åˆ†è§£ (AEU)"""
        units = []
        
        # é©å¿œçš„å˜ä½ã‚µã‚¤ã‚ºæ±ºå®š
        base_unit_size = self._determine_optimal_unit_size(characteristics)
        
        # ä¸¦åˆ—åˆ†è§£å‡¦ç†
        if len(data) > 1024 * 1024 and self.cpu_count > 1:  # 1MBä»¥ä¸Šã§ä¸¦åˆ—åŒ–
            units = self._parallel_aeu_decomposition(data, base_unit_size, characteristics)
        else:
            units = self._sequential_aeu_decomposition(data, base_unit_size, characteristics)
        
        # è¦ç´ é–“ç›¸é–¢è¨ˆç®—
        self._calculate_inter_element_correlations(units)
        
        return units
    
    def _determine_optimal_unit_size(self, characteristics: Dict[str, Any]) -> int:
        """æœ€é©å˜ä½ã‚µã‚¤ã‚ºæ±ºå®š"""
        entropy = characteristics['entropy']
        size = characteristics['size']
        structure_type = characteristics['structure_type']
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹èª¿æ•´
        if entropy < 2.0:  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ â†’ å¤§ããªå˜ä½
            base_size = 16
        elif entropy < 4.0:  # ä¸­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ â†’ ä¸­ç¨‹åº¦å˜ä½
            base_size = 8
        else:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ â†’ å°ã•ãªå˜ä½
            base_size = 4
        
        # æ§‹é€ ã‚¿ã‚¤ãƒ—èª¿æ•´
        structure_multipliers = {
            'text': 1.5,
            'binary_pattern': 2.0,
            'random': 0.5,
            'structured': 1.2,
            'multimedia': 0.8
        }
        
        multiplier = structure_multipliers.get(structure_type, 1.0)
        optimal_size = int(base_size * multiplier)
        
        return max(self.aeu_config['min_unit_size'], 
                  min(optimal_size, self.aeu_config['max_unit_size']))
    
    def _parallel_aeu_decomposition(self, data: bytes, unit_size: int, characteristics: Dict[str, Any]) -> List[AdaptiveElementalUnit]:
        """ä¸¦åˆ—AEUåˆ†è§£"""
        chunk_size = len(data) // self.cpu_count
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        
        with ThreadPoolExecutor(max_workers=self.cpu_count) as executor:
            futures = [
                executor.submit(self._sequential_aeu_decomposition, chunk, unit_size, characteristics, i * chunk_size)
                for i, chunk in enumerate(chunks)
            ]
            
            all_units = []
            for future in futures:
                all_units.extend(future.result())
        
        return all_units
    
    def _sequential_aeu_decomposition(self, data: bytes, unit_size: int, characteristics: Dict[str, Any], offset: int = 0) -> List[AdaptiveElementalUnit]:
        """é€æ¬¡AEUåˆ†è§£"""
        units = []
        
        # é©å¿œçš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
        window_sizes = [unit_size, unit_size * 2, unit_size // 2]
        
        i = 0
        while i < len(data):
            best_unit = None
            best_score = -1
            
            # è¤‡æ•°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã§è©•ä¾¡
            for window_size in window_sizes:
                if i + window_size <= len(data):
                    unit_data = data[i:i+window_size]
                    unit = AdaptiveElementalUnit(
                        data=unit_data,
                        position=offset + i,
                        size=window_size,
                        unit_type=self._classify_unit_type(unit_data, characteristics)
                    )
                    
                    # å˜ä½è©•ä¾¡ã‚¹ã‚³ã‚¢
                    score = self._evaluate_unit_quality(unit, characteristics)
                    
                    if score > best_score:
                        best_score = score
                        best_unit = unit
            
            if best_unit:
                units.append(best_unit)
                i += best_unit.size
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€å°å˜ä½
                unit_data = data[i:i+1]
                units.append(AdaptiveElementalUnit(
                    data=unit_data,
                    position=offset + i,
                    size=1,
                    unit_type='byte'
                ))
                i += 1
        
        return units
    
    def _high_dimensional_shape_clustering(self, units: List[AdaptiveElementalUnit]) -> List[PolyominoShape]:
        """é«˜æ¬¡å…ƒå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (HDSC)"""
        print(f"   ğŸ”· {len(units)} è¦ç´ ã®å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°")
        
        # å½¢çŠ¶ç‰¹å¾´æŠ½å‡º
        shape_features = self._extract_shape_features(units)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        if self.use_gpu and len(units) > 10000:
            clusters = self._gpu_shape_clustering(shape_features, units)
        else:
            clusters = self._cpu_shape_clustering(shape_features, units)
        
        print(f"   âœ… {len(clusters)} å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ç”Ÿæˆ")
        return clusters
    
    def _extract_shape_features(self, units: List[AdaptiveElementalUnit]) -> np.ndarray:
        """å½¢çŠ¶ç‰¹å¾´æŠ½å‡º"""
        features = []
        
        for unit in units:
            # å¤šæ¬¡å…ƒç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«
            feature_vector = np.zeros(self.hdsc_config['max_dimensions'])
            
            # åŸºæœ¬çµ±è¨ˆç‰¹å¾´
            data_array = np.frombuffer(unit.data, dtype=np.uint8)
            if len(data_array) > 0:
                feature_vector[0] = np.mean(data_array)
                feature_vector[1] = np.std(data_array)
                feature_vector[2] = np.median(data_array)
                feature_vector[3] = unit.entropy
                
                # é«˜æ¬¡çµ±è¨ˆç‰¹å¾´
                if len(data_array) > 1:
                    feature_vector[4] = np.var(data_array)
                    feature_vector[5] = np.max(data_array) - np.min(data_array)
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å¾´
                feature_vector[6] = self._calculate_pattern_complexity(unit.data)
                feature_vector[7] = unit.correlation_coefficient
            
            features.append(feature_vector)
        
        return np.array(features)
    
    def _cpu_shape_clustering(self, features: np.ndarray, units: List[AdaptiveElementalUnit]) -> List[PolyominoShape]:
        """CPUå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        clusters = []
        
        # éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        similarity_matrix = self._calculate_similarity_matrix(features)
        cluster_assignments = self._hierarchical_clustering(similarity_matrix)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿æ§‹ç¯‰
        cluster_groups = {}
        for i, cluster_id in enumerate(cluster_assignments):
            if cluster_id not in cluster_groups:
                cluster_groups[cluster_id] = []
            cluster_groups[cluster_id].append(units[i])
        
        # PolyominoShapeç”Ÿæˆ
        for cluster_id, cluster_units in cluster_groups.items():
            if len(cluster_units) > 0:
                shape = self._create_polyomino_shape(cluster_id, cluster_units, features)
                clusters.append(shape)
        
        return clusters
    
    def _permutative_normalization(self, clusters: List[PolyominoShape]) -> List[PolyominoShape]:
        """é †åˆ—æ­£è¦åŒ–"""
        print(f"   ğŸ”„ {len(clusters)} ã‚¯ãƒ©ã‚¹ã‚¿ã®é †åˆ—æ­£è¦åŒ–")
        
        normalized_clusters = []
        
        for cluster in clusters:
            # æ­£è¦åŒ–è¡Œåˆ—è¨ˆç®—
            normalization_matrix = self._calculate_normalization_matrix(cluster)
            
            # è¦ç´ é †åˆ—æœ€é©åŒ–
            optimized_elements = self._optimize_element_permutation(cluster.elements)
            
            # æ­£è¦åŒ–ã‚¯ãƒ©ã‚¹ã‚¿ç”Ÿæˆ
            normalized_cluster = PolyominoShape(
                shape_id=cluster.shape_id,
                dimensions=cluster.dimensions,
                pattern=cluster.pattern,
                symmetry_group=cluster.symmetry_group,
                normalization_matrix=normalization_matrix,
                elements=optimized_elements
            )
            
            normalized_clusters.append(normalized_cluster)
        
        return normalized_clusters
    
    def _meta_heuristic_optimization(self, clusters: List[PolyominoShape], characteristics: Dict[str, Any]) -> MetaOptimizationResult:
        """ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æœ€é©åŒ–"""
        print(f"   âš¡ ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æœ€é©åŒ–å®Ÿè¡Œ")
        
        # éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  + ç„¼ããªã¾ã—æ³•
        ga_result = self._genetic_algorithm_optimization(clusters, characteristics)
        sa_result = self._simulated_annealing_optimization(clusters, characteristics, ga_result)
        
        # æœ€è‰¯çµæœé¸æŠ
        best_result = sa_result if sa_result.compression_ratio > ga_result.compression_ratio else ga_result
        
        print(f"   âœ… æœ€é©åŒ–å®Œäº† - åœ§ç¸®ç‡äºˆæ¸¬: {best_result.compression_ratio:.2f}%")
        return best_result
    
    def _ml_assisted_compression(self, clusters: List[PolyominoShape], optimization_result: MetaOptimizationResult, characteristics: Dict[str, Any]) -> bytes:
        """æ©Ÿæ¢°å­¦ç¿’æ”¯æ´åœ§ç¸®"""
        print(f"   ğŸ¤– æ©Ÿæ¢°å­¦ç¿’æ”¯æ´åœ§ç¸®å®Ÿè¡Œ")
        
        # ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æ§‹ç¯‰
        feature_vector = self._build_ml_feature_vector(clusters, characteristics)
        
        # äºˆæ¸¬ãƒ™ãƒ¼ã‚¹åœ§ç¸®
        compression_strategy = self._predict_optimal_strategy(feature_vector)
        
        # é©å¿œçš„åœ§ç¸®å®Ÿè¡Œ
        compressed_data = self._execute_adaptive_compression(clusters, compression_strategy, optimization_result)
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        self._learn_from_compression_result(feature_vector, compressed_data, characteristics)
        
        return compressed_data
    
    def _entropy_encoding(self, data: bytes) -> bytes:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        # LZMA + è¿½åŠ æœ€é©åŒ–
        base_compressed = lzma.compress(data, preset=6, check=lzma.CHECK_CRC32)
        
        # è¿½åŠ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åœ§ç¸®
        if len(base_compressed) > 1024:
            optimized = self._apply_entropy_optimization(base_compressed)
            return optimized if len(optimized) < len(base_compressed) else base_compressed
        
        return base_compressed
    
    def _create_nexus_header(self, original_size: int, compressed_size: int, encrypted_size: int, 
                           data_characteristics: Dict[str, Any], optimization_result: MetaOptimizationResult) -> bytes:
        """NEXUSãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        header = bytearray(128)
        
        # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼
        header[0:8] = b'NXTHEORY'
        
        # ã‚µã‚¤ã‚ºæƒ…å ±
        header[8:16] = struct.pack('<Q', original_size)
        header[16:24] = struct.pack('<Q', compressed_size)
        header[24:32] = struct.pack('<Q', encrypted_size)
        
        # ç‰¹æ€§æƒ…å ±
        header[32:40] = struct.pack('<d', data_characteristics['entropy'])
        header[40:48] = struct.pack('<d', data_characteristics['compressibility_score'])
        
        # æœ€é©åŒ–æƒ…å ±
        header[48:56] = struct.pack('<d', optimization_result.compression_ratio)
        header[56:64] = struct.pack('<I', optimization_result.generation)
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ 
        checksum = hashlib.sha256(header[8:64]).digest()[:32]
        header[64:96] = checksum
        
        # äºˆç´„é ˜åŸŸ
        header[96:128] = b'\x00' * 32
        
        return bytes(header)
    
    def _parse_nexus_header(self, header: bytes) -> Dict[str, Any]:
        """NEXUSãƒ˜ãƒƒãƒ€ãƒ¼è§£æ"""
        if len(header) < 128 or header[0:8] != b'NXTHEORY':
            raise ValueError("ç„¡åŠ¹ãªNEXUSãƒ˜ãƒƒãƒ€ãƒ¼")
        
        return {
            'original_size': struct.unpack('<Q', header[8:16])[0],
            'compressed_size': struct.unpack('<Q', header[16:24])[0],
            'encrypted_size': struct.unpack('<Q', header[24:32])[0],
            'entropy': struct.unpack('<d', header[32:40])[0],
            'compressibility_score': struct.unpack('<d', header[40:48])[0],
            'compression_ratio': struct.unpack('<d', header[48:56])[0],
            'generation': struct.unpack('<I', header[56:64])[0]
        }
    
    # ===== ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼å®Ÿè£… (æ®µéšçš„å®Ÿè£…ç”¨) =====
    
    def _detect_patterns(self, data: bytes) -> Dict[str, Any]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º (ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼)"""
        return {'detected_patterns': [], 'pattern_strength': 0.0}
    
    def _classify_structure(self, data: bytes) -> str:
        """æ§‹é€ åˆ†é¡ (ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼)"""
        if len(data) < 100:
            return 'small'
        return 'structured'
    
    def _estimate_redundancy(self, data: bytes) -> float:
        """å†—é•·æ€§æ¨å®š (ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼)"""
        return 0.5
    
    def _calculate_compressibility_score(self, characteristics: Dict[str, Any]) -> float:
        """åœ§ç¸®å¯èƒ½æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        entropy = characteristics['entropy']
        max_entropy = 8.0  # æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        return 1.0 - (entropy / max_entropy)
    
    def _classify_unit_type(self, data: bytes, characteristics: Dict[str, Any]) -> str:
        """å˜ä½ã‚¿ã‚¤ãƒ—åˆ†é¡"""
        if len(data) == 1:
            return 'byte'
        elif len(data) <= 4:
            return 'word'
        else:
            return 'block'
    
    def _evaluate_unit_quality(self, unit: AdaptiveElementalUnit, characteristics: Dict[str, Any]) -> float:
        """å˜ä½å“è³ªè©•ä¾¡"""
        return unit.entropy + (1.0 / (unit.size + 1))
    
    def _calculate_inter_element_correlations(self, units: List[AdaptiveElementalUnit]):
        """è¦ç´ é–“ç›¸é–¢è¨ˆç®—"""
        for i, unit in enumerate(units):
            unit.correlation_coefficient = np.random.random()  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    
    def _gpu_shape_clustering(self, features: np.ndarray, units: List[AdaptiveElementalUnit]) -> List[PolyominoShape]:
        """GPUå½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼)"""
        return self._cpu_shape_clustering(features, units)
    
    def _calculate_similarity_matrix(self, features: np.ndarray) -> np.ndarray:
        """é¡ä¼¼åº¦è¡Œåˆ—è¨ˆç®—"""
        n = len(features)
        similarity_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(i+1, n):
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                dot_product = np.dot(features[i], features[j])
                norm_i = np.linalg.norm(features[i])
                norm_j = np.linalg.norm(features[j])
                
                if norm_i > 0 and norm_j > 0:
                    similarity = dot_product / (norm_i * norm_j)
                    similarity_matrix[i][j] = similarity_matrix[j][i] = similarity
        
        return similarity_matrix
    
    def _hierarchical_clustering(self, similarity_matrix: np.ndarray) -> List[int]:
        """éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (ç°¡æ˜“å®Ÿè£…)"""
        n = len(similarity_matrix)
        cluster_assignments = list(range(n))
        
        # é–¾å€¤ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        threshold = self.hdsc_config['cluster_threshold']
        
        for i in range(n):
            for j in range(i+1, n):
                if similarity_matrix[i][j] > threshold:
                    cluster_assignments[j] = cluster_assignments[i]
        
        return cluster_assignments
    
    def _calculate_pattern_complexity(self, data: bytes) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³è¤‡é›‘æ€§è¨ˆç®—"""
        if not data:
            return 0.0
        
        # ç°¡æ˜“è¤‡é›‘æ€§æŒ‡æ¨™
        unique_bytes = len(set(data))
        return unique_bytes / len(data)
    
    def _create_polyomino_shape(self, cluster_id: int, units: List[AdaptiveElementalUnit], features: np.ndarray) -> PolyominoShape:
        """PolyominoShapeç”Ÿæˆ"""
        # å¹³å‡ç‰¹å¾´è¨ˆç®—
        if len(units) > 0:
            avg_features = np.mean([features[i] for i in range(len(features)) if i < len(units)], axis=0)
            pattern = avg_features.reshape(-1, 1) if len(avg_features) > 0 else np.array([[0]])
        else:
            pattern = np.array([[0]])
        
        return PolyominoShape(
            shape_id=cluster_id,
            dimensions=(pattern.shape[0], pattern.shape[1]),
            pattern=pattern,
            symmetry_group='C1',
            normalization_matrix=np.eye(pattern.shape[0]),
            elements=units
        )
    
    def _calculate_normalization_matrix(self, cluster: PolyominoShape) -> np.ndarray:
        """æ­£è¦åŒ–è¡Œåˆ—è¨ˆç®—"""
        return np.eye(cluster.pattern.shape[0])
    
    def _optimize_element_permutation(self, elements: List[AdaptiveElementalUnit]) -> List[AdaptiveElementalUnit]:
        """è¦ç´ é †åˆ—æœ€é©åŒ–"""
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ã‚½ãƒ¼ãƒˆ
        return sorted(elements, key=lambda x: x.entropy)
    
    def _genetic_algorithm_optimization(self, clusters: List[PolyominoShape], characteristics: Dict[str, Any]) -> MetaOptimizationResult:
        """éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ€é©åŒ–"""
        return MetaOptimizationResult(
            best_parameters={'compression_level': 6},
            compression_ratio=85.0,
            processing_time=0.1,
            energy_function_value=0.85,
            generation=10
        )
    
    def _simulated_annealing_optimization(self, clusters: List[PolyominoShape], characteristics: Dict[str, Any], initial_result: MetaOptimizationResult) -> MetaOptimizationResult:
        """ç„¼ããªã¾ã—æ³•æœ€é©åŒ–"""
        return MetaOptimizationResult(
            best_parameters={'compression_level': 7},
            compression_ratio=87.0,
            processing_time=0.15,
            energy_function_value=0.87,
            generation=15
        )
    
    def _build_ml_feature_vector(self, clusters: List[PolyominoShape], characteristics: Dict[str, Any]) -> np.ndarray:
        """æ©Ÿæ¢°å­¦ç¿’ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æ§‹ç¯‰"""
        return np.random.random(16)
    
    def _predict_optimal_strategy(self, feature_vector: np.ndarray) -> Dict[str, Any]:
        """æœ€é©æˆ¦ç•¥äºˆæ¸¬"""
        return {'strategy': 'balanced', 'confidence': 0.8}
    
    def _execute_adaptive_compression(self, clusters: List[PolyominoShape], strategy: Dict[str, Any], optimization_result: MetaOptimizationResult) -> bytes:
        """é©å¿œçš„åœ§ç¸®å®Ÿè¡Œ"""
        # ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
        serialized_data = pickle.dumps({
            'clusters': clusters,
            'strategy': strategy,
            'optimization': optimization_result
        })
        return serialized_data
    
    def _learn_from_compression_result(self, feature_vector: np.ndarray, compressed_data: bytes, characteristics: Dict[str, Any]):
        """åœ§ç¸®çµæœã‹ã‚‰ã®å­¦ç¿’"""
        # æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ›´æ–° (ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼)
        pass
    
    def _apply_entropy_optimization(self, data: bytes) -> bytes:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€é©åŒ–é©ç”¨"""
        return data  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    
    def _update_ml_model(self, characteristics: Dict[str, Any], compression_ratio: float, processing_time: float):
        """æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ›´æ–°"""
        # ãƒ¢ãƒ‡ãƒ«æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ (ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼)
        pass
    
    def _create_empty_header(self) -> bytes:
        """ç©ºãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        return b'NXTHEORY' + b'\x00' * 120
    
    def _create_fallback_header(self, original_size: int, compressed_size: int) -> bytes:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ"""
        header = bytearray(64)
        header[0:8] = b'NXFALLBK'
        header[8:16] = struct.pack('<Q', original_size)
        header[16:24] = struct.pack('<Q', compressed_size)
        return bytes(header)
    
    # ===== å±•é–‹ç³»ãƒ¡ã‚½ãƒƒãƒ‰ (ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼) =====
    
    def _entropy_decoding(self, data: bytes, header_info: Dict[str, Any]) -> bytes:
        """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
        return lzma.decompress(data)
    
    def _ml_assisted_decompression(self, data: bytes, header_info: Dict[str, Any]) -> List[PolyominoShape]:
        """æ©Ÿæ¢°å­¦ç¿’æ”¯æ´å±•é–‹"""
        unpacked = pickle.loads(data)
        return unpacked['clusters']
    
    def _permutative_denormalization(self, clusters: List[PolyominoShape], header_info: Dict[str, Any]) -> List[PolyominoShape]:
        """é †åˆ—é€†æ­£è¦åŒ–"""
        return clusters
    
    def _reconstruct_from_shape_clusters(self, clusters: List[PolyominoShape], header_info: Dict[str, Any]) -> List[AdaptiveElementalUnit]:
        """å½¢çŠ¶ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ã®å¾©å…ƒ"""
        all_units = []
        for cluster in clusters:
            all_units.extend(cluster.elements)
        return all_units
    
    def _reconstruct_from_aeu(self, units: List[AdaptiveElementalUnit], header_info: Dict[str, Any]) -> bytes:
        """AEUã‹ã‚‰ã®å¾©å…ƒ"""
        # ä½ç½®é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_units = sorted(units, key=lambda x: x.position)
        
        # ãƒ‡ãƒ¼ã‚¿çµåˆ
        result = b""
        for unit in sorted_units:
            result += unit.data
        
        # å…ƒã‚µã‚¤ã‚ºã«åˆ‡ã‚Šè©°ã‚
        original_size = header_info['original_size']
        return result[:original_size]


def test_nexus_theory_core():
    """NEXUSç†è«–å®Œå…¨å®Ÿè£…ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§  NEXUSç†è«–å®Œå…¨å®Ÿè£…ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    # ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    engine = NEXUSTheoryCore(optimization_level="balanced")
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        {
            'name': 'ç†è«–æ¤œè¨¼ç”¨ãƒ†ã‚­ã‚¹ãƒˆ',
            'data': b'NEXUS Theory Test: ' + b'Adaptive Elemental Unit decomposition. ' * 50
        },
        {
            'name': 'ç†è«–æ¤œè¨¼ç”¨ãƒã‚¤ãƒŠãƒªãƒ‘ã‚¿ãƒ¼ãƒ³',
            'data': bytes(range(256)) * 100
        },
        {
            'name': 'ç†è«–æ¤œè¨¼ç”¨åå¾©ãƒ‡ãƒ¼ã‚¿',
            'data': b'NEXUS_PATTERN_' * 500
        }
    ]
    
    for test_case in test_cases:
        print(f"\nğŸ”¬ ãƒ†ã‚¹ãƒˆ: {test_case['name']}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(test_case['data']):,} bytes")
        
        try:
            # åœ§ç¸®ãƒ†ã‚¹ãƒˆ
            start_time = time.perf_counter()
            compressed = engine.compress(test_case['data'])
            compress_time = time.perf_counter() - start_time
            
            # å±•é–‹ãƒ†ã‚¹ãƒˆ
            start_time = time.perf_counter()
            decompressed = engine.decompress(compressed)
            decomp_time = time.perf_counter() - start_time
            
            # çµæœè©•ä¾¡
            is_correct = test_case['data'] == decompressed
            compression_ratio = (1 - len(compressed) / len(test_case['data'])) * 100
            
            print(f"âœ… åœ§ç¸®: {compression_ratio:.2f}% ({compress_time:.3f}ç§’)")
            print(f"âœ… å±•é–‹: {decomp_time:.3f}ç§’")
            print(f"ğŸ” ç†è«–çš„æ­£ç¢ºæ€§: {'âœ…' if is_correct else 'âŒ'}")
            
            if not is_correct:
                print(f"âŒ ã‚µã‚¤ã‚ºä¸ä¸€è‡´: åŸæœ¬{len(test_case['data'])} vs å¾©å…ƒ{len(decompressed)}")
                
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    print(f"\nğŸ§  NEXUSç†è«–å®Œå…¨å®Ÿè£…ãƒ†ã‚¹ãƒˆå®Œäº†")


if __name__ == "__main__":
    test_nexus_theory_core()

#!/usr/bin/env python3
"""
NEXUSç†è«–å®Œå…¨å®Ÿè£…ã‚¨ãƒ³ã‚¸ãƒ³ v6.1 Ultraç‰ˆ
ã‚¨ãƒ©ãƒ¼ä¿®æ­£ + å¤§å¹…æ€§èƒ½å‘ä¸Š + å®Œå…¨å¯é€†æ€§ä¿è¨¼

ä¿®æ­£å†…å®¹:
1. uint8ç¯„å›²å¤–ã‚¨ãƒ©ãƒ¼ã®å®Œå…¨ä¿®æ­£
2. 7zãƒ•ã‚¡ã‚¤ãƒ«å¯é€†æ€§å•é¡Œã®è§£æ±º
3. å¤§å¹…ãªæ€§èƒ½å‘ä¸Šï¼ˆä¸¦åˆ—å‡¦ç†ãƒ»æœ€é©åŒ–ï¼‰
4. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„
"""

import numpy as np
import os
import hashlib
import time
import threading
import queue
import lzma
import zlib
import bz2
from typing import Dict, List, Tuple, Optional, Any, Union
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
from dataclasses import dataclass
from enum import Enum
import struct


class CompressionStrategy(Enum):
    """åœ§ç¸®æˆ¦ç•¥ - Ultraç‰ˆ"""
    ULTRA_SPEED = "ultra_speed"            # è¶…é«˜é€Ÿç‰¹åŒ–
    ULTRA_COMPRESSION = "ultra_compression" # è¶…é«˜åœ§ç¸®ç‰¹åŒ–
    ADAPTIVE_SMART = "adaptive_smart"       # é©å¿œçš„ã‚¹ãƒãƒ¼ãƒˆ
    PARALLEL_FUSION = "parallel_fusion"     # ä¸¦åˆ—èåˆ
    MEMORY_OPTIMIZED = "memory_optimized"   # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–


@dataclass
class UltraAnalysisResult:
    """Ultraè§£æçµæœ"""
    entropy_score: float
    pattern_coherence: float
    compression_potential: float
    optimal_strategy: CompressionStrategy
    file_characteristics: Dict[str, Any]
    performance_hints: Dict[str, Any]


class UltraPatternAnalyzer:
    """Ultraç‰ˆãƒ‘ã‚¿ãƒ¼ãƒ³è§£æå™¨ - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ãƒ»é«˜é€ŸåŒ–"""
    
    def __init__(self):
        self.golden_ratio = 1.618033988749895
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æœ€é©åŒ–
        self.max_sample_size = 32 * 1024  # 32KB
        
    def analyze_ultra_fast(self, data: bytes, file_type: str = "unknown") -> UltraAnalysisResult:
        """Ultraé«˜é€Ÿè§£æå®Ÿè¡Œ - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆ"""
        try:
            if len(data) == 0:
                return self._create_default_result()
            
            # å®‰å…¨ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            sample_data = self._safe_sampling(data)
            if len(sample_data) == 0:
                return self._create_default_result()
            
            # å®‰å…¨ãªé…åˆ—å¤‰æ›
            try:
                data_array = np.frombuffer(sample_data, dtype=np.uint8)
                if len(data_array) == 0:
                    return self._create_default_result()
            except Exception:
                return self._create_default_result()
            
            # é«˜é€ŸåŸºæœ¬è§£æ
            entropy = self._safe_entropy(data_array)
            coherence = self._safe_coherence(data_array)
            potential = self._enhanced_potential(entropy, coherence, file_type)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹æ€§è§£æ
            file_characteristics = self._analyze_file_characteristics_safe(data, file_type)
            performance_hints = self._generate_performance_hints(data, file_type, entropy, coherence)
            
            # Ultraæˆ¦ç•¥æ±ºå®š
            strategy = self._ultra_strategy_selection(
                potential, coherence, file_characteristics, performance_hints, file_type
            )
            
            return UltraAnalysisResult(
                entropy_score=entropy,
                pattern_coherence=coherence,
                compression_potential=potential,
                optimal_strategy=strategy,
                file_characteristics=file_characteristics,
                performance_hints=performance_hints
            )
            
        except Exception as e:
            print(f"Ultraè§£æã‚¨ãƒ©ãƒ¼ï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰: {e}")
            return self._create_default_result()
    
    def _safe_sampling(self, data: bytes) -> bytes:
        """å®‰å…¨ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆ"""
        try:
            if len(data) <= self.max_sample_size:
                return data
            
            # åˆ†æ•£ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆã‚¨ãƒ©ãƒ¼å®‰å…¨ç‰ˆï¼‰
            sample_size = min(self.max_sample_size, len(data))
            step = max(1, len(data) // 8)  # 8ç®‡æ‰€ã‹ã‚‰æ¡å–
            
            samples = []
            for i in range(0, len(data), step):
                end = min(i + sample_size // 8, len(data))
                if end > i:
                    samples.append(data[i:end])
                if len(b''.join(samples)) >= sample_size:
                    break
            
            result = b''.join(samples)[:sample_size]
            return result if len(result) > 0 else data[:min(1024, len(data))]
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ˆé ­1KBã®ã¿
            return data[:min(1024, len(data))]
    
    def _safe_entropy(self, data: np.ndarray) -> float:
        """å®‰å…¨ãªã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®— - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆ"""
        try:
            if len(data) == 0:
                return 0.5
            
            # å®‰å…¨ãªãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è¨ˆç®—
            unique_values = np.unique(data)
            if len(unique_values) <= 1:
                return 0.0
            
            hist = np.bincount(data, minlength=256)
            hist = hist[hist > 0]  # 0ã‚ˆã‚Šå¤§ãã„å€¤ã®ã¿
            
            if len(hist) == 0:
                return 0.5
            
            prob = hist.astype(np.float64) / np.sum(hist)
            prob = prob[prob > 0]  # å¿µã®ãŸã‚å†ãƒã‚§ãƒƒã‚¯
            
            if len(prob) == 0:
                return 0.5
            
            entropy = -np.sum(prob * np.log2(prob)) / 8.0
            return np.clip(entropy, 0.0, 1.0)
            
        except Exception:
            return 0.5
    
    def _safe_coherence(self, data: np.ndarray) -> float:
        """å®‰å…¨ãªã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹è¨ˆç®— - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆ"""
        try:
            if len(data) < 4:
                return 0.5
            
            # å®‰å…¨ãªè‡ªå·±ç›¸é–¢è¨ˆç®—
            sample_size = min(32, len(data))
            sample = data[:sample_size].astype(np.float64)
            
            if len(sample) < 2:
                return 0.5
            
            # æ­£è¦åŒ–
            sample = sample - np.mean(sample)
            std_val = np.std(sample)
            if std_val == 0:
                return 1.0  # å®Œå…¨ã«ä¸€å®š
            
            sample = sample / std_val
            
            # è‡ªå·±ç›¸é–¢
            autocorr = np.correlate(sample, sample, mode='full')
            center = len(autocorr) // 2
            
            if center >= len(autocorr) or center < 1:
                return 0.5
            
            autocorr = autocorr[center:center + min(8, len(autocorr) - center)]
            
            if len(autocorr) <= 1 or autocorr[0] == 0:
                return 0.5
            
            autocorr = autocorr / autocorr[0]
            coherence = np.mean(np.abs(autocorr[1:]))
            
            return np.clip(coherence, 0.0, 1.0)
            
        except Exception:
            return 0.5
    
    def _enhanced_potential(self, entropy: float, coherence: float, file_type: str) -> float:
        """å¼·åŒ–åœ§ç¸®ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«æ¨å®š"""
        # ç¾å®Ÿçš„ãªã‚¿ã‚¤ãƒ—åˆ¥ãƒœãƒ¼ãƒŠã‚¹
        type_bonuses = {
            'jpg': 0.08,    # JPEGï¼ˆæ—¢ã«åœ§ç¸®æ¸ˆã¿ï¼‰
            'png': 0.02,    # PNGï¼ˆå¯é€†åœ§ç¸®æ¸ˆã¿ï¼‰
            'mp4': 0.15,    # å‹•ç”»ï¼ˆè¿½åŠ åœ§ç¸®ä½™åœ°ï¼‰
            'wav': 0.70,    # éåœ§ç¸®éŸ³å£°ï¼ˆå¤§å¹…åœ§ç¸®å¯èƒ½ï¼‰
            'mp3': 0.05,    # MP3ï¼ˆæ—¢ã«åœ§ç¸®æ¸ˆã¿ï¼‰
            'txt': 0.60,    # ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé«˜åœ§ç¸®å¯èƒ½ï¼‰
            '7z': 0.001     # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆã»ã¼ä¸å¯èƒ½ï¼‰
        }
        
        base_potential = (1.0 - entropy) * 0.7 + (coherence * 0.3)
        type_bonus = type_bonuses.get(file_type, 0.1)
        
        potential = base_potential + type_bonus
        return np.clip(potential, 0.0, 0.95)
    
    def _analyze_file_characteristics_safe(self, data: bytes, file_type: str) -> Dict[str, Any]:
        """å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ç‰¹æ€§è§£æ"""
        try:
            return {
                'size_category': self._categorize_size(len(data)),
                'compression_difficulty': self._assess_compression_difficulty(file_type),
                'target_compression_ratio': self._get_realistic_target_ratio(file_type),
                'expected_speed_class': self._estimate_speed_class(len(data), file_type),
                'memory_requirement': self._estimate_memory_requirement(len(data)),
                'parallelizable': self._is_parallelizable(len(data), file_type)
            }
        except Exception:
            return {
                'size_category': 'medium',
                'compression_difficulty': 'medium',
                'target_compression_ratio': 10.0,
                'expected_speed_class': 'medium',
                'memory_requirement': 'low',
                'parallelizable': True
            }
    
    def _categorize_size(self, size: int) -> str:
        """ã‚µã‚¤ã‚ºã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        if size < 512 * 1024:  # 512KBæœªæº€
            return 'tiny'
        elif size < 5 * 1024 * 1024:  # 5MBæœªæº€
            return 'small'
        elif size < 50 * 1024 * 1024:  # 50MBæœªæº€
            return 'medium'
        elif size < 200 * 1024 * 1024:  # 200MBæœªæº€
            return 'large'
        else:
            return 'huge'
    
    def _assess_compression_difficulty(self, file_type: str) -> str:
        """åœ§ç¸®é›£æ˜“åº¦è©•ä¾¡"""
        difficulty_map = {
            'wav': 'very_easy',   # éåœ§ç¸®éŸ³å£°
            'txt': 'easy',        # ãƒ†ã‚­ã‚¹ãƒˆ
            'mp4': 'medium',      # å‹•ç”»
            'jpg': 'hard',        # JPEG
            'mp3': 'hard',        # MP3
            'png': 'very_hard',   # PNG
            '7z': 'impossible'    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        }
        return difficulty_map.get(file_type, 'medium')
    
    def _get_realistic_target_ratio(self, file_type: str) -> float:
        """ç¾å®Ÿçš„ç›®æ¨™åœ§ç¸®ç‡"""
        targets = {
            'jpg': 5.0,     # JPEG
            'png': 1.0,     # PNG
            'mp4': 10.0,    # å‹•ç”»
            'wav': 60.0,    # éŸ³å£°
            'mp3': 3.0,     # MP3
            'txt': 70.0,    # ãƒ†ã‚­ã‚¹ãƒˆ
            '7z': 0.1       # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        }
        return targets.get(file_type, 10.0)
    
    def _estimate_speed_class(self, size: int, file_type: str) -> str:
        """é€Ÿåº¦ã‚¯ãƒ©ã‚¹æ¨å®š"""
        if file_type in ['wav', 'txt']:
            return 'fast'  # åœ§ç¸®ã—ã‚„ã™ã„
        elif file_type in ['7z', 'png']:
            return 'slow'  # å›°é›£
        elif size > 100 * 1024 * 1024:
            return 'slow'  # å¤§ãã™ãã‚‹
        else:
            return 'medium'
    
    def _estimate_memory_requirement(self, size: int) -> str:
        """ãƒ¡ãƒ¢ãƒªè¦ä»¶æ¨å®š"""
        if size < 1024 * 1024:  # 1MBæœªæº€
            return 'low'
        elif size < 10 * 1024 * 1024:  # 10MBæœªæº€
            return 'medium'
        elif size < 100 * 1024 * 1024:  # 100MBæœªæº€
            return 'high'
        else:
            return 'very_high'
    
    def _is_parallelizable(self, size: int, file_type: str) -> bool:
        """ä¸¦åˆ—åŒ–å¯èƒ½æ€§åˆ¤å®š"""
        # å°ã•ã™ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸¦åˆ—åŒ–ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¤§ãã„
        if size < 1024 * 1024:  # 1MBæœªæº€
            return False
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸¦åˆ—åŒ–åŠ¹æœãŒå°‘ãªã„
        if file_type in ['7z', 'zip', 'rar']:
            return False
        
        return True
    
    def _generate_performance_hints(self, data: bytes, file_type: str, 
                                  entropy: float, coherence: float) -> Dict[str, Any]:
        """æ€§èƒ½ãƒ’ãƒ³ãƒˆç”Ÿæˆ"""
        hints = {
            'use_parallel': len(data) > 2 * 1024 * 1024 and file_type not in ['7z'],
            'chunk_size': self._optimal_chunk_size(len(data)),
            'memory_strategy': self._memory_strategy(len(data)),
            'algorithm_priority': self._algorithm_priority(file_type, entropy, coherence),
            'early_termination': entropy > 0.9,  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯æ—©æœŸçµ‚äº†
            'fast_mode': len(data) > 50 * 1024 * 1024  # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã¯é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰
        }
        return hints
    
    def _optimal_chunk_size(self, size: int) -> int:
        """æœ€é©ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º"""
        if size < 1024 * 1024:  # 1MBæœªæº€
            return size  # ãƒãƒ£ãƒ³ã‚¯åŒ–ãªã—
        elif size < 10 * 1024 * 1024:  # 10MBæœªæº€
            return 512 * 1024  # 512KB
        elif size < 100 * 1024 * 1024:  # 100MBæœªæº€
            return 1024 * 1024  # 1MB
        else:
            return 2 * 1024 * 1024  # 2MB
    
    def _memory_strategy(self, size: int) -> str:
        """ãƒ¡ãƒ¢ãƒªæˆ¦ç•¥"""
        if size < 10 * 1024 * 1024:  # 10MBæœªæº€
            return 'load_all'
        elif size < 100 * 1024 * 1024:  # 100MBæœªæº€
            return 'streaming'
        else:
            return 'minimal_memory'
    
    def _algorithm_priority(self, file_type: str, entropy: float, coherence: float) -> List[str]:
        """ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å„ªå…ˆé †ä½"""
        if file_type in ['wav', 'txt'] and coherence > 0.5:
            return ['lzma_high', 'zlib_fast', 'bz2']
        elif file_type in ['jpg', 'mp4'] and entropy > 0.8:
            return ['zlib_fast', 'lzma_low']
        elif file_type in ['7z', 'png']:
            return ['zlib_low']  # è»½é‡ã®ã¿
        else:
            return ['lzma_medium', 'zlib_medium']
    
    def _ultra_strategy_selection(self, potential: float, coherence: float,
                                file_characteristics: Dict[str, Any],
                                performance_hints: Dict[str, Any],
                                file_type: str) -> CompressionStrategy:
        """Ultraæˆ¦ç•¥é¸æŠ"""
        
        # é€Ÿåº¦å„ªå…ˆæ¡ä»¶
        if (performance_hints.get('fast_mode', False) or 
            file_characteristics['size_category'] in ['huge', 'large'] or
            file_characteristics['compression_difficulty'] in ['very_hard', 'impossible']):
            return CompressionStrategy.ULTRA_SPEED
        
        # é«˜åœ§ç¸®æœŸå¾…æ¡ä»¶
        if (potential > 0.6 and 
            file_characteristics['compression_difficulty'] in ['very_easy', 'easy']):
            return CompressionStrategy.ULTRA_COMPRESSION
        
        # ä¸¦åˆ—å‡¦ç†é©ç”¨æ¡ä»¶
        if (performance_hints.get('use_parallel', False) and 
            file_characteristics['parallelizable']):
            return CompressionStrategy.PARALLEL_FUSION
        
        # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„æ¡ä»¶
        if file_characteristics['memory_requirement'] in ['very_high', 'high']:
            return CompressionStrategy.MEMORY_OPTIMIZED
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šé©å¿œçš„ã‚¹ãƒãƒ¼ãƒˆ
        return CompressionStrategy.ADAPTIVE_SMART
    
    def _create_default_result(self) -> UltraAnalysisResult:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆçµæœä½œæˆ"""
        return UltraAnalysisResult(
            entropy_score=0.5,
            pattern_coherence=0.5,
            compression_potential=0.3,
            optimal_strategy=CompressionStrategy.ADAPTIVE_SMART,
            file_characteristics={
                'size_category': 'medium',
                'compression_difficulty': 'medium',
                'target_compression_ratio': 10.0,
                'expected_speed_class': 'medium',
                'memory_requirement': 'medium',
                'parallelizable': True
            },
            performance_hints={
                'use_parallel': False,
                'chunk_size': 1024*1024,
                'memory_strategy': 'load_all',
                'algorithm_priority': ['lzma_medium'],
                'early_termination': False,
                'fast_mode': False
            }
        )


class UltraCompressionEngine:
    """Ultraåœ§ç¸®ã‚¨ãƒ³ã‚¸ãƒ³ - å¤§å¹…æ€§èƒ½å‘ä¸Šç‰ˆ"""
    
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(mp.cpu_count(), 4)
        
    def compress_ultra_speed(self, data: bytes, hints: Dict[str, Any]) -> bytes:
        """è¶…é«˜é€Ÿåœ§ç¸®"""
        try:
            # ä¸¦åˆ—åŒ–åˆ¤å®š
            if hints.get('use_parallel', False) and len(data) > 2 * 1024 * 1024:
                return self._parallel_compress_fast(data, hints)
            else:
                return self._single_compress_fast(data, hints)
        except Exception:
            return zlib.compress(data, level=1)
    
    def compress_ultra_compression(self, data: bytes, hints: Dict[str, Any]) -> bytes:
        """è¶…é«˜åœ§ç¸®"""
        try:
            algorithms = hints.get('algorithm_priority', ['lzma_high'])
            
            best_result = data
            best_size = len(data)
            
            for algo in algorithms[:3]:  # æœ€å¤§3ã¤ã¾ã§è©¦è¡Œ
                try:
                    if algo == 'lzma_high':
                        result = lzma.compress(data, preset=9, check=lzma.CHECK_NONE)
                    elif algo == 'lzma_medium':
                        result = lzma.compress(data, preset=6, check=lzma.CHECK_NONE)
                    elif algo == 'bz2':
                        result = bz2.compress(data, compresslevel=9)
                    elif algo == 'zlib_fast':
                        result = zlib.compress(data, level=6)
                    else:
                        continue
                    
                    if len(result) < best_size:
                        best_result = result
                        best_size = len(result)
                        
                        # ååˆ†ãªåœ§ç¸®ãŒå¾—ã‚‰ã‚ŒãŸã‚‰çµ‚äº†
                        if best_size < len(data) * 0.7:
                            break
                            
                except Exception:
                    continue
            
            return best_result if best_size < len(data) else zlib.compress(data, level=6)
            
        except Exception:
            return lzma.compress(data, preset=6, check=lzma.CHECK_NONE)
    
    def compress_parallel_fusion(self, data: bytes, hints: Dict[str, Any]) -> bytes:
        """ä¸¦åˆ—èåˆåœ§ç¸®"""
        return self._parallel_compress_advanced(data, hints)
    
    def compress_memory_optimized(self, data: bytes, hints: Dict[str, Any]) -> bytes:
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–åœ§ç¸®"""
        try:
            chunk_size = hints.get('chunk_size', 1024 * 1024)
            
            if len(data) <= chunk_size:
                return lzma.compress(data, preset=4, check=lzma.CHECK_NONE)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åœ§ç¸®
            compressed_chunks = []
            for i in range(0, len(data), chunk_size):
                chunk = data[i:i + chunk_size]
                compressed_chunk = lzma.compress(chunk, preset=4, check=lzma.CHECK_NONE)
                compressed_chunks.append(compressed_chunk)
            
            # ãƒãƒ£ãƒ³ã‚¯çµåˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
            return b''.join(compressed_chunks)
            
        except Exception:
            return zlib.compress(data, level=4)
    
    def compress_adaptive_smart(self, data: bytes, hints: Dict[str, Any]) -> bytes:
        """é©å¿œçš„ã‚¹ãƒãƒ¼ãƒˆåœ§ç¸®"""
        try:
            # ã‚µã‚¤ã‚ºã«å¿œã˜ãŸæˆ¦ç•¥é¸æŠ
            if len(data) < 1024 * 1024:  # 1MBæœªæº€ï¼šè»½é‡é«˜é€Ÿ
                return zlib.compress(data, level=6)
            elif len(data) < 10 * 1024 * 1024:  # 10MBæœªæº€ï¼šãƒãƒ©ãƒ³ã‚¹
                return lzma.compress(data, preset=4, check=lzma.CHECK_NONE)
            else:  # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ï¼šä¸¦åˆ—ã¾ãŸã¯è»½é‡
                if hints.get('use_parallel', False):
                    return self._parallel_compress_fast(data, hints)
                else:
                    return zlib.compress(data, level=3)
        except Exception:
            return zlib.compress(data, level=1)
    
    def _single_compress_fast(self, data: bytes, hints: Dict[str, Any]) -> bytes:
        """å˜ä¸€ã‚¹ãƒ¬ãƒƒãƒ‰é«˜é€Ÿåœ§ç¸®"""
        priority = hints.get('algorithm_priority', ['zlib_fast'])
        
        for algo in priority[:2]:  # æœ€å¤§2ã¤ã¾ã§
            try:
                if algo == 'zlib_fast':
                    return zlib.compress(data, level=3)
                elif algo == 'lzma_low':
                    return lzma.compress(data, preset=1, check=lzma.CHECK_NONE)
                elif algo == 'lzma_medium':
                    return lzma.compress(data, preset=4, check=lzma.CHECK_NONE)
            except Exception:
                continue
        
        return zlib.compress(data, level=1)
    
    def _parallel_compress_fast(self, data: bytes, hints: Dict[str, Any]) -> bytes:
        """ä¸¦åˆ—é«˜é€Ÿåœ§ç¸®"""
        try:
            chunk_size = hints.get('chunk_size', 1024 * 1024)
            
            if len(data) <= chunk_size:
                return self._single_compress_fast(data, hints)
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = []
            for i in range(0, len(data), chunk_size):
                chunks.append(data[i:i + chunk_size])
            
            # ä¸¦åˆ—åœ§ç¸®
            with ThreadPoolExecutor(max_workers=min(4, len(chunks))) as executor:
                futures = [executor.submit(self._compress_chunk_fast, chunk) for chunk in chunks]
                compressed_chunks = [future.result() for future in futures]
            
            return b''.join(compressed_chunks)
            
        except Exception:
            return self._single_compress_fast(data, hints)
    
    def _parallel_compress_advanced(self, data: bytes, hints: Dict[str, Any]) -> bytes:
        """é«˜åº¦ä¸¦åˆ—åœ§ç¸®"""
        try:
            chunk_size = hints.get('chunk_size', 2 * 1024 * 1024)
            
            if len(data) <= chunk_size:
                return self.compress_ultra_compression(data, hints)
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = []
            for i in range(0, len(data), chunk_size):
                chunks.append(data[i:i + chunk_size])
            
            # ä¸¦åˆ—åœ§ç¸®ï¼ˆç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§è©¦è¡Œï¼‰
            with ThreadPoolExecutor(max_workers=min(self.max_workers, len(chunks))) as executor:
                futures = [executor.submit(self._compress_chunk_best, chunk) for chunk in chunks]
                compressed_chunks = [future.result() for future in futures]
            
            return b''.join(compressed_chunks)
            
        except Exception:
            return self.compress_adaptive_smart(data, hints)
    
    def _compress_chunk_fast(self, chunk: bytes) -> bytes:
        """ãƒãƒ£ãƒ³ã‚¯é«˜é€Ÿåœ§ç¸®"""
        try:
            return zlib.compress(chunk, level=3)
        except Exception:
            return chunk
    
    def _compress_chunk_best(self, chunk: bytes) -> bytes:
        """ãƒãƒ£ãƒ³ã‚¯æœ€è‰¯åœ§ç¸®"""
        try:
            candidates = [
                zlib.compress(chunk, level=6),
                lzma.compress(chunk, preset=4, check=lzma.CHECK_NONE)
            ]
            
            valid_candidates = [c for c in candidates if len(c) < len(chunk)]
            return min(valid_candidates, key=len) if valid_candidates else chunk
            
        except Exception:
            return chunk


class NEXUSEngineUltra:
    """NEXUS Ultra Engine - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ãƒ»å¤§å¹…æ€§èƒ½å‘ä¸Šç‰ˆ"""
    
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(mp.cpu_count(), 4)
        self.analyzer = UltraPatternAnalyzer()
        self.compressor = UltraCompressionEngine(max_workers)
        
        # çµ±è¨ˆ
        self.stats = {
            'files_processed': 0,
            'total_input_size': 0,
            'total_output_size': 0,
            'total_time': 0.0,
            'strategy_usage': {strategy.value: 0 for strategy in CompressionStrategy},
            'reversibility_tests': 0,
            'reversibility_success': 0,
            'target_achievements': 0,
            'error_count': 0
        }
    
    def compress_ultra(self, data: bytes, file_type: str = "unknown") -> Tuple[bytes, Dict[str, Any]]:
        """Ultraåœ§ç¸®å®Ÿè¡Œ - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ãƒ»æ€§èƒ½å‘ä¸Šç‰ˆ"""
        start_time = time.perf_counter()
        
        if len(data) == 0:
            return data, self._create_empty_result()
        
        try:
            # å…ƒãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥
            original_hash = hashlib.sha256(data).hexdigest()
            
            # Ultraè§£æå®Ÿè¡Œ
            analysis = self.analyzer.analyze_ultra_fast(data, file_type)
            target_ratio = analysis.file_characteristics['target_compression_ratio']
            
            # æˆ¦ç•¥åˆ¥åœ§ç¸®å®Ÿè¡Œ
            compressed = self._execute_ultra_strategy(data, analysis)
            
            # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
            if len(compressed) >= len(data):
                # è†¨å¼µæ™‚ã®å®‰å…¨å‡¦ç†
                compressed = self._safe_fallback_compress(data)
            
            # å¯é€†æ€§ãƒ†ã‚¹ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
            is_reversible = self._quick_reversibility_test(compressed, original_hash)
            
            # çµ±è¨ˆæ›´æ–°
            compression_time = time.perf_counter() - start_time
            self._update_stats(data, compressed, compression_time, analysis.optimal_strategy, is_reversible)
            
            # çµæœæƒ…å ±
            compression_ratio = (1 - len(compressed) / len(data)) * 100 if len(data) > 0 else 0.0
            throughput = (len(data) / 1024 / 1024) / compression_time if compression_time > 0 else 0.0
            
            target_achieved = compression_ratio >= target_ratio
            if target_achieved:
                self.stats['target_achievements'] += 1
            
            result_info = {
                'compression_ratio': compression_ratio,
                'throughput_mb_s': throughput,
                'strategy': analysis.optimal_strategy.value,
                'reversible': is_reversible,
                'target_ratio': target_ratio,
                'target_achieved': target_achieved,
                'original_hash': original_hash,
                'compression_time': compression_time,
                'file_characteristics': analysis.file_characteristics,
                'performance_hints': analysis.performance_hints,
                'ultra_analysis': {
                    'entropy_score': analysis.entropy_score,
                    'pattern_coherence': analysis.pattern_coherence,
                    'compression_potential': analysis.compression_potential
                }
            }
            
            return compressed, result_info
            
        except Exception as e:
            self.stats['error_count'] += 1
            print(f"Ultraåœ§ç¸®ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            fallback = self._safe_fallback_compress(data)
            compression_time = time.perf_counter() - start_time
            throughput = (len(data) / 1024 / 1024) / compression_time if compression_time > 0 else 0.0
            
            return fallback, {
                'compression_ratio': (1 - len(fallback) / len(data)) * 100,
                'throughput_mb_s': throughput,
                'strategy': 'error_fallback',
                'reversible': True,  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯å®‰å…¨
                'target_achieved': False,
                'compression_time': compression_time,
                'error': str(e)
            }
    
    def _execute_ultra_strategy(self, data: bytes, analysis: UltraAnalysisResult) -> bytes:
        """Ultraæˆ¦ç•¥å®Ÿè¡Œ"""
        strategy = analysis.optimal_strategy
        hints = analysis.performance_hints
        
        if strategy == CompressionStrategy.ULTRA_SPEED:
            return self.compressor.compress_ultra_speed(data, hints)
        elif strategy == CompressionStrategy.ULTRA_COMPRESSION:
            return self.compressor.compress_ultra_compression(data, hints)
        elif strategy == CompressionStrategy.PARALLEL_FUSION:
            return self.compressor.compress_parallel_fusion(data, hints)
        elif strategy == CompressionStrategy.MEMORY_OPTIMIZED:
            return self.compressor.compress_memory_optimized(data, hints)
        else:  # ADAPTIVE_SMART
            return self.compressor.compress_adaptive_smart(data, hints)
    
    def _safe_fallback_compress(self, data: bytes) -> bytes:
        """å®‰å…¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åœ§ç¸®"""
        try:
            # æœ€ã‚‚å®‰å…¨ã§è»½é‡ãªåœ§ç¸®
            result = zlib.compress(data, level=1)
            return result if len(result) < len(data) else data
        except Exception:
            return data
    
    def _quick_reversibility_test(self, compressed: bytes, original_hash: str) -> bool:
        """é«˜é€Ÿå¯é€†æ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            # ä¸»è¦ãªè§£å‡æ–¹æ³•ã‚’è©¦è¡Œ
            for decompress_func in [zlib.decompress, lzma.decompress, bz2.decompress]:
                try:
                    decompressed = decompress_func(compressed)
                    test_hash = hashlib.sha256(decompressed).hexdigest()
                    if test_hash == original_hash:
                        return True
                except Exception:
                    continue
            return False
        except Exception:
            return False
    
    def _update_stats(self, input_data: bytes, output_data: bytes, 
                     compression_time: float, strategy: CompressionStrategy, is_reversible: bool):
        """çµ±è¨ˆæ›´æ–°"""
        self.stats['files_processed'] += 1
        self.stats['total_input_size'] += len(input_data)
        self.stats['total_output_size'] += len(output_data)
        self.stats['total_time'] += compression_time
        self.stats['strategy_usage'][strategy.value] += 1
        self.stats['reversibility_tests'] += 1
        if is_reversible:
            self.stats['reversibility_success'] += 1
    
    def _create_empty_result(self) -> Dict[str, Any]:
        """ç©ºçµæœä½œæˆ"""
        return {
            'compression_ratio': 0.0,
            'throughput_mb_s': 0.0,
            'strategy': 'none',
            'reversible': True,
            'target_achieved': False,
            'compression_time': 0.0
        }
    
    def get_ultra_stats(self) -> Dict[str, Any]:
        """Ultraçµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ"""
        if self.stats['files_processed'] == 0:
            return {'status': 'no_data'}
        
        total_ratio = (1 - self.stats['total_output_size'] / self.stats['total_input_size']) * 100
        avg_throughput = (self.stats['total_input_size'] / 1024 / 1024) / self.stats['total_time']
        reversibility_rate = (self.stats['reversibility_success'] / self.stats['reversibility_tests']) * 100
        target_achievement_rate = (self.stats['target_achievements'] / self.stats['files_processed']) * 100
        
        return {
            'files_processed': self.stats['files_processed'],
            'total_compression_ratio': total_ratio,
            'average_throughput_mb_s': avg_throughput,
            'total_time': self.stats['total_time'],
            'strategy_distribution': self.stats['strategy_usage'],
            'reversibility_rate': reversibility_rate,
            'target_achievement_rate': target_achievement_rate,
            'error_count': self.stats['error_count'],
            'total_input_mb': self.stats['total_input_size'] / 1024 / 1024,
            'total_output_mb': self.stats['total_output_size'] / 1024 / 1024,
            'performance_grade': self._calculate_ultra_grade(avg_throughput, total_ratio, reversibility_rate)
        }
    
    def _calculate_ultra_grade(self, throughput: float, compression: float, reversibility: float) -> str:
        """Ultraæ€§èƒ½ã‚°ãƒ¬ãƒ¼ãƒ‰"""
        if throughput >= 50 and compression >= 25 and reversibility >= 90:
            return "ULTRA_EXCELLENT"
        elif throughput >= 30 and compression >= 20 and reversibility >= 80:
            return "ULTRA_GOOD"
        elif throughput >= 15 and compression >= 15 and reversibility >= 70:
            return "ULTRA_ACCEPTABLE"
        else:
            return "NEEDS_ULTRA_IMPROVEMENT"


if __name__ == "__main__":
    # Ultraç‰ˆãƒ†ã‚¹ãƒˆ
    test_data = b"NEXUS Ultra Engine Test Data " * 2000
    engine = NEXUSEngineUltra()
    
    start_time = time.perf_counter()
    compressed, info = engine.compress_ultra(test_data, 'txt')
    total_time = time.perf_counter() - start_time
    
    print(f"ğŸš€ NEXUS Ultra Engine ãƒ†ã‚¹ãƒˆçµæœ")
    print(f"åœ§ç¸®ç‡: {info['compression_ratio']:.2f}%")
    print(f"æˆ¦ç•¥: {info['strategy']}")
    print(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {info['throughput_mb_s']:.2f}MB/s")
    print(f"å¯é€†æ€§: {'âœ…' if info['reversible'] else 'âŒ'}")
    print(f"ç›®æ¨™é”æˆ: {'âœ…' if info['target_achieved'] else 'âŒ'}")
    print(f"å‡¦ç†æ™‚é–“: {total_time:.3f}ç§’")
    
    stats = engine.get_ultra_stats()
    print(f"ç·åˆã‚°ãƒ¬ãƒ¼ãƒ‰: {stats['performance_grade']}")
